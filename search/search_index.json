{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Wikimedial","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>This is an infrastructure developed by Medial EarlySign to streamline the creation of predictive models using EMR data for clinical applications. Existing tools often fall short for clinical use many Python libraries are not optimized for sparse time series analysis, leading to high memory consumption and, in some cases, performance that is 10\u2013100 times slower than necessary.</p> <p>Medial Infrastructure is designed to turn the Electronic Medical Record (EMR)-a complex, semi-structured time-series dataset, into a machine-learning-ready resource. Unlike images or free text, EMR data can be stored in countless formats, and its \"labels\" (the outcomes or targets you want to predict) aren\u2019t always obvious. We address this by standardizing both the storage and the processing of time-series signals. We can think about this infrastructure as \"TensorFlow\" of medical data machine learning.</p>"},{"location":"index.html#using-a-prebuilt-model","title":"Using a Prebuilt Model","text":"<p>Available models:</p> Model Name Model description Contact Details for Usage LGI/Colon-Flag Detects colon cancer using age, sex, and CBCs Roche LungFlag Detects lung cancer using age, sex, smoking infromation, and common blood tests Roche GastroFlag Detects gastric cancer using age, sex, and common blood tests Roche AAA Predicts AAA events Geisinger/TBD FluComplications Predicts flu followed by complications such as pneumonia, hospitalization, or death TBD Pred2D Predicts progression from prediabetes to diabetes Planned to be open source FastProgressors Predicts rapid decline in eGFR Planned to be open source MortatlityCMS Predicts mortality using CMS claims data TBD Unplanned COPD Admission Prediction Model Predicts COPD hospitalization using CMS claims data TBD <p>There are two options for using a model:</p> <ul> <li>The model is already depolyed and you just want to access it. What is the API? - please refer How To Use Deployed AlgoMarker</li> <li>The model is available for download, but you need to deploy it first. Please refer to AlgoMarker Deployment</li> </ul>"},{"location":"index.html#creating-a-new-model","title":"Creating a new model","text":"<p>If you want to create a new model, please follow those steps:</p> <ol> <li>Set up your environment: Compile and clone the necessary tools<ul> <li>Follow: Infrastructure Installation</li> </ul> </li> <li>Create a data repository: Follow the ETL guide</li> <li>Define your cohort: Prepare a list of patient IDs, prediction times, and outcome. See MedSamples for details on labeling. In this step you will create a file where each patient has a timestamp for prediction and a label in a CSV format. </li> <li>Specify the model architecture in JSON. Refer to: Model Json Format, infrastucture explaination </li> <li>Run our tools to train the model with the model architecture, samples and data repository - python or Flow, Optimizer to train the model, or python code</li> <li>Apply and test the model using Flow or python code</li> <li>Analyze model performance using bootstrap and test kit/AutoTest or write your own tests</li> <li>Wrap the model for deployment - follow - AlgoMarker Deployment</li> </ol>"},{"location":"index.html#other-documentation-pages","title":"Other Documentation Pages","text":"<ul> <li>Infrastructure Home Page - Information about the Infrastructure<ul> <li>Howto use AlgoMarker</li> </ul> </li> <li>Repositories - Information on how to Load data into Data Repository</li> <li>Medial Tools - A documentation of our tools used to create the models, test the models, etc.</li> <li>Python - A documentation of python wrapper of our infrastructure</li> <li>Research - A documentation of some research topics</li> <li>Installation - Other documentation</li> </ul>"},{"location":"Infrastructure%20C%20Library/index.html","title":"Infrastructure Home Page","text":""},{"location":"Infrastructure%20C%20Library/index.html#overview-of-medial-infrastructure","title":"Overview of Medial Infrastructure","text":"<p>Medial Infrastructure is designed to turn the Electronic Medical Record (EMR)-a complex, semi-structured time-series dataset, into a machine-learning-ready resource. Unlike images or free text, EMR data can be stored in countless formats, and its \"labels\" (the outcomes or targets you want to predict) aren\u2019t always obvious. We address this by standardizing both the storage and the processing of time-series signals. We can think about this infrastructure as \"TensorFlow\" of medical data machine learning. </p>"},{"location":"Infrastructure%20C%20Library/index.html#howto-use-this","title":"Howto Use this","text":"<p>Suppose you're a potential user or client interested in using a specific model. You're not concerned with how the model was built or which tools were used, you simply want to deploy and use it. Please refer to this page: Howto use AlgoMarker</p>"},{"location":"Infrastructure%20C%20Library/index.html#main-contributers-from-recent-years","title":"Main contributers from recent years:","text":"<ul> <li>Avi Shoshan</li> <li>Yaron Kinar</li> <li>Alon Lanyado</li> </ul>"},{"location":"Infrastructure%20C%20Library/index.html#challenges","title":"Challenges","text":"<ul> <li>Variety of Questions: Risk prediction (e.g., cancer, CKD), compliance, diagnostics, treatment recommendations</li> <li>Medical Data Complexity: Temporal irregularity, high dimensionality (&gt;100k categories), sparse signals, multiple data types</li> <li>Retrospective Data Issues: Noise, bias, spurious patterns, policy sensitivity</li> </ul>"},{"location":"Infrastructure%20C%20Library/index.html#goals","title":"Goals","text":"<ul> <li>Avoid reinventing common methodologies each project. Sometimes complicated code/logic with debugging</li> <li>Maintain shareable, versioned, regulatory\u2011compliant pipelines</li> <li>Facilitate reproducible transfer from research to product</li> <li>Provide end-to-end support: data import \u2192 analysis \u2192 productization</li> </ul>"},{"location":"Infrastructure%20C%20Library/index.html#platform-requirements","title":"Platform Requirements","text":"<ul> <li>Performance: Ultra-efficient in memory &amp; time (&gt;100x compare to native python pandas in some cases, mainly in preprocessing)</li> <li>Extensibility: Rich APIs, configurable pipelines, support new data types</li> <li>Minimal Rewriting &amp; Ease Of Usage: JSON\u2011driven configs, unified codebase, python API to the C library</li> <li>Comprehensive: From \"raw\" data to model deployment</li> <li>Reproducible &amp; Versioned: Track data, code, models, and parameters</li> </ul>"},{"location":"Infrastructure%20C%20Library/index.html#infrastructure-components","title":"Infrastructure Components","text":"<ol> <li>MedRepository: a high-performance EMR time-series store<ul> <li>Fast retrieval of any patient\u2019s full record or a specific signal across all patients.</li> <li>Unified representation: each signal consists of zero or more time channels plus zero or more value channels, all tied to a patient ID.<ul> <li>Static example: \"Birth year\" \u2192 no time channels, one value channel.</li> <li>Single-time example: \"Hemoglobin\" \u2192 one time channel (test date), one value channel (numeric result).</li> <li>Interval example: \"Hospitalization\" \u2192 two time channels (admission and discharge dates).</li> </ul> </li> <li>Hierarchical support for categorical medical ontologies <ul> <li>Enables seamless integration and translation between different systems when working with a frozen model or algorithm. </li> <li>Example: A query for ICD-10 codes starting with \"J\" (respiratory diseases) will also automatically map to corresponding categories in systems like Epic. When dictionary of mapping between ICD and Epic is added, no need to change the model. </li> <li>Ontology mappings are managed by MedDictionary, which supports many-to-many hierarchical relationships across coding systems.</li> </ul> </li> </ul> </li> <li>Modular processing pipeline (sklearn-style)<ul> <li>Rep Processors: Clean or derive \"raw\" virtual signals, while preventing leakage of future data<ul> <li>Example: Outlier cleaner that omits values only when abnormality is detected by future readings (e.g., a hemoglobin value on 2023-Feb-04 flagged only by a 2023-May-21 test remains until after May 21).</li> <li>Example: Virtual BMI signal computed from weight/height, or imputed when only two of three inputs exist</li> </ul> </li> <li>Feature Generators: Convert cleaned signals into predictive features.<ul> <li>Examples:<ul> <li>\"Last hemoglobin in past 365 days\"</li> <li>\"Hemoglobin slope over three years\"</li> <li>\"COPD diagnosis code during any emergency admission in last three years\"</li> </ul> </li> </ul> </li> <li>Feature Processors: Operate on the feature matrix\u2014imputation, selection, PCA, etc. </li> <li>Predictors/Classifiers: LightGBM, XGBoost, or custom algorithms.</li> <li>Post-processing: Score calibration, explainability layers, fairness adjustments, etc.</li> </ul> </li> <li>JSON-driven pipeline configuration - Define every processor, feature generator, and model step in a single JSON file. Json Format     Example json for training a model:</li> </ol> Click to expend example json<pre><code>   {\n    \"model_json_version\": \"2\",\n    \"serialize_learning_set\": \"0\",\n    \"model_actions\": [\n        \"json:full_rep_processors.json\", // Import a json from current folder with other componenets - in this case, outlier cleaners, signal panel completers, etc.\n    // Features\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"age\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"gender\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"unified_smoking\",\n            \"tags\": \"smoking\",\n            \"smoking_features\": \"Current_Smoker, Ex_Smoker, Unknown_Smoker, Never_Smoker, Passive_Smoker, Smok_Days_Since_Quitting , Smok_Pack_Years_Max, Smok_Pack_Years_Last,Smoking_Years,Smoking_Intensity\"\n        },\n        // Cancers in Dx\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=10950\"\n            ],\n            \"time_unit\": \"Days\",\n            \"sets\": [\n                \"ICD9_CODE:140-149,ICD9_CODE:150-159,ICD9_CODE:160-165,ICD9_CODE:170,ICD9_CODE:171,ICD9_CODE:172,ICD9_CODE:174,ICD9_CODE:175,ICD9_CODE:176,ICD9_CODE:179-189,ICD9_CODE:200-208,ICD9_CODE:209.0,ICD9_CODE:209.1,ICD9_CODE:209.2,ICD9_CODE:290.3,ICD9_CODE:230-234\"\n            ],\n            \"signal\": \"ICD9_Diagnosis\",\n            \"in_set_name\": \"Cancers\"\n        },\n    // Statistical features - will take: last, average, min, max, etc. for each time window: 0-180, 0-365. 365-730, 0-1095 prior prediction day in days and for each signal: Hemoglobin, WBC...\n    // In total will create: 8*4*4 = 128 features\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": [\n                \"last\",\n                \"last_delta\",\n                \"avg\",\n                \"max\",\n                \"min\",\n                \"std\",\n                \"slope\",\n                \"range_width\"\n            ],\n            \"window\": [\n                \"win_from=0;win_to=180\",\n                \"win_from=0;win_to=365\",\n                \"win_from=365;win_to=730\",\n                \"win_from=0;win_to=1095\"\n            ],\n            \"time_unit\": \"Days\",\n            \"tags\": \"labs_and_measurements,need_imputer,need_norm\",\n            \"signal\": [\n                \"Hemoglobin\",\n                \"WBC\",\n                \"Platelets\",\n                \"Albumin\"\n            ]\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": [\n                \"last_time\"\n            ],\n            \"window\": [\n                \"win_from=0;win_to=180\",\n                \"win_from=0;win_to=365\",\n                \"win_from=365;win_to=730\",\n                \"win_from=0;win_to=1095\"\n            ],\n            \"time_unit\": \"Days\",\n            \"tags\": \"labs_and_measurements,need_imputer,need_norm\",\n            //Take only panels - to remove repititions:\n            \"signal\": [\n                \"BMI\",\n                \"Creatinine\",\n                \"WBC\",\n                \"Cholesterol\",\n                \"Glucose\",\n                \"Hemoglobin\",\n                \"Albumin\"\n            ]\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"category_depend\",\n            \"signal\": \"DIAGNOSIS\",\n            \"window\": [\n                \"win_from=0;win_to=10950;tags=numeric.win_0_10950\",\n                \"win_from=0;win_to=365;tags=numeric.win_0_365\"\n            ],\n            \"time_unit_win\": \"Days\",\n            \"regex_filter\": \"ICD10_CODE:.*\",\n            \"min_age\": \"40\",\n            \"max_age\": \"90\",\n            \"age_bin\": \"5\",\n            \"min_code_cnt\": \"200\",\n            \"fdr\": \"0.01\",\n            \"lift_below\": \"0.7\",\n            \"lift_above\": \"1.3\",\n            \"stat_metric\": \"mcnemar\",\n            \"max_depth\": \"50\",\n            \"max_parents\": \"100\",\n            \"use_fixed_lift\": \"1\",\n            \"sort_by_chi\": \"1\",\n            \"verbose\": \"1\",\n            \"take_top\": \"50\"\n        },\n        // Feature selector to remove features with 99.9% same value, there are other options, like lasso, by model importance, etc.\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                {\n                    \"fp_type\": \"remove_deg\",\n                    \"percentage\": \"0.999\"\n                }\n            ]\n        },\n        // Imputer - simple choise of choosing median value by stratifying to age, gender and smoking status - will commit for all features with \"need_imputer\" tag\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                {\n                    \"fp_type\": \"imputer\",\n                    \"strata\": \"Age,40,100,5:Gender,1,2,1:Current_Smoker,0,1,1:Ex_Smoker,0,1,1\",\n                    \"moment_type\": \"median\",\n                    \"tag\": \"need_imputer\",\n                    \"duplicate\": \"1\"\n                }\n            ]\n        },\n        // Normalizer - will commit for all features with \"need_imputer\" tag\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                {\n                    \"fp_type\": \"normalizer\",\n                    \"resolution_only\": \"0\",\n                    \"resolution\": \"5\",\n                    \"tag\": \"need_norm\",\n                    \"duplicate\": \"1\"\n                }\n            ]\n        }\n    ],\n    \"predictor\": \"xgb\",\n    \"predictor_params\": \"tree_method=auto;booster=gbtree;objective=binary:logistic;eta=0.050;alpha=0.000;lambda=0.010;gamma=0.010;max_depth=6;colsample_bytree=0.800;colsample_bylevel=1.000;min_child_weight=10;num_round=200;subsample=0.800\" }\n</code></pre> <ol> <li>Comprehensive evaluation toolkit<ul> <li>Bootstrap-based cohort analysis allows batch testing across thousands of user-defined subgroups (e.g., age 50\u201380, males only, prediction window of 365 days, COPD patients).</li> <li>Automatically extracts AUC, ROC points at each 1% FPR increment, odds ratios, PPV/NPV, and applies incidence-rate adjustments or KPI weights</li> <li>Includes explainability and fairness audits</li> </ul> </li> <li>Unified API wrapper for production deployment<ul> <li>Ready for productization out of the box, no need to reinvent integration or design a new interface each time. See AlgoMarker</li> <li>Packages the entire end-to-end pipeline (raw time-series ingestion through inference) into a single, stable SDK.</li> <li>Core infrastructure implemented in C++ for performance and portability, with a lightweight Python wrapper for seamless integration.</li> <li>Although powered by C++, the team mainly uses and maintains workflows via the Python SDK, ensuring rapid development and minimal friction. Experienced user might use the C++ API more often, since the python interface is more limited. </li> </ul> </li> </ol>"},{"location":"Infrastructure%20C%20Library/index.html#basic-pages","title":"Basic Pages","text":"<ul> <li>MedModel learn and apply\u00a0</li> <li>RepProcessors:<ul> <li>RepProcessors Practical Page</li> </ul> </li> <li>FeatureGenerators:<ul> <li>Feature Generator Practical Guide</li> </ul> </li> <li>FeatureProcessors:<ul> <li>FeatureProcessor practical guide</li> </ul> </li> <li>MedPredictors<ul> <li>MedPredictors practical guide</li> </ul> </li> <li>PostProcessors:<ul> <li>PostProcessors Practical Guide</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/index.html#other-links","title":"Other links","text":"<p>Home page for in depth pages explaining several different aspects in the infrastructure Some interesting pages:</p> <ul> <li>Setup Environment</li> <li>How to Serialize : learn the SerializableObject libarary secrets.</li> <li>PidDynamicRecs and versions</li> <li>Virtual Signals</li> </ul>"},{"location":"Infrastructure%20C%20Library/Infrastructure%20Application%20Version.html","title":"Infrastructure Application Version","text":"<p>If using\u00a0medial::io::ProgramArgs_base wrapper class instead of ProgramArgs of boost you get several things: You get \"help\", option to load all arguments form file using --base_config and more.. Running our apps with --debug will print the app version and --version will just output the application version and exit.</p> <ul> <li>To compile all our tools with version info use build scripts in Scripts Repo: $MR_ROOT/Projects/Scripts/Bash-Scripts/full_build.sh</li> <li>Another option is to compile with</li> </ul> <p><pre><code>smake_rel.sh \"Version\\ comment\\ string\\ each\\ space\\ is\\ escaped\"\n</code></pre> will compile our program with this comment in version\\debug</p> <ul> <li>example of version output running: bootstrap --version with full_build.sh Version Info: Build on 14-01-2019_17:42:20 =&gt;Libs Git Head: 8fdbf55f32fc550420f300d391c88295bcb3a46a by avi at 2019-01-14 Last Commit Note: pushing the correct val in the diabetes registry =&gt;Tools Git Head: 97e7e06988a606781dda39625507d9702e146ac9 by avi at 2019-01-14 Last Commit Note: a diabetes read code list capabale of working with a registry rep processor</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html","title":"MedModel JSON Format","text":"<p>This guide explains the structure and usage of the MedModel JSON format for defining machine learning pipelines within the Medial infrastructure. The JSON file orchestrates all model steps, from raw data processing to prediction and post-processing, making your workflow modular, reproducible, and easy to configure.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#what-is-a-medmodel-json","title":"What is a MedModel JSON?","text":"<p>A MedModel JSON file describes:</p> <ul> <li>The pipeline of data processing and machine learning components (like data cleaning, feature generation, modeling, etc.)</li> <li>The order and configuration of each processing step</li> <li>The parameters for each component (as key-value pairs). This will call the component \"init\" function with the key value pairs to initialize the component. This allows a simpler way to add components and pass arguments to them from the json.</li> <li>How to reference additional configuration files or value lists</li> </ul> <p>This enables flexible, versioned, and shareable model definitions\u2014ideal for both research and production.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#how-to-write-a-medmodel-json-step-by-step","title":"How to Write a MedModel JSON: Step-by-Step","text":"<p>Let\u2019s walk through building a JSON model file, explaining each section.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#1-general-fields","title":"1. General Fields","text":"<p>These fields configure the overall behavior of the pipeline. Most are optional and have sensible defaults.</p> <pre><code>{\n  \"model_json_version\": \"2\",                // Required: version of the format (always use \"2\")\n  \"serialize_learning_set\": \"0\",            // Optional: whether to save training samples in the model (default \"0\")\n  \"generate_masks_for_features\": \"0\",       // Optional: track which features were imputed (default \"0\")\n  \"max_data_in_mem\": 100000,                // Optional: controls batch size for large data (default is unlimited)\n  \"take_mean_pred\": \"1\"                     // Optional: use mean for prediction (default \"1\")\n}\n</code></pre> <p>Tip: Only <code>model_json_version</code> is required for most users. The rest can typically be left out.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#2-the-pipeline-model_actions","title":"2. The Pipeline: <code>model_actions</code>","text":"<p>This is the heart of the model definition\u2014a list of components executed in order. Each component is an object specifying:</p> <ul> <li><code>action_type</code>: What kind of step this is (data cleaning, feature generation, etc.)</li> <li>Other keys: Parameters specific to the step</li> </ul> <p>Component types:</p> <ul> <li><code>rep_processor</code> or <code>rp_set</code>: Cleans or derives raw signals</li> <li><code>feat_generator</code>: Creates features from cleaned signals</li> <li><code>fp_set</code>: Post-processes the feature matrix (imputation, selection, normalization)</li> <li><code>predictor</code>: The machine learning algorithm</li> <li><code>post_processor</code>: Final calibration or adjustment</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#example-walkthrough","title":"Example Walkthrough","text":"<p>Let\u2019s walk through an example and explain each major step:</p> <pre><code>{\n  \"model_json_version\": \"2\",\n  \"serialize_learning_set\": \"0\",\n  \"model_actions\": [\n    // Step 1: Load additional rep_processors from another file for modularity\n    \"json:full_rep_processors.json\",\n\n    // Step 2: Generate simple features for age, gender, and smoking status\n    { \"action_type\": \"feat_generator\", \"fg_type\": \"age\" },\n    { \"action_type\": \"feat_generator\", \"fg_type\": \"gender\" },\n    { \"action_type\": \"feat_generator\", \"fg_type\": \"unified_smoking\", \"tags\": \"smoking\", \"smoking_features\": \"Current_Smoker,Ex_Smoker,...\" },\n\n    // Step 3: Generate categorical features (e.g., cancer diagnosis in the last 30 years)\n    {\n      \"action_type\": \"feat_generator\",\n      \"fg_type\": \"basic\",\n      \"type\": \"category_set\",\n      \"window\": [\"win_from=0;win_to=10950\"], // Days\n      \"sets\": [\"ICD9_CODE:140-149,ICD9_CODE:150-159,...\"],\n      \"signal\": \"ICD9_Diagnosis\",\n      \"in_set_name\": \"Cancers\"\n    },\n\n    // Step 4: Generate statistical features (last, avg, min, max, etc.) for signals and time windows\n    {\n      \"action_type\": \"feat_generator\",\n      \"fg_type\": \"basic\",\n      \"type\": [\"last\", \"avg\", \"max\", \"min\"],\n      \"window\": [\n        \"win_from=0;win_to=180\",\n        \"win_from=0;win_to=365\"\n      ],\n      \"signal\": [\"Hemoglobin\", \"WBC\", \"Platelets\"]\n    },\n\n    // Step 5: Feature selection (remove features with near-constant values)\n    {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        { \"fp_type\": \"remove_deg\", \"percentage\": \"0.999\" }\n      ]\n    },\n\n    // Step 6: Imputation (fill missing values using the median stratified by age, gender, smoking)\n    {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        { \"fp_type\": \"imputer\", \"strata\": \"Age,40,100,5:Gender,1,2,1\", \"moment_type\": \"median\", \"tag\": \"need_imputer\", \"duplicate\": \"1\" }\n      ]\n    },\n\n    // Step 7: Normalization (for features needing normalization)\n    {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        { \"fp_type\": \"normalizer\", \"resolution_only\": \"0\", \"resolution\": \"5\", \"tag\": \"need_norm\", \"duplicate\": \"1\" }\n      ]\n    }\n  ],\n\n  // Step 8: Specify the predictor and its parameters\n  \"predictor\": \"xgb\", // e.g., XGBoost\n  \"predictor_params\": \"tree_method=auto;booster=gbtree;objective=binary:logistic;...\"\n}\n</code></pre> <p>How it works:</p> <ol> <li>The pipeline loads additional processors from a separate JSON file (for modularity).</li> <li>It generates demographic and behavioral features.</li> <li>It creates diagnosis-based categorical features.</li> <li>It computes statistical features over defined time windows.</li> <li>It removes features with little variation.</li> <li>It imputes missing values using well-defined rules.</li> <li>It normalizes certain features.</li> <li>It trains the model using XGBoost, with custom parameters.</li> </ol> <p>This example uses an additional file \"full_rep_processors.json\" next to it. Here is the content inside</p> full_rep_processors.json full_rep_processors.json<pre><code>{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"conf_cln\",\n      \"conf_file\":\"path_rel:cleanDictionary.csv\",\n      \"time_channel\":\"0\",\n      \"clean_method\":\"confirmed\",\n      \"signal\":\"file_rel:all_rules_sigs.list\",\n      \"print_summary\" : \"1\",\n      \"nrem_suff\":\"nRem\"\n      //,\"verbose_file\":\"/tmp/cleaning.log\"\n    },\n    {\n      \"rp_type\":\"conf_cln\",\n      \"conf_file\":\"path_rel:cleanDictionary.csv\",\n      \"val_channel\":[\"0\", \"1\"],\n      \"clean_method\":\"confirmed\",\n      \"signal\": [\"BP\"],\n      \"print_summary\" : \"1\",\n      \"nrem_suff\":\"nRem\"\n      //,\"verbose_file\":\"/tmp/cleaning.log\"\n    }\n  ]\n},\n{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"sim_val\",\n      \"signal\":\"file_rel:all_rules_sigs.list\",\n      \"type\":\"remove_diff\",\n      \"debug\":\"0\"\n    }\n\n  ]\n},\n{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"rule_cln\",\n      \"addRequiredSignals\":\"1\",\n      \"time_window\":\"0\",\n      \"tolerance\":\"0.1\",\n      \"calc_res\":\"0.1\",\n      \"rules2Signals\":\"path_rel:ruls2Signals.tsv\",\n      \"consideredRules\":[ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\" ] \n\n      //,\"verbose_file\":\"/tmp/panel_cleaning.log\"\n    }\n  ]\n},\n{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"complete\",\n      \"sim_val\":\"remove_diff\",\n      \"config\":\"path_rel:completion_metadata\",\n      \"panels\":[\"red_line\", \"white_line\", \"gcs\", \"bmi\"]\n    },\n    {\n      \"rp_type\":\"calc_signals\",\n      \"calculator\":\"eGFR\",\n      \"missing_value\":\"-65336\",\n      \"work_channel\":\"0\",\n      \"names\":\"eGFR_CKD_EPI\",\n      \"signals_time_unit\":\"Date\",\n      \"max_time_search_range\":\"0\",\n      \"signals\":\"Creatinine,GENDER,BDATE\",\n      \"calculator_init_params\":\"{mdrd=0;ethnicity=0}\"\n    },\n    {\n      \"rp_type\":\"calc_signals\",\n      \"calculator\":\"eGFR\",\n      \"missing_value\":\"-65336\",\n      \"work_channel\":\"0\",\n      \"names\":\"eGFR_MDRD\",\n      \"signals_time_unit\":\"Date\",\n      \"max_time_search_range\":\"0\",\n      \"signals\":\"Creatinine,GENDER,BDATE\",\n      \"calculator_init_params\":\"{mdrd=1;ethnicity=0}\"\n    }\n  ]\n}  \n</code></pre> <ul> <li>It uses <code>conf_cln</code> for configuring simple and fixed outliers by valid range and configuration file <code>cleanDictionary.csv</code> with those ranges.</li> <li>It uses <code>all_rules_sigs.list</code> to list down all the avaible signals are create cleaner for each of those signals. See List Expension fo mkore details</li> <li>It uses <code>sim_val</code> to remove inputs on the same date with contradicting values and remove duplicate rows if the values are the sames</li> <li>It uses <code>rule_cln</code> to clear outliers based on equations and relations between signals. Gor example: BMI=Weight/Height^2, if a difference of more than tolerance (default is 10%) observed the values will be dropped. We can configure using <code>ruls2Signals.tsv</code> what happens if contradiction observed, whather to drop all signals in the relation, or just one specific - for example drop only the BMI. </li> <li>It uses <code>complete</code> - to complete missing values in panels from other relational signals. For example BMI is missing and we have Weight, Height. It uses <code>completion_metadata</code> to control the resulted signals resolution. </li> <li>It uses <code>calc_signals</code> to generate virtual signals for eGFR.</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#3-referencing-other-files","title":"3. Referencing Other Files","text":"<p>To keep your pipeline modular and maintainable, you can reference external files directly in your JSON configuration. Here are the supported reference types:</p> <ul> <li><code>\"json:somefile.json\"</code>: Imports another JSON file containing additional pipeline components.</li> <li><code>\"file_rel:signals.list\"</code>: Loads a list of values from a file and expands them as a JSON array. Useful for features or signals lists. For details on how lists are expanded, see List Expansion.</li> <li><code>\"path_rel:config.csv\"</code>: Uses a relative path to point to configuration files, resolved relative to the current JSON file's location.</li> <li><code>\"comma_rel:somefile.txt\"</code>: Reads a file line by line and produces a single comma-separated string of values (<code>\"line1,line2,...\"</code>). Unlike <code>\"file_rel\"</code>, this will not create a JSON list, but rather a flat, comma-delimited string.</li> </ul> <p>These options allow you to keep configuration modular, re-use existing resources, and simplify large or complex pipelines.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#4-advanced-list-expansion","title":"4. Advanced: List Expansion","text":"<p>If you use lists for fields (e.g., multiple signals or time windows), the pipeline automatically expands to cover all combinations (Cartesian product).</p> <p><pre><code>{\n  \"type\": [\"last\", \"avg\"],\n  \"window\": [\"win_from=0;win_to=180\", \"win_from=0;win_to=365\"]\n}\n</code></pre> This generates steps for each type \u00d7 window combination.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#5-reference-lists","title":"5. Reference Lists","text":"<p>At the end of your JSON, you can define reusable value lists, such as drug codes or signals:</p> <p><pre><code>\"diabetes_drugs\": \"ATC_A10,ATC_A11,ATC_A12\"\n</code></pre> Reference these in your pipeline using <code>\"ref:diabetes_drugs\"</code>.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#ready-to-write-your-own","title":"Ready to Write Your Own?","text":"<p>By following this walkthrough, you can confidently define new model JSON files:</p> <ul> <li>Start with the general fields</li> <li>List your pipeline steps in <code>model_actions</code></li> <li>Modularize and reuse with references</li> <li>Expand lists for coverage</li> <li>Define your predictor and parameters</li> </ul> <p>Tip: For a new project, copy and adapt the example above to fit your own signals, features, and model goals.</p>"},{"location":"Infrastructure%20C%20Library/MedModel%20json%20format.html#further-reading","title":"Further Reading","text":"<ul> <li>Rep Processors Practical Guide</li> <li>Feature Generator Practical Guide</li> <li>FeatureProcessor practical guide</li> <li>MedPredictor practical guide</li> <li>PostProcessors Practical Guide</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/index.html","title":"InfraMed Library page","text":"<p>Git Link:\u00a0https://github.com/Medial-EarlySign/MR_Libs</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/index.html#general","title":"General","text":"<p>The InfraMed Library is a non-sql data structure tailored to hold medical records data. The main goal is to provide an efficient way to get a vector of time signals (date/time, value) for a pair of patient and signal type (pid , sid). Main toolsets available in the library:</p> <ul> <li>Converting a data set to the InfraMed Repository format. (see here)</li> <li>Adding a signal to the data set, fixing a signal</li> <li>Reading a data set or a part of it to memory</li> <li>Get the vector of signals for (pid, sid) , date/time sorted</li> <li>Create a \"by-pid\" transposed way of keeping the data</li> <li>Get all signals for a pid \"by-pid\"</li> <li>Free/Lock/Unlock/Load mechanisms for signals</li> <li>Work with dictionaries\u00a0</li> <li>Manage versions of data for a signal</li> <li>Option to load data in memory to create a repository \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/index.html#the-inframed-data-model-in-5-sentences","title":"The InfraMed data model in 5 sentences.","text":"<ul> <li>each patient has one or more signal vectors.</li> <li>each signal is of a specific type.</li> <li>In general any type can be added (see more in the MedSignals page) but a signal is composed of a constant number of time channels (0 or more) and a constant number of value channels (0 or more).</li> <li>Except for the (new) rep in memory features the library assumes the data is for read-only and updated once in a very long time, so the efficiency of creating a repository is less important.</li> <li>The library allows an extremely fast access for a signal vector given a query of a pid + signal_id (or signal name), and this is its main usage.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/index.html#general-classes","title":"General Classes","text":"<p>It is recommeded to read and understand the following pages explaining the main classes used in the InfraMed library:</p> <ul> <li>MedRepository , MedPidRepository : most used classes packing options to read repositories and query them.</li> <li>MedConvert : class used when creating a new repository.</li> <li>MedSignals: class used to handle signal files and signal properties, also contains the unified way of accessing signals.</li> <li>MedDictionary , MedDictionarySections : classes used to read dictionaries and use them More Advanced:</li> <li>PidDynamicRec : class with an advanced option to hold versions of the same signal.</li> <li>InMem repository : the InMemRepData class and its usage in a MedRepository to load data into it. Even More... (internal important classes and methods):</li> <li>IndexTable : class to read data for a specific signal on a subset (or all) pids, with a memory efficient fast index.</li> <li>MedSparseVec : a general key,value datastructure. Memory efficient and fast. The baseline for many of the indexes used in a MedRepository. \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html","title":"Generic (Universal) Signal Vectors","text":""},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#medrepository-signaltypes","title":"MedRepository SignalTypes","text":"<p>Legacy signal types were previously referenced by numeric IDs\u2014this approach is now deprecated: <pre><code>string GenericSigVec::get_type_generic_spec(SigType t)\n{\n    switch (t) {\n    case T_Value: return \"V(f)\";                    //  0\n    case T_DateVal: return \"T(i),V(f)\";             //  1\n    case T_TimeVal: return \"T(l),V(f),p,p,p,p\";     //  2\n    case T_DateRangeVal: return \"T(i,i),V(f)\";      //  3\n    case T_TimeStamp: return \"T(l)\";                //  4\n    case T_TimeRangeVal: return \"T(l,l),V(f),p,p,p,p\"; //  5\n    case T_DateVal2: return \"T(i),V(f,us),p,p\";     //  6\n    case T_TimeLongVal: return \"T(l),V(l)\";         //  7\n    case T_DateShort2: return \"T(i),V(s,s)\";        //  8\n    case T_ValShort2: return \"V(s,s)\";              //  9\n    case T_ValShort4: return \"V(s,s,s,s)\";          // 10\n    case T_CompactDateVal: return \"T(us),V(us)\";    // 11\n    case T_DateRangeVal2: return \"T(i,i),V(f,f)\";   // 12\n    case T_DateFloat2: return \"T(i),V(f,f)\";        // 13\n    case T_TimeRange: return \"T(l,l)\";              // 14\n    case T_TimeShort4: return \"T(i),p,p,p,p,V(s,s,s,s)\"; //15\n    default:\n        MTHROW_AND_ERR(\"Cannot get generic spec of signal type %d\\n\", t);\n    }\n    return 0;\n}\n</code></pre></p> <p>When defining C structs, compilers may add padding bytes to align fields for optimal memory access. For certain types (like T_TimeVal, T_TimeRangeVal, T_DateVal2, and T_TimeShort4), padding was added to maintain compatibility with older C structures. For example, T_TimeRangeVal corresponds to the GSV string \"T(l,l),V(f),p,p,p,p\", meaning each record contains two <code>long long</code> time channels, one <code>float</code> value channel, and four padding bytes.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#gsv-declarations-in-signal-files","title":"GSV Declarations in .signal Files","text":"<p>In repository <code>.signal</code> files, you can specify a signal\u2019s type using either the legacy numeric code or a GSV string specification. If multiple signals share the same type, you can define a type alias to avoid repetition and improve clarity. Define an alias using: GENERIC_SIGNAL_TYPE [type_alias] [GSV_string_spec] (Remember: <code>.signal</code> files are tab-separated.)</p> <p>Example : .signal file example <pre><code>GENERIC_SIGNAL_TYPE mytype0 V(f)\nGENERIC_SIGNAL_TYPE mytype1 T(i),V(f)\nSIGNAL  GENDER  100 16:mytype0  Male=1,Female=2 0\nSIGNAL  BMI 902 16:mytype1  0\nSIGNAL  ICD9_Hospitalization    2300    T(i,i),V(f) 1\nSIGNAL  RBC 1001    1   0\n...\n</code></pre> In this example:</p> <ul> <li>The first two lines define type aliases (<code>mytype0</code>, <code>mytype1</code>).  </li> <li>The next two lines use these aliases for the GENDER and BMI signals.  </li> <li>The following lines show direct type specification and use of a legacy type code.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#using-gsv","title":"Using GSV","text":"<p>The <code>GenericSigVec</code> class implements GSVs, replacing the old USV system. You can still use <code>UniversalSigVec</code> as an alias for <code>GenericSigVec</code> for backward compatibility: <pre><code>typedef class GenericSigVec UniversalSigVec;\n</code></pre> The previous USV implementation is now called <code>UniversalSigVec_legacy</code> and will be removed in the future.</p> <p>Initialization can be done as follows: <pre><code>    GenericSigVec()\n    GenericSigVec(const string&amp; signalSpec, int time_unit = MedTime::Undefined)\n    GenericSigVec(SigType sigtype, int time_unit = MedTime::Undefined)\n    GenericSigVec(const GenericSigVec&amp; other)\n\u00a0\n    void init(const SignalInfo &amp;info)\n    void init_from_spec(const string&amp; signalSpec);\n    void init_from_sigtype(SigType sigtype);\n    void init_from_repo(MedRepository&amp; repo, int sid);\n</code></pre> The string specification replaces the old <code>SigType</code>. You can also initialize from a <code>SignalInfo</code> object for better performance, as it avoids runtime parsing.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#implementation","title":"Implementation","text":"<p>The core logic is in the value getter function: <pre><code>    template&lt;typename T = float&gt;\n    T Val(int idx, int chan, const void* data_) const {\n        auto field_ptr = ((char*)data_) + idx * struct_size + val_channel_offsets[chan];\n        switch (val_channel_types[chan]) {\n        case type_enc::FLOAT32: return (T)(*(float*)(field_ptr));\n        case type_enc::INT16:   return (T)(*(short*)(field_ptr));\n        case type_enc::UINT16:  return (T)(*(unsigned short*)(field_ptr));\n        case type_enc::UINT8:   return (T)(*(unsigned char*)(field_ptr));\n        case type_enc::UINT32:  return (T)(*(unsigned int*)(field_ptr));\n        case type_enc::UINT64:  return (T)(*(unsigned long long*)(field_ptr));\n        case type_enc::INT8:    return (T)(*(char*)(field_ptr));\n        case type_enc::INT32:   return (T)(*(int*)(field_ptr));\n        case type_enc::INT64:   return (T)(*(long long*)(field_ptr));\n        case type_enc::FLOAT64: return (T)(*(double*)(field_ptr));\n        case type_enc::FLOAT80: return (T)(*(long double*)(field_ptr));\n        }\n        return 0;\n    }\n</code></pre></p> <ul> <li>This template function retrieves values (defaulting to <code>float</code>, but other types are supported).</li> <li>It calculates the value\u2019s location using <code>val_channel_offsets</code>.</li> <li>The correct type cast is chosen based on <code>val_channel_types</code>.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#changes-for-gsv-support","title":"Changes for GSV Support","text":"<ul> <li><code>MedConvert</code> now supports GSVs when creating new repositories.</li> <li><code>MedPyExport</code> can export GSVs to Python. Exported field names are now <code>time0</code>, <code>time1</code>, ..., <code>val0</code>, <code>val1</code>, ...</li> <li>Virtual signals now support the new type specification strings.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#performance","title":"Performance","text":"<p>Performance testing shows a slight improvement over the old implementation, likely because the new approach avoids C function callbacks, allowing better compiler optimization.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedDictionary.html","title":"MedDictionary","text":"<p>The <code>MedDictionary</code> class provides methods for mapping integer values to strings, which is essential since MedRepository does not support free text and relies on numerical codes for efficiency (e.g., drug names or read codes). This class enables translating these codes back to their textual descriptions.</p> <p>Additionally, MedDictionary supports defining sets of values and assigning them names, which is useful for organizing hierarchical classifications (such as ICD codes, drug categories, or cancer types). Efficient membership testing for these sets is also provided.</p> <p>To handle multiple dictionaries with potentially overlapping numerical codes, MedDictionary uses a sections mechanism.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedDictionary.html#dictionary-file-format","title":"Dictionary File Format","text":"<p>When loading a repository, dictionary files are read with these rules:</p> <ul> <li>Ignore empty lines and lines starting with <code>#</code> (comments).</li> <li>All other lines are tab-delimited.</li> <li>Section definition: <code>SECTION &lt;comma-separated list of section names&gt;</code>   Each section can have multiple names. Typically, a section contains all relevant signal names.</li> <li>Value definition: <code>DEF &lt;numerical int value&gt; &lt;string&gt;</code>   Multiple names can be assigned to the same value. Each set must have a DEF line for its name. Each int value must be unique.</li> <li>Set membership: <code>SET &lt;set name&gt; &lt;member name&gt;</code>   Sets can include other sets, but avoid cyclic definitions.</li> </ul> <p>Example dictionary file: <pre><code># Section definition\nSECTION RC_Diagnosis,Cancer_Location,DM_Registry,HT_Registry,CVD_MI,CVD_HeartFailure,CVD_HemorhagicStroke,CVD_IschemicStroke,CKD_State,DEATH\n\n# Value definitions\nDEF     0       DM_Registry_Non_diabetic\nDEF     1       DM_Registry_Pre_diabetic\nDEF     2       DM_Registry_Diabetic\n\n# Set definitions (each set gets a unique int value)\nDEF     21000   Colon_Cancer\nDEF     21001   CRC_Cancer\nDEF     21002   Stomach_Cancer\nDEF     21003   Rectum_Cancer\n\n# Set memberships\nSET     Colon_Cancer    Digestive Organs,Digestive Organs,Colon\nSET     Stomach_Cancer  Digestive Organs,Digestive Organs,Stomach\nSET     Rectum_Cancer   Digestive Organs,Digestive Organs,Rectum\nSET     CRC_Cancer      Colon_Cancer\nSET     CRC_Cancer      Rectum_Cancer\nSET     CRC_and_Stomach_Cancer  CRC_Cancer\nSET     CRC_and_Stomach_Cancer  Stomach_Cancer\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedDictionary.html#meddictionary-vs-meddictionarysections","title":"MedDictionary vs. MedDictionarySections","text":"<ul> <li>MedDictionary: Handles a single dictionary with one namespace for numerical values.</li> <li>MedDictionarySections: Manages multiple dictionaries, each in its own section, with APIs for section management.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedDictionary.html#initializing-dictionaries","title":"Initializing Dictionaries","text":"<p>Dictionaries are automatically initialized when a repository is loaded, using the dictionary files specified in the repository config. To manually initialize, use <code>read(vector&lt;string&gt; &amp;input_dictionary_files)</code> for either MedDictionary or MedDictionarySections. Dictionaries are typically used within a MedRepository, but can be used independently.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedDictionary.html#key-methods","title":"Key Methods","text":"<p>MedDictionary:</p> <ul> <li><code>int id(const string &amp;name)</code>: Get id from name.</li> <li><code>string name(int id)</code>: Get name from id (returns the last name if multiple exist).</li> <li><code>map&lt;int, vector&lt;string&gt;&gt; Id2Names</code>: Maps id to all its names.</li> <li><code>int is_in_set(int member_id, int set_id)</code>: Check if member id is in set.</li> <li><code>int is_in_set(const string&amp; member, const string&amp; set_name)</code>: Same as above, using names.</li> <li><code>int prep_sets_lookup_table(const vector&lt;string&gt; &amp;set_names, vector&lt;char&gt; &amp;lut)</code>: Creates a fast lookup table for set membership.</li> <li><code>int prep_sets_indexed_lookup_table(const vector&lt;string&gt; &amp;set_names, vector&lt;unsigned char&gt; &amp;lut)</code>: Similar, but notes the serial number of the set for each member.</li> </ul> <p>MedDictionarySections:</p> <ul> <li><code>int section_id(const string &amp;name)</code>: Get section id from name.</li> <li><code>vector&lt;MedDictionary&gt; dicts</code>: Access a dictionary by section id and use all MedDictionary methods.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedDictionary.html#code-example","title":"Code Example","text":"<pre><code>// Initialize repository (which initializes MedDictionarySections)\nMedRepository rep;\n\n// Print the name for a drug code\nint section_id = rep.dict.section_id(\"Drug\");\ncout &lt;&lt; \"Name of Drug code \" &lt;&lt; drug_val &lt;&lt; \" is \" &lt;&lt; rep.dict.dicts[section_id].name(drug_val);\n\n// Scan a list of patient IDs for drug usage in specific sets\nvector&lt;int&gt; pids; // Assume populated\nvector&lt;string&gt; drug_sets = {\"ATC_B04A_B__\", \"ATC_C10A_A__\", \"ATC_B01A____\"}; // Example sets\n\n// Prepare lookup table for fast membership testing\nvector&lt;char&gt; lut;\nrep.dict.dicts[rep.dict.section_id(\"Drug\")].prep_sets_lookup_table(drug_sets, lut);\n\nUniversalSigVec usv;\nint drug_sid = rep.sigs.sid(\"Drug\");\n\n// Iterate over patients\nfor (auto &amp;pid : pids) {\n    rep.uget(pid, drug_sid, usv);\n    for (int i = 0; i &lt; usv.len; i++) {\n        int drug_val = (int)usv.Val(i, 0);\n        if (lut[drug_val])\n            cout &lt;&lt; \"Patient \" &lt;&lt; pid &lt;&lt; \" is using drug code \" &lt;&lt; drug_val &lt;&lt; \" at time \" &lt;&lt; usv.Time(i,0); // Print usage\n    }\n}\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html","title":"MedRepository","text":""},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#overview","title":"Overview","text":"<p><code>MedRepository</code> and its extension <code>MedPidRepository</code> are core classes in the InfraMed library, designed to facilitate reading, managing, and querying repository data. Familiarity with these classes is essential for effective use of the library.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#medrepository-vs-medpidrepository","title":"MedRepository vs. MedPidRepository","text":"<p><code>MedPidRepository</code> builds on <code>MedRepository</code> by providing APIs to load a complete patient record (<code>PidRec</code>). This is particularly useful when iterating over patient IDs and needing all associated data for each one.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#initializing-and-loading-data","title":"Initializing and Loading Data","text":"<p>There are several ways to initialize and load data into a <code>MedRepository</code>:</p> <ul> <li> <p>Using <code>read_all()</code>   Loads the repository configuration, a list of signal names (or all signals if the list is empty), and a list of patient IDs (or all if empty). The function loads the full cartesian product of requested pids and signals into memory, making the repository ready for queries.</p> </li> <li> <p>Using <code>init()</code>   Initializes the repository with the configuration file, loading signals, dictionaries, and file locations, but not the actual data. Data can be loaded later using <code>load()</code> and released with <code>free()</code>.</p> </li> <li> <p>Using <code>load()</code> and <code>free()</code> <code>load()</code> loads data for a specific signal and list of pids (or all if the list is empty). <code>free()</code> releases memory for a specific signal, which is useful for handling large datasets.</p> </li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-initializing-and-loading-a-repository","title":"Example: Initializing and Loading a Repository","text":"<pre><code>// Define repository variable\nMedRepository rep;\n\n// Repository configuration file\nstring rep_conf = \"/home/Repositories/THIN/thin.repository\";\n\n// Option 1: Load all signals and pids\nrep.read_all(rep_conf);\n\n// Option 2: Load specific signals and pids\nvector&lt;string&gt; sigs = {\"BDATE\", \"GENDER\", \"Glucose\", \"HbA1C\", \"BMI\", \"Drug\"};\nvector&lt;int&gt; pids = {5000001, 50000002, 10000000};\nrep.read_all(rep_conf, pids, sigs);\n\n// Option 3: Initialize without loading data\nrep.init(rep_conf);\n\n// Load a subset of signals and pids\nrep.load(sigs, pids);\n\n// Free signals from memory\nrep.free(sigs);\nrep.free_all_sigs();\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#accessing-data-in-medrepository","title":"Accessing Data in MedRepository","text":"<p>Once data is loaded, you can access it using the <code>get()</code> function (Old API), which returns a time-sorted vector of values for a specific pid and signal. The result is a void pointer that should be cast to the appropriate signal type. Please use <code>uget()</code> instead and get a generic signal of type <code>UniversalSigVec</code></p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-accessing-data","title":"Example: Accessing Data","text":"<pre><code>// Repository variable\nMedRepository rep;\n\n// Assume repository is initialized and loaded\n\nint pid;\nstring sig;\n\n// Convert signal name to signal ID for performance\nint sid = rep.sigs.sid(sig);\n\n// Output variable for length\nint len;\n\n// Access DateVal signal with old API - Don't use it (and almost not exists anymore) \nSDateVal *sdv = (SDateVal *)rep.get(pid, sig, len);\n// Access Generic signal with variable time and values channels\nUniversalSigVec usv;\nrep.uget(pid, sig, usv);\n\n\nprintf(\"read %d items\\n\", len);\nif (len &gt; 0)\n    printf(\"First item: time %d, val %f. Last item: time %d, val %f\\n\", sdv[0].date, sdv[0].val, sdv[len-1].date, sdv[len-1].val);\n\n// Faster access using signal ID - please use uget\nsdv = (SDateVal *)rep.get(pid, sid, len);\nrep.uget(pid, sid, usv);\n\n// Access a different signal type - please use uget\nSVal *sv = (SVal *)rep.get(pid, \"BDATE\", len);\nrep.uget(pid, \"BDATE\", usv);\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#medpidrepository-initialization-and-usage","title":"MedPidRepository: Initialization and Usage","text":"<p><code>MedPidRepository</code> enables loading data for individual pids into memory, which is useful for scanning large repositories with minimal memory usage. Each thread can hold data for a single patient, rather than all data at once.</p> <p>To use <code>MedPidRepository</code>, ensure the repository is transposed during its creation. After setup, use a <code>PidRec</code> object to store data for a specific patient ID and access signals with either <code>get()</code> or <code>uget()</code>.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-using-medpidrepository-and-pidrec","title":"Example: Using MedPidRepository and PidRec","text":"<pre><code>// Create MedPidRepository and PidRec instances\nMedPidRepository pid_rep;\nPidRec rec;\n\n// Initialize the repository\npid_rep.init(rep_conf_file);\n\n// Load data for a specific patient\npid_rep.get_pid_rec(pid, rec);\n\n// Access a signal using the legacy API\nSDateVal *sdv = rec.get(sig_name, len);\n\n// Access a signal using the recommended API\nUniversalSigVec usv;\nrec.uget(sig_name, usv);\n</code></pre> <p>You can also initialize a <code>PidRec</code> from an existing <code>MedRepository</code> using <code>init_from_rep()</code>. This copies the patient record and allows for dynamic modifications.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-initializing-pidrec-from-medrepository","title":"Example: Initializing PidRec from MedRepository","text":"<pre><code>MedRepository rep;\nPidRec rec;\n\n// Repository should be initialized as shown earlier\n\nvector&lt;int&gt; sids; // Leave empty to load all signals\nrec.init_from_rep(&amp;rep, pid, sids);\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedRepository.html#performance-tips","title":"Performance Tips","text":"<ul> <li>Store repositories locally (e.g., <code>/home/Repositories</code> on Linux) for optimal speed.</li> <li>When processing many patient IDs, use signal IDs instead of names for faster lookups:   <pre><code>int pid;\nstring sig_name;\nint sid = rep.sigs.id(sig_name);\nUniversalSigVec usv;\n\nfor (...) {\n    rec.uget(sig_name, usv);\n    // ...process data...\n}\n</code></pre></li> <li>Load only the signals you need</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html","title":"MedSignals / Unified Signals","text":"<p>MedSignals is an internal class in MedRepository that defines signal types and manages <code>.signals</code> files\u2014dictionaries mapping signal names to their properties. Each MedRepository instance has a <code>sigs</code> object for basic <code>.signals</code> operations. All signal definitions are found in <code>MedSignals.h</code>.</p> <p>When initializing a repository, one of the first steps is to read the signals configuration file (specified in the repository config) and set up all signal definitions.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#what-is-a-signal","title":"What is a signal?","text":"<p>A signal type is a class defined in <code>MedSignals.h</code> and acts as a placeholder for signals. Signals represent actual data stored in the repository, each with a specific type. For example:</p> <ul> <li>BYEAR, GENDER, TRAIN: Store a single float value, typically using the <code>SVal</code> type, or as a generic signal with one integer value channel: \"V(i)\"</li> <li>Hemoglobin, Creatinine: Store a date and a float value, using <code>SDateVal</code> or <code>STimeVal</code>, or as a generic signal with one integer time channel and one float value channel: \"T(i),V(f)\"</li> <li>BP: Stores a date and two short values (diastolic and systolic blood pressure), using <code>SDateShort2</code>, or as a generic signal: \"T(i),V(s,s)\"</li> </ul> <p>Each signal has several properties, defined in the <code>SignalInfo</code> class:</p> <ul> <li>name: Signal name</li> <li>type: Type code (from the <code>SigType</code> enum) or a generic type string. You can use <code>get_signal_generic_spec()</code> on <code>UniversalSigVec</code> to get its generic signal string representation.</li> <li>sid: Unique integer signal ID from the signals config file, used for fast indexing and efficient access. Maximum SID is <code>MAX_SID_NUM</code> (currently 100,000).</li> <li>bytes_len: Number of bytes needed to store one element.</li> <li>n_time_channels: Number of time channels.</li> <li>n_val_channels: Number of value channels.</li> </ul> <p>Signals must currently have a fixed size.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#unified-signals-part-i","title":"Unified Signals - Part I","text":"<p>Most medical record signals can be described as having zero or more time channels and zero or more value channels. Examples:</p> <ul> <li>BYEAR: 0 time channels, 1 float value channel</li> <li>Creatinine: 1 integer time channel, 1 float value channel</li> <li>BP: 1 integer time channel, 2 float value channels</li> <li>DM_Registry: 2 integer time channels, 1 int/char value channel</li> </ul> <p>When time channels are integers and value channels are floats, signals can be accessed in a unified way, regardless of their type. This allows for generalized code in components like <code>RepProcessors</code> and <code>FeatureGenerators</code> in MedProcessTools.</p> <p>See Part II for usage examples.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#current-signal-types","title":"Current Signal Types","text":"<p>All signal types are defined in <code>MedSignals.h</code>. Available types include:</p> Signal Type Time Channels Value Channels Placeholders Example Signals SVal 0 1 float BYEAR, TRAIN, GENDER SDateVal 1 1 int : float Hemoglobin, Creatinine STimeVal 1 1 long long : float Creatinine (mimic) SDateRangeVal 2 1 int, int : float DM_Registry STimeRangeVal 2 1 long long, long long : float (mimic) STimeStamp 0 1 : long long (mimic) SDateVal2 1 2 int : float, unsigned short Drugs (MHS) STimeLongVal 1 1 long long : long long (mimic) SDateShort2 1 2 int : short, short BP SValShort2 0 2 : short, short (mimic) SValShort4 0 4 : short, short, short, short (mimic)"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#adding-a-new-signal-type","title":"Adding a New Signal Type","text":"<p>To add a new legacy signal type:</p> <ol> <li>Add an enum value for the type (used for encoding in the signals file).</li> <li>Define the signal class and its methods.</li> <li>Update <code>MedConvert</code> to read the new signal type (add parsing logic if needed).</li> <li>Make sure the new signal class inherits from <code>UnifiedSig</code> for unified signal compatibility.</li> <li>Implement these methods:    - <code>n_time_channels()</code>    - <code>n_val_channels()</code>    - <code>time_unit()</code>    - <code>int Time(int chan)</code>    - <code>float Val(int chan)</code>    - <code>SetVal(chan, _val)</code>    - <code>Set(int *times, float *vals)</code>    - Comparison operators (<code>&lt;</code>, <code>==</code>)    - Output operator (<code>&lt;&lt;</code>)</li> </ol>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#unified-signals-part-ii","title":"Unified Signals - Part II","text":"<p>To work with unified signals, use the <code>UniversalSigVec</code> (<code>usv</code>) class. Instead of the standard <code>get()</code> method, use <code>uget()</code> to load signal data into a <code>usv</code> object. Key features:</p> <ul> <li>Initialize for a specific signal type using only the enum.</li> <li>Use <code>uget()</code> to load data (no copies; uses pointers and virtual functions).</li> <li>Access any time or value channel uniformly across all signal types.</li> </ul> <p>This lets you write type-independent code. Initialization has a performance cost, but repeated use with the same type is efficient.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#example-usage","title":"Example Usage","text":"<pre><code>// Initialize repository and UniversalSigVec\nMedRepository rep;\nUniversalSigVec usv;\n\n// ... initialize repository ...\n\nrep.uget(pid, sid, usv);\n\n// Print all time and value channels for each element\nfor (int i = 0; i &lt; usv.len; i++) {\n    for (int t_ch = 0; t_ch &lt; usv.n_time_channels(); t_ch++)\n        cout &lt;&lt; \"Element \" &lt;&lt; i &lt;&lt; \", time channel \" &lt;&lt; t_ch &lt;&lt; \": \" &lt;&lt; usv.Time(i, t_ch);\n    for (int v_ch = 0; v_ch &lt; usv.n_val_channels(); v_ch++)\n        cout &lt;&lt; \"Element \" &lt;&lt; i &lt;&lt; \", value channel \" &lt;&lt; v_ch &lt;&lt; \": \" &lt;&lt; usv.Val(i, v_ch);\n}\n\n// Usage with PidRec\nMedPidRepository pid_rep;\nPidRec rec;\n\n// ... initialize pid_rep ...\n\npid_rep.get_pid_rec(pid, rec);\nrec.uget(sid, usv);\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#signals-config-file","title":"Signals Config File","text":"<p>For details on the signals file</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html","title":"PidDynamicRec","text":"<p>The <code>PidDynamicRec</code> class provides advanced access to signal data, allowing both reading and modification, as well as maintaining multiple \"versions\" of a signal vector. This is particularly useful when implementing RepProcessors in the <code>MedProcessTools</code> classes, where signals may need to be cleaned, altered, or removed before feature creation.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#why-are-multiple-versions-necessary","title":"Why are multiple versions necessary?","text":"<p>When running a RepProcessor and modifying a signal at timepoint T, data from future timepoints (t &gt; T) might inadvertently influence the change, leading to data leakage during model training and testing. For example, if testing at T_test (where T_test \u2265 T) and using data from t &gt; T_test, future information contaminates the test.</p> <p>The complexity increases when testing at multiple timepoints for a patient (e.g., T_test1 and T_test2, with T_test2 &gt; T_test1). The allowed \"horizon\" for each test is:</p> <ul> <li>For T_test1: t \u2264 T_test1</li> <li>For T_test2: t \u2264 T_test2</li> </ul> <p>Values generated by RepProcessors for t \u2264 T_test1 may differ between these horizons, requiring snapshots (\"versions\") of the data as seen up to each timepoint. Efficient management of these versions is essential for performance and memory usage.</p> <p><code>PidDynamicRec</code> addresses these challenges by enabling efficient versioning.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#version-numbering","title":"Version Numbering","text":"<ul> <li>Version 0: The original data from the repository.</li> <li>Versions 1+: Created and managed using <code>PidDynamicRec</code> tools, typically corresponding to different prediction timepoints.</li> </ul>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#initializing-a-piddynamicrec","title":"Initializing a PidDynamicRec","text":"<p><code>PidDynamicRec</code> inherits from <code>PidRec</code> and is initialized similarly, with the addition of specifying the number of versions to maintain (usually matching the number of prediction timepoints). Use the <code>set_n_versions()</code> method:</p> <pre><code>int init_from_rep(MedRepository *rep, int pid, vector&lt;int&gt; &amp;sids_to_use, int n_versions);\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#reading-from-a-piddynamicrec","title":"Reading from a PidDynamicRec","text":"<p>To read data, specify both the signal and the version:</p> <pre><code>void *get(int sid, int version, int &amp;len);\nvoid *get(string &amp;sig_name, int version, int &amp;len);\nvoid *uget(int sid, int version, UniversalSigVec &amp;_usv);\nvoid *uget(const string &amp;sig_name, int version, UniversalSigVec &amp;_usv);\n</code></pre> <p>Each <code>PidRec</code> maintains a <code>usv</code> object for thread safety and efficiency.</p>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#modifying-versions-in-piddynamicrec","title":"Modifying Versions in PidDynamicRec","text":"<p>The core purpose of <code>PidDynamicRec</code> is to create and manage versions that differ from the original. Initially, all versions reference version 0.</p> <p>Key methods for version management:</p> <pre><code>// Set a version to new data\nint set_version_data(int sid, int version, void *datap, int len);\n\n// Copy original data into a new version\nint set_version_off_orig(int sid, int version);\n\n// Point one version to another's data\nint point_version_to(int sid, int v_src, int v_dst);\n\n// Remove an element from a version\nint remove(int sid, int version, int idx);\n\n// Remove from one version and place in another\nint remove(int sid, int v_in, int idx, int v_out);\n\n// Change an element in a version\nint change(int sid, int version, int idx, void *new_elem);\n\n// Change in one version and copy to another\nint change(int sid, int v_in, int idx, void *new_elem, int v_out);\n\n// Batch update: changes and removals\nint update(int sid, int v_in, vector&lt;pair&lt;int, void *&gt;&gt;&amp; changes, vector&lt;int&gt;&amp; removes);\n\n// Batch update with value channel\nint update(int sid, int v_in, int val_channel, vector&lt;pair&lt;int, float&gt;&gt;&amp; changes, vector&lt;int&gt;&amp; removes);\n</code></pre> <p>Example: Cleaning Glucose Signal</p> <pre><code>MedRepository rep;\nint pid;\n// ... load repository and set pid ...\n\nPidDynamicRec pdr;\nint sid = rep.sigs.sid(\"Glucose\");\nvector&lt;int&gt; sids = { sid };\npdr.init_from_rep(&amp;rep, pid, sids, 1); // Initialize with one extra version\n\nUniversalSigVec usv;\npdr.uget(sid, 0, usv); // Get original data\n\n// Option 1: Create a new vector and set as version 1\nvector&lt;SDataVal&gt; new_glu;\nfor (int i = 0; i &lt; usv.len; i++) {\n    if (usv.Val(i, 0) &gt; 0) {\n        SDataVal sdv;\n        sdv.date = usv.Time(i, 0);\n        sdv.val = (float)((int)usv.Val(i, 0));\n        new_glu.push_back(sdv);\n    }\n}\npdr.set_version_data(sid, 1, &amp;new_glu[0], (int)new_glu.size());\n\n// Option 2: Batch update\nvector&lt;pair&lt;int, float&gt;&gt; changes;\nvector&lt;int&gt; removes;\nfor (int i = 0; i &lt; usv.len; i++) {\n    if (usv.Val(i, 0) &gt; 0) {\n        changes.push_back({i, (float)((int)usv.Val(i, 0))});\n    } else {\n        removes.push_back(i);\n    }\n}\npdr.update(sid, 1, 0, changes, removes);\n\n// Read modified version\npdr.uget(sid, 1, usv);\n// usv now contains the cleaned data\n</code></pre>"},{"location":"Infrastructure%20C%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#efficiency-version-pointing-and-iteration","title":"Efficiency: Version Pointing and Iteration","text":"<p>Often, multiple versions are identical (e.g., when only considering data up to a given timepoint). To optimize, versions can \"point\" to the same data, only splitting when changes are needed.</p> <p>Mechanisms: 1. Version Pointing: A version can reference another's data, splitting only when modifications occur. 2. Iterators: Use iterators to process blocks of versions sharing the same data.</p> <p>API Examples:</p> <pre><code>class PidDynamicRec : public PidRec {\n    // Point one version to another\n    int point_version_to(int sid, int v_src, int v_dst);\n\n    // Check if two versions share data\n    int versions_are_the_same(int sid, int v1, int v2);\n    int versions_are_the_same(set&lt;int&gt; sids, int v1, int v2);\n    // ...\n};\n\n// Iterator for blocks of identical versions\nclass differentVersionsIterator : public versionIterator {\n    int jVersion;\n    int init();\n    int next();\n    bool done() { return iVersion &lt; 0; }\n    inline int block_first() { return jVersion + 1; }\n    inline int block_last() { return iVersion; }\n};\n</code></pre> <p>Iterating Over Versions Example:</p> <pre><code>_apply(PidDynamicRec&amp; rec, vector&lt;int&gt;&amp; time_points, ... ) {\n    differentVersionsIterator vit(rec, reqSignalIds);\n    for (int iver = vit.init(); !vit.done(); iver = vit.next()) {\n        // Process versions from vit.block_first() to vit.block_last()\n        // All versions in this block point to the same data, optimizing performance\n    }\n}\n</code></pre>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/index.html","title":"Rep Processors Practical Guide","text":"<p>This page intends to list current RepProcessors with explanations on parameters and a json line example.</p> <ul> <li>basic_cln : Basic outlier cleaner : learn ditribution and throw extreme values</li> <li>nbrs_outlier_cleaner : Neighborhood outlier cleaner</li> <li>configured_outlier_cleaner: Configured outlier cleaner - Wrapper for\u00a0basic_cln\u00a0 with fixed bounderies for each signal</li> <li>rulebased_outlier_cleaner: Rule Based outlier cleaner - Rules based on panels (for example BMI) to remove mismatched values in panels</li> <li>calculator: virtual signals calculator - Virtual signals calculator - linear sum, log, and more..</li> <li>complete: panel completer - Completes signals using panels calculations. searches for signals in exact same time that are relate to calculate (TODO: time window support). for example BMI = Weight/Height^2</li> <li>req : check requirement processor</li> <li>sim_val : sim val processor\u00a0 - When signal has more than one value in same time - which one to choose</li> <li>signal_rate : signal rate processor - divide signal value by time diff in 2 time channels that describe the signal period</li> <li>combine : combine signals processor - combines 2 signals to 1 signal with same name</li> <li>split : split signal processor - splits signal by rule for 2 different signals (to apply different rules for example on each split)</li> <li>aggregation_period : create periods signal of categorical signals</li> <li>basic_range_cleaner : range cleaner</li> <li>aggregate : get signals in or out of a given period signal</li> <li>create_registry : create a virtual signal of a registry to some common medical situations (Diabetes, Hypertension)</li> <li>limit_history : limit or eliminate signals. Needed mainly as a pre processor \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/index.html#rep-processors-usage-in-a-medmodel","title":"Rep Processors usage in a MedModel","text":"<p>Rep processors are the first elements running in a MedModel run. They are packed into MultiProcessors, each containing 1 or more rep processors. Each such multi processor contains processors allowed to run in parallel to each other, and indeed the MedModel will parallelize those runs. This is true for both learn and apply stages (mainly for learn, as in apply we anyway parallize on the pids records). Hence once defined it is important to pack the processors in the way that allows maximal parallelism. Example of packing processors when defining them in a json file: <pre><code>// new format example\n\u00a0\n\"model_json_version\": \"2\",\n\"serialize_learning_set\": \"0\",\n\u00a0\n// pack all actions (rp, fg, fp) into model_actions\n  \"model_actions\": [\n// add a group of first rep processors (will run first and in parallel)\n    { \"action_type\": \"rp_set\",\n      \"members\": [\n        { \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"ALT\", \"Na\" ] }, //Not used in LGI\n        { \"rp_type\": \"basic_cln\", \"type\": \"iterative\", \"range\": \"range_min=0.0001;range_max=10000\", \"trimming_sd_num\": \"7\",\"removing_sd_num\": \"14\", \"signal\": \"Hemoglobin\"} //USED in LGI\n      ]\n    },\n// add another group (will run after first group and in parallel)\n    { \"action_type\": \"rp_set\",\n      \"members\": [\n        { \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"RBC\", \"WBC\" ] },\n        { \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": \"Creatinine\"}\n      ]\n    }\n\u00a0\n// continue with your json ...\n    ]\n\u00a0\n// old format example\n  \"processes\": {\n// rep processors in group 0 (running first and in parallel)\n    \"process\" : { \"process_set\": \"0\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"ALT\", \"Na\" ] },\n    \"process\" : { \"process_set\": \"0\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": \"Hemoglobin\" },\n// rep processors in group 1 (running after first and in parallel)\n    \"process\" : { \"process_set\": \"1\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"RBC\", \"WBC\" ] },\n    \"process\" : { \"process_set\": \"1\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": \"Creatinine\" }\n\u00a0\n// continue with your json ....\n}\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/index.html#running-with-a-pre-processor","title":"Running with a pre processor","text":"<p>The pre processor mechainsm allows one to define new rep processors to be added to a ready model, in order to be run before all the pre processors of the model. This allows for example limiting the history of a model, or deleting signals in an elegant way when searching for signal importance and minimal requirements.\u00a0 A pre processor hence, will typically be one that has an empty learn() and relies only on user given parameters, and is used only at apply time. The most convenient way to use rep_processors is using a json defining them and the\u00a0add_pre_processors_json_string_to_model API in MedModel. Example json: <pre><code># example : limiting some signals to tests done at a window of 1 year before prediction time\n{\n        \"pre_processors\" : [ {\"rp_type\" : \"history_limit\" , \"signal\" : \"ref:signals\", \"win_from\" : \"0\" , \"win_to\" : \"365\"} ] ,\n        \"signals\" : [\"Hemoglobin\", \"MCV\", \"MCH\"]\n}\n</code></pre> \u00a0 The Flow App has an option to get predictions of a model with an added pre_processors file. Use the following: <pre><code>Flow --rep data.repository --get_model_preds --f_model my.model --f_samples test.samples --f_pre_json pre.json --f_preds output.preds\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/index.html#create-registy-processor","title":"Create Registy Processor","text":"<p>Creates a registry for the chosen medical condition. Currently implemented are hypertension and diabetes.</p> <ul> <li>name : \"create_registry\" Rough registry definition:</li> </ul> <ol> <li>Diabetes:<ol> <li>Diabetes : one of:<ol> <li>2 tests within 2y of Glucose above 125, or HbA1C above 6.5 (the second one)\u00a0\u00a0</li> <li>1 test of Glucose above 200, or HbA1C above 8.0</li> <li>A diagnostic code</li> <li>Starting point of using diabetes drugs.</li> </ol> </li> </ol> </li> <li> <p>PreDiabetes:  </p> <ol> <li>Non Diabetic</li> <li>1 Glucose test above 110, or 1 HbA1C above 5.7</li> <li>2 Glucose tests within 2y above 100 (the second), but still not diabetic.</li> <li>Not taking diabetes drugs</li> </ol> </li> <li> <p>Healthy   </p> <ol> <li>Non Diabetic</li> <li>Non PreDiabetic</li> <li>Normal range Glucose/HbA1C tests period.</li> </ol> </li> <li>HyperTension:\u00a0</li> </ol> <p>Example: Example<pre><code>    {\n      \"rp_type\":\"create_registry\",\n      \"registry\":\"ht\",\n      \"names\":\"my_HT_Registry\",\n      \"ht_systolic_first\": \"1\",\n      \"ht_drugs\":\"list_rel:registries/ht_drugs.full\",\n      \"ht_identifiers\":\"list_rel:registries/ICD10/hyper_tension.2020.desc\",\n      \"chf_identifiers\":\"list_rel:registries/ICD10/heart_failure_events.2020.desc\",\n      \"mi_identifiers\":\"list_rel:registries/ICD10/mi.2020.desc\",\n      \"af_identifiers\":\"list_rel:registries/ICD10/AtrialFibrilatioReadCodes.2020.desc\",\n      \"ht_chf_drugs\":\"ATC_C03_____\",\n      \"ht_dm_drugs\":\"ATC_C09A____,ATC_C09C____\",\n      \"ht_extra_drugs\":\"ATC_C07A_A__,ATC_C07A_B__\",\n      \"signals\":\"BP,DIAGNOSIS,Drug,BDATE,_v_DM_Registry\"\n    },\n</code></pre></p> <ol> <li>The code (RepCreateRegistry) uses the following definitions -\u00a0<ol> <li>High blood pressure -\u00a0Diastolic BP over 90 or Systolic BP over 140 (for people younger than 60) or 150 (for people 60 or older)</li> <li>Drugs - Relevant drugs are divided into 4 groups. The first is always indicative of HT. The second is indicative of HT unless there are other indication of CHF. The third indicative of HT unless there are other indicaiton of Diabetes. The last is indicative of HT uless other indications of CHF, Diabetes or MI </li> </ol> </li> <li>An individual is considered non hyper-tensive, as long as there are no tests showing high blood pressure, no Read (ICD9/10) codes and no drugs indicative of hypertension</li> <li>Conditions for HT positive:   <ol> <li>The first appearance of HT diagnosis (note - until 3/3/25 we waited for 2nd indication)</li> <li>Two consequtives high BP tests, without normal BP test between them</li> <li>Drug indication after high BT test</li> <li>Two drug indications, less than\u00a0'ht_drugs_gap' days apart</li> <li>With just one bad test / drug indication - the status is 'gray' however considered as not HT</li> </ol> </li> <li>After first HT positive =&gt; always HT positive</li> <li>Proteinuria: <ol> <li>Goes over a list of urine tests</li> <li>For the categorial ones: each has the categories that match states of normal, medium, or severe proteinuria.   </li> <li>For the value based tests : ranges are given for the normal, medium and proteinuria stages.</li> <li>The code goes over all the given tests, and in each day there's a test records if it was normal (0) , medium (1) , or severe (2) .</li> <li>If there are several tests in the same day, the worst of them is considered as the result to use.</li> </ol> </li> <li>The output is a DateVal signal with times and values of 0,1,2 matching normal, medium, severe states (note this is DIFFERENT from the 0-4 proteinuria states we used in the past and which are calculated in the Diabetes registries calculator.</li> <li>CKD: <ol> <li>Goes over eGFR and Proteinuria states and calculates at each time point the CKD state (0-4) based on the last known values of eGFR and Proteinuria.</li> <li>In many cases it would be recommended to have a double layered calculation in which:<ol> <li>Layer 1 creates the virtual signals needed for eGFR and Proteinuria</li> <li>Layer 2 calculates the CKD levels based on Layer1 results.</li> </ol> </li> </ol> </li> </ol> <p>parameters: general:</p> <ul> <li>registry : \"dm\" for siabetes, \"ht\" for hypertension</li> <li>names : the name/names of the virtual signals created by the processor, these will hold the actual registry signal.</li> <li>signals : the signals the rep depends on, in case of working with slightly different signal names than the defaults<ul> <li>defaults for dm :\u00a0\"Glucose\",\"HbA1C\",\"Drug\",\"RC\"</li> <li>defaults for ht :\u00a0\"BP\",\"RC\",\"Drug\",\"BYEAR\",\"DM_Registry\"</li> <li>defaults for proteinuria: all relevant rine tests :\u00a0\"Urine_Microalbumin\", \"UrineTotalProtein\" , \"UrineAlbumin\" , \"Urine_dipstick_for_protein\" , \"Urinalysis_Protein\" , \"Urine_Protein_Creatinine\" , \"UrineAlbumin_over_Creatinine\"</li> <li>defaults for ckd : in ckd it is always reccomended to create the proteinuria signal and then use it, see the examples below.</li> </ul> </li> <li>time_unit : of repository (can rely on default though)</li> <li>registry_values : the names of the registry values created in a dictionary (first will be 0, second 1, and on....) diabetes related:</li> <li>dm_drug_sig ,\u00a0dm_diagnoses_sig ,\u00a0dm_glucose_sig ,\u00a0dm_hba1c_sig : the names for the matching signals, defaults are respectively Drug , RC , Glucose , HbA1C</li> <li>dm_drug_sets : the drug sets to be used as defining a diabetic patient.</li> <li>dm_diagnoses_sets : the set of codes for diabetes</li> <li>dm_diagnoses_severity : 3 or 4 : 3 means a diagnoses needs more supporting evidence (such as bad glucose tests) to decided diabetic, 4 means the code is enough on its own. hypertension related:</li> <li>ht_identifiers: Read (ICD9/10) codes for hypertension</li> <li>chf_idientifiers, mi_identifiers, af_identifiers: Read (ICD9/10) codes for CHF, MI and AF retrospectively</li> <li>ht_drugs: Drugs sets for hyper-tenstion</li> <li>ht_chf_drugs, ht_dm_drugs, ht_extra_drugs: Drugs sets for hyper-tension unless there is other indication of CHF, Diabetes and CHF/Diabetes/MI, retrospectively</li> <li>ht_drugs_gap: See above for details proteinuria related:</li> <li>urine_tests_categories : a listing of the urine tests to use, each with a bit static if it is numeric or not, and then categories or ranges for the signal for the normal, medium and severe states.\u00a0</li> <li>Example:\u00a0 \"Urine_Microalbumin:1:0,30:30,300:300,1000000/UrineTotalProtein:1:0,0.15:0.15,0.60:0.60,1000000/UrineAlbumin:1:0,30:30,300:300,1000000/Urine_dipstick_for_protein:0:Urine_dipstick_for_protein_normal:Urine_dipstick_for_protein_medium:Urine_dipstick_for_protein_severe\"</li> <li>Note the usage of the '/' separator between signals, the use of ':' between the five fields for each signal, and the use of ',' within the ranges or categories fields. ckd related:</li> <li>ckd_egfr_sig : the name of the signal holding the eGFR</li> <li>ckd_proteinuria : the name of the signal holding the proteinuria signal (the 3 levels one\u00a0!!!) \u00a0 Json Examples:</li> </ul> <p><pre><code>//\n// creating a diabetes registry\n//\n{\"rp_type\": \"create_registry\", \"registry\" : \"dm\", \"names\" : \"_v_DM_Registry\",\n                          \"dm_drug_sets\" : \"list:/nas1/UsersData/avi/MR/Tools/Registries/Lists/diabetes_drug_codes.full\",\n                     \"dm_diagnoses_sets\" : \"list:/nas1/UsersData/avi/MR/Tools/Registries/Lists/diabetes_read_codes_registry.full.striped\"}\n\u00a0\n//\n// creating a hyper tension registry\n// It is important to clean and handle simultanous values beforehand\n// note that default lists of codes are in $MR_ROOT/Tools/Registries/Lists/\n//\n{\"action_type\":\"rep_processor\",\"rp_type\":\"basic_outlier_cleaner\",\"range_min\":\"0.001\",\"range_max\":\"100000\",\"val_channel\":\"0\",\"signal\":\"BP\"},\n{\"action_type\":\"rep_processor\",\"rp_type\":\"basic_outlier_cleaner\",\"range_min\":\"0.001\",\"range_max\":\"100000\",\"val_channel\":\"1\",\"signal\":\"BP\"},\n{\"action_type\":\"rep_processor\",\"rp_type\":\"sim_val\",\"type\":\"min\",\"signal\":\"BP\"},\n{\"action_type\":\"rep_processor\",\"rp_type\":\"create_registry\",\"registry\":\"ht\",\"names\":\"my_HT_Registry\"},\n//\n// creating a proteinuria registry\n//\n{\"action_type\":\"rep_processor\",\"rp_type\":\"create_registry\",\"registry\":\"proteinuria\",\"names\":\"_v_Proteinuria_State\"},\n\n//\n// creating a CKD registry, note we use the previously defined proteinuria registry\n//\n{\"action_type\":\"rep_processor\",\"rp_type\":\"create_registry\",\"registry\":\"ckd\", \"names\" : \"_v_CKD_State\" , \n                \"signals\" : \"_v_Proteinuria_State,eGFR_CKD_EPI\" ,\n                \"ckd_egfr_sig\" : \"eGFR_CKD_EPI\" , \n                \"ckd_proteinuria_sig\" : \"_v_Proteinuria_State\"},\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Cleaners%20Json%20Examples.html","title":"Cleaners Json Examples","text":"<p>Examples for creating Jsons with default cleaner. An Example of cleaner can be found in git:\u00a0full_rep_processors.json \u00a0 full_cleaners is based on:</p> <ul> <li>configured simple cleaner with strict\u00a0boundaries\u00a0for each signal\u00a0</li> <li>sim_val - when signal appears with the same time and different value which value to take? (from my observations in THIN - taking the first value is better)</li> <li>panels calculation and remove mismatches of biological rules. For example wrong calculation of BMI Notes:</li> <li>There is a problem with\u00a0Cholesterol_over_HDL\u00a0signal in the loading process,\u00a0\u00a0so using full_cleaner is not recommended for now with the rule of the\u00a0Cholesterol_over_HDL activated (rules 15,17)</li> <li>You can now use Flow with \"pids_sigs_print\" mode or Yaron print program to print pids and signals after rep_processors. It may be\u00a0useful\u00a0for cleaners, virtual signals and more. I had created a\u00a0different\u00a0print (not in Flow) that shows the difference between 2 run modes of rep_processings (or compare run with rep processing to no rep processing at all) and prints the removed rows with \"[REMOVED]\" in each removed row to see what happened. for more information contact me. \u00a0 <pre><code>{\n  \"model_json_version\": \"2\",\n  \"serialize_learning_set\": \"0\",\n  \"model_actions\": [\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"time_channel\":\"0\",\n          \"clean_method\":\"confirmed\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\"\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        },\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"val_channel\":[\"0\", \"1\"],\n          \"clean_method\":\"confirmed\",\n          \"signal\": [\"BP\"]\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"sim_val\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\",\n          \"type\":\"first\",\n          \"debug\":\"0\"\n\n        }\n\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"rule_cln\",\n          \"addRequiredSignals\":\"1\",\n          \"time_window\":\"0\",\n          \"tolerance\":\"0.1\",\n          \"calc_res\":\"0.1\",\n          \"rules2Signals\":\"../settings/ruls2Signals.tsv\",\n          \"consideredRules\":[ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\" ] \n          //,\"verbose_file\":\"/tmp/panel_cleaning.log\"\n        }\n      ]\n    }\n\n  ]\n}\n</code></pre> \u00a0 This was tested with Cleaner Program that checks for filtered stats with examples. here are the filtered stats on THIN: Stats of simple cleaner To create this table with full examples: <pre><code>Flow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --rep_processor_print --sigs /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/all_rules_sigs.list --max_examples 10 --seed 0 --f_output /tmp/test.log --cleaner_path /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/only_configure.json \n</code></pre> \u00a0 Simple Filtering - only Configure Rules for stricted bounderies</li> </ul> Signal TOTAL_CNT TOTAL_CNT_NON_ZERO TOTAL_CLEANED CLEAN_PERCENTAGE CLEAN_NON_ZERO_PERCENTAGE TOTAL_PIDS PIDS_FILTERED PIDS_FILTERED_NON_ZEROS PIDS_FILTER_PERCENTAGE PIDS_FILTER_NON_ZERO_PERCENTAGE comment eGFR_MDRD 30992945 30976157 30258567 2.37% 2.32% 5328855 427719 414278 8.03% 7.77% Remove filter - not needed UrineCreatinine 2603228 2602550 2556325 1.80% 1.78% 694781 17213 17210 2.48% 2.48% Maybe problem in units that can be solved - checking PlasmaViscosity 1258162 1257236 1237837 1.62% 1.54% 459295 8899 8147 1.94% 1.77% Maybe problem in units that can be solved - checking Transferrin 386039 386034 382521 0.91% 0.91% 232202 2709 2704 1.17% 1.16% OK HDL_over_nonHDL 14593886 14580124 14459042 0.92% 0.83% 3414705 77777 77335 2.28% 2.26% OK eGFR_CKD_EPI 30992945 30992374 30763090 0.74% 0.74% 5328855 171719 171715 3.22% 3.22% Remove filter - not needed Height 18860856 18780829 18714169 0.78% 0.35% 9334026 124126 57892 1.33% 0.62% Maybe problem in units that can be solved - checking CA125 252667 252550 251745 0.36% 0.32% 193979 599 598 0.31% 0.31% OK BP 90295429 89497237 89264282 1.14% 0.26% 9410580 525587 178924 5.59% 1.90% bugfix to work on each channel MCHC-M 25816227 25786164 25719334 0.38% 0.26% 5459883 56481 38450 1.03% 0.70% OK BMI 35211327 35194623 35118884 0.26% 0.22% 8293631 73201 59596 0.88% 0.72% OK Phosphore 4571174 4570766 4561223 0.22% 0.21% 1866755 3057 2683 0.16% 0.14% Lymphocytes% 24656284 24652334 24610137 0.19% 0.17% 5312408 26240 26119 0.49% 0.49% Neutrophils% 24740548 24736537 24695583 0.18% 0.17% 5321777 25181 25046 0.47% 0.47% INR 8505951 8496622 8488159 0.21% 0.10% 455102 5643 4012 1.24% 0.88% PDW 384442 383910 383555 0.23% 0.09% 109155 772 772 0.71% 0.71% PlasmaAnionGap 21530 21530 21513 0.08% 0.08% 5491 15 15 0.27% 0.27% WBC 26610249 26593657 26572901 0.14% 0.08% 5554159 25789 12033 0.46% 0.22% FreeT3 506119 505466 505091 0.20% 0.07% 205428 877 314 0.43% 0.15% RBC 25905761 25876600 25857697 0.19% 0.07% 5471987 28169 27732 0.51% 0.51% Ca 7173055 7172113 7167232 0.08% 0.07% 2582464 4826 3942 0.19% 0.15% Mg 182248 182218 182094 0.08% 0.07% 107908 134 108 0.12% 0.10% T4 475830 473666 473368 0.52% 0.06% 213799 2235 277 1.05% 0.13% Hematocrit 25862482 25836512 25822132 0.16% 0.06% 5463443 27793 10406 0.51% 0.19% TEMP 1801163 1786361 1785441 0.87% 0.05% 964098 12126 917 1.26% 0.10% Digoxin 93861 93617 93584 0.30% 0.04% 43469 215 212 0.49% 0.49% SerumAnionGap 59666 59661 59640 0.04% 0.04% 22779 24 24 0.11% 0.11% K+ 28662323 28620055 28610424 0.18% 0.03% 5174781 35073 6931 0.68% 0.13% MPV 3443266 3442461 3441344 0.06% 0.03% 980181 1361 978 0.14% 0.10% Bicarbonate 3175248 3174729 3173852 0.04% 0.03% 741895 1332 844 0.18% 0.11% Cholesterol 18909856 18888280 18883203 0.14% 0.03% 3882355 22236 4367 0.57% 0.11% Albumin 23700344 23664696 23658553 0.18% 0.03% 4850137 25065 4754 0.52% 0.10% Na 29000033 28945701 28940315 0.21% 0.02% 5195665 26686 5191 0.51% 0.10% Iron_Fe 613710 613514 613409 0.05% 0.02% 363871 284 102 0.08% 0.03% Weight 39402271 39199410 39192705 0.53% 0.02% 9808932 167114 6298 1.70% 0.06% NonHDLCholesterol 14588087 14587707 14585866 0.02% 0.01% 3413836 1912 1909 0.06% 0.06% RandomGlucose 710956 701691 701603 1.32% 0.01% 423583 6726 87 1.59% 0.02% Platelets_Hematocrit 3400998 3399872 3399528 0.04% 0.01% 972680 1193 1191 0.12% 0.12% Hemoglobin 27748929 27701627 27698933 0.18% 0.01% 5689777 24331 2523 0.43% 0.04% LDL 12668050 12639848 12638813 0.23% 0.01% 3124704 20626 965 0.66% 0.03% MCV 26246642 26230251 26228319 0.07% 0.01% 5510085 12313 1833 0.22% 0.03% CO2 262289 261884 261866 0.16% 0.01% 44949 216 18 0.48% 0.04% Glucose 16484078 16472703 16471686 0.08% 0.01% 4466133 9937 928 0.22% 0.02% Amylase 355012 354296 354275 0.21% 0.01% 275843 685 20 0.25% 0.01% CorrectedCa 6448360 6447552 6447174 0.02% 0.01% 2381369 1087 357 0.05% 0.01% Triglycerides 14035783 14004268 14003515 0.23% 0.01% 3295899 19785 707 0.60% 0.02% LDH 223522 218968 218958 2.04% 0.00% 102148 3316 10 3.25% 0.01% ALKP 24501543 24491562 24490462 0.05% 0.00% 4975550 8773 844 0.18% 0.02% PULSE 5607620 5597890 5597659 0.18% 0.00% 2221767 7933 224 0.36% 0.01% Cl 6074637 6068936 6068789 0.10% 0.00% 1205272 4050 146 0.34% 0.01% Protein_Total 15053134 15051906 15051694 0.01% 0.00% 3350737 1336 188 0.04% 0.01% FreeT4 8375885 8347195 8347096 0.34% 0.00% 2572146 19968 99 0.78% 0.00% AST 5017954 5016921 5016887 0.02% 0.00% 1391446 972 33 0.07% 0.00% Platelets 26572350 26551880 26551731 0.08% 0.00% 5546115 15154 133 0.27% 0.00% ALT 20504083 20485612 20485528 0.09% 0.00% 4431602 12872 82 0.29% 0.00% MCH 25858658 25846546 25846464 0.05% 0.00% 5463374 7628 82 0.14% 0.00% B12 3015306 3013860 3013860 0.05% 0% 1607426 1212 0 0.08% 0% RDW 4969565 4969446 4969563 4.02E-07 0.00% 1579652 2 2 0.00% 0.00% Urea 22375296 22373145 22373971 0.01% 0.00% 4367623 1291 1289 0.03% 0.03% Monocytes% 24411657 24387478 24388588 0.09% 0.00% 5276761 13670 13512 0.26% 0.26% VitaminD_25 352787 352738 352775 0.00% -0.01% 228066 12 12 0.01% 0.01% Fibrinogen 196551 196504 196550 0.00% -0.02% 143822 1 1 0.00% 0.00% HDL_over_LDL 12596626 12590998 12594958 0.01% -0.03% 3118377 1509 1507 0.05% 0.05% Urine_Dipstick_pH 136097 136047 136091 0.00% -0.03% 64581 6 6 0.01% 0.01% Transferrin_Saturation_Index 252126 251899 251984 0.06% -0.03% 162901 134 134 0.08% 0.08% Ferritin 3729644 3728208 3729625 0.00% -0.04% 1809350 18 18 0.00% 0.00% Bilirubin 23598709 23588476 23598656 0.00% -0.04% 4904303 50 50 0.00% 0.00% Sex_Hormone_Binding_Globulin 133505 133441 133503 0.00% -0.05% 110127 2 2 0.00% 0.00% Lymphocytes# 24916931 24903489 24916729 0.00% -0.05% 5337938 159 159 0.00% 0.00% HbA1C 7597400 7592635 7597179 0.00% -0.06% 1520278 163 160 0.01% 0.01% TIBC 151135 151012 151114 0.01% -0.07% 97981 21 21 0.02% 0.02% Prolactin 407790 407328 407663 0.03% -0.08% 297447 91 91 0.03% 0.03% Cholesterol_over_HDL 15035230 15018683 15031577 0.02% -0.09% 3421173 3150 2923 0.09% 0.09% Follic_Acid 2703458 2701120 2703458 0% -0.09% 1472362 0 0 0% 0% HDL_over_Cholesterol 14670753 14656530 14669390 0.01% -0.09% 3420769 1342 1331 0.04% 0.04% Neutrophils# 24970648 24946553 24970234 0.00% -0.09% 5346509 383 378 0.01% 0.01% FSH 1045438 1044297 1045438 0% -0.11% 688285 0 0 0% 0% Testosterone 388036 387595 388027 0.00% -0.11% 292007 9 9 0.00% 0.00% LDL_over_HDL 12606025 12591423 12605655 0.00% -0.11% 3118883 357 349 0.01% 0.01% Globulin 8009111 7999593 8008795 0.00% -0.12% 1896822 254 253 0.01% 0.01% Creatinine 31070691 31033058 31069711 0.00% -0.12% 5331326 753 732 0.01% 0.01% eGFR 16442849 16422354 16442849 0% -0.12% 3426403 0 0 0% 0% Monocytes# 24612290 24579520 24612015 0.00% -0.13% 5298350 234 219 0.00% 0.00% CK 942771 938846 940828 0.21% -0.21% 481006 1367 1364 0.28% 0.28% Uric_Acid 1151516 1147621 1150405 0.10% -0.24% 655507 1073 1068 0.16% 0.16% Erythrocyte 7239836 7222198 7239830 8.29E-07 -0.24% 2647537 6 5 0.00% 0.00% Lithium 274155 273378 274113 0.02% -0.27% 21344 26 26 0.12% 0.12% GGT 9354116 9324434 9354066 0.00% -0.32% 2369564 49 49 0.00% 0.00% UrineAlbumin 1366696 1361871 1366696 0% -0.35% 391018 0 0 0% 0% GFR 3225746 3213478 3225746 0% -0.38% 882940 0 0 0% 0% LUC 1330513 1324624 1330144 0.03% -0.42% 324604 250 215 0.08% 0.07% HDL 14818204 14749529 14813285 0.03% -0.43% 3431910 3956 2632 0.12% 0.08% UrineTotalProtein 386928 384724 386787 0.04% -0.54% 179141 115 115 0.06% 0.06% Reticulocyte 69816 69418 69814 0.00% -0.57% 49484 2 2 0.00% 0.00% UrineAlbumin_over_Creatinine 2451211 2431361 2451204 0.00% -0.82% 678233 7 7 0.00% 0.00% PSA 1926651 1906549 1926420 0.01% -1.04% 660296 159 157 0.02% 0.02% Serum_Oestradiol 394083 389241 394083 0% -1.24% 274758 0 0 0% 0% PFR 5264955 5164019 5264358 0.01% -1.94% 1355995 569 535 0.04% 0.04% TSH 16173524 15848163 16172909 0.00% -2.05% 4501457 528 450 0.01% 0.01% LuteinisingHormone 847445 830210 847445 0% -2.08% 585366 0 0 0% 0% Rheumatoid_Factor 461205 450087 460648 0.12% -2.35% 364740 494 492 0.14% 0.13% Progesterone 233686 227831 233686 0% -2.57% 156019 0 0 0% 0% CRP 5980408 5820488 5980374 0.00% -2.75% 2320376 30 30 0.00% 0.00% Urine_Protein_Creatinine 324747 314833 324741 0.00% -3.15% 151145 6 6 0.00% 0.00% Urine_Epithelial_Cell 299537 287827 299537 0% -4.07% 145624 0 0 0% 0% Eosinophils% 24273861 22801564 24270972 0.01% -6.44% 5260185 2185 1951 0.04% 0.04% Eosinophils# 24445262 22942339 24445176 0.00% -6.55% 5281030 70 59 0.00% 0.00% Urine_Microalbumin 1386074 1269197 1386072 0.00% -9.21% 375904 2 0 0.00% 0% Basophils# 23695244 14449541 23692629 0.01% -63.97% 5194395 2007 1665 0.04% 0.03% Basophils% 23660542 9994176 23648885 0.05% -136.63% 5193897 9473 3998 0.18% 0.08% NRBC 2113870 3939 2113869 4.73E-07 -53565.10% 733525 1 0 0.00% 0% <p>Examples of filtered row log from program: EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20090210\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 12225\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100204\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 14970\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110304\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 15219\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120319\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 15607\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20130221\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 14.8 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20131115\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 13 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20141124\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20160122\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11134171\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20091215\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6694\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11134171\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20111115\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20080424\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3686\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20090918\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4630\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100611\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 13229\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110106\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 13349\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110404\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3275\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0 \u00a0\u00a06835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20111202\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 5063\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120315\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3616\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120628\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 5262\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20130306\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 2.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20140221\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4.3 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20150527\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20160822\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3.6 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 8685466 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20091113\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 11823\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20081124\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 5452\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100125\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6526\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100624\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4616\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110715\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3074\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120615\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3674\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20130611\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4.2 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20140607\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20140722\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3 STATS\u00a0\u00a0 UrineCreatinine TOTAL_CNT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2603228 TOTAL_CNT_NON_ZERO\u00a0\u00a0\u00a0\u00a0\u00a0 2602550 TOTAL_CLEANED\u00a0\u00a0 2556325 CLEAN_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.80172%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_NON_ZERO_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.77614%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_PIDS\u00a0\u00a0\u00a0 694781\u00a0\u00a0 PIDS_FILTERED\u00a0\u00a0 17213\u00a0\u00a0 PIDS_FILTERED_NON_ZEROS 17210\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 2.47747%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 2.47704%\u00a0 1. Height- looks like there is factor 100 sometimes EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20041111\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20050302\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060522\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060607\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060607\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20071109\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20090617\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19980408\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 17\u00a0\u00a0\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20040209\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20061212\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20091203\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20111122\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20121113\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11310017\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19930902\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 150 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11310017\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19990422\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 12\u00a0\u00a0\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20031118\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20031118\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20040809\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 158 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20041120\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060901\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 158 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20071010\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 159 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 10759614\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19930602\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 168 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 10759614\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19980225\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [REMOVED] STATS\u00a0\u00a0 Height\u00a0 TOTAL_CNT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 18860856\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_CNT_NON_ZERO\u00a0\u00a0\u00a0\u00a0\u00a0 18780829\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_CLEANED\u00a0\u00a0 18714169\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.777732%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_NON_ZERO_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.354936%\u00a0\u00a0\u00a0\u00a0 TOTAL_PIDS\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a09334026 PIDS_FILTERED\u00a0\u00a0 124126\u00a0 PIDS_FILTERED_NON_ZEROS 57892\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 1.32982%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 0.620225% 1. MCHC-M \u2013 looks like factor 10 problem: EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20010622\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 357\u00a0\u00a0\u00a0\u00a0 [*] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20060711\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090505\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 19950502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 330\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 19950605\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 326\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20021114\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20050328\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20070111\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20070201\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20071030\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20071112\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20080422\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.7 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20081030\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.8 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090611\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090715\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20091230\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.6 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100106\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 30 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100126\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100204\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.6 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100225\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.2 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100301\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.7 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100308\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100315\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 29.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 15517308\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20040511\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 313\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 15517308\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20071022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.7 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 19991116\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 326\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20050808 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value\u00a0\u00a0 34.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20051026\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20051222\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.8 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20040728\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value\u00a0\u00a0 33.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090522\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 396\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090623\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 35 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090918\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 35.3 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20091123\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100319\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 370\u00a0\u00a0\u00a0\u00a0 [***] STATS\u00a0\u00a0 MCHC-M\u00a0 TOTAL_CNT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 25816227\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0TOTAL_CNT_NON_ZERO\u00a0\u00a0\u00a0\u00a0\u00a0 25786164\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_CLEANED\u00a0\u00a0 25719334\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.375318%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_NON_ZERO_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.25917%\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_PIDS\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5459883 PIDS_FILTERED\u00a0\u00a0 56481\u00a0\u00a0 PIDS_FILTERED_NON_ZEROS 38450\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 1.03447%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 0.704228% \u00a0 Stats of Full (with panels check) <pre><code>Flow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --rep_processor_print --sigs \"Cholesterol_over_HDL,HDL_over_Cholesterol,UrineAlbumin,HDL,UrineAlbumin_over_Creatinine,UrineCreatinine,LDL,Cholesterol,LDL_over_HDL,HDL_over_LDL,Platelets_Hematocrit,MPV,NonHDLCholesterol,HDL_over_nonHDL,MCV,RBC,Hematocrit,Platelets,MCH,MCHC-M,Protein_Total,Hemoglobin,Height,Albumin,Basophils#,Eosinophils#,Monocytes#,Lymphocytes#,Neutrophils#,NRBC,WBC,BMI,Weight,UrineTotalProtein,ALKP,ALT,Amylase,AST,B12,Basophils%,Bicarbonate,Bilirubin,Ca,CA125,CK,Cl,CO2,CorrectedCa,Creatinine,CRP,Digoxin,eGFR,Eosinophils%,Erythrocyte,Ferritin,Fibrinogen,Follic_Acid,FreeT3,FreeT4,FSH,GFR,GGT,Globulin,Glucose,HbA1C,INR,Iron_Fe,K+,LDH,Lithium,LUC,LuteinisingHormone,Lymphocytes%,Mg,Monocytes%,Na,Neutrophils%,PDW,PFR,Phosphore,PlasmaAnionGap,PlasmaViscosity,Progesterone,Prolactin,PSA,PULSE,RandomGlucose,RDW,Reticulocyte,Rheumatoid_Factor,Serum_Oestradiol,SerumAnionGap,Sex_Hormone_Binding_Globulin,T4,Testosterone,TIBC,Transferrin,Transferrin_Saturation_Index,Triglycerides,TSH,Urea,Uric_Acid,Urine_Dipstick_pH,Urine_Epithelial_Cell,Urine_Microalbumin,Urine_Protein_Creatinine,VitaminD_25,TEMP\" --max_examples 10 --seed 0 --f_output /tmp/test.log --cleaner_path_before /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/configure_sim_val.json --cleaner_path /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/full_cleaners.json \n</code></pre></p> Signal TOTAL_CNT TOTAL_CNT_NON_ZERO TOTAL_CLEANED CLEAN_PERCENTAGE TOTAL_PIDS PIDS_FILTERED PIDS_FILTERED_NON_ZEROS PIDS_FILTER_PERCENTAGE PIDS_FILTER_NON_ZERO_PERCENTAGE Comment UrineAlbumin 1364175 1359350 1301695 4.58% 391018 33583 33206 8.59% 8.49% \u00a0Urine_panel, lot of real errors UrineAlbumin_over_Creatinine 2444786 2424936 2382427 2.55% 678233 33467 30981 4.93% 4.57% \u00a0Urine_panel, lot of real errors UrineCreatinine 2550757 2550079 2488398 2.44% 694781 33467 33438 4.82% 4.81% \u00a0Urine_panel, lot of real errors Cholesterol_over_HDL 14653976 14653976 14375111 1.90% 3421173 115517 115517 3.38% 3.38% there is problem in load of wrong source HDL_over_LDL 12594284 12588656 12368827 1.79% 3118377 129667 128275 4.16% 4.11% Platelets_Hematocrit 3398224 3398224 3337846 1.78% 972680 19763 19763 2.03% 2.03% MPV 3438309 3438309 3377931 1.76% 980181 19763 19763 2.02% 2.02% HDL 14751464 14682789 14546126 1.39% 3431910 114884 98162 3.35% 2.86% HDL_over_Cholesterol 14665993 14651770 14467583 1.35% 3420769 86917 86067 2.54% 2.52% there is problem in load of wrong source Cholesterol 18855175 18855175 18649837 1.09% 3882355 114884 114884 2.96% 2.96% NonHDLCholesterol 14585464 14585084 14428102 1.08% 3413836 83548 83450 2.45% 2.44% LDL_over_HDL 12605642 12591040 12550215 0.44% 3118883 27352 26925 0.88% 0.86% LDL 12633401 12633401 12577976 0.44% 3124704 37293 37293 1.19% 1.19% MCV 26166367 26166367 26084584 0.31% 5510085 44007 44007 0.80% 0.80% HDL_over_nonHDL 14459035 14445273 14414026 0.31% 3414705 19387 18580 0.57% 0.54% RBC 25832962 25803801 25758846 0.29% 5471987 38602 34538 0.71% 0.63% Hematocrit 25784644 25784644 25725376 0.23% 5463443 29769 29769 0.54% 0.54% Platelets 26511344 26511344 26450966 0.23% 5546115 19763 19763 0.36% 0.36% MCH 25791202 25791202 25735104 0.22% 5463374 36564 36564 0.67% 0.67% MCHC-M 25697538 25697538 25655308 0.16% 5459883 25802 25802 0.47% 0.47% Protein_Total 15030698 15030698 15014686 0.11% 3350737 10857 10857 0.32% 0.32% Hemoglobin 27672196 27672196 27648170 0.09% 5689777 16426 16426 0.29% 0.29% Albumin 23630785 23630785 23614773 0.07% 4850137 10857 10857 0.22% 0.22% Basophils# 23647655 14401952 23636707 0.05% 5194395 9378 3885 0.18% 0.07% Eosinophils# 24411167 22908244 24400219 0.04% 5281030 9378 7428 0.18% 0.14% Monocytes# 24561221 24528451 24550273 0.04% 5298350 9378 9178 0.18% 0.17% Lymphocytes# 24824132 24810690 24813184 0.04% 5337938 9378 9326 0.18% 0.17% Neutrophils# 24907730 24883635 24896782 0.04% 5346509 9378 9272 0.18% 0.17% NRBC 2113731 3800 2112819 0.04% 733525 646 503 0.09% 0.07% WBC 26543214 26543214 26532266 0.04% 5554159 9378 9378 0.17% 0.17% BMI 35030317 35030317 35017055 0.04% 8293631 12869 12869 0.16% 0.16% UrineTotalProtein 385952 383748 385824 0.03% 179141 125 91 0.07% 0.05% Height 18664187 18664187 18664187 0% 9334026 0 0 0% 0% Weight 39068717 39068717 39068717 0% 9808932 0 0 0% 0% ALKP 24458720 24458720 24458720 0% 4975550 0 0 0% 0% ALT 20471720 20471720 20471720 0% 4431602 0 0 0% 0% Amylase 353637 353637 353637 0% 275843 0 0 0% 0% AST 5013392 5013392 5013392 0% 1391446 0 0 0% 0% B12 3011237 3011237 3011237 0% 1607426 0 0 0% 0% Basophils% 23647760 9981394 23647760 0% 5193897 0 0 0% 0% Bicarbonate 3171108 3171108 3171108 0% 741895 0 0 0% 0% Bilirubin 23542527 23532294 23542527 0% 4904303 0 0 0% 0% Ca 7095160 7095160 7095160 0% 2582464 0 0 0% 0% CA125 252115 251998 252115 0% 193979 0 0 0% 0% CK 939752 935827 939752 0% 481006 0 0 0% 0% Cl 6064395 6064395 6064395 0% 1205272 0 0 0% 0% CO2 261631 261631 261631 0% 44949 0 0 0% 0% CorrectedCa 6440715 6440715 6440715 0% 2381369 0 0 0% 0% Creatinine 31019168 30981535 31019168 0% 5331326 0 0 0% 0% CRP 5976368 5816448 5976368 0% 2320376 0 0 0% 0% Digoxin 92754 92510 92754 0% 43469 0 0 0% 0% eGFR 16433805 16413310 16433805 0% 3426403 0 0 0% 0% Eosinophils% 24269187 22796890 24269187 0% 5260185 0 0 0% 0% Erythrocyte 7233824 7216186 7233824 0% 2647537 0 0 0% 0% Ferritin 3722680 3721244 3722680 0% 1809350 0 0 0% 0% Fibrinogen 196069 196022 196069 0% 143822 0 0 0% 0% Follic_Acid 2700849 2698511 2700849 0% 1472362 0 0 0% 0% FreeT3 504917 504917 504917 0% 205428 0 0 0% 0% FreeT4 8340984 8340984 8340984 0% 2572146 0 0 0% 0% FSH 1044260 1043119 1044260 0% 688285 0 0 0% 0% GFR 3223523 3211255 3223523 0% 882940 0 0 0% 0% GGT 9337150 9307468 9337150 0% 2369564 0 0 0% 0% Globulin 7992922 7983404 7992922 0% 1896822 0 0 0% 0% Glucose 16291642 16291642 16291642 0% 4466133 0 0 0% 0% HbA1C 7510232 7505467 7510232 0% 1520278 0 0 0% 0% INR 8402552 8402552 8402552 0% 455102 0 0 0% 0% Iron_Fe 610270 610270 610270 0% 363871 0 0 0% 0% K+ 28586751 28586751 28586751 0% 5174781 0 0 0% 0% LDH 218679 218679 218679 0% 102148 0 0 0% 0% Lithium 273406 272629 273406 0% 21344 0 0 0% 0% LUC 1328863 1322974 1328863 0% 324604 0 0 0% 0% LuteinisingHormone 844893 827658 844893 0% 585366 0 0 0% 0% Lymphocytes% 24605312 24601362 24605312 0% 5312408 0 0 0% 0% Mg 181832 181832 181832 0% 107908 0 0 0% 0% Monocytes% 24385874 24361695 24385874 0% 5276761 0 0 0% 0% Na 28917933 28917933 28917933 0% 5195665 0 0 0% 0% Neutrophils% 24692458 24688447 24692458 0% 5321777 0 0 0% 0% PDW 383011 382479 383011 0% 109155 0 0 0% 0% PFR 5127271 5026335 5127271 0% 1355995 0 0 0% 0% Phosphore 4557642 4557642 4557642 0% 1866755 0 0 0% 0% PlasmaAnionGap 21500 21500 21500 0% 5491 0 0 0% 0% PlasmaViscosity 1237193 1237193 1237193 0% 459295 0 0 0% 0% Progesterone 233310 227455 233310 0% 156019 0 0 0% 0% Prolactin 406527 406065 406527 0% 297447 0 0 0% 0% PSA 1924332 1904230 1924332 0% 660296 0 0 0% 0% PULSE 5527058 5527058 5527058 0% 2221767 0 0 0% 0% RandomGlucose 700394 700394 700394 0% 423583 0 0 0% 0% RDW 4960073 4959954 4960073 0% 1579652 0 0 0% 0% Reticulocyte 69336 68938 69336 0% 49484 0 0 0% 0% Rheumatoid_Factor 460276 449158 460276 0% 364740 0 0 0% 0% Serum_Oestradiol 393299 388457 393299 0% 274758 0 0 0% 0% SerumAnionGap 59530 59525 59530 0% 22779 0 0 0% 0% Sex_Hormone_Binding_Globulin 132646 132582 132646 0% 110127 0 0 0% 0% T4 473023 473023 473023 0% 213799 0 0 0% 0% Testosterone 386501 386060 386501 0% 292007 0 0 0% 0% TIBC 149863 149740 149863 0% 97981 0 0 0% 0% Transferrin 382006 382006 382006 0% 232202 0 0 0% 0% Transferrin_Saturation_Index 251706 251479 251706 0% 162901 0 0 0% 0% Triglycerides 13994934 13994934 13994934 0% 3295899 0 0 0% 0% TSH 16164536 15839175 16164536 0% 4501457 0 0 0% 0% Urea 22350867 22348716 22350867 0% 4367623 0 0 0% 0% Uric_Acid 1149741 1145846 1149741 0% 655507 0 0 0% 0% Urine_Dipstick_pH 136044 135994 136044 0% 64581 0 0 0% 0% Urine_Epithelial_Cell 299102 287392 299102 0% 145624 0 0 0% 0% Urine_Microalbumin 1379741 1262864 1379741 0% 375904 0 0 0% 0% Urine_Protein_Creatinine 321517 311603 321517 0% 151145 0 0 0% 0% VitaminD_25 349534 349485 349534 0% 228066 0 0 0% 0% TEMP 1779120 1779120 1779120 0% 964098 0 0 0% 0%"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Full%20Rep%20Processor.html","title":"Full Rep Processor","text":"<p>This Page will describe and demonstrate a full rep processor json with all out capabilities:</p> <ul> <li>Cleaners - basic and rules</li> <li>Panel Compelition</li> <li>Common Virtual Signals: for example eGFR</li> <li>Registries \u00a0 The json can be found in git under\u00a0https://github.com/Medial-EarlySign/AM_Lung/blob/main/configs/training/full_rep_processors.json</li> </ul> <p><pre><code>{\n  \"model_json_version\": \"2\",\n  \"serialize_learning_set\": \"0\",\n  \"model_actions\": [\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"time_channel\":\"0\",\n          \"clean_method\":\"confirmed\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\"\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        },\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"val_channel\":[\"0\", \"1\"],\n          \"clean_method\":\"confirmed\",\n          \"signal\": [\"BP\"]\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"sim_val\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\",\n          \"type\":\"first\",\n          \"debug\":\"0\"\n\n        }\n\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"rule_cln\",\n          \"addRequiredSignals\":\"1\",\n          \"time_window\":\"0\",\n          \"tolerance\":\"0.1\",\n          \"calc_res\":\"0.1\",\n          \"rules2Signals\":\"../settings/ruls2Signals.tsv\",\n          \"consideredRules\":[ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\" ] \n          //,\"verbose_file\":\"/tmp/panel_cleaning.log\"\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"complete\",\n          \"sim_val\":\"remove_diff\",\n          \"config\":\"../settings/panel_completer.cfg\",\n          \"panels\":[\"red_line\", \"white_line\", \"platelets\", \"lipids\", \"egfr\", \"bmi\", \"gcs\"]\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"create_registry\", \n          \"registry\":\"dm\", \n          \"names\":\"_v_DM_Registry\",\n          \"dm_drug_sets\":\"list:../settings/registries/diabetes_drug_codes.full\",\n          \"dm_diagnoses_sets\":\"list:../settings/registries/diabetes_read_codes_registry.full.striped\"\n        },\n        {\n          \"rp_type\":\"create_registry\",\n          \"registry\":\"ht\",\n          \"names\":\"my_HT_Registry\"\n        },\n        {\n          \"rp_type\":\"create_registry\",\n          \"registry\":\"proteinuria\",\n          \"names\":\"_v_Proteinuria_State\"\n        },\n        {\n          \"action_type\":\"rep_processor\",\n          \"rp_type\":\"create_registry\",\n          \"registry\":\"ckd\",\n          \"names\":\"_v_CKD_State\",\n          \"signals\":\"_v_Proteinuria_State,eGFR_CKD_EPI\",\n          \"ckd_egfr_sig\":\"eGFR_CKD_EPI\",\n          \"ckd_proteinuria_sig\":\"_v_Proteinuria_State\"\n        }\n      ]\n    }\n\n  ]\n}\n</code></pre> to include use: $MR_ROOT/Projects/Resources/configs/include_jsons/full_rep_processors.inc.json</p> <p>Debug &amp; Print repository after applying rep_processors: <pre><code>#prints signal _v_CKD_State after applying full_rep_processors\nFlow --rep /home/Repositories/THIN/thin_2018/thin.repository  --pids_sigs_print_raw  --model_rep_processors $MR_ROOT/Projects/Resources/configs/full_model_jsons/full_rep_processors.json --sigs _v_CKD_State\n\u00a0\nRead 0 signals, 0 pids :: data  0.000GB :: idx  0.000GB :: tot  0.000GB\nRead data time 0.126360 seconds\nMedModel:: init model from json file [/nas1/UsersData/alon/MR/Projects/Resources/configs/full_model_jsons/full_rep_processors.json]:\nread 126 lines from: /nas1/UsersData/alon/MR/Projects/Resources/configs/full_model_jsons/../settings/all_rules_sigs.list\nread 126 lines from: /nas1/UsersData/alon/MR/Projects/Resources/configs/full_model_jsons/../settings/all_rules_sigs.list\nNOTE: no [predictor] node found in file\nMedRepository: read config file /home/Repositories/THIN/thin_2018/thin.repository\nMedRepository: reading signals: Urinalysis_Protein,BYEAR,eGFR_CKD_EPI,GENDER,UrineTotalProtein,UrineCreatinine,UrineAlbumin_over_Creatinine,Urine_dipstick_for_protein,Urine_Protein_Creatinine,Urine_Microalbumin,UrineAlbumin,Creatinine,\nRead 12 signals, 17030409 pids :: data  0.523GB :: idx  0.205GB :: tot  0.728GB\nRead data time 1.730095 seconds\n#Print# :: Done processed all 17030409. Took 215.3 Seconds in total\npid     signal_name     description_name        description_value       ...\n5000001 _v_CKD_State    Time_ch_0       20111201        Val_ch_0        4\n5000002 _v_CKD_State    Time_ch_0       20051206        Val_ch_0        4\n5000002 _v_CKD_State    Time_ch_0       20080508        Val_ch_0        4\n5000002 _v_CKD_State    Time_ch_0       20120516        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20080630        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20120626        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20131125        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20170531        Val_ch_0        4\n5000009 _v_CKD_State    Time_ch_0       20050908        Val_ch_0        4\n</code></pre> Debug the changes of rep_processors (for example use cleaner and see before\\after): <pre><code>#Compares repository from state after cleaner_path_before  to repository state after cleaner_path\n#cleaner_path_before - if empty will compare to repository without rep_processors\nFlow --rep /home/Repositories/THIN/thin_2018/thin.repository --rep_processor_print --sigs Hemoglobin --cleaner_path_before \"\" --cleaner_path $MR_ROOT/Projects/Resources/configs/full_model_jsons/full_rep_processors.json --max_examples 5 --f_output /tmp/repositry_after_rep_processors.log\n\u00a0\nhead /tmp/repositry_after_rep_processors.log\nEXAMPLE pid     10419961        Signal  Hemoglobin      Time    20050310        Value   14.8    [REMOVED]\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19961001        Value   12.1\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19970115        Value   12.4\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19970519        Value   12.8\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19981022        Value   12.1\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20010123        Value   13.7\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20010831        Value   14.8\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20011025        Value   14.2\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20030116        Value   8.9\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20040528        Value   12.3\n...\nSTATS   Hemoglobin      TOTAL_CNT       30787105        TOTAL_CNT_NON_ZERO      30735288        TOTAL_CLEANED   30464195        CLEAN_PERCENTAGE        1.04885%        CLEAN_NON_ZERO_PERCENTAGE       0.882025%\n       TOTAL_PIDS      6168092 PIDS_FILTERED   241868  PIDS_FILTERED_NON_ZEROS 164564  PIDS_FILTER_PERCENTAGE  3.92128%        PIDS_FILTER_PERCENTAGE  2.66799%\n</code></pre> in the above example we can see 5 examples of patients that has at least 1 change from the original signal - we can see that the hemoglobin value of\u00a0 10419961 is being filtered. The last line in output file is summary stats on the signal Hemoglobin compared to the original one. :\u00a0</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/History%20Limit%20repo%20processor.html","title":"History Limit repo processor","text":"<p>A rep processor that allows to limit the history of a signal to a given time range relative to the prediction time point. There is also an option to virtually erase the signal completely so when the model will try to access the signal it will be empty. Mainly used as a pre processor in model apply to see how history impact performance.</p> <ul> <li>name : \"limit_history\" or \"history_limit\" parameters:</li> <li>signal - signal name time_channel - the time channel to limit by</li> <li>win_from , win_to - the time window to select</li> <li>delete_sig : if 1 : delete the signal from record.</li> <li>rep_time_unit , win_time_unit : global by default, otherwise as stated. Example json lines: <pre><code># limit Hemoglobin and Creatinine to tests done up to one year before the prediction time\n{\"rp_type\" : \"history_limit\" , \"signal\" : [\"Hemoglobin\" , \"Creatinine\"] , \"win_from\" : \"0\" , \"win_to\" : \"365\"}\n\u00a0\n# delete GENDER and Hemoglobin signals from each record\n{\"rp_type\" : \"history_limit\" , \"signal\" : [\"GENDER\", \"Hemoglobin\"], \"delete_sig\" : \"1\" }\n\u00a0\n</code></pre> Example as pre processor: A pre processor json file that limits history for RDW: \u00a0 <pre><code>{\n        \"pre_processors\" : [ {\"rp_type\" : \"history_limit\" , \"signal\" : [\"RDW\"], \"win_from\" : \"0\" , \"win_to\" : \"365\"} ] ,\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20print%2C%20how%20many%20outliers%20were.html","title":"How to print, how many outliers were","text":"<p>print_clear_info.json <pre><code>{\n  \"changes\": [\n       {\n                        \"change_name\":\"Turn on verbose cleaning\",\n                        \"object_type_name\":\"RepBasicOutlierCleaner\",\n                        \"json_query_whitelist\": [],\n                        \"json_query_blacklist\": [],\n                        \"change_command\": \"print_summary=0.00001;print_summary_critical_cleaned=0.00001\",\n                        \"verbose_level\":\"2\"\n           }\n     ]\n}\n</code></pre> \u00a0 Full flow command that uses this when model is applied: <pre><code>Flow --get_model_preds --rep $REP --f_samples $SMAPLES_PATH --f_model $MODEL_PATH --f_preds $OUTPUT_PREDS\u00a0--change_model_file print_clear_info.json\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html","title":"How to remove signal during admissions","text":""},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#motivation","title":"Motivation","text":"<p>When the repository includes info from inpatients, we might want to remove lab, measurements and drugs from admission periods, as it might be biased by the temporarily medical situation. Note that we are likely to keep diagnosis, as we are typically interested in chronic issues.</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#example","title":"Example","text":"<p>In the following example:</p> <ul> <li>We use**** rep processor</li> <li>We have **** signal in the repository</li> <li>We set get_values_in_range to 0 (will keep just input from inside of timeslots of ranges_sig_name) , as default is 1 (outside timeslots)</li> <li>We need to define signal appropriate ****</li> <li>The signal_options:<ul> <li>Can have a list of input&amp;output, all of them must have the same\u00a0****</li> <li>The default output_name is combination of the signal_name and ranges_sig_name - in this example (if we have not wrote output_name=BP), it would have been BP_ADMISSION. However, keeping the original name is continent when this rep processor is added in the beginning of existing process.\u00a0 \u00a0 <pre><code>{\n    \"action_type\": \"rp_set\",\n    \"members\": [\n    {\n        \"rp_type\":\"basic_range_cleaner\",\n        \"ranges_sig_name\":\"ADMISSION\",\n        \"time_channel\":\"0\",\n        \"range_time_channel\":\"0\",\n        \"get_values_in_range\":\"0\",\n        \"range_operator\":\"all\",\n        \"output_type\":\"T(i),V(s,s)\",\n        \"signal_options\": [ \n            \"signal_name=BP;output_name=BP\"\n        ]\n    }\n]}\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#remove-just-from-predict","title":"Remove just from predict","text":"<p>When a model was trained without the need to remove admissions (repository without inpatient data), but we apply the model on a repository with inpatient info, we need to remove admissions before predict, and this pre processor is not integral part of the model. We would run: <pre><code>Flow --get_model_preds --rep $REP --f_model $MODEL --f_samples $SAMPLES --f_preds $PREDS --f_pre_json $REMOVE_ADMISSIONS\n</code></pre> \u00a0where the REMOVE_ADMISSIONS json is as above, covered with <pre><code>{\n\"pre_processors\":[\nadd the json here\n]\n}\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#important","title":"Important","text":"<p>The new REMOVE_ADMISSIONS pre-processor should come before the original model pre-processors executed. Otherwise, it might be a problem. For instance, if the original model pre-processors are calculating virtual signal or creating registry, they would use the data during admissions (before the REMOVING). To avoid that, the default in Flow is that the new pre-processor comes first. If, from any reason you want it differently, set the parameter add_rep_processor_first to False. If we have panel completer in our json, after the basic_range_cleaner, it might 'return' the signal during admission, if we have not cleaned all relevant signals. For instance, removing just Hemoglobin is not enough as the panel completer add it back using other signals.</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20rename%20signal.html","title":"How to rename signal","text":"<p>When a model is trained on repository A and test on repository B, then we might face signal naming issue. Example:</p> <ul> <li>model A:\u00a0ICD9_Diagnosis</li> <li>model B: DIAGNOSIS \u00a0To resolve we adjust the model with the following json <pre><code>adjust_model --preProcessors rename_signal_diag.json --skip_model_apply 1 --learn_rep_proc 0 --inModel ${IN_MODEL} --out ${OUT_MODEL} \n</code></pre> While rename_siganl_diag.json is: <pre><code>{ \"pre_processors\" : [\n   {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"filter_channels\",\n          \"signal\":\"DIAGNOSIS\",\n          \"output_name\":\"ICD9_Diagnosis\",\n          \"signal_type\":\"T(i),V(i)\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Howto%20write%20RepProcessor.html","title":"How to Write a RepProcessor","text":"<p>RepProcessors in MedModel follow a defined lifecycle. Below is the typical sequence of method calls and their purpose:</p> <ol> <li> <p>Constructor    - Initializes the RepProcessor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the processor.</p> </li> <li> <p>Initialization    - During learning: <code>init(map&lt;string, string&gt;&amp; mapper)</code> parses arguments from a key-value map.    - During application:      Arguments were loaded from disk (based on <code>ADD_SERIALIZATION_FUNCS</code>).</p> </li> <li> <p>fit_for_repository(MedPidRepository)    - Adapts the processor to the repository (e.g., creates virtual signals if needed).    - During learning:</p> <ul> <li><code>get_required_signal_names()</code>    Identifies which signals are needed for processing.</li> <li><code>filter()</code>  Determines if the processor should be applied, based on whether it affects any required signals. The list of affected signals is stored in the <code>aff_signals</code> variable. You can override the <code>filter</code> logic if needed.  </li> </ul> </li> <li> <p>Virtual Signal Management    - <code>add_virtual_signals()</code>      Lists virtual signals to generate and their types.    - <code>register_virtual_section_name_id()</code>      Registers categorical virtual signals in the dictionary.</p> </li> <li> <p>Signal ID Setup    - <code>set_affected_signal_ids(MedDictionarySections)</code>      Defines output signal IDs.    - <code>set_required_signal_ids(MedDictionarySections)</code>      Defines input signal IDs.    - <code>set_signal_ids(MedSignals)</code>      Sets input/output signal settings (often overlaps with above).</p> </li> <li> <p>Final Initialization    - <code>init_tables(MedDictionarySections, MedSignals)</code>      Finalizes processor settings using repository data.</p> </li> <li> <p>Attribute Initialization    - <code>init_attributes()</code>      Sets up additional processor attributes in MedSamples. For example store fields to document outlier cleaning</p> </li> <li> <p>Signal Requirement</p> <ul> <li><code>get_required_signal_names()</code>   (May be called again) Ensures all required signals are fetched.</li> </ul> </li> <li> <p>Application</p> <ul> <li><code>conditional_apply(PidDynamicRec, MedIdSamples)</code>   Applies processor logic to patient data in memory. Uses <code>PidDynamicRec</code> which is editable in-memory repository for a single patient. It also protects us from changing data for other patients.</li> </ul> </li> <li> <p>Summary</p> <ul> <li><code>make_summary()</code>   Generates a summary after processing (e.g., outlier percentages).   Useful for parallel execution and feature generation.</li> </ul> </li> </ol>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Howto%20write%20RepProcessor.html#steps-to-implement-a-repprocessor","title":"Steps to Implement a RepProcessor:","text":"<ol> <li>Create a new <code>.h</code> file for your class and a corresponding <code>.cpp</code> file that includes the header. In the header, include <code>\"RepProcess.h\"</code>.</li> <li>Set up default values in <code>init_defaults()</code> or the constructor. For example, set <code>processor_type</code> using <code>RepProcessorTypes</code> (optional).</li> <li>Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</li> <li>Set up serialization:    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of the <code>.h</code> file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify only the parameters that need to be stored on disk after learning. Do not include temporary or repository-specific variables.</li> <li>Configure key variables for pipeline integration:    - Assign <code>virtual_signals_generic</code> after <code>init</code> if your processor creates virtual signals.    - Set <code>req_signals</code> to define required/input signals. This helps manage dependencies and ensures prerequisite processors run first. You can set this in <code>init_tables</code> or after <code>init</code>.    - Set <code>aff_signals</code> to specify output/affected signals, aiding pipeline dependency tracking. This can also be set in <code>init_tables</code> or after <code>init</code>.</li> <li>Override necessary functions as needed:    - <code>register_virtual_section_name_id</code> (for virtual categorical signals)    - <code>init_tables</code> (for initializing temporary variables using the repository - both in learn\\apply)    - <code>set_required_signal_ids</code>, <code>set_affected_signal_ids</code> (for custom signal ID logic; usually, using <code>aff_signals</code> and <code>req_signals</code> is sufficient)    - <code>fit_for_repository</code> (for repository-specific adjustments, e.g., virtual signal checks) (optional).    - <code>_learn</code> (override only if learning logic is needed; default is empty)    - <code>_apply</code> (main logic for applying the processor)    - <code>print</code> (optional for debugging)    - Any other required virtual functions</li> <li>Register your new RepProcessor in <code>RepProcess.h</code>:    - Add a new type to <code>RepProcessorTypes</code> before <code>REP_PROCESS_LAST</code>. In the documentation comment, specify the name in <code>rep_processor_name_to_type</code> for Doxygen reference.</li> <li>Register your new RepProcessor in <code>RepProcess.cpp</code>:    - Add your class to <code>rep_processor_name_to_type</code>    - Add your class to <code>RepProcessor::new_polymorphic</code>    - Add your class to <code>RepProcessor::make_processor(RepProcessorTypes processor_type)</code></li> </ol>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Noiser.html","title":"Noiser","text":"<p>This rep processor induces random noise into lab signals.\u00a0 This is divided into 3 kinds of noise, which can be used in tandem: 1. time_noise (-1&lt;int) - for noise size t, lowers the date by random value sampled from\u00a0 uniform_int(0, t).\u00a0 2. value_noise (0.0&lt;float)\u00a0- for noise size v, the rep processor first calculates the std of the lab signal across all patients. Then, it adds to each value of this lab a random noise, sampled from gaussian(0, v*std). 3. drop_probability (0.0&lt;float&lt;1.0) - for noise size d, each lab signal will be randomly dropped with probability d. In addition, one can truncate the resulting values to n digits by using truncation=n - important to truncate, as we are dealing with randomly sampled floats. In apply_in_test, if 0 it will apply noise in train only. If 1, will apply noise also in test. \u00a0 The rep processor is defined as such: <pre><code>{ \"pre_processors\" : [\n{\n\"action_type\":\"rp_set\",\n\"members\":[\n{\n\"rp_type\":\"noiser\",\n\"truncation\":\"2\",\n\"time_noise\":\"_TIME_NOISE_\",\n\"value_noise\":\"_VALUE_NOISE_\",\n\"drop_probability\":\"_DROP_NOISE_\",\n \"apply_in_test\":\"_ON_APPLY_STR_\",\n\"signal\": [\"RBC\",\"MPV\",\"Hemoglobin\",\"Hematocrit\",\"MCV\",\"MCH\",\"MCHC-M\",\"Platelets\",\"Neutrophils%\",\"Lymphocytes%\",\"Monocytes%\", \"WBC\",\"Eosinophils#\",\"Eosinophils%\",\"Basophils%\",\"Basophils#\",\"Neutrophils#\",\"Lymphocytes#\",\"Monocytes#\", \"RDW\"]\n} \n]\n}\n] }\n</code></pre> \u00a0 We are currently using this processor in two capacities. The first is to take a trained model and apply it on noised data, to see how much the model is sensitive to noise at prediction. This is much cheaper (adjust_model), and is incorporated into the autotests as\u00a015.test_noise_sensitivity_analysis.py (see Development kit). The second is to noise a model at training time,\u00a0to see how much the model is sensitive to noise at training. For this purpose, see\u00a0U:\\Itamar\\MR\\Projects\\Shared\\test_noiser\\train_experiment\\example_experiment. The file create_preds_train.py is a python script calling shell commands, to test crc_model.json in same directory. The trained models and preds subdirectories contain outputs of the test, including analysis for noising just time, just value, or just drop probability.\u00a0</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Rep%20Calculator.html","title":"Rep Calculator","text":"<p>Rep processor to calculate things: \u00a0 examples/rep_processor_calc.json Full parameter list:\u00a0https://Medial-EarlySign.github.io/MR_LIBS/classRepCalcSimpleSignals.html \u00a0 The input signals is given with comma separated string in \"signals\" parameter - for example: \"\"</p> <ul> <li>calculator - the calculator type, list of types can be found in next section</li> <li>output_signal_type - string type of output signal. For example \"T(i),V(f)\" to generate signal with 1 time channel of type int and 1 value channel of type float. This is the default</li> <li>max_time_search_range\u00a0 -integer that specify what is the maximal time gap to construct the virtual signal based on all signals. The date of the new signal will be the latest date.\u00a0</li> <li>signals_time_unit - the time unit to use in\u00a0max_time_search_range\u00a0(default Days)</li> <li>names - the name of the virtual signal</li> <li>time_channel - integer that specify which time channel to use in all input signals (default 0)</li> <li>work_channel -\u00a0integer that specify which value channel to use in all input signals (default 0). (If signal has less channels, will use last channel. not common usage)</li> <li>calculator_init_params - additional arguments string based on \"calculator\" parameter. since it's can be multiple arguments, you need to escape the string with \"{}\" and put arguments inside of brackets as in the examples. \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Rep%20Calculator.html#calculator-type-calculator-parameters","title":"Calculator type - \"calculator\" parameters","text":"<p>All \"calculator\" parameter options can be found in here\u00a0https://Medial-EarlySign.github.io/MR_LIBS/classSimpleCalculator.html:</p> <ul> <li>sum - linear combination of multiple signals\u00a0 \u00a0 res := b0 + sum_sigma(i=1..N){ factor[i] * input[i]},\u00a0 \u00a0 where b0 is a bias<ul> <li>calculator_init_params - can receive \"b0\" - to specify constant bias argument + \"factors\" which is comma separated numbers that correspond each input signal (default is list of ones)</li> </ul> </li> <li>log - calculates log on signal</li> <li>ratio - divides signals, accepts \"factor\" as final factor after dividing (default 1),.\u00a0\u00a0res := factor * V1^power_mone / V2^power_base<ul> <li>calculator_init_params - \u00a0\"power_mone\", \"power_base\" which default value is 1, and \"factor\" which the default is also 1</li> </ul> </li> <li>multiply -multiply of signals.\u00a0res := b0 * pie_multiply(i=1..N) {input[i]^powers[i]}\u00a0<ul> <li>calculator_init_params \"b0\" and \"powers\" which is comma separated numbers that correspond each input signal (default is list of ones)</li> </ul> </li> <li>kfre -\u00a0 Implements calculation of 3,4 or 8-variable Kidney Failure Risk Equations (KFRE). \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0The code can be found under \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Libs/Internal/MedProcessTools/MedProcessTools/RepProcess.h \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Libs/Internal/MedProcessTools/MedProcessTools/RepProcess.cpp</li> <li>empty - dummy virtual signal to create empty signal</li> <li>exists -\u00a0res := in_range_val if signal exists otherwise out_range_val<ul> <li>calculator_init_params\u00a0 \u00a0- \"out_range_val\", \"in_range_val\"</li> </ul> </li> <li>range -\u00a0A simple Range check that return \"in_range_val\" if within range and returns \"out_range_val\" if outside range. Accepts also \"min_range\", \"max_range\"<ul> <li>calculator_init_params - \"in_range_val\", \"out_range_val\", \"min_range\", \"max-range\"</li> </ul> </li> <li>set -\u00a0\u00a0res := \"in_range_val\" if is in set otherwise \"out_range_val\"<ul> <li>calculator_init_params\u00a0 \u00a0- \"out_range_val\", \"in_range_val\", \"sets\" or \"sets_file\" to specify list of codes of file path to read codes.\u00a0</li> </ul> </li> <li>eGFR - calculates eGFR from Creatinine, Gender, Age, based on CKD_EPI or MDRD equations.\u00a0<ul> <li>calculator_init_params\u00a0 -\u00a0You can pass \"mdrd\" to control if to use MDRD or CKD_EPI equation. You can also pass \"ethnicity\". 0 -for white, \"1\" for black \u00a0</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Rep%20Calculator.html#examples","title":"Examples:","text":"<p><pre><code>{\n                    \"rp_type\":\"calc_signals\",\n                    \"calculator\":\"ratio\",\n                    \"names\":\"PaO2_over_FiO2\",\n                    \"signals\":\"Art_PaO2,FiO2\",\n                    \"max_time_search_range\":\"180\",\n                    \"signals_time_unit\":\"Minutes\",\n                    \"calculator_init_params\":\"{factor=100}\",\n                    \"unconditional\":\"0\"\n}\n{\n                    \"rp_type\":\"calc_signals\",\n                    \"calculator\":\"set\",\n                    \"names\":\"Ventilation_proc\",\n                    \"signals\":\"PROCEDURE\",\n                    \"max_time_search_range\":\"0\",\n                    \"signals_time_unit\":\"Minutes\",\n                    \"calculator_init_params\":\"{sets=Invasive_Ventilation,Non-invasive_Ventilation}\",\n                    \"unconditional\":\"0\"\n},\n{\n                    \"rp_type\":\"calc_signals\",\n                    \"calculator\":\"range\",\n                    \"names\":\"Ventilation_1\",\n                    \"signals\":\"Peak_Insp_Pressure\",\n                    \"max_time_search_range\":\"0\",\n                    \"signals_time_unit\":\"Minutes\",\n                    \"calculator_init_params\":\"{min_range=0;max_range=1000}\",\n                    \"unconditional\":\"0\"\n },\n</code></pre> \u00a0 Run Flow with those signals (print them): <pre><code>#Flow with rep_processors: pids arg can be omitted to print all pids\nFlow --rep /home/Repositories/MIMIC/Mimic3/mimic3.repository --model_rep_processors $MR_ROOT/Projects/Resources/examples/rep_processor_calc.json --pids_sigs_print --sigs \"PaO2_over_FiO2,Art_PaO2,FiO2\" --pids 100000027\n\u00a0\npid     signal_name     description_name        description_value       ...\n100000027       PaO2_over_FiO2  Time_ch_0       142142154|21700404-19:54        Val_ch_0        147\n100000027       PaO2_over_FiO2  Time_ch_0       142142160|21700404-20:00        Val_ch_0        210\n100000027       PaO2_over_FiO2  Time_ch_0       142142997|21700405-09:57        Val_ch_0        134\n100000027       PaO2_over_FiO2  Time_ch_0       142143060|21700405-11:00        Val_ch_0        67\n100000027       PaO2_over_FiO2  Time_ch_0       142143120|21700405-12:00        Val_ch_0        67\n100000027       PaO2_over_FiO2  Time_ch_0       142143134|21700405-12:14        Val_ch_0        57\n100000027       PaO2_over_FiO2  Time_ch_0       142143180|21700405-13:00        Val_ch_0        57\n100000027       PaO2_over_FiO2  Time_ch_0       142144290|21700406-07:30        Val_ch_0        74\n100000027       PaO2_over_FiO2  Time_ch_0       142144335|21700406-08:15        Val_ch_0        80\n100000027       PaO2_over_FiO2  Time_ch_0       142146240|21700407-16:00        Val_ch_0        98.5714\n100000027       Art_PaO2        Time_ch_0       142142154|21700404-19:54        Time_ch_1       142142154|21700404-19:54        Val_ch_0        147\n100000027       Art_PaO2        Time_ch_0       142142599|21700405-03:19        Time_ch_1       142142599|21700405-03:19        Val_ch_0        89\n100000027       Art_PaO2        Time_ch_0       142142997|21700405-09:57        Time_ch_1       142142997|21700405-09:57        Val_ch_0        67\n100000027       Art_PaO2        Time_ch_0       142143134|21700405-12:14        Time_ch_1       142143134|21700405-12:14        Val_ch_0        57\n100000027       Art_PaO2        Time_ch_0       142143846|21700406-00:06        Time_ch_1       142143846|21700406-00:06        Val_ch_0        52\n100000027       Art_PaO2        Time_ch_0       142143900|21700406-01:00        Time_ch_1       142143900|21700406-01:00        Val_ch_0        57\n100000027       Art_PaO2        Time_ch_0       142144007|21700406-02:47        Time_ch_1       142144007|21700406-02:47        Val_ch_0        63\n100000027       Art_PaO2        Time_ch_0       142144148|21700406-05:08        Time_ch_1       142144148|21700406-05:08        Val_ch_0        74\n100000027       Art_PaO2        Time_ch_0       142144335|21700406-08:15        Time_ch_1       142144335|21700406-08:15        Val_ch_0        80\n100000027       Art_PaO2        Time_ch_0       142145347|21700407-01:07        Time_ch_1       142145347|21700407-01:07        Val_ch_0        86\n100000027       Art_PaO2        Time_ch_0       142146228|21700407-15:48        Time_ch_1       142146228|21700407-15:48        Val_ch_0        69\n100000027       Art_PaO2        Time_ch_0       142146662|21700407-23:02        Time_ch_1       142146662|21700407-23:02        Val_ch_0        71\n100000027       Art_PaO2        Time_ch_0       142147017|21700408-04:57        Time_ch_1       142147017|21700408-04:57        Val_ch_0        89\n100000027       Art_PaO2        Time_ch_0       142148289|21700409-02:09        Time_ch_1       142148289|21700409-02:09        Val_ch_0        64\n100000027       FiO2    Time_ch_0       142142085|21700404-18:45        Time_ch_1       142142085|21700404-18:45        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142142100|21700404-19:00        Time_ch_1       142142100|21700404-19:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142142148|21700404-19:48        Time_ch_1       142142148|21700404-19:48        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142142160|21700404-20:00        Time_ch_1       142142160|21700404-20:00        Val_ch_0        70\n100000027       FiO2    Time_ch_0       142142340|21700404-23:00        Time_ch_1       142142340|21700404-23:00        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142142400|21700405-00:00        Time_ch_1       142142400|21700405-00:00        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142142820|21700405-07:00        Time_ch_1       142142820|21700405-07:00        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142142850|21700405-07:30        Time_ch_1       142142850|21700405-07:30        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142143060|21700405-11:00        Time_ch_1       142143060|21700405-11:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143120|21700405-12:00        Time_ch_1       142143120|21700405-12:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143180|21700405-13:00        Time_ch_1       142143180|21700405-13:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143360|21700405-16:00        Time_ch_1       142143360|21700405-16:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143435|21700405-17:15        Time_ch_1       142143435|21700405-17:15        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142144290|21700406-07:30        Time_ch_1       142144290|21700406-07:30        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142144560|21700406-12:00        Time_ch_1       142144560|21700406-12:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142144800|21700406-16:00        Time_ch_1       142144800|21700406-16:00        Val_ch_0        70\n100000027       FiO2    Time_ch_0       142146000|21700407-12:00        Time_ch_1       142146000|21700407-12:00        Val_ch_0        70\n100000027       FiO2    Time_ch_0       142146240|21700407-16:00        Time_ch_1       142146240|21700407-16:00        Val_ch_0        70\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html","title":"Virtual Signals","text":"<p>Sometimes, it's necessary to dynamically add new signals to a repository, assigning them unique names and types, and enabling data insertion. This approach is especially useful when calculating features for specific problems, as storing features or intermediate results as new signals often simplifies the process.</p> <p>Example: If eGFR is not available in your repository (or you need a different calculation method), and you want to generate features like last, min, max, or average values across various time windows, the best solution is to create a virtual signal for eGFR. You can then calculate its values in memory, add them to the repository, and use this signal like any other for feature generation.</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html#defining-virtual-signals","title":"Defining Virtual Signals","text":"<p>To create a virtual signal, use the <code>insert_virtual_signal</code> API from <code>MedSignals.h</code>:</p> <ul> <li><code>int insert_virtual_signal(const string &amp;sig_name, int type)</code> or <code>int insert_virtual_signal(const string &amp;sig_name, const string&amp; signalSpec)</code> with generic string definition<ul> <li><code>sig_name</code>: The new signal's name (must be unique within the repository)</li> <li><code>type</code>: The signal's data type or signalSpec for generic signal definition</li> </ul> </li> </ul> <p>This function validates the input and sets up the necessary internal structures.</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html#adding-data-to-a-virtual-signal","title":"Adding Data to a Virtual Signal","text":"<p>There are two main methods:</p> <ol> <li> <p>In-memory mode:    The repository operates entirely in memory, allowing you to add or remove data freely. Refer to the relevant documentation for details.</p> </li> <li> <p>Dynamic records:    Use the dynamic records API (see PidDynamicRec) to modify or add signal data. This method is used in the MedProcessTools library and is described below.</p> </li> </ol> <p>In MedProcessTools, rep processor stages run first, each with <code>learn</code> and <code>apply</code> methods. For virtual signals that only require calculation (not learning), you simply need to compute and insert the new signal. More complex cases involve learning parameters or models, which are then used for signal calculation. The most advanced scenarios involve virtual signals needed during the learning stage of another processor or feature generator, which are not fully supported yet.</p> <p>During the apply stage, the library creates a <code>PidDynamicRec</code> for each patient, containing all necessary time points and signals for future processing. If the required virtual signals are already defined, you only need to add data to them:</p> <ul> <li>Calculate the time and value channels for your virtual signal at all relevant time points.</li> <li>Note: If dynamic record versions differ, generate a separate data set for each version.</li> <li>Use one of these APIs in <code>PidDynamicRec</code>:<ul> <li><code>int set_version_data(int sid, int version, void *datap, int len)</code><ul> <li><code>sid</code>: Signal ID (retrieve using <code>rep.sigs.sid(string name)</code>)</li> <li><code>version</code>: Target version</li> <li><code>datap</code>: Pointer to an array of the signal's type (e.g., <code>vector&lt;sigType&gt; data;</code> and use <code>&amp;data[0]</code>)</li> <li><code>len</code>: Number of items</li> </ul> </li> <li><code>int set_version_universal_data(int sid, int version, int *_times, float *_vals, int len)</code><ul> <li><code>sid</code>: Signal ID</li> <li><code>version</code>: Target version</li> <li><code>_times</code>, <code>_vals</code>: Arrays for time and value channels; if multiple channels exist, arrange them sequentially for each item</li> <li><code>len</code>: Number of items</li> </ul> </li> </ul> </li> </ul> <p>If multiple versions should share the same data, you can link them using:</p> <ul> <li><code>int point_version_to(int sid, int v_src, int v_dst);</code></li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html#creating-a-rep-processor-for-virtual-signals","title":"Creating a Rep Processor for Virtual Signals","text":"<p>You can either write a processor from scratch or use the <code>RepCalcSimpleSignals</code> processor, which simplifies many common cases.</p> <p>From scratch: 1. Each <code>RepProcessor</code> has a <code>virtual_signals_generic</code> vector (defined in the base class) to store all virtual signals it creates. Initialize this vector in the processor's <code>init()</code> function. The model will call <code>add_virtual_signals</code> at the appropriate time. 2. Implement <code>init()</code>, <code>init_tables()</code>, <code>learn()</code>, and <code>apply()</code> as needed. 3. When creating a new virtual signal:    - Loop over dynamic record versions    - Calculate the signal from the dynamic record    - Insert it using either <code>set_version_data</code> or <code>set_version_universal_data</code></p> <p>Using RepCalcSimpleSignals: The <code>RepCalcSimpleSignals</code> processor acts as a wrapper for simple virtual signal calculations. You can implement a new <code>SimpleCalculator</code> and integrate it via <code>RepCalcSimpleSignals::make_calculator</code>. Refer to the code for several straightforward examples.</p>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20create%20an%20empty%20signal/index.html","title":"How to create an empty signal","text":"<p>When a model is trained on repository A and test on repository B, then we might face missing signal. Example:</p> <ul> <li>model A: has RDW signal</li> <li>model B: do not have RDW signal \u00a0To resolve we adjust the model with the following json <pre><code>adjust_model --preProcessors add_empty_signal.json --skip_model_apply 1 --learn_rep_proc 0 --inModel ${IN_MODEL} --out ${OUT_MODEL}\n</code></pre> \u00a0 While add_empty_signal.json is: <pre><code>{ \"pre_processors\" : [\n   {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"calc_signals\",\n          \"calculator\":\"empty\",\n          \"names\":\"RDW\",\n          \"signals\":\"BDATE\" \n      }\n      ]\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20C%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20create%20an%20empty%20signal/Howto%20create%20a%20signal%20with%20constant%20value.html","title":"Howto create a signal with constant value","text":"<p>In some cases you want to create \"signal\" with constant value, for example, Race, all population are \"White\". You need to add this block: <pre><code>{ \"pre_processors\" : [\n  {\n    \"action_type\":\"rp_set\",\n    \"members\":[\n        {\n            \"rp_type\":\"calc_signals\",\n            \"calculator\":\"exists\",\n            \"names\":\"Race\",\n            \"signals\":\"MEMBERSHIP\",\n            \"max_time_search_range\":\"0\",\n            \"signals_time_unit\":\"Minutes\",\n            \"calculator_init_params\":\"{in_range_val=1}\",\n            \"unconditional\":\"0\"\n        }\n    ]\n  }\n] }\n</code></pre> The input signal in : \"signals\" should contain signal with \"time channel\" you can't pass BDATE or GENDER (we might change the code to support this). the\u00a0 in_range_val=\"1\" contains the output of the signal, 1 for all records.\u00a0 If you want to use this as dictionary, you will need to specify a dictionary with the mapping of the value \"1\" into the desired string.\u00a0 In our example, that we want to generate \"Race=White\" for all, we will need to add this dict to the repository (TODO: in the future we might configure the dictionary virtually and skip that). <pre><code>SECTION Race\nDEF 1   White\n</code></pre> and add this dicts to the repository</p>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html","title":"Feature Generator Practical Guide","text":"<p>Main page for describing Feature Generators We have and how to use them.</p>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#specific-fgs","title":"Specific FGs\u00a0:","text":"<ul> <li>Basic : \"basic\" : generating a wide range of simple (powerful) features such as last, min, max, avg, etc...</li> <li>Age : \"age\"\u00a0 : generating age , taking into account time units and the birth year / date.</li> <li>Singleton : \"singleton\" :\u00a0take the value of a time-less signal</li> <li>Gender : \"gender\" : special case of Singleton</li> <li>Binned LM : \"binnedLM\" :\u00a0creating linear model for esitmating feature in time points</li> <li>Smoking : \"smoking\" : very THIN related : converting THIN basic smoking signals to features.</li> <li>KP Smoking : \"kp_smoking\" : very KP related : converting KP basic smoking signals to features.</li> <li>Range : \"range\" : features related to range signals (such as some registries)</li> <li>Drug Intake : \"drugIntake\" : coverage of prescription times</li> <li>Alcohol : \"alcohol\" : very THIN related : convertin raw alcohol THIN signals to features</li> <li>Model : \"model\" : using a pre trained MedModel to generate features as its predictions.</li> <li>Time : \"time\" :\u00a0creating sample-time features (e.g. differentiate between times of day, season of year, days of the week, etc.)</li> <li>Attributes : \"attr\" : creating features from Samples attributes (allows \"loading\" of more data in the Samples file).</li> <li>Signal Dependency\u00a0: \"category depend\" : Select categorial features with correlation to the outcome.</li> <li>Embedding : \"embedding\" : Use a pre trained embedding model to generate features. \u00a0 Some more in depth information on Feature Generators \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#what-is-a-feature-generator","title":"What is a Feature Generator?","text":"<p>Formally, within our infrastructure a Feature Generator is running after all the rep processors. Each Feature Generator can create a constant number of features for all the given time points to a specific pid given in the dynamic record of the pid, after all rep processors had been applied to it. The features are then added to the right position in the MedFeatures object for the model. In subsequent stages after all FGs were run, Features Processors applied to the MedFeatures matrix will be run (such as imputers, normalizers, etc), and the matrix will be ready for training/prediction. Several points to remember, esp. when writing a new FG:</p> <ul> <li>Parallelism : Feature Generators are parallelized on pids in apply stages. So when writing one it should be:<ul> <li>Thread Safe : avoid static variables in FG, and if you do use them : guard them.</li> <li>No need to work hard to parallelize inside as it's already very efficiently parallelized from above.</li> </ul> </li> <li>Empty Learn Stage : many FGs are like that : that is the simplest, you don't need to do anything here.</li> <li>Learn Stage : There's an issue here as the FG needs to get all the Samples and the Rep and do the FG training process. The problem is :<ul> <li>Remeber you may need to apply the rep processors needed for the FG.<ul> <li>Since Rep Processors are currently running on dynamic recs, this may become an issue and needs careful coding.</li> <li>If you are only using signals not affected by previous rep processors, you can indeed work directly with the Samples and Rep.</li> </ul> </li> <li>Learn stage is NOT parallelized, hence you should take care of parallelizing it in your Feature Generator code.</li> </ul> </li> <li>Each FG must fill in the required repository signals list in req_signals. This should be filled in first init() time , and serialized (for actual apply). \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#feature-generator-init-from-parameters","title":"Feature Generator init from parameters","text":"<p>Several things happen in the init() routine:</p> <ul> <li>parameters are parsed</li> <li>names are created (use set_names() for that)</li> <li>req_signals are generated. \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#feature-generator-learnapply-stage-sequence","title":"Feature Generator Learn/Apply Stage sequence","text":"<ul> <li>Model will first scan to find out which generators need to be run (using MedModel::get_applied_generators()). Only those passing will be actually used.</li> <li>As part of the init_all procedure, the following will be called for each generator:</li> <li>set_signal_ids : allowing generator to translate signal names to ids using the rep signals.</li> <li>init_tables : allowing generator to initialize any internal table that requires knowing the dictionaries of the repository.</li> <li>Model will collect all required signals, hence every FG must implement the\u00a0get_required_signal_names() function or fill the req_signals vector</li> <li>The easiest way to ensure this is to make sure the req_signals vector is filled in init() . Make sure you add req_signals to the serialization.</li> <li>If this is done, the default\u00a0get_required_signal_names() is enough as it will simply add the signals in req_signals.</li> <li>In learn stage: The learn() is called, the user is responsible to apply the rep processors of the model inside the specific learn routine.</li> <li>In apply stage</li> <li>optional prepare() function getting MedFeatures, rep and samples is called. This is sometimes needed (for example in the Model FG).</li> <li>features is initialized and get_p_data() is called for each generator (usually the default one is good enough)</li> <li>for each pid , all (required) rep processors are invoked by their order, and then all the (reuired) generators. Parallelism is on the pids. \u00a0  </li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#signal-dependency-fg-category_depend","title":"Signal Dependency FG (\"category_depend\")","text":"<p>Using Signal Dependency to generate categorical signal. chi-square statistical test for independency between outcome and appearance of the category value for Gender+Age stratas. It selects the top k categories with the best p value and Lift.  It also works on sets and hierarchical categories like ATC, ICD10\u00a0 Feature Generator Arguments:</p> <ul> <li>\"signal\" - the categorical signal in repository</li> <li>\"win_from,win_to,time_unit_win\" - time window arguments to define time window</li> <li>\"regex_filter\" - filter categories bt name - for example take only \"ATC\" drug codes. use \"ATC_.*\"</li> <li>\"min_age,max_age,age_bin\" - Age strata for the statistical test. Also uses gender</li> <li>\"min_code_cnt\" - filters categories below this count of apearences</li> <li>\"fdr\" - p value filter</li> <li>\"lift_below,lift_above\" - filters on Lift values of category on average</li> <li>\"max_depth,max_parents\" - hierarchical\u00a0arguments, how many parents to take for each category value (in the example till 5 parentes), and maximal number of parents in that depth</li> <li>\"take_top\" - how many features to create, based on the categories. sorted by P value, lift and chi-square score by this order</li> <li>\"stat_test\" - chi-square or mcnemar (TODO add support for Cochran\u2013Mantel\u2013Haenszel statistics, fisher excat test). mcnemar is not exactly mcnemar because our data is not pairwise matched, but manipulated test to mimic this behaivor. from my experience it's more robust and gives better results</li> <li>\"verbose\" - if 1 prints the taken categories with Lift, total_count and some stats</li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#example-of-using-categorydependencygenerator","title":"Example of using\u00a0CategoryDependencyGenerator:","text":"Config Example<pre><code>{\n    \"action_type\":\"feat_generator\",\n    \"fg_type\":\"category_depend\",\n    \"signal\":\"RC\",\n    \"win_from\":\"0\",\n    \"win_to\":\"1825\",\n    \"time_unit_win\":\"Days\",\n    \"regex_filter\":\"ICD10_CODE:.*\",\n    \"min_age\":\"18\",\n    \"max_age\":\"90\",\n    \"age_bin\":\"5\",\n    \"min_code_cnt\":\"1000\",\n    \"fdr\":\"0.01\",\n    \"lift_below\":\"0.8\",\n    \"lift_above\":\"1.2\",\n    \"stat_metric\":\"mcnemar\",\n    \"max_depth\":\"5\",\n    \"max_parents\":\"10\",\n    \"use_fixed_lift\":\"0\",\n    \"verbose\":\"1\",\n    \"take_top\":\"100\"\n}\n</code></pre> <p>Running train MedModel with this+Age+Gender on death from Flu model with AUC=0.92 compared to Age+Gender only with AUC=0.88 \u00a0</p>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#todo","title":"TODO:","text":"<ul> <li>Use Stratas instead of fixed:Age,Gender</li> <li>Improve logic for filterHirarchy - to use Entropy or better measure to filter parent\\child\u00a0</li> <li>profilling - improve speed</li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/BasicFeatGenerator.html","title":"BasicFeatGenerator","text":"<p>This feature generator is used to create features for most common signals with 1 time channel and for a specific value channel (for example lab test like Hemoglobin). This feature calculate some stats in time window. It can be last value, mean value, slope, last_time in days of last test, etc. Here is an example block to define this feature generator - you should put this under \"model_actions\" element and after all the rep_processors and before feature_processors: <pre><code>{\n    \"fg_type\": \"basic\",\n    //ELEMENT initialization of BasicFeatGenerator \n}\n</code></pre> \u00a0 Full example on numeric values: <pre><code>{\n    \"fg_type\": \"basic\",\n    \"type\": [ \"last\", \"max\", \"min\", \"avg\", \"slope\", \"last_time\", \"last2\" ],\n    \"window: [ \"win_from=0;win_to=365\" ], //defines time window. I used list in here to be able to use this feature generator on multiple features. we can also use seperatly \"win_from\":\"0\", \"win_to\":\"365\"\n    \"signal\": [ \"Hemoglobin\", \"WBC\", \"Glucose\" ], //The signal to operate on\n    \"val_channel\":\"0\", //if signal has more than 1 channel (for example BP) we can specify on each value channel to work. Deafult is 0\n    \"tags\": \"labs,numeric,need_imputer\" //we can specify \"tags\" for this group of feature to later refer all of them. For example, do imputations for all features with \"need_imputer\" tag\n}\n</code></pre> Categorical example: <pre><code>{\n    \"fg_type\": \"basic\",\n    \"type\": \"category_set\" , //The operator, can also be category_set_count to store the count\n    \"window: [ \"win_from=0;win_to=365\" ], //defines time window. I used list in here to be able to use this feature generator on multiple features. we can also use seperatly \"win_from\":\"0\", \"win_to\":\"365\"\n    \"signal\": \"Drug\" , //The signal to operate on\n    \"val_channel\":\"0\", //if signal has more than 1 channel (for example BP) we can specify on each value channel to work. Deafult is 0\n    \"sets\": \"ATC_A10B_A02,ATC_A10B_A01\", //defined code or list of codes (comma separated) to construct the \"set\". You can also refer to codes in file with \"comma_rel:$FILE_PATH_OF_CODES\"\n    \"in_set_name\": \"Diabetes_drug\", //Optional argument to give name to this set. Otherwise a default of concatinating codes from \"sets\" will be created as the name of the feature\n    \"tags\": \"categorical\" //we can specify \"tags\" for this group of feature to later refer all of them. For example, do imputations for all features with \"need_imputer\" tag\n}\n</code></pre> \u00a0 \u00a0 For full list of arguments, please refer to: https://Medial-EarlySign.github.io/MR_LIBS/classBasicFeatGenerator.html \u00a0 For full list of \"type\" params: https://Medial-EarlySign.github.io/MR_LIBS/FeatureGenerator_8h.html Here are some common operators/types:</p> <ul> <li>\"last\" - take last value in the time window</li> <li>\"max\" - take maximal value</li> <li>\"last_time\" - take last time in days of the test in time window (in days)</li> <li>\"category_set\" - for categorical signal. boolean result. Will test if has value part of \"sets\" parameter in the defined time window</li> <li>\"category_set_count\" - for categorical signal. numeric result. Will count how many times found a value that is part of \"sets\" parameter in the defined time window \u00a0 For full json model format refer to\u00a0MedModel json format</li> </ul>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/Howto%20write%20Feature%20Generator.html","title":"How to Write a Feature Generator","text":"<p>A Feature Generator is a processing unit that takes raw input signals directly from a data repository or EMR. Its process has two main stages:</p> <ul> <li>It runs all relevant rep processors to pre-process the input signals. This prepares the data before it can be used to generate new features. This is being called by the infrastructure.</li> <li>It calls the generate function, which receives this pre-processed, patient-specific data and produces the final output.</li> </ul> <p>Feature Generators in MedModel follow a specific sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the Feature Generator object.</p> </li> <li> <p>init_defaults()    - Sets default values for the generator. please update <code>generator_type</code> to hold genertor type</p> </li> <li> <p>Initialization    - During learning: <code>init(map&lt;string, string&gt;&amp; mapper)</code> parses parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      Please make sure to update <code>req_signals</code> as required input signals for the feature generator      please set <code>tags</code> variable     - During application:      Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</p> </li> <li> <p>fit_for_repository(MedPidRepository)    - Adapts the generator to the repository, e.g., modifies logic if certain signals are missing.</p> </li> <li> <p>Signal Requirements and Setup    - <code>get_required_signal_ids()</code>      Returns the list of required signal IDs for learning or applying the generator.    - <code>set_required_signal_ids(MedDictionarySections)</code>      Stores required signal IDs using dictionary sections.    - <code>set_signal_ids(MedSignals)</code>      Stores required signal IDs using signal objects.    - <code>init_tables(MedDictionarySections)</code>      Initializes tables and stores needed signal IDs using dictionary sections.    - <code>set_names</code> - stores the output names of the feature generator - please override.</p> </li> <li> <p>Feature Filtering    - <code>filter_features()</code>      Determines if this generator is needed (e.g., after feature selection). Returns <code>true</code> if the generator should be kept. Uses by default <code>names</code> variable set by <code>set_names</code> to check if the feature generator is needed and if one of his output names is needed in the pipeline.</p> </li> <li> <p>Signal Names    - <code>get_required_signal_names()</code>      Returns all signal names needed to run this generator.</p> </li> <li> <p>Learning Phase    - <code>learn()</code>      Performs learning logic (called only during training).</p> </li> <li> <p>Preparation    - <code>prepare()</code>      Prepares features, attributes, and allocates space.</p> </li> <li> <p>Output Initialization</p> <ul> <li><code>get_p_data()</code>   Initializes the address for the generator\u2019s output (useful for parallelism).</li> </ul> </li> <li> <p>Feature Generation</p> <ul> <li><code>generate()</code>   Generates the feature for each sample. The infrastructure already execuated all relavent rep processors for the desired input signals the feature generator is using. </li> </ul> </li> <li> <p>Summary</p> <ul> <li><code>make_summary()</code>   Summarizes results after generation (e.g., collects statistics across all data).</li> </ul> </li> </ol>"},{"location":"Infrastructure%20C%20Library/02.Feature%20Generator%20Practical%20Guide/Howto%20write%20Feature%20Generator.html#steps-to-implement-a-feature-generator","title":"Steps to Implement a Feature Generator","text":"<ol> <li> <p>Create Class Files    - Make a new <code>.h</code> header and <code>.cpp</code> source file for your feature generator class. Include <code>\"FeatureGenerator.h\"</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Exclude temporary or repository-specific variables.</p> </li> <li> <p>Signal and Table Setup    - Implement or override (if needed):</p> <ul> <li><code>set_names</code> Update feature generator output features</li> <li><code>get_required_signal_ids()</code> and <code>get_required_signal_names()</code> - only if needed. The deafult is to use <code>req_signals</code></li> <li><code>set_required_signal_ids(MedDictionarySections)</code>  - only if needed. The deafult is to use <code>req_signals</code></li> <li><code>set_signal_ids(MedSignals)</code> - only if needed to do more setup. </li> <li><code>init_tables(MedDictionarySections)</code></li> <li><code>get_required_signal_categories</code> - if the feature generator uses categorical signals - this will need to list all \"required\" categorical values the feature generator is using</li> </ul> </li> <li> <p>Feature Filtering    - Overide (if needed) <code>filter_features()</code> if your generator should be skipped under certain conditions (e.g., after feature selection). The default is to use <code>names</code> to identify if the feature generator is needed.</p> </li> <li> <p>Learning and Preparation    - Implement <code>learn()</code> for training logic (if needed).    - Implement <code>prepare()</code> to allocate resources and set up attributes.</p> </li> <li> <p>Feature Generation    - Implement <code>generate()</code> to produce the feature for each sample.    - Implement <code>get_p_data()</code> if your generator supports parallel output.</p> </li> <li> <p>Summary    - Implement <code>make_summary()</code> to collect and report statistics after feature generation.</p> </li> <li> <p>Register Your Feature Generator in header file in <code>FeatureGenerator.h</code>    - register a new type in <code>FeatureGeneratorTypes</code> before <code>FTR_GEN_LAST</code> In the documentation comment, specify the name in <code>FeatureGeneratorTypes</code> for Doxygen reference. </p> </li> <li> <p>Register Your Feature Generator in cpp file <code>FeatureGenerator.cpp</code>    - Add your type conversion to <code>ftr_generator_name_to_type</code>    - Add your class to <code>FeatureGenerator::new_polymorphic</code>    - Add your class to <code>FeatureGenerator::make_processor(FeatureGeneratorTypes generator_type)</code></p> </li> </ol> <p>Tip: Follow the structure and naming conventions used in existing feature generators for consistency and easier maintenance.</p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/index.html","title":"FeatureProcessor practical guide","text":"FTR_PROCESS_NORMALIZER <p>\"normalizer\" to create\u00a0FeatureNormalizer</p> FTR_PROCESS_IMPUTER <p>\"imputer\" to create\u00a0FeatureImputer</p> FTR_PROCESS_DO_CALC <p>\"do_calc\" to create\u00a0DoCalcFeatProcessor</p> FTR_PROCESS_UNIVARIATE_SELECTOR <p>\"univariate_selector\" to create\u00a0UnivariateFeatureSelector</p> FTR_PROCESSOR_MRMR_SELECTOR <p>\"mrmr\" or \"mrmr_selector\" to create\u00a0MRMRFeatureSelector</p> FTR_PROCESSOR_LASSO_SELECTOR <p>\"lasso\" to create\u00a0LassoSelector</p> FTR_PROCESSOR_TAGS_SELECTOR <p>\"tags_selector\" to create\u00a0TagFeatureSelector</p> FTR_PROCESSOR_IMPORTANCE_SELECTOR <p>\"importance_selector\" to create\u00a0ImportanceFeatureSelector</p> FTR_PROCESSOR_ITERATIVE_SELECTOR <p>\"iterative_selector\" applies bottom-up or top-down iteration for feature selection. Creates\u00a0IterativeFeatureSelector</p> FTR_PROCESS_REMOVE_DGNRT_FTRS <p>\"remove_deg\" to create\u00a0DgnrtFeatureRemvoer</p> FTR_PROCESS_ITERATIVE_IMPUTER <p>\"iterative_imputer\" to create\u00a0IterativeImputer</p> FTR_PROCESS_ENCODER_PCA <p>\"pca\" to create\u00a0FeaturePCA</p> FTR_PROCESS_ONE_HOT <p>\"one_hot\" to create\u00a0OneHotFeatProcessor\u00a0- make one-hot features from a given feature</p> FTR_PROCESS_GET_PROB <p>\"get_prob\" to create\u00a0GetProbFeatProcessor\u00a0- replace categorical feature with probability of outcome in training set</p> FTR_PROCESS_PREDICTOR_IMPUTER <p>\"predcitor_imputer\" to create\u00a0PredictorImputer</p> FTR_PROCESS_MULTIPLIER <p>\"multiplier\" to create\u00a0MultiplierProcessor\u00a0- to multiply feature by other feature</p> FTR_PROCESS_RESAMPLE_WITH_MISSING <p>\"resample_with_missing\" to create\u00a0ResampleMissingProcessor\u00a0- adds missing values to learn matrix</p> FTR_PROCESS_DUPLICATE <p>\"duplicate\" to create\u00a0DuplicateProcessor\u00a0- duplicates samples in order to do multiple imputations.</p> FTR_PROCESS_MISSING_INDICATOR <p>\"missing_indicator\" to create\u00a0MissingIndicatorProcessor\u00a0- creates a feature that indicates if a feature is missing or not</p> FTR_PROCESS_BINNING <p>\"binning\" to create\u00a0BinningFeatProcessor\u00a0- binning with one hot on the bins</p> <p>- \"remove_deg\" - removes features that most of the time are same value (or missing)\u00a0 <pre><code>{\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n          \"fp_type\": \"remove_deg\",\n          \"percentage\": \"0.999\"\n        }\n      ]\n  }\n</code></pre> - \"normalizer\" - to normalize features: <pre><code> {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n          \"fp_type\": \"normalizer\",\n          \"resolution_only\":\"0\",\n          \"resolution\":\"5\",\n          \"tag\":\"need_norm\",\n          \"duplicate\":\"1\"\n        }\n      ]\n    }\n</code></pre>   tag + duplicate=1, results in wrapping this feature processor with MultiFeatureProcessors that iterates over all features and filter out features by \"tag\" to apply this normalization processing. I tagged all the required relevant features with tag \"need_norm\" - \"imputer\" by strata of age+sex+other features. Take median,common, average, sample, etc:</p> <p><pre><code>{\n\"action_type\": \"fp_set\",\n\"members\": [\n{\n\"fp_type\": \"imputer\",\n\"strata\": \"Age,0,80,10:Gender,1,2,1\",\n\"moment_type\":\"common\",\n\"tag\":\"need_imputer\",\n\"duplicate\":\"1\"\n}\n]\n}\n</code></pre> - \"do_calc\" - calculator of some features from others. For example \"or\" on features, can also \"sum\" features <pre><code>{\n        \"action_type\": \"fp_set\",\n          \"members\": [\n            {\n              \"fp_type\": \"do_calc\",\n              \"calc_type\": \"or\",\n              \"source_feature_names\": \"Current_Smoker,Ex_Smoker\",\n              \"name\":  \"Ex_or_Current_Smoker\"\n            }\n        ]\n    }\n</code></pre></p> <ul> <li>importance_selector - selection of features based on most important features in a model that is trained on the data.\u00a0</li> <li>iterative_selector - please use the tool to do it, it takes forever!!\u00a0Iterative Feature Selector</li> <li>resample_with_missing - used in training to \"generate\" more samples with missing values and increasing data size (not doing imputations, that</li> </ul> <pre><code>{\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n             \"fp_type\": \"resample_with_missing\",\n             \"missing_value\":\"-65336\",\n             \"grouping\":\"BY_SIGNAL_CATEG\",\n             \"selected_tags\":\"labs_numeric\",\n             //\"removed_tags\":\"\",\n             \"duplicate_only_with_missing\":\"0\",\n             \"add_new_data\":\"3000000\",\n             \"sample_masks_with_repeats\":\"1\",\n             \"limit_mask_size\":\"2\",\n             \"uniform_rand\":\"0\",\n             \"use_shuffle\":\"0\",\n             \"subsample_train\":\"3000000\",\n             \"verbose\":\"1\"\n        }\n      ]\n    }\n</code></pre> <p>No need for duplicate, it is scanning the features by it's own and operating on \"selected_tags\". add_new_data- - how many new data points to add. grouping is used to generate masks of missing values in groups and not feature by features. This is another feature processor job). similar to data augmentation in imaging</p> <ul> <li> <p>\"binning\" - binning feature value - can be specified directly the cutoffs or using some binning_method, equal width, minimal observations in each bin, etc. <pre><code>{\n    \"action_type\": \"fp_set\",\n    \"members\": [\n        {\n            \"action_type\": \"feat_processor\",\n            \"fp_type\": \"binning\",\n            \"bin_sett\": \"{bin_cutoffs=0,1,15,30,100}\",\n            \"one_hot\": 0,\n            \"tag\": \"Smok_Pack_Years_Last\", \n            \"duplicate\": 1\n        }\n    ]\n    }\n</code></pre></p> </li> <li> <p>\"predcitor_imputer\" - much more complicated/smart imputer based on model. Gibbs samplings, masked GAN, univariate sampling from features distributions, etc..\u00a0 <pre><code>{\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n          \"fp_type\": \"predictor_imputer\",\n          \"tag\":\"need_imputer\",\n          \"duplicate\":\"0\",\n          \"gen_type\":\"GIBBS\",\n          \"verbose_learn\":\"1\",\n          \"verbose_apply\":\"1\",\n          \"use_parallel_learn\":\"0\",\n          \"use_parallel_apply\":\"0\",\n          \"generator_args\":\"{calibration_save_ratio=0.2;bin_settings={split_method=iterative_merge;min_bin_count=100;binCnt=50};calibration_string={calibration_type=isotonic_regression;verbose=0};predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=1;num_trees=10;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=20;feature_fraction=0.8;bagging_fraction=0.25;bagging_freq=4;is_unbalance=true;num_leaves=80;silent=2};selection_count=200000}\",\n          \"sampling_args\":\"{burn_in_count=20;jump_between_samples=5;find_real_value_bin=1}\"\n        }\n      ]\n    }\n</code></pre> Instead of GIBBS, GAN, can select:</p> </li> <li> <p>RANDOM_DIST\u00a0- random value from normal dist around 0,5 (not related to feature dist)</p> </li> <li>UNIVARIATE_DIST - strata by some features, store distribution in each strata. In apply, find strata and select value randomly from dist</li> <li>MISSING - put missing value</li> <li>GAN -\u00a0generator_args is path to trained model. Please refer to this path to train:\u00a0TrainingMaskedGAN</li> <li>GIBBS - arguments\u00a0<ul> <li>sampling_args -\u00a0<ul> <li>burn_in_count - how many round to do in the start and to ignore them till stablize on reasonable vector.\u00a0</li> <li>jump_between_samples - how many rounds to do before generating new sample. when continuing to iterate in rounds, after several loops we end up with different sample</li> <li>find_real_value_bin - if true will round values to existing values only from feature values. When you try to see if model can discriminate between real data and generated data, the resolution of the feature values is important. tree can detect different between 3 nd 3.000001. If true this will cause 3.00001 to be 3. No good reason why to turn off</li> <li>samples_count - how many samples to extract</li> </ul> </li> <li>generator_args<ul> <li>calibration_save_ratio - what percentage of data to keep for clibration to probability. 0.2 (20%) is good number</li> <li>bin_settings - how to split the feature value into bins. The prediction problem will be multi category to those binned values. For example, hemoglobin last : 13.1-13.3, 13.4-13.6, etc... The target will be what's the probability to have hemoglobin in each value range. after having this, we can sample from this distribution bin value for the feature.\u00a0</li> <li>calibration_string - how to calibarte. keep as isotonic_regression, it's good</li> <li>predictor_type - predictor type for the multi class prediction</li> <li>predictor_args - arguments for the predictor. please pay attention this is multi category prediction! For example objective for lightGBM is \"objective=multiclass\"</li> <li>num_class_setup - since this is multiclass, some predictors requires to setup how many classes there are. This argument controls the name of this parameters. In LightGBM for example it's called \"num_class\"</li> <li>selection_count - down sampling for training those models to speedup</li> </ul> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Charlson.html","title":"Charlson","text":"<p>Json to generate Charlson score. Path: $MR_ROOT/Projects/Resources/examples/Charlson/charlson.pretify.json \u00a0 This is for THIN,\u00a0 input is DIAGNOSIS signal <pre><code>{\n    \"model_json_version\": \"2\",\n    \"serialize_learning_set\": \"0\",\n    \"model_actions\": [\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"age\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"MI\",\n            \"sets\": \"comma_rel:Diseases/MI.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"CHF\",\n            \"sets\": \"comma_rel:Diseases/CHF.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"PVD\",\n            \"sets\": \"comma_rel:Diseases/PVD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"CerebrovascularD\",\n            \"sets\": \"comma_rel:Diseases/CerebrovascularD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Dementia\",\n            \"sets\": \"comma_rel:Diseases/Dementia.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"ChronicPulmD\",\n            \"sets\": \"comma_rel:Diseases/ChronicPulmD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"RheumaticD\",\n            \"sets\": \"comma_rel:Diseases/RheumaticD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"PepticUlcer\",\n            \"sets\": \"comma_rel:Diseases/PepticUlcer.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"MildLiver\",\n            \"sets\": \"comma_rel:Diseases/MildLiver.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"DiabetesNComplx\",\n            \"sets\": \"comma_rel:Diseases/DiabetesNComplx.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"DiabetesWComplx\",\n            \"sets\": \"comma_rel:Diseases/DiabetesWComplx.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"XPlegia\",\n            \"sets\": \"comma_rel:Diseases/XPlegia.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Renal\",\n            \"sets\": \"comma_rel:Diseases/Renal.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Cancers\",\n            \"sets\": \"comma_rel:Diseases/Cancers.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"SevereLiver\",\n            \"sets\": \"comma_rel:Diseases/SevereLiver.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Metastatic\",\n            \"sets\": \"comma_rel:Diseases/Metastatic.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"HIV\",\n            \"sets\": \"comma_rel:Diseases/HIV.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Leukemia\",\n            \"sets\": \"comma_rel:Diseases/Leukemia.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Lymphoma\",\n            \"sets\": \"comma_rel:Diseases/Lymphoma.list\"\n        },\n        //Processings\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                //Age binnings\n                {\n                    \"fp_type\": \"binning\",\n                    \"name\": \"Age\",\n                    \"bin_sett\": \"{bin_cutoffs=49,59,69,79;bin_repr_vals=0,1,2,3,4}\",\n                    \"bin_format\": \"%1.0f\",\n                    \"remove_origin\": \"0\",\n                    \"one_hot\": \"1\",\n                    \"keep_original_val\": \"1\"\n                },\n                //substraction of intersecting conditioins:\n                {\n                    \"fp_type\": \"do_calc\",\n                    \"calc_type\": \"and\",\n                    \"source_feature_names\": \"DiabetesNComplx,DiabetesWComplx\",\n                    \"name\": \"Double_Diabetes\"\n                },\n                {\n                    \"fp_type\": \"do_calc\",\n                    \"calc_type\": \"and\",\n                    \"source_feature_names\": \"MildLiver,SevereLiver\",\n                    \"name\": \"Double_Liver\"\n                },\n                {\n                    \"fp_type\": \"do_calc\",\n                    \"calc_type\": \"and\",\n                    \"source_feature_names\": \"Cancers,Metastatic\",\n                    \"name\": \"Double_Cancer\"\n                }\n            ]\n        },\n        {\n            \"action_type\": \"feat_processor\",\n            \"fp_type\": \"do_calc\",\n            \"calc_type\": \"sum\",\n            \"source_feature_names\": \"Age.BINNED_1,Age.BINNED_2,Age.BINNED_3,Age.BINNED_4,MI,CHF,PVD,CerebrovascularD,Dementia,ChronicPulmD,RheumaticD,PepticUlcer,MildLiver,SevereLiver,DiabetesNComplx,DiabetesWComplx,XPlegia,Renal,Cancers,HIV,Leukemia,Lymphoma,Double_Diabetes,Double_Liver,Metastatic,Double_Cancer\",\n            \"weights\": \"1,2,3,4,1,1,1,1,1,1,1,1,1,3,1,2,2,2,2,2,2,6,-1,-1,6,-2\",\n            \"name\": \"Charlson\",\n            \"duplicate\": \"0\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Framingham%20Feature%20Processor.html","title":"Framingham Feature Processor","text":"<pre><code>//Diabetes registry virtual signal (rep processor) to create signal DM_Registry:\n{\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"create_registry\", \n          \"registry\":\"dm\", \n          \"names\":\"DM_Registry\",\n          \"dm_diagnoses_sig\":\"DIAGNOSIS\",\n          \"dm_drug_sets\":\"list_rel:registries/diabetes_drug_codes.2020.full\",\n          \"dm_diagnoses_sets\":\"list_rel:registries/diabetes_read_codes_registry.full.striped\",\n           \"signals\":\"Glucose,HbA1C,Drug,DIAGNOSIS\"\n        }\n    ]\n},\n\u00a0\n//Age + Gender input features:\n{ \"action_type\": \"feat_generator\", \"fg_type\": \"age\" },\n{ \"action_type\": \"feat_generator\", \"fg_type\": \"gender\" },\n//DM last value:\n{\n      \"action_type\":\"feat_generator\",\n            \"fg_type\":\"range\",\n            \"type\":\"ever\",\n            \"window\":\"win_from=0;win_to=100000\",\n            \"time_unit\":\"Days\",\n            \"signal\":\"DM_Registry\",\n            \"sets\":\"DM_Registry_Diabetic\"\n},\n//Other signals:\n{    \"feat_generator\":\"basic\",\n    \"type\": \"last\",\n    \"window:  \"win_from=0;win_to=1095\" ,\n    \"signal\": [ \"Cholesterol\", \"HDL\" ], //The signal to operate on\n    \"val_channel\":\"0\"\n},\n{    \"feat_generator\":\"basic\",\n    \"type\": \"last\",\n    \"window:  \"win_from=0;win_to=1095\" ,\n    \"signal\": [ \"BP\" ], //The signal to operate on\n    \"val_channel\":[\"0\", \"1\"]\n},\n\u00a0\n{\n  \"action_type\":\"fp_set\",\n  \"members\": [\n    {\n        \"fp_type\":\"do_calc\",\n        \"calc_type\":\"framingham_chd\",\n        \"source_feature_names\":\"Gender,Age,DM_Registry,Current_Smoker,BP.last.win_0_1095,BP.last.win_0_1095.t0v1,Cholesterol.last.win_0_1095,HDL.last.win_0_1095,Drug.category_set_hypertension_drugs.win_0_1095\",\n        \"name\":\"Framingham_feature_name\"\n    }\n  ]\n}\n</code></pre>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Howto%20write%20Feature%20Processor.html","title":"How to Write a Feature Processor","text":"<p>Feature Processors are components that operate on the feature matrix produced by the Feature Generator. They take a matrix of features as input, process it (e.g., normalization, feature selection, PCA), and output a transformed feature matrix.</p> <p>Feature Processors in MedModel follow a defined sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the Feature Processor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the processor. Be sure to update <code>processor_type</code> to reflect the processor type.</p> </li> <li> <p>Initialization    - During learning:      Implement <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      If your processor affects a single feature, you may want to use <code>feature_name</code> to specify the output feature.    - During application:      Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</p> </li> <li> <p>Repository Setup    - Calls <code>set_feature_name</code> to configure the processor using repository information.</p> </li> <li> <p>Feature Filtering    - Methods like <code>update_req_features_vec</code>, <code>are_features_affected</code>, and <code>filter</code> determine if this Feature Processor is needed for prediction.      If the processor does not affect any required features, it will be skipped.      By default, <code>filter</code> uses <code>feature_name</code> to check if the processor is necessary.</p> </li> <li> <p>select_learn_matrix    - Usually not required. In special cases, you may want to create a copy of the original feature matrix and store it under a different name for use by other processors in the pipeline.</p> </li> <li> <p>Learning Phase    - <code>learn()</code>      Implements any learning logic needed during training.</p> </li> <li> <p>Feature Processing    - <code>apply()</code>      Applies the processor logic to the feature matrix.</p> </li> </ol>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Howto%20write%20Feature%20Processor.html#steps-to-implement-a-feature-processor","title":"Steps to Implement a Feature Processor","text":"<ol> <li> <p>Create Class Files    - Create a new <code>.h</code> header and <code>.cpp</code> source file for your feature processor class. Include <code>\"FeatureProcess.h\"</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Do not include temporary or repository-specific variables.</p> </li> <li> <p>Custom Setup (if needed)    - Implement or override:</p> <ul> <li><code>filter</code> (update logic if your processor affects a specific set of features)</li> </ul> </li> <li> <p>Learning    - Implement <code>learn()</code> for any required training logic.</p> </li> <li> <p>Apply    - Implement <code>apply()</code> to process the features.</p> </li> <li> <p>Register Your Feature Processor in the Header (<code>FeatureProcess.h</code>)    - Add a new type to <code>FeatureProcessorTypes</code> before <code>FTR_PROCESS_LAST</code>. In the documentation comment, specify the name in <code>FeatureProcessorTypes</code> for Doxygen reference.</p> </li> <li> <p>Register Your Feature Processor in the Source (<code>FeatureProcess.cpp</code>)    - Add your type conversion to <code>feature_processor_name_to_type</code>    - Add your class to <code>FeatureProcessor::new_polymorphic</code>    - Add your class to <code>FeatureProcessor::make_processor(FeatureProcessorTypes processor_type)</code></p> </li> </ol> <p>Tip: Follow the structure and naming conventions of existing feature processors for consistency and easier maintenance.</p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/index.html","title":"Embeddings","text":"<p>The Embeddings tool set is designed to allow generation of strong features embedding a very large feature space into a much smaller dimension, and giving the tools to train these Embeddings, and to use them as a feature generator within the InfraStructure. The general plan is the following:</p> <ol> <li>Define the (large) feature set (use the embed_params many options for that, see below)</li> <li>Create x,y matrices to train the embedding : this will also create the .scheme file which is the recipe file of how to create a line in the x matrix - this will be needed later.</li> <li>Train the embedding using Keras (Embedder.py script) : this will also set the dimension of the embedding.</li> <li>Use the .scheme file and the keras layers file to define embedded features to be generated on your prediction problem - the new embedded features will be added to your train/test matrices, the .scheme and layers information will be serialized into your trained model. \u00a0 We will cover the technical details of how to perform each of the steps above, and also take a look at the Embedding WalkThrough Example page to see an actual example. \u00a0</li> </ol>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html","title":"Embeddings WalkThrough Example","text":"<p>In this example we will first create an unsupervised embedding space, and then use it as features for predicting the first CVD MI. All work done here can be found in:\u00a0/nas1/Work/Users/Avi/test_problems/embedding_example The code needed for this is either in MR_LIBS or in MR/Projects/Shared/Embeddings. Quick Jump:</p> <ul> <li>Step 1 : Plan</li> <li>Step 2: Prepare basic lists , and cohort</li> <li>Step 3 : Design and prepare the training dates for embeddings</li> <li>Step 4 : Prepare the embed_params files for x and y</li> <li>Step 5: Create the x and y matrices </li> <li>Step 6: Train the embedding using Keras</li> <li>Step 7 : Testing we get the same embeddings in Keras and Infrastructure</li> <li>Step 8 : Using Embedding In a MedModel</li> <li>Step 9 : Results for MI with/without Embedding , with/without SigDep </li> </ul>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-1-plan","title":"Step 1 : Plan","text":"<p>Our plan is to use the CVD_MI registry, then use TRAIN=1 group for training/cross validation , and TRAIN=2 group for actual validation. We will need to sacrifice some data in order to train our embedding. We do it simply by splitting the TRAIN=1 group into two random groups: 1A and 1B , 1A will be used in training our embedding, 1B will be used in training our model. We will give scores at random times before the outcome (and slice with bootstrap later), say one random sample every 180 days. We will also make sure our samples have at least a BP or LDL or Glucose reading at least 3 years before the sample. This gives us the plan for the outcome training. For the embedding training - we have lots of options, and it is not clear which is the best. In this example we will choose the option of using the same dates and outcome times as in the outcome training group, generate a feature rich large x (source) matrix at the sample times, and a large (but smaller) destiny matrix y for the times (outcome,\u00a0outcome + T) , T = 1y , which will also include our outcome variable. We will then use the embedded layer as features added to the CVD model and see if we see any improvement in perfomance. General Vocabulary of terms we use:</p> <ul> <li>sparse matrix : a matrix in which most values are 0 , and hence can be defined by showing only the few non-zero elements. Very efficient in holding large sets of sparse categorial features. We use the MedSparseMat class to handle these objects. Usually we create such matrices with a prefix followed by a suffix :</li> <li>.smat : the actual sparse matrix, lines of ,, <li>.meta : containing the list of pid,time matching each line</li> <li>.dict : a dictionary giving the names of the features in each column</li> <li>.scheme : the serialized scheme file for this matrix. Allows recreating such matrices with the same rules/dictionaries on a different set of samples.</li> <li>x,y matrices - the marices we train the embedding on. The embedding will start from the (sparse) features in x[i] vector run through a network of several layers, one of which is the embedding layer, and end up in the y[i] (sparse) vector.</li> <li>embed_params : a file containing the parameters (init_from_string format) defining how to generate a sparse matrix, for example which categorial signals to use, on which sets in which time windows, etc.</li> <li>scheme file : a serialized EmbedMatCreator object (from MedEmbed.h) , containing a ready to use object that can be deserialized and used to generate sparse matrices using a predefined set of rules that was learnt.</li> <li>layers file : a file containing a keras embedding model in a format we can read and use in our infrastructure.</li> <li>outcome training : the process of training the actual model for the outcome (in this example cvd_mi)</li> <li>embedding training : the process of preparing the x,y matrices, the scheme files, train a deep learning embedding model in keras, and get the layers file. Scheme and layer files will later be used as a feature generator in the infrastructure.</li>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-2-prepare-basic-lists-and-cohort","title":"Step 2: Prepare basic lists , and cohort","text":"<p><pre><code># creating a cohort file for cvd_mi : first verified mi, censoring cases with other mi's before , and cases with no BP or Glucose or LDL tests\n\u00a0\n/nas1/UsersData/avi/MR/Projects/Shared/Embeddings/Linux/Release/Embeddings --build_example\n\u00a0\n# creating samples for training/testing for TRAIN=1 and TRAIN=2\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --cohort_fname ./cvd_mi.cohort --cohort_sampling \"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=1;min_age=35;max_age=90;min_year=2004;max_year=2017;max_samples_per_id=4\" --out_samples ./train_1.samples\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --cohort_fname ./cvd_mi.cohort --cohort_sampling \"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=2;min_age=35;max_age=90;min_year=2004;max_year=2017;max_samples_per_id=4\" --out_samples ./train_2.samples\n\u00a0\n\u00a0\n# now filter and match on train_1 samples : making sure samples have needed information, and raising the case to control ratio in learning set\n# on the validation set we only apply the filter for needed information\n# generate validate_1 validate_2 subsets\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --filter_and_match --in_samples ./train_1.samples --out_samples ./validate_1.samples --filter_params \"min_sample_time=20040101;max_sample_time=20160101;bfilter=sig_name,BP,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,Glucose,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,LDL,win_from,0,win_to,730,min_Nvals,1;min_bfilter=1\"\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --filter_and_match --in_samples ./train_2.samples --out_samples ./validate_2.samples --filter_params \"min_sample_time=20040101;max_sample_time=20160101;bfilter=sig_name,BP,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,Glucose,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,LDL,win_from,0,win_to,730,min_Nvals,1;min_bfilter=1\"\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --filter_and_match --in_samples ./validate_1.samples --out_samples ./learn_1.samples --match_params \"priceRatio=200;maxRatio=10;verbose=1;strata=time,year,1\"\n\u00a0\n# prepare pids groupA (for embedding) and groupB (for model)\nless validate_1.samples | awk '(NR&gt;1){print $2}' | uniq | awk '{print $1, 1+(rand()&lt;0.5)}' &gt; train_pids_groups\nless train_pids_groups | awk '($2==1){print $1}' &gt; pids_group_A\nless train_pids_groups | awk '($2==2){print $1}' &gt; pids_group_B\n# prepare validate_1_A , validate_1_B , learn_1_A , learn_1_B\nintersect.pl validate_1.samples 1 pids_group_A 0 &gt; validate_1_A.samples\nintersect.pl validate_1.samples 1 pids_group_B 0 &gt; validate_1_B.samples\nintersect.pl learn_1.samples 1 pids_group_A 0 &gt; learn_1_A.samples\nintersect.pl learn_1.samples 1 pids_group_B 0 &gt; learn_1_B.samples\n</code></pre> \u00a0 We now intend to use validate_1_A.samples or learn_1_A.samples to design an embedding training, and then use learn_1_B.samples to train a predictor for cvd_mi, once without embeddings signals, and once with, and compare the results (on validate_2_B in CV, and eventually on validate_2).</p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-3-design-and-prepare-the-training-dates-for-embeddings","title":"Step 3 : Design and prepare the training dates for embeddings","text":"<p>To train the embedding we need: 1. x matrix , at some points in time, could be random points, or points similar to how we choose validation or learning samples. We will use the pid, time of validate_1_A.samples for that (~1.6M points, ~660K patients) 2. y matrix : lots of options:</p> <ol> <li>take y to be x : this is exactly learning an autoencoder.</li> <li> <p>simply take the y time to be x time + some translation t from a set T of optional time translations:</p> </li> <li> <p>T =\u00a0{0} (semi-autoencoder) , T={365} (one year forward) , T={730} (two years forward) , T={-365} (one year backwards) etc... </p> </li> <li>T={0,365,730} , all options or random choice of one for each sample. We will use two options: semi-autoencoder (T for y is 0), and option 2b , and randomly set the y time to be 1y or 2y ahead from the sampling point. We keep that in a samples file where the time is the time for x and the outcomeTime is the time for y. Our code later supports this.</li> </ol> <p><pre><code># preparing embedding_1_A.samples (using awk this time)\n# we could have started from learn_1_A samples, but decided to be more general and use lots of data, hence starting from validate_1_A\n\u00a0\nless validate_1_A.samples | awk '(/EVENT/){print}(/SAMPLE/){a=1+(rand()&gt;0.5);a=a*10000;print \"SAMPLE\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$3+a\"\\t\"$6}' &gt; embedding_1_A.samples\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-4-prepare-the-embed_params-files-for-x-and-y","title":"Step 4 : Prepare the embed_params files for x and y","text":"<p>The embed_params file holds all the parameters needed in order to create a sparse matrix for an embedding. For categorial signals we can choose the sets we are interested in (could be a very large number), and a few more settings such as time windows, if to keep as counts, if to shrink, etc. For continous signals we can choose the ranges we are interested in. We can also use a MedModel generated before from a json file and add the features it creates (this option is still in dev/testing) We could use the same embed_params for x and y matrices, but to make things interesting we will use different plans for the x and y matrices. some preparations:</p> <p><pre><code># preparing the list of read codes we'll use to generate features (that's ~97K features !)\nless /home/Repositories/THIN/thin_jun2017/dict.read_codes | awk '(/DEF/ &amp;&amp; substr($3,0,2)==\"G_\"){print $3}' &gt; rc.codes\n\n### preparing the list of atc codes we'll use to generate features (~6k features)\nless /home/Repositories/THIN/thin_jun2017/dict.drugs_defs | awk '(/DEF/ &amp;&amp; substr($3,0,4)==\"ATC_\"){print $3}' | grep -v \":\" &gt; atc.codes\n</code></pre> \u00a0 x_embed_params :</p> <p><pre><code>sigs={\n# sigs are sig=&lt;&gt; format with | between sigs\n# dummy signal is needed to make sure there's at least one entry for each line , helps in matching x,y lines\nsig=dummy;type=dummy|\n# age category with a binning to 10 groups\nsig=BYEAR;type=age;ranges=0,10,20,30,40,50,60,70,80,90,1000;do_shrink=0|\n# gender with 2 categories\nsig=GENDER;type=continuous;ranges=1,2,3;do_shrink=0|\n# RC signals : categorial , use rc.codes list as categories, look at a large (ever) time window, use hierarchy and count each category used\nsig=RC;type=categorial;categories=list:rc.codes;win_from=0;win_to=36500;add_hierarchy=1;do_counts=1|\n# RC signals : categorial same as previous but for a short (1y) time window\nsig=RC;type=categorial;categories=list:rc.codes;win_from=0;win_to=365;add_hierarchy=1;do_counts=1|\n# Drug signals , last 2 years\nsig=Drug;type=categorial;categories=list:atc.codes;win_from=0;win_to=730;add_hierarchy=1;do_counts=1|\n# Adding LDL distribution to bins in last 5 years\nsig=LDL;type=continuous;ranges=0,50,70,100,120,150,200,300,10000;do_shrink=0;win_from=0;win_to=1800;do_counts=1|\n# Adding Glucose distribution to bins in last 5 years\nsig=Glucose;type=continuous;ranges=0,50,70,90,100,110,125,150,200,10000;do_shrink=0;win_from=0;win_to=1800;do_counts=1|\n# Adding HbA1C distribution to bins in last 5 years\nsig=HbA1C;type=continuous;ranges=0,4.0,5.0,5.7,6.0,6.5,7.0,8.0,10.0,10000;do_shrink=0;win_from=0;win_to=1800|\n# Adding Cretinine distribution to bins in last 5 years\nsig=Creatinine;type=continuous;ranges=0,0.5,0.8,1.0,1.2,1.5,2.0,3.0,4.0,10000;do_shrink=0;win_from=0;win_to=1800|\n# Adding BP distribution to bins in last 5 years\nsig=BP;type=continuous;val_chan=0;ranges=0,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,10000;do_shrink=0;win_from=0;win_to=1800|\nsig=BP;type=continuous;val_chan=1;ranges=0,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,10000;do_shrink=0;win_from=0;win_to=1800\n};\n</code></pre> \u00a0 y_embed params: preferring do_counts=0 as easier to build loss function for. <pre><code>sigs={\n# sigs are sig=&lt;&gt; format with | between sigs\n# dummy signal is needed to make sure there's at least one entry for each line , helps in matching x,y lines\nsig=dummy;type=dummy|\n# age category with a binning to 10 groups\nsig=BYEAR;type=age;ranges=0,10,20,30,40,50,60,70,80,90,1000;do_shrink=0|\n# gender with 2 categories\nsig=GENDER;type=continuous;ranges=1,2,3;do_shrink=0|\n# RC signals : categorial same as previous but for a short (1y) time window\nsig=RC;type=categorial;categories=list:rc.codes;win_from=0;win_to=365;add_hierarchy=1;do_counts=0|\n# Drug signals , last 2 years\nsig=Drug;type=categorial;categories=list:atc.codes;win_from=0;win_to=365;add_hierarchy=1;do_counts=0|\n# Adding LDL distribution to bins in last year\nsig=LDL;type=continuous;ranges=0,50,70,100,120,150,200,300,10000;do_shrink=0;win_from=0;win_to=365;do_counts=0|\n# Adding Glucose distribution to bins in last year\nsig=Glucose;type=continuous;ranges=0,50,70,90,100,110,125,150,200,10000;do_shrink=0;win_from=0;win_to=365;do_counts=0|\n# Adding HbA1C distribution to bins in last year\nsig=HbA1C;type=continuous;ranges=0,4.0,5.0,5.7,6.0,6.5,7.0,8.0,10.0,10000;do_shrink=0;win_from=0;win_to=365;do_counts=0|\n# Adding Past/Future Registries: past, 1y ahead, 5y ahead\n# In y matrices it is perfectly OK to peek into the future (!) we only use them for training\nsig=CVD_MI;type=categorial;categories=CVD_MI_Confirmed_Event,CVD_MI_Untimed_Event,CVD_MI_History;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CVD_MI;type=categorial;categories=CVD_MI_Confirmed_Event,CVD_MI_Untimed_Event,CVD_MI_History;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_MI;type=categorial;categories=CVD_MI_Confirmed_Event,CVD_MI_Untimed_Event,CVD_MI_History;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_IschemicStroke;type=categorial;categories=CVD_IschemicStroke_Confirmed_Event,CVD_IschemicStroke_Untimed_Event,CVD_IschemicStroke_History;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CVD_IschemicStroke;type=categorial;categories=CVD_IschemicStroke_Confirmed_Event,CVD_IschemicStroke_Untimed_Event,CVD_IschemicStroke_History;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_IschemicStroke;type=categorial;categories=CVD_IschemicStroke_Confirmed_Event,CVD_IschemicStroke_Untimed_Event,CVD_IschemicStroke_History;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HemorhagicStroke;type=categorial;categories=CVD_HemorhagicStroke_Confirmed_Event,CVD_HemorhagicStroke_Untimed_Event,CVD_HemorhagicStroke_History;win_from=0;win_to=0;do_counts=10000;do_shrink=0|\nsig=CVD_HemorhagicStroke;type=categorial;categories=CVD_HemorhagicStroke_Confirmed_Event,CVD_HemorhagicStroke_Untimed_Event,CVD_HemorhagicStroke_History;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HemorhagicStroke;type=categorial;categories=CVD_HemorhagicStroke_Confirmed_Event,CVD_HemorhagicStroke_Untimed_Event,CVD_HemorhagicStroke_History;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HeartFailure;type=categorial;categories=CVD_HeartFailure_First_Indication;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CVD_HeartFailure;type=categorial;categories=CVD_HeartFailure_First_Indication;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HeartFailure;type=categorial;categories=CVD_HeartFailure_First_Indication;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CKD_State;type=categorial;categories=CKD_State_Normal,CKD_State_Level_1,CKD_State_Level_2,CKD_State_Level_3,CKD_State_Level_4;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CKD_State;type=categorial;categories=CKD_State_Normal,CKD_State_Level_1,CKD_State_Level_2,CKD_State_Level_3,CKD_State_Level_4;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CKD_State;type=categorial;categories=CKD_State_Normal,CKD_State_Level_1,CKD_State_Level_2,CKD_State_Level_3,CKD_State_Level_4;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=DM_Registry;type=categorial;categories=DM_Registry_Pre_diabetic,DM_Registry_Diabetic;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=DM_Registry;type=categorial;categories=DM_Registry_Pre_diabetic,DM_Registry_Diabetic;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=DM_Registry;type=categorial;categories=DM_Registry_Pre_diabetic,DM_Registry_Diabetic;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=HT_Registry;type=categorial;categories=HT_Registry_Hypertensive;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=HT_Registry;type=categorial;categories=HT_Registry_Hypertensive;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=HT_Registry;type=categorial;categories=HT_Registry_Hypertensive;win_from=-1825;win_to=0;do_counts=0;do_shrink=0\n};\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-5-create-the-x-and-y-matrices","title":"Step 5: Create the x and y matrices","text":"<p>Finally we are ready for this stage. We will start with our samples file, and create 2 matrices from it, the x will use time, the y will use outcomeTime. The matrices will use the embed rules we defined in the previous step, and we will also shrink them to only contain values that appear at least 1e-3 of the samples (to have roughly at least ~1000 cases of it appearing), and less than 0.75 of the samples , to screen to frequent or too rare columns. Our samples file is the one we prepared before: embedding_1_A.samples (1.6M training points) To actually create the matrices we use the Embeddings project, with the --gen_mat option.</p> <p><pre><code># command line to create x matrix\nEmbeddings --gen_mat --rep &lt;rep&gt; --f_samples ./embedding_1_A.samples --embed \"pFile=x_embed_params\" --min_p 0.001 --max_p 0.75 --prefix x\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#files-created-ls-l","title":"files created (ls -l) :","text":"<pre><code>-rwxrwxrwx 1 root root 6521109879 Feb 13 17:58 x.smat           &lt;---- the sparse matrix. And yes, that is a 6.5GB matrix....\n-rwxrwxrwx 1 root root   40597062 Feb 13 18:00 x.meta           &lt;---- pid,time for each line (same number of lines as in our samples file)\n-rwxrwxrwx 1 root root    1892057 Feb 13 18:00 x.scheme         &lt;---- our serialized x scheme file for this matrix : we will need it when we later create features within a model !\n-rwxrwxrwx 1 root root    1256521 Feb 13 18:00 x.dict           &lt;---- the names of all the columns in the shrunk matrix\n</code></pre>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#and-now-creating-the-y-matrix","title":"and now creating the y matrix","text":"<p><pre><code>Embeddings --gen_mat --rep &lt;rep&gt; --f_samples ./embedding_1_A.samples --embed \"pFile=./y_embed_params\" --min_p 0.001 --max_p 0.75 --prefix y\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-6-train-the-embedding-using-keras","title":"Step 6: Train the embedding using Keras","text":"<p>We are now at a state in which we have our x &amp; y matrices ready. Our goal now is to calculate a deep learning model starting from x[i] and ending in y[i], while flowing through a narrow layer with few (say 100-200) neurons.\u00a0 To do that use the Embedder.py script, or copy it and change what you need inside. The Embedder.py script allows the following:</p> <ol> <li>Train through 3 layers (last is the embedding)\u00a0</li> <li>Train through 5 simetric layers with the middle as the embedding layer.</li> <li> <p>Control parameters of network:</p> </li> <li> <p>layers sizes</p> </li> <li> <p>l1, l2, dropout regularizers on each layer</p> </li> <li>control of extra weight given to cases in the loss function</li> <li>control leaky ReLU function parameters</li> <li>control noise added to embedding layer in training</li> <li>control number of epochs in training, also allow to continue training of a saved model.</li> <li>savind model in a way that can be later used in the infrastructure (.layers file)</li> <li>test modes to generate predictions/embedding layer results on a set of examples (to allow testing vs. the infrastructure) The Embedder.py script is in the git in .../MR/Projects/Shared/Embeddings/scripts/ \u00a0 <pre><code># example run training a model\npython ./Embedder.py --xfile ./x.smat --yfile ./y.smat --nepochs 5 --dim 400 200 100 --l1 1e-7 0 0 --out_model emodel --train --wgt 10 --dropout 0.0 0.0 0.0 --noise 0.3 --gpu 1 --full_decode\n\u00a0\n# output explained ... : (# lines added to explain)\nUsing TensorFlow backend.\n# full parameter list\n('arguments----&gt;', Namespace(dim=[400, 200, 100], dropout=[0.0, 0.0, 0.0], embed=False, full_decode=True, gpu=[1], in_model='', l1=[1e-07, 0.0, 0.0], l2=[0.0, 0.0, 0.0], leaky=[0.1], nepochs=[5], no_shuffle=False, noise=[0.3], out_model=['emodel'], test=False, train=True, wgt=[10.0], xdim=-1, xfile=['./x.smat'], ydim=-1, yfile=['./y.smat']))\n# using only gpu 1 , not setting this will allow running on all gpus in parallel, or on another (say gpu 0)\nusing gpu  1\nTrain mode\n# reading input matrices (can be quite slow, as matrices can be huge)\n('reading: ', ['./x.smat'], ['./y.smat'])\nreading csv file:  ./x.smat 13:11:18.800850\npreparing sparse mat 13:14:27.758752\nreading csv file:  ./y.smat 13:14:40.032091\npreparing sparse mat 13:15:34.727952\n# printing some basic info on sparse matrices x,y : note for example the ratio of 0 to non 0 in x is 43.1 and in y 48.08 (!), showing how sparse they are.\n('xtrain : ', (1623366, 11933), &lt;1623366x11933 sparse matrix of type '&lt;type 'numpy.float32'&gt;'\n        with 451035611 stored elements in Compressed Sparse Row format&gt;, 'non zero: ', 449412245, 43.10435840038137)\n('ytrain : ', (1623366, 3986), &lt;1623366x3986 sparse matrix of type '&lt;type 'numpy.float32'&gt;'\n        with 136188699 stored elements in Compressed Sparse Row format&gt;, ' non zero: ', 134565333, 48.08621010881012)\n('ORIGDIM----&gt; ', 11933, 3986, -1, -1)\n# and now the actual training results\nRunning model definition\n10.0\n10.0\nTraining on xtrain\nTrain on 1298692 samples, validate on 324674 samples\nEpoch 1/5\n2019-02-20 13:15:51.200332: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-02-20 13:15:52.226714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:81:00.0\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\n2019-02-20 13:15:52.226779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2019-02-20 13:15:52.504954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)\n# each epoch has a line , giving results on train and validation, the validation set is always the last 20% of the data.\n# see below some more explaining on how to understand the measures and results printed.\n1298692/1298692 [==============================] - 249s 192us/step - loss: 0.1734 - w_bin_cross: 0.1637 - espec: 0.0289 - enpv: 0.0039 - ppv: 0.3799 - sens: 0.8203 - val_loss: 0.1578 - val_w_bin_cross: 0.1476 - val_espec: 0.0232 - val_enpv: 0.0037 - val_ppv: 0.4295 - val_sens: 0.8253\nEpoch 2/5\n1298692/1298692 [==============================] - 302s 233us/step - loss: 0.1339 - w_bin_cross: 0.1238 - espec: 0.0240 - enpv: 0.0028 - ppv: 0.4370 - sens: 0.8735 - val_loss: 0.1362 - val_w_bin_cross: 0.1261 - val_espec: 0.0219 - val_enpv: 0.0030 - val_ppv: 0.4558 - val_sens: 0.8564\nEpoch 3/5\n1298692/1298692 [==============================] - 329s 253us/step - loss: 0.1221 - w_bin_cross: 0.1120 - espec: 0.0224 - enpv: 0.0024 - ppv: 0.4584 - sens: 0.8885 - val_loss: 0.1345 - val_w_bin_cross: 0.1245 - val_espec: 0.0214 - val_enpv: 0.0029 - val_ppv: 0.4626 - val_sens: 0.8640\n# note epoch 4 started to climb back in val_loss ... this might be a sign for a need to better regularize the model, as it seems to start entering an overfit state\nEpoch 4/5\n1298692/1298692 [==============================] - 304s 234us/step - loss: 0.1156 - w_bin_cross: 0.1056 - espec: 0.0215 - enpv: 0.0023 - ppv: 0.4706 - sens: 0.8967 - val_loss: 0.1542 - val_w_bin_cross: 0.1442 - val_espec: 0.0306 - val_enpv: 0.0025 - val_ppv: 0.3752 - val_sens: 0.8867\n# last epoch , seems val_loss started to go down again, so we got a better model at the end. Still loss is smaller than val_loss, showing a potential for better regularizing.\nEpoch 5/5\n 1298692/1298692 [==============================] - 333s 257us/step - loss: 0.1111 - w_bin_cross: 0.1011 - espec: 0.0209 - enpv: 0.0021 - ppv: 0.4798 - sens: 0.9024 - val_loss: 0.1213 - val_w_bin_cross: 0.1114 - val_espec: 0.0216 - val_enpv: 0.0024 - val_ppv: 0.4627 - val_sens: 0.8862\n# now follows is the ending in which the output files are created \n\u00a0Writing model to files\n('History: ', &lt;keras.callbacks.History object at 0x6fa5690&gt;, {'val_espec': [0.023174003757131564, 0.021862880609978624, 0.021418394041715545, 0.03055855377804435, 0.021563341807379736], 'val_w_bin_cross': [0.14756845901125903, 0.12613012184193115, 0.12453776885398905, 0.14417843291107432, 0.11137183862486143], 'val_sens': [0.8253292080568654, 0.8563854553455446, 0.8639981839122339, 0.8866606718251948, 0.8861758074101623], 'val_enpv': [0.0037238269219488115, 0.003042069682342851, 0.002891325553422261, 0.0024709458833944, 0.0024423540647124592], 'val_ppv': [0.4295036114702129, 0.4557987501766982, 0.4626011239199966, 0.3751608240688261, 0.4627356967393517], 'enpv': [0.003947158052682541, 0.0027687939362956364, 0.002437750546613879, 0.0022583779117050936, 0.002131666025525145], 'val_loss': [0.15775366971853721, 0.13624100354358956, 0.13452252385947847, 0.15421950637508852, 0.12133861386228599], 'ppv': [0.3798841048923378, 0.43695378672384044, 0.4583713526026837, 0.47056864791872133, 0.4798394423932855], 'w_bin_cross': [0.16372003288450798, 0.12379132172316154, 0.11200136639712739, 0.10560965182129696, 0.10111237579286037], 'sens': [0.8202563503519625, 0.8735020021500306, 0.8885229580472642, 0.8966689669803886, 0.9024332567701953], 'loss': [0.17342430566917966, 0.13392729285731664, 0.1220678161909718, 0.11563198370958765, 0.11113599371116904], 'espec': [0.028870824286393815, 0.02403844574438139, 0.02241259005779445, 0.021534086913695155, 0.020883613924218173]})\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_1 (InputLayer)         (None, 11933)             0\n_________________________________________________________________\ndropout_1 (Dropout)          (None, 11933)             0\n_________________________________________________________________\ndense_1 (Dense)              (None, 400)               4773600\n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 400)               0\n_________________________________________________________________\ndropout_2 (Dropout)          (None, 400)               0\n_________________________________________________________________\ndense_2 (Dense)              (None, 200)               80200\n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 200)               0\n_________________________________________________________________\ndropout_3 (Dropout)          (None, 200)               0\n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               20100\n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 100)               0\n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400\n_________________________________________________________________\ngaussian_noise_1 (GaussianNo (None, 100)               0\n_________________________________________________________________\ndropout_4 (Dropout)          (None, 100)               0\n_________________________________________________________________\ndropout_5 (Dropout)          (None, 100)               0\n_________________________________________________________________\ndense_4 (Dense)              (None, 200)               20200\n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 200)               0\n_________________________________________________________________\ndropout_6 (Dropout)          (None, 200)               0\n_________________________________________________________________\ndense_5 (Dense)              (None, 400)               80400\n_________________________________________________________________\ndense_6 (Dense)              (None, 3986)              1598386\n=================================================================\nTotal params: 6,573,286\nTrainable params: 6,573,086\nNon-trainable params: 200\n_________________________________________________________________\nPrinting model layers\nNLAYERS= 19\nLAYER type=dropout;name=dropout_1;drop_rate=0.000000\nLAYER type=dense;name=dense_1;activation=linear;in_dim=11933;out_dim=400;n_bias=400\nLAYER type=leaky;name=leaky_re_lu_1;leaky_alpha=0.100000\nLAYER type=dropout;name=dropout_2;drop_rate=0.000000\nLAYER type=dense;name=dense_2;activation=linear;in_dim=400;out_dim=200;n_bias=200\nLAYER type=leaky;name=leaky_re_lu_2;leaky_alpha=0.100000\nLAYER type=dropout;name=dropout_3;drop_rate=0.000000\nLAYER type=dense;name=dense_3;activation=linear;in_dim=200;out_dim=100;n_bias=100\nLAYER type=leaky;name=leaky_re_lu_3;leaky_alpha=0.100000\nLAYER type=batch_normalization;name=batch_normalization_1;dim=100\nLAYER type=dropout;name=dropout_4;drop_rate=0.000000\nLAYER type=dropout;name=dropout_5;drop_rate=0.000000\nLAYER type=dense;name=dense_4;activation=linear;in_dim=100;out_dim=200;n_bias=200\nLAYER type=leaky;name=leaky_re_lu_4;leaky_alpha=0.100000\nLAYER type=dropout;name=dropout_6;drop_rate=0.000000\nLAYER type=dense;name=dense_5;activation=linear;in_dim=200;out_dim=400;n_bias=400\nLAYER type=dense;name=dense_6;activation=sigmoid;in_dim=400;out_dim=3986;n_bias=3986\n# that's it !! we succesfully trained the model\n# at the end these are the files created :\n# file created by keras to contain the model structure\n-rwxrwxrwx. 1 root root       6599 Feb 20 13:41 emodel.json\n# the model parameters in keras bin format\n-rwxrwxrwx. 1 root root   78938688 Feb 20 13:41 emodel.h5\n# the model parameters in our simple .layers format that can be read using our infrastructure\n-rwxrwxrwx. 1 root root   62422454 Feb 20 13:41 emodel.layers\n# keeping the command line used to create the model : very useful and important if going to continue training or repeat it\n-rwxrwxrwx. 1 root root        170 Feb 20 13:41 emodel_command_line.txt\n# history of results on train and validation, can be used to draw charts of model training process\n-rwxrwxrwx. 1 root root      35160 Feb 20 13:41 emodel.history\n# you can pick up a model you trained and continue its training. Here we use the emodel trained before to continue and save into emodel2\n# the model will initialize itself from the emodel.json and emodel.h5 files, and then continue training and save to emodel2\npython ./Embedder.py --xfile ./x.smat --yfile ./y.smat --nepochs 5 --out_model emodel2 --in_model emodel --train --gpu 1\n\u00a0\n# skipping ...\n\u00a0\nEpoch 1/5\n1298692/1298692 [==============================] - 294s 227us/step - loss: 0.1073 - w_bin_cross: 0.0975 - espec: 0.0204 - enpv: 0.0020 - ppv: 0.4870 - sens: 0.9074 - val_loss: 0.1181 - val_w_bin_cross: 0.1082 - val_espec: 0.0185 - val_enpv: 0.0026 - val_ppv: 0.5010 - val_sens: 0.8770\nEpoch 2/5\n1298692/1298692 [==============================] - 226s 174us/step - loss: 0.1052 - w_bin_cross: 0.0954 - espec: 0.0202 - enpv: 0.0020 - ppv: 0.4904 - sens: 0.9101 - val_loss: 0.1156 - val_w_bin_cross: 0.1059 - val_espec: 0.0181 - val_enpv: 0.0025 - val_ppv: 0.5089 - val_sens: 0.8804\n# ...\n# we were able to improve a little more.\n\u00a0\n</code></pre> Which is the Embedding layer? The one just before the gaussian noise. We take the third layer, then batch normalize it , so that it is in the N(0,1) distribution on each channel, and then add noise (only in training). This is done in order to make sure our embedding layer gives numbers in a reasonable numerical range, is normalized, and that it is immune to noising each channel, making it a more stable embedding. \u00a0 Some explanation on the model loss and evaluation:</li> </ol> <ul> <li>Loss function : we treat the y vector as a binary prediction goal, and to the whole problem as predicting together a vector of binary predictions (in the example above a vector of length 3986). The loss we take is the logloss on each channel , summed over all channels. You will note that our last layer before getting to the y layer is using sigmoid as activation into the y layer, hence predicting a probablily for each channel. Since the y vectors are very sparse we multiply by a weight the cases (y[i][j] == 1) , pushing the model to try better to be right on 1 predictions rather than 0 predictions.</li> <li>Evaluation : last 20% of x.y matrices (as they appear in the input files) are always used as evaluation data for the embedding training process, and you see that evaluation after each epoch.</li> <li>Evaluation metrics (all metrics with val_ prefix are the same but on the validation 20% set). All our done at a point which sets &gt;=0.5 predictions as 1 and the others as 0.<ul> <li>loss : overall loss value (as explained above). Big differences between train and validation group hints towards over fit we can try to regilarize (finding the best regularization usually improves results). Note that when using large data sets less regularization is needed. More data is always the best regularizer.</li> <li>w_bin_cross : loss without the regularization terms (l1, l2).</li> <li>ppv : #(true==1 &amp;&amp; prob&gt;=0.5) / # (prob &gt;= 0.5) : the probability of being right when predicting positive (larger is better)</li> <li>sens : #(true==1 &amp;&amp; prob&gt;=0.5) / # (true == 1) : how many of the positives caught when predicting positive (larger is better)</li> <li>enpv : #(true==1 &amp;&amp; prob&lt;0.5) / #(prob&lt;0.5) : the probability for error when predicting negative (lower is better)</li> <li>espec : 1-#(true==0 &amp;&amp; prob&lt;0.5)/#(true == 0) : percentage of true negatives not predicted right out of all negatives (lower is better) \u00a0</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-7-testing-we-get-the-same-embeddings-in-keras-and-infrastructure","title":"Step 7 : Testing we get the same embeddings in Keras and Infrastructure","text":"<p>This is a needed sanity in order to verify the model we trained is indeed the one our infrastructure will use.</p> <p><pre><code># assuming we prepared t_1.samples : a samples file with a single line\n# we first create a sparse mat for this sample, we use the x.scheme file that was created when we first generated the x matrix\nEmbeddings --gen_mat_from_scheme --f_samples ./t_1.samples --f_scheme ../x.scheme --prefix xtest\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#this-created-the-xtestsmat-file","title":"this created the xtest.smat file","text":""},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#we-can-now-check-it-directly-with-keras-using-the-following-line-the-model-will-initialize-from-the-json-and-h35-files","title":"we can now check it directly with keras using the following line (the model will initialize from the .json and .h35 files):","text":"<p><pre><code>python ../Embedder.py --embed --in_model ../emodel --xfile ./xtest.smat  \n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#result-is","title":"result is :","text":"<pre><code>Using TensorFlow backend.\n('arguments----&gt;', Namespace(dim=[400, 200, 100], dropout=[0.0, 0.0, 0.0], embed=True, full_decode=True, gpu=[0], in_model=['../emodel'], l1=[1e-07, 0.0, 0.0], l2=[0.0, 0.0, 0.0], leaky=[0.1], nepochs=-1, no_shuffle=False, noise=[0.3], out_model='', test=False, train=False, wgt=[10.0], xdim=[11933], xfile=['./xtest.smat'], ydim=[3986], yfile='my_y.smat'))\nusing gpu  0\nEmbed mode\n('reading: ', ['./xtest.smat'])\nreading csv file:  ./xtest.smat 18:14:50.269859\npreparing sparse mat 18:14:50.275748\n('ORIGDIM----&gt; ', 11933, 3986, [11933], [3986])\nRunning model definition\n10.0\n10.0\n2019-02-20 18:14:50.625090: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-02-20 18:14:51.702925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:02:00.0\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\n2019-02-20 18:14:51.702988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2019-02-20 18:14:52.051737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\nGenerating an embedding (for testing)\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_1 (InputLayer)         (None, 11933)             0\n_________________________________________________________________\ndropout_1 (Dropout)          (None, 11933)             0\n_________________________________________________________________\ndense_1 (Dense)              (None, 400)               4773600\n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 400)               0\n_________________________________________________________________\ndropout_2 (Dropout)          (None, 400)               0\n_________________________________________________________________\ndense_2 (Dense)              (None, 200)               80200\n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 200)               0\n_________________________________________________________________\ndropout_3 (Dropout)          (None, 200)               0\n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               20100\n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 100)               0\n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400\n=================================================================\nTotal params: 4,874,300\nTrainable params: 4,874,100\nNon-trainable params: 200\n_________________________________________________________________\n[[ 0.5779214  -1.1393576  -1.2145319   0.719882    1.152698    2.7561216\n   4.7062497  -1.9202161   1.4628897  -0.28924894 -5.909066   -3.8524284\n   0.89937544 -1.7450757  -1.6244755   4.2109747   1.8999414   2.4285064\n   2.119122   -1.1817946   2.5119438   2.4076338  -1.6450381   2.5745249\n   1.3517776   5.6222005   3.1432505   0.97051287 -1.9321308   1.6599026\n  -1.135961    4.1045027  -5.8931293  -0.9852152  -0.22657108  3.0653677\n   2.6770768   1.325161   -3.7317533   0.78288555  2.749711   -0.71579885\n  -0.6447115  -0.54217243  2.1220136   0.16115046  1.6993942   4.197215\n  -1.103509   -2.8239236  -2.4093542   1.7677689  -3.7956572   1.0639849\n  -4.50788    -0.5759249   1.5213671  -0.01162529  2.6363235  -2.1107693\n   4.447809    4.9245405   1.1112337   1.6995897   2.810319   -2.2490163\n  -0.57630396  3.6462874   2.2628431   5.652481   -2.4730139   5.1024203\n   3.0555716   2.2935085  -1.472085   -0.3556919   0.70053387  3.6700687\n   4.944693   -6.186803    1.0003986   3.6284337  -4.352747    3.9384065\n  -0.5298457   1.7390456   0.1143899   1.6540909   2.9988813   0.4738245\n  -5.7533255   5.286108    4.581069    1.6037283  -0.42494774  2.3381634\n  -5.95912     2.1251287   5.0158515  -0.31940365]]\n</code></pre> <p>We now want to do the same using our layers file and infrastructure: layer 9 is usually the layer of the Embedding if you didn't change the Embedder.py script to run a different network</p> <p><pre><code>./Embeddings --get_embedding --f_samples ./t_1.samples --f_scheme ../x.scheme --f_layers ../emodel.layers --to_layer 9\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#results","title":"results ...","text":"<p><pre><code>initializing rep /home/Repositories/THIN/thin_jun2017/thin.repository\nRead 0 signals, 0 pids :: data  0.000GB :: idx  0.000GB :: tot  0.000GB\nRead data time 0.102909 seconds\nread_binary_data_alloc [../x.scheme] with crc32 [-63879080]\nread_from_file [../x.scheme] with crc32 [-63879080] and size [1892057]\nMedSamples: reading ./t_1.samples\nWARNING: header line contains unused fields [EVENT_FIELDS,]\n[date]=2, [id]=1, [outcome]=3, [outcome_date]=4, [split]=5,\nread [1] samples for [1] patient IDs. Skipped [0] records\nsorting samples by id, date\nGenerating sparse mat for 1 lines\nReading layers file ../emodel.layers\nApplyKeras: Reading 0 : LAYER   type=dropout;name=dropout_1;drop_rate=0.000000\nApplyKeras: Reading 1 : LAYER   type=dense;name=dense_1;activation=linear;in_dim=11933;out_dim=400;n_bias=400\nApplyKeras: Reading 2 : LAYER   type=leaky;activation=leaky;name=leaky_re_lu_1;leaky_alpha=0.100000\nApplyKeras: Reading 3 : LAYER   type=dropout;name=dropout_2;drop_rate=0.000000\nApplyKeras: Reading 4 : LAYER   type=dense;name=dense_2;activation=linear;in_dim=400;out_dim=200;n_bias=200\nApplyKeras: Reading 5 : LAYER   type=leaky;activation=leaky;name=leaky_re_lu_2;leaky_alpha=0.100000\nApplyKeras: Reading 6 : LAYER   type=dropout;name=dropout_3;drop_rate=0.000000\nApplyKeras: Reading 7 : LAYER   type=dense;name=dense_3;activation=linear;in_dim=200;out_dim=100;n_bias=100\nApplyKeras: Reading 8 : LAYER   type=leaky;activation=leaky;name=leaky_re_lu_3;leaky_alpha=0.100000\nApplyKeras: Reading 9 : LAYER   type=batch_normalization;name=batch_normalization_1;dim=100\nApplyKeras: Reading 10 : LAYER  type=dropout;name=dropout_4;drop_rate=0.000000\nApplyKeras: Reading 11 : LAYER  type=dropout;name=dropout_5;drop_rate=0.000000\nApplyKeras: Reading 12 : LAYER  type=dense;name=dense_4;activation=linear;in_dim=100;out_dim=200;n_bias=200\nApplyKeras: Reading 13 : LAYER  type=leaky;activation=leaky;name=leaky_re_lu_4;leaky_alpha=0.100000\nApplyKeras: Reading 14 : LAYER  type=dropout;name=dropout_6;drop_rate=0.000000\nApplyKeras: Reading 15 : LAYER  type=dense;name=dense_5;activation=linear;in_dim=200;out_dim=400;n_bias=400\nApplyKeras: Reading 16 : LAYER  type=dense;name=dense_6;activation=sigmoid;in_dim=400;out_dim=3986;n_bias=3986\nEmbedding[0] :  0.577931, -1.139361, -1.214531, 0.719889, 1.152678, 2.756100, 4.706246, -1.920236, 1.462890, -0.289272, -5.909092, -3.852452, 0.899393, -1.745082, -1.624470, 4.210994, 1.899885, 2.428522, 2.119110, -1.181803, 2.511931, 2.407662, -1.645038, 2.574553, 1.351748, 5.622195, 3.143245, 0.970485, -1.932137, 1.659895, -1.135962, 4.104524, -5.893116, -0.985202, -0.226589, 3.065342, 2.677109, 1.325170, -3.731724, 0.782854, 2.749709, -0.715786, -0.644691, -0.542146, 2.122007, 0.161158, 1.699387, 4.197234, -1.103530, -2.823920, -2.409330, 1.767769, -3.795663, 1.063999, -4.507883, -0.575921, 1.521367, -0.011617, 2.636321, -2.110771, 4.447805, 4.924530, 1.111231, 1.699584, 2.810309, -2.249007, -0.576356, 3.646280, 2.262861, 5.652493, -2.473031, 5.102398, 3.055582, 2.293494, -1.472100, -0.355688, 0.700531, 3.670066, 4.944662, -6.186786, 1.000359, 3.628438, -4.352755, 3.938388, -0.529864, 1.739041, 0.114385, 1.654087, 2.998851, 0.473810, -5.753366, 5.286112, 4.581021, 1.603754, -0.424938, 2.338176, -5.959126, 2.125135, 5.015850, -0.319395,\n</code></pre> \u00a0 and as can be easily seen we indeed create the same Embedding !! We're good to go and use this embedding in our infrastructure. note we got the same embedding up to ~1e-5 error which is just a numerical error difference. Our embedding is much more stable that this small difference since we trained it with added noise 4 orders of magnitude larger.\u00a0 Another way to test this is to use the Flow --get_json_mat option with the right samples file, and the right json file (see below) \u00a0\u00a0</p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-8-using-embedding-in-a-medmodel-finally","title":"Step 8 : Using Embedding In a MedModel (Finally\u00a0!!)","text":"<p>Once we have the .scheme file and the matching .layers file we can generate features using the \"embedding\" feature generator. This feature generator will generate for each sample it's sparse x line, then run it through the embedding model , and add the layer output as features in the MedFeatures matrix. To use embeddings as features add the following to your json</p> <p><pre><code>    { \"action_type\":    \"feat_generator\", \"fg_type\": \"embedding\", \n                        // the name of the feature will be FTR_&lt;num&gt;.&lt;name_prefix&gt;.col_&lt;embedding column number&gt; , if using several embedding FGs give them different name_prefix names\n                        \"name_prefix\" : \"Semi_AutoEncoder\",\n                        // your x matrix scheme file\n                        \"f_scheme\" : \"/nas1/Work/Users/Avi/test_problems/embedding_example/mats/x.scheme\",\n                        // your layers file\n                        \"f_layers\" : \"/nas1/Work/Users/Avi/test_problems/embedding_example/mats/emodel_auto_xx.layers\",\n                        // layer to use for embedding, typical is 9 if you used the default Embedder.py script\n                        \"to_layer\" : \"9\"},\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-9-results-for-mi-withwithout-embedding-withwithout-sigdep","title":"Step 9 : Results for MI with/without Embedding , with/without SigDep","text":"<p>We can now easily train models for our problem. We can use learn_1_B as our training samples and validate_1_B as our cross validation set, and validate_2 as an external test set. Doing these tests in 4 flavors:</p> <ul> <li>No categorial signals<ul> <li>We used\u00a0\"last\", \"min\", \"max\", \"avg\", \"last_delta\", \"last_time\" on several time windows\u00a0</li> <li>signals :\u00a0<ul> <li>\"Glucose\", \"BMI\", \"HbA1C\", \"Triglycerides\", \"LDL\", \"Cholesterol\", \"HDL\", \"Creatinine\", \"eGFR_CKD_EPI\", \"Urea\", \"Proteinuria_State\", \"ALT\", \"AST\", \"ALKP\", \"WBC\", \"RBC\", \"Hemoglobin\", \"Hematocrit\", \"RDW\", \"K+\", \"Na\", \"GGT\", \"Bilirubin\", \"CRP\",\"BP\"</li> </ul> </li> </ul> </li> <li>Used also Smoking information, age, gender</li> <li>Using Signal Dependency on Drugs and RC (choosing top 100 codes in each) to select correlated read codes.</li> <li>Using Embeddings:<ul> <li>Embeddings were trained once as an autoencoder (y matrix created at same times of x matrix) and once as a future-encoder in which y matrix was created on times 1 or 2 years a head.</li> <li>Embedding dimension was 100</li> </ul> </li> <li>Using both SigDep and Embeddings together. \u00a0 Predictor used was the same for all cases: lightgbm with slightly optimized parameters. Results are given for the 30-730 time window, ages 40-80 , we compare AUCs</li> </ul> Model CV AUC Test AUC Labs 0.762 0.763 +SigDep 0.782 0.782 +Embeddings 0.785 0.787 +SigDep +Embeddings 0.787 0.789 <p>CI intervals are more or less +- 0.005 Some points:</p> <ul> <li>We see a very minor improvement for using Embeddings.<ul> <li>This is a nice result , as it verifies the whole concept works.</li> <li>There is a trend towards better results even in this model, and when going to 3y, 5y time windows it gets stronger</li> </ul> </li> <li>Since the SigDep option is much simpler also when interprating the models , it is not clear Embeddings are preffered in this problem.</li> <li>On the other hand the Embedding option is quite general and the same set of features could be used in many different models.</li> <li>Also : it may be that finiding/training the right Embedding model will give an even larger boost.</li> <li>interestingly there was also a very small improvement trend when using both together:<ul> <li>Means most of the information captured by both methods is the same, but still each has some small information parts the other doesn't. \u00a0 \u00a0 \u00a0 \u00a0</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html","title":"MedAlgo Library","text":""},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html#general","title":"General","text":"<p>The MedAlgo library (together with its accompanying algorithm libraries) is a general wrapper for several ML algorithms allowing learn , predict and parameters configuration. \u00a0 \u00a0 General Usage Example: \u00a0 Using MedPredictor <pre><code>#include &lt;MedAlgo/MedAlgo/MedAlgo.h&gt;\n\u00a0\n// .... create your train and test matrices ...\nMedMat&lt;float&gt; Xtrain,Ytrain,Xtest,Ytest;\n\u00a0\n// define a MedPredictor pointer\nMedPredicor *predictor;\n\u00a0\n// Create model of choice (here in example linear model)\npredictor = MedPredictor::make_predictor(\"linear_model\");\n\u00a0\n// Initialize parameters for this model (here in example putting rfactor , the 1-ridge , to be 0.9)\npredictor-&gt;init_from_string(\"rfactor=0.9\");\n\u00a0\n// Learn on Train , this will build a model. Here we use MedMat&lt;float&gt; matrices for X and Y\n// There are other API's as well - for example c like API's , please look at the MedPredictor class for more options\npredictor-&gt;learn(Xtrain,Ytrain);\n\u00a0\n// Predict on Test , here we use an API that uses an Xtest MedMat&lt;float&gt; for test, and a vector&lt;float&gt; for predictions\n// There are other API's as well - for example c like API's , please look at the MedPredictor class for more options\nvector&lt;float&gt; preds;\npredictor-&gt;predict(Xtest,preds);\n\u00a0\n// That's it ... now you have the predictions and can test the performance.\n// More on options to test performance and serialize and save models below.\n\u00a0\n\u00a0\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html#predictors-and-their-parameters","title":"Predictors and their parameters","text":""},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html#linear-model","title":"Linear Model","text":"<ul> <li>Use\u00a0MedPredictor::make_predictor(\"linear_model\")</li> <li>Parameters:<ul> <li>rfactor - 1.0: no ridge , closer to 0.0 - stronger ridge. Reccomended: 0.9 for regular runs, 0.3 for highly regularaized runs. Linear Model example <pre><code>MedPredicor *predictor;\npredictor = MedPredictor::make_predictor(\"linear_model\");\npredictor-&gt;init_from_string(\"rfactor=0.9\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html#xgboost","title":"XGBoost","text":"<ul> <li>Use\u00a0MedPredictor::make_predictor(\"xgb\")</li> <li>Parameters<ul> <li>seed</li> <li>booster</li> <li>objective</li> <li>eta - step size in each iteration , should be slow enough to avoid overfitting, and fast enough to get somewhere. If num_round is large, use a small eta and vice versa.</li> <li>num_round - how many trees to build. Running time is linear with this parameter.</li> <li>gamma</li> <li>max_depth - of trees</li> <li>min_child_weight - limiting size of leaves (larger = more regularization)</li> <li>missing_value - you can sign the algorithm what are the missing values (if there are any) in your matrix.</li> <li>lambda</li> <li>alpha</li> <li>scale_pos_weight - allows to fix imbalances in data</li> <li>tree_method \u00a0 XGBoost example <pre><code>MedPredicor *predictor;\npredictor = MedPredictor::make_predictor(\"xgb\");\npredictor-&gt;init_from_string(\"booster=gbtree;objective=binary:logistic;eta=0.05;gamma=1;max_depth=5;num_round=50;min_child_weight=6\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html#qrf","title":"QRF","text":"<ul> <li>Use\u00a0MedPredictor::make_predictor(\"qrf\")</li> <li>Parameters:<ul> <li>ntrees - number of trees to build</li> <li>maxq - max number of quantized cells for a parameter</li> <li>type - one of: binary , regression , categorical_chi2 , categorical_entropy</li> <li>min_node - split only nodes of size above this (larger = more regularization)</li> <li>ntry - how many random features to test in each node. -1 (or 0) is default and means sqrt(num_features), a specific number is the actual requested ntry. (smaller = more regularization)</li> <li>max_samp - how many samples to bag for each tree (total neg+pos). 0 means - bag at the number of input and is default. (smaller = more regularization)</li> <li>n_categ - number of categories. 0/1 for regression , 2 for binary problems, 3 and more for multicategorical data</li> <li>spread - in regression trees nodes with difference from max to min below spread will not split.</li> <li>sampsize - a vector (with , delimeter) stating how many samples to take for each tree from each category. example: sampsize=5000,1000 for a binary problem means bag 5000 neg and 1000 pos for each tree.</li> <li>get_count -\u00a0<ul> <li>0 : avg majority of nodes (less recommended)</li> <li>1 : avg probabilities in nodes (in regression = weighted average of nodes , taking their size into account) - recommended</li> <li>2: \u00a0avg counts in nodes - recommended</li> </ul> </li> <li>get_only_this_categ -\u00a0<ul> <li>-1 : get predictions for all categories one after the other (output size is nsamples*n_categ)</li> <li>0...n_categ-1 : get only the predictions for this categ (output size is nsamples)</li> </ul> </li> <li>learn_nthreads - how many threads to use in learn (use 8 for windows, and 24 for linux servers)</li> <li>predict_nthreads -\u00a0how many threads to use in predict (use 8 for windows, and 24 for linux servers) \u00a0 QRF example <pre><code>MedPredicor *predictor;\npredictor = MedPredictor::make_predictor(\"qrf\");\n\u00a0\n// classification example\npredictor-&gt;init_from_string(\"type=categorical_entropy;ntrees=200;min_node=30;n_categ=2;get_only_this_categ=1;sampsize=15000,5000;learn_nthreads=24;predict_nthreads=24\");\npredictor-&gt;learn(Xtrain,Ytrain);\n\u00a0\n// regression example\npredictor-&gt;init_from_string(\"type=regression;ntrees=200;min_node=100;n_categ=1;spread=0.1;learn_nthreads=24;predict_nthreads=24\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/index.html#gdlm","title":"GDLM","text":"<p>The gdlm package provides algorithms for linear and logistic regression with ridge and lasso regularizations. The solution is via gradient descent.</p> <ul> <li>use \"gdlm\" as the name for the predictor.</li> <li>Parameters:<ul> <li>method: one of full , sgd or logistic_sgd<ul> <li>full : full exact solution to the linear regression problem. Can be slow on huge matrices. Less recommended, but works. Not supporting lasso.</li> <li>sgd : gradient descent solution to the linear problem with least square loss and optional ridge and/or lasso regularizers.</li> <li>logistic_sgd : gradient descent solution to the logistic loss function with optional ridge and/or lasso regularizers.</li> </ul> </li> <li>normalize : 0/1 : use 1 if you want the algorithm to normalize the matrix before the optimization. Note that the algorithms converge only when data is normalized, so use this if data was not prepared normalized.</li> <li>l_ridge : the ridge gamma</li> <li>l_lasso : the lasso gamma (you'll have to play and find the gamma value that works for you. Typically very small values are needed (0.01, 0.001 , etc).</li> <li>max_iter : maximal number of iterations (an iteration is a full epoch through all the data)</li> <li>err_freq : print summary and check stop condition each err_freq iterations</li> <li>batch_size : the batch size for the gradient descent (coefficients are updated after every batch of course)</li> <li>rate : learning rate</li> <li>rate_decay : allow rate to slowly decrease (or stay constant if decay is 1).</li> <li>momentum : for gradient descent</li> <li>stop_at_err : once the relative improvement in loss falls below this value, the optimazation will stop.</li> <li>last_is_bias : leave 0 usually, is there for cases where a bias is given with the x values.</li> <li>nthreads : number of threads for matrix operations. Number of cores (12 in our nodes) is typically a good choice. MedGDLM init examples <pre><code>MedPredictor *predictor;\npredictor = MedPredictor::make_predictor(\"gdlm\");\n\n// classification example with logistic regression , lasso of 0.01 , learning rate of 0.001 and normalization pre running\npredictor-&gt;init_from_string(\"method=logistic_sgd;last_is_bias=0;stop_at_err=1e-4;batch_size=2048;momentum=0.95;rate=0.001;rate_decay=1;l_ridge=0;l_lasso=0.01;err_freq=10;nthreads=12;normalize=1\");\npredictor-&gt;learn(Xtrain,Ytrain);\n\n// same but regression example with least squares and lasso\npredictor-&gt;init_from_string(\"method=sgd;last_is_bias=0;stop_at_err=1e-4;batch_size=2048;momentum=0.95;rate=0.001;rate_decay=1;l_ridge=0;l_lasso=0.01;err_freq=10;nthreads=12;normalize=1\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/Feature%20Importance.html","title":"Feature Importance","text":"<p>The Feature Importance is virtual function of MedPredictor called calc_feature_importance. If you want to support Feature Importance for your predicotr you need to implement the virtual function: calc_feature_importance Some notes:</p> <ul> <li>This function must be called after learn method only! otherwise and exception will be raised.</li> <li>The functions returns the features importance score in vector with same order of the learned features matrix order (sorted by ABC because we use map object). Currently the method is implemented by:</li> <li>QRF - no additional parameters are required to run</li> <li>LightGBM -\u00a0has a parameter \"importance_type\" which has 2 options:\u00a0<ul> <li>\"gain\" -\u00a0the average gain of the feature when it is used in trees (in each split of tree node we have loss gain - averaging that)</li> <li>\"frequency\" -\u00a0the number of times a feature is used to split the data across all trees</li> </ul> </li> <li>XGB - has a parameter \"importance_type\" which has 3 options:\u00a0<ul> <li>\"gain\" -\u00a0the average gain of the feature when it is used in trees (in each split of tree node we have loss gain - averaging that)</li> <li>\"gain_total\" -\u00a0sum of gain the of the feature when it is used in trees (not normalized by number of appearances)</li> <li>\"weight\" -\u00a0the number of times a feature is used to split the data across all trees</li> <li>\"cover\" -\u00a0the average coverage of the feature when it is used in trees. it sums the number of samples in the leaves that in their path have this feature (so this feature was used to split and deciede on this observations). it calc's the average coverage \u00a0 Example run for XGB <pre><code>vector&lt;float&gt; feautres_scores;\nMedPredictor *predictor = MedPredicotr::make_predictor(\"xgb\"); //load trained model\nMedFeatures data; //data.read_from_file(matrix_file); //load dataMatrix or already trained model\n//predictor-&gt;learn(data); //or load trained model, otherwise it will throw exception\n\u00a0\n//now do learn and run again\npredictor-&gt;calc_feature_importance(feautres_scores, \"importance_type=gain\"); //the second argument is additional parameters for feature importance\n//sort and print features\nmap&lt;string, vector&lt;float&gt;&gt;::iterator it = data.data.begin();\nvector&lt;pair&lt;string, float&gt;&gt; ranked((int)feautres_scores.size());\nfor (size_t i = 0; i &lt; feautres_scores.size(); ++i) {\n    ranked[i] = pair&lt;string, float&gt;(it-&gt;first, feautres_scores[i]);\n    ++it;\n}\nsort(ranked.begin(), ranked.end(), [](const pair&lt;string, float&gt; &amp;c1, const pair&lt;string, float&gt; &amp;c2)\n{\n    return c1.second &gt; c2.second;\n});\nfor (size_t i = 0; i &lt; ranked.size(); ++i)\n    printf(\"FEATURE %s : %2.3f\\n\", ranked[i].first.c_str(), ranked[i].second);\n\u00a0\n//QRF- have no additional options - empty string\n//XGB - has parameter - \"importance_type\" with those options: \"gain,weight,cover\"\n//LightGBM  - has parameter \"importance_type\" with those options: \"frequency,gain\". gain is like xgboost and frequency is like weight in xgboost\n\u00a0\n\u00a0\n</code></pre> Using FeatureImportance as Selector: cat /server/Work/Users/Alon/UnitTesting/examples/\"general config files\"/importance_example.json <pre><code>\"process\":{\n            \"process_set\":\"3\",\n            \"fp_type\":\"importance_selector\",\n            \"duplicate\":\"no\",\n            \"numToSelect\":\"10\",\n            \"predictor\":\"qrf\",\n            \"predictor_params\":\"{type=categorical_entropy;ntrees=100;min_node=20;n_categ=2;get_only_this_categ=1;learn_nthreads=40;predict_nthreads=40;ntry=100;maxq=5000;spread=0.1}\",\n            \"importance_params\":\"{}\" //you may pass other parameters for feature importance in here. for example importance_type=gain\n        }\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/Machine%20Learning%20Algorithms%20-%20Parameter%20Tuning%20%28C%2B%2B%20Code%29.html","title":"Machine Learning Algorithms - Parameter Tuning (C++ Code)","text":""},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/Machine%20Learning%20Algorithms%20-%20Parameter%20Tuning%20%28C%2B%2B%20Code%29.html#qrf-regression-tree","title":"QRF Regression Tree:","text":"<p>To Initialize MedQRF predictor you can use MedQRF\u00a0object. You MUST initialize params.type =\u00a0QRF_TreeType::QRF_REGRESSION_TREE. there are 5 different parameters for regression random forest: 1. params.ntrees - which controls the number of trees created in the forest. when this value is big enougth it not suppose to have a big impact making it bigger. the prediction value is the mean average of all trees. theoritically it suppose to converge in big numbers (&gt; 100's) 2. params.maxq- controls how many bins a signal would be divided into (if there are less unique values than this number, it won't do a thing). will be used later for choosing threshold to decide where to split the tree in the signal node. there is a linear shrinkage between min to max value of signal divide by\u00a0maxq number. if bigger number than the runnint time increases, not suppose to increase overfitting \u00a0because it's only searches for split threshold in smaller bin jumps. 3. params.ntry - how many random tries to select signal to split node in the tree .default value is sqrt(number_of_features) 4. params.min_node - split node condition - how many samples should be in node, less than this will not split \u00a0node. default is 100 5. params.spread \u00a0- split node condition - the minimum diff between min and max in signal to split node. for example, if all the signal values in the the tree node only changes(max_value-min_value) less than spread , it will not split the node.\u00a0default value is 0.1. depends on the signal resulotion sometime 0.1 has big meaning and sometime it has small meaning \u00a0 Missing Parameters: 1. spliting function creteria - Gini, info gain? other measures 2. configure special params for special signals. for example in some signals you would like to choose diffrent spread \u00a0</p>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/Machine%20Learning%20Algorithms%20-%20Parameter%20Tuning%20%28C%2B%2B%20Code%29.html#sgd","title":"SGD:","text":"<p>These are the parameters which effects learning_rate in SGD: 1. B - Blocking Value for W in L2 norm - you need to search for a solution in blocked space. esstimate the L2 norm of your parameter optimal solution (W are the parameters of the model) 2. P - max change in derivate. more specificcly maximal change in (f(x+h)-f(x) )/ h. f(x) can be also not diffrential. you can calculate max diff in each signal maximum value- minimum value and divide with h_size which can be small and not important (0.01, 0.1) 3. h_size - the numeric step for calculating derivate 4. T_Steps - the number of steps the SGD will do 5. sample_size - the sample_size for stochastic gradient decend to calculate derievate, need to be not too small (50, 100 are ok to have enougth samples to calculate gradient) - not very important param and it's not effecting learning rate.There is a simple equation that takes all those params into account in yield the learning_rate and esstimate eppsilon error from optimal solution with a good confidence levelIn my code you can ran:learner.set_blocking(B);\u00a0//for projection step when solution is outside boundleaner.set_gradient_params(sample_size, h_size);learner.set_learning_rate(B, P, T_Steps);leaner.output_num = T_Steps\u00a0/ 5;\u00a0// If you want the SGD to output each T_steps/5 rounds the error. if set to 0 which is default won't output anything</p>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/but_why%20-%20feature%20contribs.html","title":"but_why - feature contribs","text":"<p>feature contributions (AKA but_why) is an attempt to explain a specific prediction of a classifier, by providing an interpretable simple model of simple features. We saw 3 works in this field:</p> <ol> <li>LIME - (https://arxiv.org/pdf/1602.04938.pdf) - build a complex model, then build a local simple model (Lasso) for the predictions of the complex model. By local they mean - generate samples, and weight them according to their distance from the interesting sample (patient).</li> <li>tree path interpreter - (https://blog.datadive.net/interpreting-random-forests/) - for tree ensembles such as XGB, follow the decision path for the interesting sample (patient), and accumulate the changes in the outcome mean for each feature.</li> <li>SHAP - (https://arxiv.org/abs/1802.03888) - an improvement over (2), which promises consistency when the impact of a feature stays the same but the structure of the trees changes.\u00a0 Yaron Kinar built LinearizeScore - a local c++ version of LIME, which uses real samples (LIME are using artificial samples which are generated assuming feature independence and normality). we added MedPredictor::calc_feature_contribs, which supports (2) and (3), only for XGB. In Likely we are doing a somewhat strange hybrid solution: we are building a strong XGB model using all the features, then we build a second XGB model with only part of the features (the ones that make medical sense), and we aim to explain the predictions of the strong model. We then use calc_features_contribs for getting the weak model feature contributions, in (2) mode.\u00a0 Note that we currently do not know of a reliable way to quantify which method is better, except in artificial data experiments. Also note that for migrating to yaron's solution, we need the entire repo in memory on our production machine which is currently not feasible.\u00a0 \u00a0</li> </ol>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/index.html","title":"MedPredictor practical guide","text":"<ul> <li>Linear model : type = \"linear_model\" - linear regression using coordinate descent with Ridge factors</li> <li>Linear-SGD : type = \"linear_sgd\" - linear regression using stochastic gradient descent</li> <li>GDLM : type = \"gdlm\" - Generalized linear model using stochastic gradient descent</li> <li>QRF : type = \"qrf\" - Random Forest using quantization for efficency</li> <li>KNN : type = \"knn\" - K Nearest Neighbors</li> <li>MARS : type = \"mars\" - (quite slow) impliminationg of MARS (Multivariate Additive Regression Spline)</li> <li>GBM : type = \"gbm\" - Gradient Boosting Machine\u00a0 - imported public library used in R</li> <li>BP : type = \"BP\" - Back propogation. (Obsolete) version of NN</li> <li>Multi-Class : type = \"multi_class\" - an enevelope for multi-class problems, currently implementing one-vs-all</li> <li>XGBoost : type = \"xgb\" - public implementation of xgboost + internaly added features</li> <li>Lasso : type = \"lasso\" - Lasso linear regression using gradient descent</li> <li>MicNet : type = \"micNet\" - Neural Net</li> <li>Booster : type = \"booster\" - Envelope for boosting models</li> <li>Deep-Bit : type = \"deep_bit\" - Nir's deep bit using Yoav's implementation</li> <li>Light-GBM : type = \"light_gbm\" - public implementation of fast GBM</li> <li>SVM : type = \"svm\" - Support Vector Machine</li> <li>multi-models : type = \"multi_models\" - an envelope for multiple models for different inputs (e.g. - age specific models)</li> <li>vw : type = \"vw\" - an envelope for\u00a0Vowpal Wabbit models</li> <li>TQRF : type = \"tqrf\" - new version of quantized RF (Not ready yet ?)</li> <li>BART : type = \"bart\" - public implementation of BART (Bayesian Additive Regression Trees)</li> <li>MASK: type=\"by_missing_value_subset\" - see here </li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/Howto%20write%20MedPredictor.html","title":"How to Write a MedPredictor","text":"<p>MedPredictor is Classifier/Regressor that inputs the features matrix and output prediction/s.  It is being executed after FeatureProcessors</p> <p>MedPredictor in MedModel follow a defined sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the MedPredictor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the predictor. Be sure to update <code>classifier_type</code> to reflect the predictor type.</p> </li> <li> <p>Initialization    - During learning:      Implement <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      Medial pipeline will initialize <code>features_count</code> and <code>model_features</code> that when we apply the modle we can make sure all and only needed features are inputed to the model.      You might want to set <code>transpose_for_learn</code>, <code>transpose_for_predict</code> - if transpose of features matrix is needed for this type of predictor      You might want to set (legacy and almost not used. Please use Feature Processors) to normalize the inputs: <code>normalize_for_learn</code>, <code>normalize_y_for_learn</code>, <code>normalize_for_predict</code>    - During application:      Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</p> </li> <li> <p>Learning Phase    - <code>learn()</code>      Implements any learning logic needed during training.</p> </li> <li> <p>Applying Phase    - <code>apply()</code>  or <code>predict_single()</code> for single patient (more efficient setup for single patient, some preparation are done prior)      Applies the predictor to calculate the score</p> </li> </ol>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/Howto%20write%20MedPredictor.html#steps-to-implement-a-medpredictor","title":"Steps to Implement a MedPredictor","text":"<ol> <li> <p>Create Class Files    - Create a new <code>.h</code> header and <code>.cpp</code> source file for your predictor class. Include <code>MedAlgo.h</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Do not include temporary or repository-specific variables.</p> </li> <li> <p>Learning    - Implement <code>learn()</code> for any required training logic.</p> </li> <li> <p>Apply    - Implement <code>apply()</code> to calculate the score. If there is more efficient calculate for single call, than also implement <code>predict_single()</code></p> </li> <li> <p>Register Your Predictor in the Header (<code>MedAlgo.h</code>)    - Add a new type to <code>MedPredictorTypes</code> before <code>MODEL_LAST</code>. In the documentation comment, specify the name in <code>MedPredictorTypes</code> for Doxygen reference.</p> </li> <li> <p>Register Your Feature Processor in the Source (<code>MedAlgo.cpp</code>)    - Add your type conversion to <code>predictor_type_to_name</code> dictionary    - Add your class to <code>MedPredictor::new_polymorphic</code>    - Add your class to <code>MedPredictor::make_predictor(MedPredictorTypes model_type)</code></p> </li> </ol>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/MASK%20predictor%20-%20predict%20by_missing_value_subset.html","title":"MASK predictor - predict by_missing_value_subset","text":"<p>When a model uses BMI (for example), and a sample has no BMI, the 'standard' approach is leave the relevant features as NaN or impute (impute is usually better). MASK model takes a different approach - it would score the sample based on a model trained without BMI.</p> <p><pre><code>PARAMS=\"predictor_type=xgb;masks_params=Smoking_Intensity|BMI|Fev1|Hemoglobin,Hematocrit,Platelets,WBC,MCH,Eosinophils%,Neutrophils%,Neutrophils#,RDW,RBC,MCHC-M|ALT,ALKP,Albumin;masks_tw=0,365,365,365,365;predictor_params=${XGB_PARAMS}\"\n</code></pre> A mask is list of signals, separated by \",\". Masks are separated by \"|\". This example has 5 masks:</p> <ul> <li>Mask 0: Smoking_Intensity</li> <li>Mask 1: BMI</li> <li>Mask 2: Fev1</li> <li>Mask 3: Hemoglobin,Hematocrit,Platelets,WBC,MCH,Eosinophils%,Neutrophils%,Neutrophils#,RDW,RBC,MCHC-M</li> <li>Mask 4: ALT,ALKP,Albumin</li> </ul> <p>** **- the code would train 2 power #masks predictors. In this example - 32 predictors, each with parameter as in predictor_params, but with different list of features:</p> <ul> <li>Predictor 0 includes all the features.</li> <li>Predictor 1\u00a0includes all the features but those associated with mask 0.</li> <li>Predictor 2 includes all the features but those associated with mask 1.</li> <li>Predictor 3 includes all the features but those associated with mask 1 and mask 0.</li> <li>...</li> <li>Predictor 10 includes all the features but those associated with mask 3 and mask 1. The mask code supports only these features:</li> <li>Signals with features of type \"last in time window\", e.g. WBC.last.win_0_to_365<ul> <li>365, the time window for looking for missing value in predict, see next, is defined in the parameter mask_tw</li> <li>If WBC is signal in a mask, all WBC features would be dropped (and not just the 'indicator feature' WBC.last.win_0_to_365)</li> </ul> </li> <li>Smoking Intensity - the masks would drop smoking pack years features as well (as one is calculated from the other plus smoking duration) For every Predictor, we run Calibrator, so score is risk, and it makes more sense to compare scores coming from different predictors (in bootstrap after predict)\u00a0 ** **- to chose the right model per sample:</li> <li>For every mask, the code checks if more than 50% of the signals are missing.</li> <li>To check if a feature is missing, we look in features.masks, so predict should look like</li> </ul> <pre><code>Flow --get_model_preds --rep $REP --f_samples $SAMPLES --f_model $MODEL --f_preds $PREDS --change_model_init \"object_type_name=MedModel;change_command={generate_masks_for_features=1}\" \n</code></pre> <ul> <li>Every mask has time window, given in the mask_tw parameter.<ul> <li>For mask 1, BMI, the time window in the example is 365. The code check if\u00a0BMI.last.win0_to_365 is missing.</li> <li>If a mask has more than one feature, e.g. Mask 3 in the example, it would check for 50% missing or more.</li> <li>Smoking Intensity is treated differently - time window is irrelevant and the code check whether the signal exists ever or not.</li> </ul> </li> <li>Based on the relevant masks for each sample, the right predictor is chosen.</li> <li>In the above example, if a sample has no Smoking Intensity ever, and no BMi in the last 365 days, but he has Fev1 in the last 365 days as well as the majority of signals from Mask 4 and 5, then his score would come from predictor 3.</li> <li>Standard output of predict (per prediction batch) includes:<ul> <li>frequency of each mask - % of missing values per mask</li> <li>frequency of predictor used The predictor return scores before and after calibration.</li> </ul> </li> </ul> <ul> <li>How to choose features for masks?<ul> <li>Based on feature importance</li> <li>Grouped by blood panels</li> <li>If Some signals are highly correlated, they should be in the same mask (because if we mask one of them, the sub-model will use the other one)</li> </ul> </li> </ul> <p>We ran two tests:</p> <ul> <li>LungFlag: train on KP, tests mostly on THIN</li> <li>AAA: train and test on Geisinger with cross validation In both cases we saw no significant change from XGB straightforward, with and without calibration\u00a0 Note that for each model we used the same XGB parameters as in the full model - i.e. we haven't run optimization on top of the mask model, as we cannot do it in our current infrastructure and we decided to skip testing it manually. We hoped for better results. But, the good news is the strength of XGB \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/XGBoost%20added%20features.html","title":"XGBoost added features","text":"<ul> <li>When using gbtree::updater_colmaker::spit_evaluator==ElasticNet, it is possible to add feature specific additive penalties<ul> <li>Parameter format - a string \"fid:valfid:val,...\"</li> <li>When spliting according to id fid, loss-change is decreased by val</li> <li>if a given feature-id is not given in the string, the corresponding penalty is 0</li> <li>In MedXGB intialization, give\u00a0\"feature-name:valfeature-name:val,...\" \u00a0</li> </ul> </li> <li> </li> <li>It is actually XGB standard constraint, but we need some translation to reach the required format</li> <li>Our parameter format:\u00a0monotone_constraints=f1:d1#f2:d2#...</li> <li>Where:<ul> <li>f is part of a unique feature name, and</li> <li>d is a direction: 1 for up and -1 for down</li> </ul> </li> <li>Currently NO defense against raw values/format, and</li> <li>Bad outcome (feature format not recognized by XGB) yield all predictions = 0.5 without warning</li> </ul>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/index.html","title":"PostProcessors Practical Guide","text":"<p>PostProcessors are processors which occours after predicitons. Full PostProcessor code doc:\u00a0https://Medial-EarlySign.github.io/MR_LIBS/classPostProcessor.html Some PostProcessor, for full list of types and json values to put in names please reffer to PostProcessor_Types. \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0We have now MultiPostProcessor (for parallelism) \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Calibrator \u2013 to calibrate scores to probabilities for example \u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ModelExplainer \u2013 Several options for ButWhy:  \u00a0 PostProcessor API:  \u00a0 ModelExplainer API:  \u00a0 Examples:</p> <ul> <li>Calibration</li> <li>Explainer of prediction \"But Why\" \u00a0 Example for calibration: <pre><code>{\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"calibrator\",\n      \"calibration_type\":\"binning\",\n      \"min_preds_in_bin\":\"200\",\n      \"min_prob_res\":\"0.005\",\n      //\"calibration_samples\":\"\", //on train or give your samples to calibrate on\n      \"verbose\":\"1\"\n}\n</code></pre> Example for explainer: \u00a0 Tree Shapley: <pre><code>{\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"tree_shap\",\n      \"approximate\":\"0\",\n      \"interaction_shap\":\"0\"\n}\n</code></pre> LIME: <pre><code>{\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"lime_shap\",\n      //\"pp_type\":\"shapley\",\n      \"gen_type\":\"GIBBS\",\n      \"n_masks\":\"500\", //how many masks to sample for learn\n      \"generator_args\":\"{kmeans=0;select_with_repeats=0;max_iters=0;predictor_type=qrf;predictor_args={spread=0;type=categorial_entropy;learn_nthreads=40;predict_nthreads=40;ntrees=50;maxq=500;min_node=300;get_only_this_categ=-1};num_class_setup=n_categ;bin_settings={split_method=iterative_merge;min_bin_count=500;binCnt=150};selection_ratio=1.0}\", //when using Gibbs, otherwise give GAN path here\n      \"sampling_args\":\"{burn_in_count=50;jump_between_samples=10;samples_count=1;find_real_value_bin=1;use_cache=0}\" //in GAN not needed\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html","title":"ButWhy Practical Guide","text":""},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#global-feature-importance","title":"Global Feature Importance","text":"<p>You can compute global feature importance for each model (tree-based predictors like XGBoost, LightGBM, QRF) directly, without extra preparation. Importance can also be calculated for groups of features (e.g., signals).</p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#option-1-simple-but-redundant","title":"Option 1 - Simple (but redundant)","text":"<p>Use Flow's <code>print_model_info</code> to get feature importance using SHAP values:</p> <p><pre><code>Flow --print_model_info --f_model $MODEL_PATH --rep $REPOSITORY --f_samples $SAMPLES_TO_ANALYSE_SHAPLEY --max_samples $DOWN_SAMPLE_SAMPLES_TO_THIS_COUNT_OPTIONAL_TO_SPEEDUP --importance_param \"importance_type=shap\"\n</code></pre> This command reports SHAP-based feature importance for each feature in tree models.</p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#option-2-detailed-report","title":"Option 2 - Detailed Report","text":"<p>Use Flow's <code>shap_val_request</code> to generate a report with average outcome/score/SHAP values across different feature value groups:</p> <p><pre><code>Flow --shap_val_request --f_model $MODEL_PATH --rep $REPOSITORY --f_samples $SAMPLES_TO_ANALYSE_SHAPLEY --max_samples $DOWN_SAMPLE_SAMPLES_TO_THIS_COUNT_OPTIONAL_TO_SPEEDUP --group_signals \"\" --bin_method \"split_method=iterative_merge;binCnt=50;min_bin_count=100;min_res_value=0.1\" --f_output $REPORT_STORE_PATH\n</code></pre> This creates a report at the path specified by <code>--f_output</code>. Key arguments:</p> <ul> <li>group_signals: Leave empty to report for each feature. Use:</li> <li><code>BY_SIGNAL_CATEG_TREND</code>: group by signals, separating value and trend features</li> <li><code>BY_SIGNAL_CATEG</code>: group by signals, combining value and trend features</li> <li>Or provide a tab-delimited file mapping features to groups</li> <li>bin_method: Controls how features are binned. The recommended value is <code>split_method=iterative_merge;binCnt=50;min_bin_count=100;min_res_value=0.1</code>, which creates up to 50 bins with at least 100 samples per bin and a minimum value resolution of 0.1. Bins are merged greedily to ensure minimum counts in each bin.</li> <li>cohort_filter: Optionally filter samples using a bootstrap cohort file or a cohort string (e.g., <code>Age:40,60;Time-Window:0,365</code>). For filters beyond Age, Gender, or Time-window, specify <code>f_json</code> to define filter features.</li> <li>keep_imputers: If true, model imputers are used to fill missing values during SHAP analysis. By default, imputers are skipped and missing values are binned separately.</li> <li>max_samples: controls maximal samples count and randomly subsample the row to the number if the file is larger. If 0, will do nothing (default: 0) </li> <li>f_model, rep, f_samples: needed inputs for calculation. We need data repository, full model with predictor and samples files to calcualte the feature importance for</li> </ul> <p>Advanced flags (usually not needed): - <code>normalize</code>: Normalize SHAP values to percentages (default: 1) - <code>normalize_after</code>: If 1, normalizes after summing global importance; if 0, normalizes per prediction (default: 1; only applies if <code>normalize</code> is on) - <code>remove_b0</code>: Remove the baseline/prior score (default: 1). If 0, keeps and prints the baseline (b0), which is the constant added to all predictions.</p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#inspecting-the-raw-output","title":"Inspecting the Raw Output","text":"<p>To inspect the first few rows of the output file (<code>shap_val_output.tsv</code>) and transpose them for easier reading:</p> <pre><code>cat shap_val_output.tsv | head -n 4 | awk ' { for (i=1;i&lt;=NF;i++) a[i]=a[i]\"\\t\"$i; } END { for (i in a) {printf(\"%d%s\\n\", i, a[i])} }' | sort -g -k1\n</code></pre> <p>This produces a table like:</p> Feature Age ADMISSION.category_dep_set_Hospital_Emergency_Department.win_0_3650 DIAGNOSIS.category_dep_set_ICD10_CODE:J00-J99.win_0_1825 1 Importance 5.3 4.9 4.15 2 SHAP::Low_Mean 15.4 -4.3 -5.1 3 SHAP::Low_Std 8.6 1.0 1.4 4 SHAP::Low_Prctile10 4.5 -5.6 -6.8 5 SHAP::Low_Prctile50 15.0 -4.3 -5.2 6 SHAP::Low_Prctile90 27.2 -3.1 -3.3 7 FEAT_VAL::Low_Mean 5.27 0 0 9 FEAT_VAL::Low_Prctile0 0 0 0 10 FEAT_VAL::Low_Prctile10 2 0 0 11 FEAT_VAL::Low_Prctile50 6 0 0 12 FEAT_VAL::Low_Prctile90 9 0 0 13 FEAT_VAL::Low_Prctile100 9 0 0 14 SHAP::Medium_Mean -4.9 -1.2 -1.0 ... ... ... ... ... 19 FEAT_VAL::Medium_Mean 44.96 0.3 0.5 21 FEAT_VAL::Medium_Prctile0 40 0 0 25 FEAT_VAL::Medium_Prctile100 50 1 1 26 SHAP::High_Mean 10.3 6.1 3.2 ... ... ... ... ... 31 FEAT_VAL::High_Mean 84.57 1 1 33 FEAT_VAL::High_Prctile0 81 1 1 37 FEAT_VAL::High_Prctile100 90 1 1 ... ... ... ... ... <p>Example (Age column): - Importance: About 5.3% of the average XGBoost raw score (before sigmoid/calibration) - SHAP::Low_Mean: 15.4 - average contribution for the \"Low\" age group (positive) - FEAT_VAL::Low_Prctile0: 0 - lowest value in \"Low\" bin; FEAT_VAL::Low_Prctile100: 9 - highest value (so \"Low\" = 0-9 years, average 5.27) - SHAP::Medium_Mean: -4.9% - negative contribution for 40\u201350 year olds, average 44.96 (Protective) - SHAP::High_Mean: 10.3% - positive contribution for 81\u201390 year olds, average 84.57</p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#generating-graphs-from-the-report","title":"Generating Graphs from the Report","text":"<p>Use the script <code>feature_importance_printer.py</code> (should be under MR_Scripts <code>Python-scripts/feature_importance_printer.py</code>).</p> <ul> <li>To output a single HTML file with the top 30 features/groups:   <pre><code>feature_importance_printer.py --report_path $REPORT_STORE_PATH --output_path $OUTPUT_PATH --num_format \"%2.3f\" --feature_name \"\" --max_count 30 --print_multiple_graphs 0\n</code></pre></li> <li>To output multiple HTML files (one per feature/group, up to 30):   <pre><code>feature_importance_printer.py --report_path $REPORT_STORE_PATH --output_path $OUTPUT_PATH --num_format \"%2.3f\" --feature_name \"\" --max_count 30 --print_multiple_graphs 1\n</code></pre><ul> <li><code>force_many_graph</code>: Forces scatter (not bar) chart if set</li> <li><code>use_median</code>: Use median instead of mean for feature bins</li> <li><code>contrib_format</code>: Controls SHAP value precision; <code>num_format</code>: feature value precision</li> <li><code>feature_name</code>: If set, outputs only that feature to a single HTML file</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#local-feature-importance-explaining-a-single-prediction","title":"Local Feature Importance \u2013 Explaining a Single Prediction","text":"<p>To use ButWhy for explaining individual predictions, follow these steps:</p> <ol> <li>Add an Explainer PostProcessor to the Model (can be done later with <code>adjust_model</code>)</li> <li>Apply the Model - Use Flow or <code>CreateExplainReport</code> to generate report</li> </ol>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#adding-an-explainer-to-an-existing-model","title":"Adding an Explainer to an Existing Model","text":"<p>For training with an explainer, see the PostProcessors Practical Guide. To add an explainer to an existing model, create a post_processor JSON like:</p> <pre><code>{\n  \"post_processors\": [\n    {\n      \"action_type\": \"post_processor\",\n      \"pp_type\": \"tree_shap\",\n      \"attr_name\": \"Tree_iterative_cov\",\n      \"filters\": \"{max_count=0;sort_mode=0}\",\n      \"processing\": \"{grouping=BY_SIGNAL_CATEG_TREND;iterative=1;group_by_sum=0;learn_cov_matrix=1;zero_missing=0;use_mutual_information=0;mutual_inf_bin_setting={split_method=iterative_merge;min_bin_count=100;binCnt=50;min_res_value=0}}\"\n    }\n  ]\n}\n</code></pre> <p>Add this post processor using <code>adjust_model</code>:</p> <pre><code>adjust_model --rep $REPOSITORY --inModel $F_MODEL_PATH --out $F_MODEL_OUTPUT_PATH_WITH_EXPLANIER --postProcessors $FILE_PATH_TO_POST_PROCESSOR_DEF --samples $SAMPLES_PATH\n</code></pre>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#creating-a-report-for-each-prediction","title":"Creating a Report for Each Prediction","text":"<p><code>CreateExplainReport</code> is part of AllTools:</p> <pre><code>CreateExplainReport --rep $REPOSITORY --samples_path $PATH_TO_SAMPLES_TO_EXPLAIN --model_path $PATH_TO_MODEL --output_path $OUTPUT_PATH_REPORT --take_max 10\n# viewer_url_base: Controls the viewer link (should contain two \"%d\" for pid and prediction time)\n# rep_settings: Controls features shown per group, e.g., rep_settings=\"min_count=2;sum_ratio=0.5\" shows at least 2 features/group and at least 50% total weight\n</code></pre>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#generating-html-reports","title":"Generating HTML Reports","text":"<p>Use <code>explainer_printer.py</code> (in MR_Scripts under <code>Python-scripts/explainer_printer.py</code>):</p> <p><pre><code>explainer_printer.py --report_path $OUTPUT_PATH_REPORT --predictor_name \"pre2d - optional argument to control graph title\" --filter_pid -1 --max_count 10 --output_path $FOLDER_OUTPUT_PATH_TO_HTMLS\n# This creates an HTML file for each sample. Use --filter_pid to select specific pids. max_count limits the number of HTMLs generated.\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/Explainers%20%28But%20Why%29.html","title":"Explainers (But Why)","text":"<p>ModelExplainer (code doc) ModelExplainer API:  </p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/Explainers%20%28But%20Why%29.html#modelexplainer-types","title":"ModelExplainer Types:","text":"<ul> <li>TreeExplainer\u00a0 \u2013 explains model with SHAPLEY implementation for trees (it has 2 additional flags \u2013 do interaction calculation for shap values or use approximate calculation which is faster):<ul> <li>If tree model has its own implementation for shapley (like XGboost, LightGBM) \u2013 will use those methods directly</li> <li>If the model is based on ensemble trees (like QRF) it will convert the model into generic tree model in learn. Than in Apply it will run SHAP algorithm for trees with the given flags. I made the function parallel</li> <li>If it's other MedPredictor \u2013 it will learn other ensemble trees model (for example XGBoost) based on parameters for the predictor. The model will \u00a0be \"proxy model\" for our model and will learn the original model output (regression problem) and will run SHAP algorithm for trees for the proxy model to explain scores It's just backup algorithm for models that are not ensemble trees\u2026</li> </ul> </li> <li>ShapleyExplainer -\u00a0Agnostic SHAP algorithm, may use Gibbs, GAN as generators for the process. It also has sampling parameters for the mask to speed up the calculation. Still very slow implementation \u2013 it's here because we already written it and maybe in the future for some specific research problems it may be useful. Maybe to compare with other methods</li> <li>LimeExplainer -\u00a0\u00a0Agnostic SHAP algorithm, may use Gibbs, GAN as generators for the process. It\u00a0has sampling parameters for the mask to speed up the calculation and fits linear model to the random masks. It's faster the Shapley because the runtime is O(Number_of_masks)generate_samples. In Shapley we do this for each feature again, with and without the specific feature. so Shapley with the same sampling args is slower by 2number of features. the linear fit time is very fast</li> <li>MissingShapExplainer \u2013 Agnostic SHAP algorithm that doesn't use Gibbs or GAN and is much faster\u2026 the Algorithm trains proxy model (with same parameters as original model) on the outcome with added masked samples. masked samples are samples where feature values were removed on the mask and replaced with missing value. It reweights all training samples to match SHAP weights for each mask. The results on the simulated data(which are very simple) looks good for lightGBM model, the runtime is also very good. The idea is that the proxy model can now handle much better missing values and we can just feed the model with missing values instead of generating \"real\" samples. Theoretically for linear\\polynomial kernel it's should work(and works) very good</li> <li>LinearExplainer - Simple Explainer for linear models to return feature_value*coeff. The implementation is generic for all models - for each feature\\group of features, the contribution is calculated as the difference between the model score with the original feature value versus the model score with the feature value set to 0. It's similar to Shapley but much faster - taking mask of all 1's so no need to generate values with Gibbs\\GAN. Very similar implementation to\u00a0MissingShapExplainer\u00a0but without proxy model</li> <li>KNNExplainer\u00a0-An explainer that calculates average score for neighbours of sample in training data, \u00a0when \u00a0neighborhood is calculatesd with and without the tested feature, and the ratio between the two is returned.</li> </ul> Explainer type string to put in json internal support for grouping of features Additional options Advantages run_time TreeExplainer \"tree_shap\" NO Support for Interaction valuesSupoprt for approximate alg - Saabas alg which is faster Accurate very fast!! ShapleyExplainer \"shapley\" YES <p>Accurate (when not sampling)</p><p>Model Agnostic</p> Very Slow, depend heavily in the number of features LimeExplainer\u00a0 \"lime_shap\" YES <p>Accurate (when not sampling)</p><p>Model Agnostic</p> Slow, but can be feasible MissingShapExplainer\u00a0 \"missing_shap\" YES Model Agnostic very fast!! LinearExplainer\u00a0 \"linear\" YES fastest!! KNNExplainer \"knn\" NO <p>May use raw score or thresholded score.</p><p>May set a threshold ifnot given.</p> Model agnostic. fast"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/Explainers%20%28But%20Why%29.html#global-arguments-for-all-explainers","title":"Global Arguments for all explainers:","text":"<ul> <li>filters<ul> <li>max_count - maximal number of features\\groups to keep for explaining\u00a0</li> <li>sum_ratio - the maximal number of features\\groups to\u00a0keep for explaining when considering the sum of top features contributions as ratio from the total\u00a0contributions . in range 0-1, 1 - means take all</li> <li>sort_mode - 0 sort feature\u00a0contributions by applying ABS (no importance for sign), +1 - sort only positive contributions\u00a0, -1 - sort only negative\u00a0contributions\u00a0</li> </ul> </li> <li>processing<ul> <li>learn_cov_matrix - If turned on will use the train_matrix to calculated Covarince matrix and will aggregate the\u00a0contributions for all correlated features\\groups according to covariance\u00a0</li> <li>cov_features - covarince matrix path, If we want to use extrernal covariance matrix instead of calculating on train matrix</li> <li>group_by_sum - If turned on will use External grouping to calculate group\u00a0contribution using sum of each feature in the group. for example in TreeExplainer this is the only way to calculate\u00a0contribution for group, the other explainers has specific special implementation for grouping</li> <li>grouping - a file path for grouping or the keyword \"BY_SIGNAL\" to group each feature by it's signal. If file path is provided the file format is tab-delimited with 2 fields: feature_name_to_search_in_features, group_name</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/FairnessPostProcessor.html","title":"FairnessPostProcessor","text":"<p>PostProcessor on score to constraint fairness between groups</p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/Howto%20write%20PostProcessor.html","title":"How to Write a PostProcessor","text":"<p>A PostProcessor is a component that takes the feature matrix and prediction results, then applies additional post-processing steps. It is executed after the MedPredictor stage in the pipeline.</p> <p>PostProcessors in MedModel follow a defined sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the PostProcessor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the processor. Make sure to update <code>processor_type</code> to indicate the processor type.</p> </li> <li> <p>Initialization    - During learning:      Implement <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      If your PostProcessor should operate on a specific subset of training samples, set either <code>use_p</code> or <code>use_split</code>:</p> <ul> <li><code>use_split</code>: Uses the \"split\" stored in MedSamples. All splits (based on patient ID) except the selected one are passed to the full model pipeline; the selected split is reserved for training this PostProcessor.</li> <li><code>use_p</code>: A value between 0 and 1 that determines the proportion of randomly selected patient IDs passed to the PostProcessor. The remainder is processed by the main MedModel pipeline.  This mechanism also supports multiple PostProcessors, each working on a different subset of the data.</li> <li>During application:  Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</li> </ul> </li> <li> <p>Pipeline Integration    - <code>init_post_processor()</code>:      Initializes the PostProcessor using the complete MedModel pipeline, allowing for any necessary adaptations before execution.</p> </li> <li> <p>Learning Phase    - <code>Learn()</code>:      Implements any learning logic required during training.</p> </li> <li> <p>Application Phase    - <code>Apply()</code>:      Applies the post-processing logic to the data.</p> </li> </ol>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/Howto%20write%20PostProcessor.html#steps-to-implement-a-postprocessor","title":"Steps to Implement a PostProcessor","text":"<ol> <li> <p>Create Class Files    - Create a new <code>.h</code> header and <code>.cpp</code> source file for your PostProcessor class. Include <code>PostProcessor.h</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Exclude temporary or repository-specific variables.</p> </li> <li> <p>Pipeline Adaptation (if needed)    - Implement <code>init_post_processor()</code> if your PostProcessor needs to adapt based on the full MedModel pipeline.</p> </li> <li> <p>Define Dependencies and Outputs    - Implement <code>get_input_fields()</code> and <code>get_output_fields()</code> to specify the inputs and outputs of your PostProcessor.  </p> <ul> <li>For features, prefix the name with <code>\"feature:\"</code>.</li> <li>For predictions, use <code>\"prediction:X\"</code> (where X is the prediction index, usually 0).</li> <li>For other sample effects, use <code>\"attr:\"</code>, <code>\"str_attr:\"</code>, or <code>\"json:\"</code> as appropriate.  The MedModel pipeline uses this information to determine if the PostProcessor is required.</li> </ul> </li> <li> <p>Learning    - Implement <code>Learn()</code> for any required training logic.</p> </li> <li> <p>Apply    - Implement <code>Apply()</code> to perform the post-processing.</p> </li> <li> <p>Register Your PostProcessor in the Header (<code>PostProcessor.h</code>)    - Add a new type to <code>PostProcessorTypes</code> before <code>FTR_POSTPROCESS_LAST</code>. In the documentation comment, specify the name in <code>PostProcessorTypes</code> for Doxygen reference.</p> </li> <li> <p>Register Your PostProcessor in the Source (<code>PostProcessor.cpp</code>)</p> <ul> <li>Add your type conversion to <code>post_processor_name_to_type</code></li> <li>Add your class to <code>PostProcessor::new_polymorphic</code></li> <li>Add your class to <code>PostProcessor::make_processor(MedPredictorTypes model_type)</code></li> </ul> </li> </ol> <p>Tip: Follow the structure and conventions of existing PostProcessors for consistency and easier integration into the MedModel framework.</p>"},{"location":"Infrastructure%20C%20Library/05.PostProcessors%20Practical%20Guide/MultipleImputations.html","title":"MultipleImputations","text":"<p>The main idea is to do multiple imputations for missing value as post processor on existing model.\u00a0 The steps are:</p> <ul> <li>Apply model till feature processor imputers (if model has one). If model doesn't have imputer complete apply till the end before predictor. The main idea is that we can't just use the model imputer (it's not stochastic and we need to replace it) so the postprocessor locate the imputer if exists and \"replaces\" with stochastic one.\u00a0</li> <li>Duplicate each row 100 times (parameter) for different imputation in each line. There is also additional parameter \"batch\" to control the maximal memory needed for Apply the post processor - mainly set to 10K, so we will end up with 1M rows in each batch that each 100 rows are duplicates of the same row and will be aggregated later.\u00a0</li> <li>Apply the stochastic imputer given in the post processor argument on all rows - so each row out of the 100 duplication will get different imputations</li> <li> <p>Aggregations, each 100 rows are aggregated - Mean,Median, STD, CI_lower, CI_Upper are extract as attributes in model apply. We can later test if the model original score (that is not impacted) is even inside the confidence interval. You can also control the post processor to override the prediction with mean value/median value of this multiple imputations \u00a0 Example config: <pre><code>{ \n  \"post_processors\": [\n    {\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"aggregate_preds\",\n      \"force_cancel_imputations\":\"1\",\n      \"use_median\":\"0\",\n      \"resample_cnt\":\"100\",\n      \"batch_size\":\"10000\",\n      \"feature_processor_type\":\"predictor_imputer\",\n      //\"feature_processor_args\":\"{gen_type=UNIVARIATE_DIST;generator_args={strata=Age,0,100,5;min_samples=50};tag=labs_numeric}\"\n      \"feature_processor_args\":\"{gen_type=GIBBS;generator_args={kmeans=0;select_with_repeats=0;max_iters=0;predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=0;num_trees=100;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=50;feature_fraction=0.8;bagging_fraction=0.25;bagging_freq=4;is_unbalance=true;num_leaves=80;silent=2};num_class_setup=num_class;calibration_string={calibration_type=isotonic_regression;verbose=0};calibration_save_ratio=0.2;bin_settings={split_method=iterative_merge;min_bin_count=200;binCnt=100};selection_ratio=1.0;selection_count=500000};sampling_args={burn_in_count=5;jump_between_samples=10;samples_count=1;find_real_value_bin=1};verbose_learn=1;tag=labs_numeric}\"\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>resample_cnt - how many times we duplicate each row</p> </li> <li>force_cancel_imputations - flag to indicate we need to find model original imputer and replace it (otherwise there will be no missing values, the imputer will impute them).</li> <li>batch_size - how many samples in each batch before duplications</li> <li>use_median - if true will replace pred_0 with median</li> <li>feature_processor_type - the feature processor init type - pay attention to use something with stochastic properties, for example \"predictor_imputer\" - you can see more imputer options in\u00a0FeatureProcessor practical guide</li> <li>feature_processor_args - arguments for the feature processors. You can see some explanations in the FeatureProcessor practical guide Run adjust model with this post processor to generate gibbs sampling on missing values in features tagged \"labs_numeric\": <pre><code>adjust_model --rep /home/Repositories/THIN/thin_jun2017/thin.repository --samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/diabetes/train.samples --inModel /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/diabetes/base_model.bin --out /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/diabetes/test_imputer.2.mdl --postProcessors $MR_ROOT/Projects/Shared/Projects/configs/UnitTesting/examples/MultipleImputations/post_processors.multipleimputations.json\n</code></pre> Apply with Flow and store attributes: <pre><code>Flow --get_model_preds --print_attr 1 --rep /home/Repositories/THIN/thin_jun2017/thin.repository --f_samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/diabetes/test.samples --f_preds /server/Linux/alon/pre2d_test.tsv --f_model /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/diabetes/test_imputer.2.mdl\n</code></pre> </li> </ul> <p>Example output for pred2d model output: Excel Results There are additional columns: attr_pred.ci_lower,attr_pred.ci_upper,attr_pred.mean,attr_pred.median,attr_pred.std \u00a0</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html","title":"AlgoMarkers","text":"<p>AlgoMarkers are wrappers to models or calculators that are able to \"talk\" to the AlgoAnalyzer via the API defined in AlgoMarker.h (the api funcs start with AM_API_... ). Since the AlgoAnalyzer uses the AlgoMarker as a shared library from a c# code, it is written is \"C\" style bare bones to allow for the integration. This page describes the following:</p> <ul> <li>How to write a new AlgoMarker</li> <li>Compile the AlgoMarker shared library</li> <li>The MedialInfra AlgoMarker<ul> <li>Configuration file</li> <li>Eligibility rules configuration</li> </ul> </li> <li>How to freeze a MedialInfra AlgoMarker</li> <li>How to test the AlgoMarker shared library </li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#how-to-write-a-new-custom-algomarker","title":"How to write a new Custom AlgoMarker","text":"<p>The AlgoMarker base class is in AlgoMarker.h in the AlgoMarker lib which is part of medial research libs git. So to work with it pull it from git first. In order to write a new AlgoMarker one has to go through the following steps:</p> <ol> <li>Write a new class that inherits from AlgoMarker (or from one of its derived classes), and make sure the following virtual functions are all filled:<ul> <li>virtual int Load(const char *config_f) : gets a config file (or NULL if you don't need one), and gets the AlgoMarker into a working state.</li> <li>virtual int Unload()\u00a0\u00a0: releases all allocated memory and closes</li> <li>virtual int AddData(int patient_id, const char signalName, \u00a0int TimeStamps_len, long long TimeStamps, int Values_len, float* Values)<ul> <li>Adds data to the AlgoMarker which later could be used to ask scores for</li> <li>Data is added per pid, for a specific signalName, and then 2 arrays giving the time and value channels. The order in each array is element by element and then each channel for each element.</li> </ul> </li> <li>virtual int ClearData() : Clear all data in the AlgoMarker</li> <li>virtual int Calculate(AMRequest request, AMResponses responses)<ul> <li>Major API : once the AlgoMarker is loaded and has data, one can give it requests and get responses (=results).</li> <li>To handle requests and responses one can check their classes and the example AlgoMarkers already implemented.</li> </ul> </li> </ul> </li> <li>Add a type to your new AlgoMarker in the enum\u00a0AlgoMarkerType</li> <li>Update the\u00a0AlgoMarker::make_algomarker routine to support your new class</li> <li>That's It\u00a0!! compile and ready to go. NO NEED to touch any of the API routines - they only rely on the implementation of the 5 routines in (1).</li> </ol> <p>[!NOTE] In most use cases(all till now), you won't need to do that and write a custom AlgoMarker. You can use TYPE = \"MEDIAL_INFRA\" and our existing implementation supports all what you need. So you can skip this step.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#compile-the-algomarker-library","title":"Compile The AlgoMarker library","text":"<ul> <li>Follow the AlgoMarker Library Setup to build the <code>libdyn_AlgoMarker.so</code> library, or use a pre-built version if available.</li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#the-medialinfra-algomarker","title":"The MedialInfra AlgoMarker","text":"<p>The MedialInfra AlgoMarker allows using any model that was trained using Medial MedProcessTools infrastructure. On top of that it allows also for some nice configuration of eligibility testing on input data, and is packaged with a configuration file that's easy to edit and work with.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#configuration-file","title":"Configuration File","text":"<p>When Loading a new MedialInfra AlgoMarker a configuration file is given. The format of the general file is explained in the next example:</p> <p>Configuration File Example<pre><code>#################################################################################\n# MedialInfra AlgoMarker config example file\n#################################################################################\n# comment lines start with #\n# all (non comments) separators are tab\n# type of AlgoMarker\n# shoule be MEDIAL_INFRA for a MedialInfra AlgoMarker, otherwise will fail loading\nTYPE    MEDIAL_INFRA\n# name : One can get the name via the AM_API\nNAME    PRE2D\n# repository file configuration : to enable load of signal names and dictionaries, or optionally also data\nREPOSITORY  thin.repository\n# basic time unit for signals used in the specific marker (typically : Date for in patient, Minutes for out patient)\nTIME_UNIT   Date\n# model file for AlgoMarker , if name does not start with '/' the file position will be relative to the directory in which the config file was at\nMODEL   pre2d.model\n#################################################################################\n# Eligibility\n#################################################################################\n# Following parts are optional: when defining eligibility rules\n# if a file is given it will be used, if \".\" is the file name, then it means this config file contains also the filters definitions.\nINPUT_TESTER_CONFIG .\n\u00a0\n# each filter is in the following format:\n# FILTER    &lt;filter type&gt;|&lt;filter params&gt;|&lt;warning_or_error&gt;|&lt;use_for_max_outliers_flag&gt;|&lt;external_rc&gt;|&lt;internal_rc&gt;|&lt;err_msg&gt;\n# &lt;filter type&gt; : currently always 'simple' or 'attr'\n# &lt;filter params&gt; : see examples below, and/or read documentation. Params should be separated with ';' .\n# &lt;warining_or_error&gt;: values are WARNING or ERROR\n# &lt;use_for_max_outliers_flag&gt;: ACC=1 or ACC=0 : state from which filters to accumulate the overall number of outliers\n# &lt;external_rc&gt; : read code to return in the message case the test did not pass\n# &lt;internal_rc&gt; : internal read code returned (another layer of error codes that is needed)\n# &lt;err_msg&gt; : string - free text that will be returned as the error message in case the test did not pass.\n# TESTER_NAME : mainly for debug prints etc...\nTESTER_NAME pre2d_tester\n\u00a0\n# example for a filter to force a minimal number of results in a certain time window (defualt given in days)\nFILTER  simple|sig=Glucose;win_from=0;win_to=730;min_Nvals=2|ERROR|ACC=0|310|310|Not enough Glucose tests in the last 2 years\n\u00a0\n# example for a filter that forces also a maximal number of results for a signal\nFILTER  simple|sig=GENDER;min_Nvals=1;max_Nvals=1|ERROR|ACC=0|310|310|Missing GENDER or more than 1 GENDER signal\n\u00a0\n# example for a filter that forces values to be in a given list (allowed_values)\nFILTER  simple|sig=GENDER;allowed_values=1,3|WARNING|ACC=0|310|310|WARNING: GENDER Value Not 1 or 3\n\u00a0\n# example for an AGE filter (ages should be &gt;= and &lt;= the given range)\nFILTER  simple|sig=AGE;min_val=50;max_val=60|WARNING|ACC=0|320|320|age not in range 50-60\n\u00a0\n# example for a filter that verifies that the values of a signal are defined in the repository dictionary (important for categorical signals)\nFILTER  simple|sig=Drug;values_in_dictionary=1|ERROR|ACC=0|320|320|Drug code not in dictionary\n\u00a0\n# example for filters that check for a maximal number of outliers\nFILTER  simple|sig=Glucose;win_from=0;win_to=3650;min_val=10;max_val=2000;max_outliers=3|ERROR|ACC=1|321|321|Too many glucose outliers\nFILTER  simple|sig=HbA1C;win_from=0;win_to=3650;min_val=3;max_val=12;max_outliers=3|ERROR|ACC=1|321|321|Too many HbA1C outliers\n# if the model has Glucose_nRem attributes we can create the following rule by testing it directly.\nFILTER  attr|attr_name=Glucose_nRem;max=0|ERROR|ACC=0|321|321|Too many glucose outliers\n\u00a0\n# max outliers allowed when summing over all the ACC=1 filters\nMAX_OVERLALL_OUTLIERS   1\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#eligibility-rules-configuration","title":"Eligibility rules configuration","text":"<p>Most of the options can be seen in the above example, here are the basics:</p> <ul> <li>Each filter is given in a line :\u00a0FILTER |||||| <li>Currently  is always 'simple' or 'attr' <li> : see examples above and definitions below <li> : one of WARNING or ERROR. A warning will run the test, report the problem if found, but not fail the example and still will give a score for it. <li>: ACC=1 or ACC=0 : state from which filters to accumulate the overall number of outliers (MAX_OVERALL_OUTLIERS) <li> : read code to return in the message case the test did not pass <li> : internal read code returned (another layer of error codes that is neededed <li> : string - free text that will be returned as the error message in case the test did not pass. \u00a0 Filter params is given in a string , separator is ';' and there should be no spaces/tabs. It has many options, see the class SanitySampleFilter to view them formally."},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#parameters-for-simple-filter-type","title":"Parameters for simple filter type","text":"<ul> <li>sig : name of the signal we want to test</li> <li>time_ch , val_ch : time and value channels to test signal with (defaults are 0 and 0)</li> <li>win_time_unit : default Days</li> <li>samples_time_unit: default Date</li> <li>min_val , max_val : min and max allowed values for a signal (&gt;=min &lt;=max) when testing for outliers.</li> <li>win_from, win_to : a time window before and relative to the sample in which we apply the test:<ul> <li>Each of the tests asked for will be done only on the signal results in the given time window.</li> <li>Default is all the tests (infinite window to the past)</li> </ul> </li> <li>min_Nvals, max_Nvals : test min or max number of values (ALL values not just those in a given range if one is given) for the given signal and window.</li> <li>max_outliers : maximal allowed number of outliers for the signal in the given time window</li> <li>min_left : minimal number of results left after throwing the outliers.</li> <li>allowed_values : list of values , seprated by comma (',') . Test verifies that all values (in the given window) are one of the given allowed values.</li> <li>values_in_dictionary : if =1 (default 0) , will test that all the values of the signal are valid values in the repository dictionary. Useful for categorical data such as Drug or RC. A filter can be configured to do one simple test, or several. As explained above many filters can be defined. All filters defined will run on every point (pid,timepoint pair) , and all the filters that did not pass will push their error message into the relevant response messages. For cases in which the sample did not pass a filter defined as ERROR , the AlgoMarker will not generate a score. However for cases which only had warnings, it will. \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#parameters-for-attribute-attr-filter-type","title":"Parameters for attribute ('attr') filter type","text":"<p>To set this filter use 'attr' in filter type (see example above). The use of this filters relies on the fact the model was built with an option to create those attributes and inject them into the resulted MedSamples object at prediction time. The classical use is to let cleaners and testers running in the model to report their results this way. Formally these actions will add an attribute to each prediction, and the filter defined here is able to create rules based on those attributes. Parameters:</p> <ul> <li>name : the name of the attribute to test</li> <li> <p>max : the maximal value allowed. An error or warning will be given for any value larger than this. \u00a0 To make sure a model creates those attributes it is needed to make it do so in its json definition.</p> </li> <li> <p>To a basic cleaner ( \"rp_type\": \"basic_cln\" in json) add :\u00a0\"nrem_attr\": \"nRem\",\"ntrim_attr\": \"nTrim\",\u00a0 \u00a0in order to get an attribute of the number of removed signals, and/or trimmed ones. Note this counts the total number and not in a specific time window....</p> </li> <li>To force a panel use for example:<ul> <li>{\"action_type\":\"rep_processor\",\"rp_type\":\"req\",\"signals\":\"Hemoglobin,RBC,Hematocrit\", \"win_from\": \"0\", \"win_to\": \"365\"},</li> <li>this will count how many of the panel signals are missing in the given time window (relative to prediction point). Note that if placed AFTER cleaners (reccomended) it will test this on the cleaned data that may have some values removed.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#how-to-freeze-a-medialinfra-algomarker-version","title":"How to freeze a MedialInfra AlgoMarker Version","text":"<ol> <li>If using a non frozen libraries version:<ol> <li>Either create a branch with a suitable name for your freeze for the Libs git</li> <li>Tag your version in the branch you want to work with.</li> </ol> </li> <li>It is reccomended to freeze also code for Tools git, and code for releavant projects (for example the Diabetes git project for a diabetes algomarker, etc).</li> <li>When freezing several different gits, make sure to tag all of them with the same tag</li> <li>Document your freeze and branches/tags names.</li> </ol> <ol> <li>Each version that is being compiled with our scripts, contains the git last commit information and date of compilation for both MR_Libs and MR_Tools. No need to tag/create branches or things you might forget. </li> <li>Make sure you have everything prepared: <ol> <li>A model file trained for the AlgoMarker. Better test this very algomarker gives the expected results on your test set, and runs with the frozen libraries version.</li> <li>The repository files you were working with: main needed:</li> <li>the .repository file</li> <li>the .signals file</li> <li>the dictionary files for the signals you are using.</li> </ol> </li> <li>Create a new directory, call it with the agreed upon algomarker+version name</li> <li>Put there: <ol> <li>The algomarker model file</li> <li>The repository files used</li> <li>Prepare an algo marker config file as explained above</li> </ol> </li> <li>Make sure the eligibility rules are the ones you want.</li> <li>Good time to run a small unit-test here to see it loads, runs and gives expected results on some prepared data set.</li> <li>Zip directory.</li> </ol> <p>It is also describred in here </p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/index.html#how-to-test-the-algomarker-library","title":"How to test the AlgoMarker library","text":"<p>The AlgoMarker project includes the DllAPITester, which can be used to test the DLL and compare its output to scores given by the Flow app with existing repository. With this way we can run score compare and check that the results using AlgoMarker library and our tools used in the research are equivelent. After all json/format conversions and using this wrapper. </p> <p>DllAPITester Help</p> <p>DllAPITester Help<pre><code>DllAPITester --h\nReading params\nProgram options:\n  --help                                produce help message\n  --rep arg (=/home/Repositories/THIN/thin_mar2017/thin.repository)\n                                        repository file name\n  --samples arg                         medsamples file to use\n  --model arg                           model file to use\n  --amconfig arg                        algo marker configuration file\n  --direct_test                         split to a dedicated debug routine\n  --test_data arg                       test data for --direct_test option\n  --date arg (=20180101)                test date\n  --egfr_test                           split to a debug routine for the simple\n                                        egfr algomarker\n</code></pre> After compiling, it can be used as follows:</p> <p>Example Run<pre><code>./Linux/Release/DllAPITester --rep /home/Repositories/THIN/thin_jun2017/thin.repository --samples /server/Work/Users/Tal/Temp/test.samples --model /server/Products/Pre2D/QA_Versions/1.0.0.1/pre2d.model --amconfig /server/Products/Pre2D/QA_Versions/1.0.0.1/pre2d.amconfig\n</code></pre> The tester will compare the scores given by both methods and will return passed/failed. For example:</p> <p>Example Output<pre><code>...\n#Res1 :: pid 5000529 time 20060308 pred 0.135475 #Res2 pid 5000529 time 20060308 pred 0.135475\n#Res1 :: pid 5000529 time 20060315 pred 0.090177 #Res2 pid 5000529 time 20060315 pred 0.090177\n#Res1 :: pid 5000529 time 20060818 pred 0.111277 #Res2 pid 5000529 time 20060818 pred 0.111277\n#Res1 :: pid 5000529 time 20070907 pred 0.085166 #Res2 pid 5000529 time 20070907 pred 0.085166\n#Res1 :: pid 5000529 time 20080225 pred 0.103451 #Res2 pid 5000529 time 20080225 pred 0.103451\n#Res1 :: pid 5000529 time 20080411 pred 0.075562 #Res2 pid 5000529 time 20080411 pred 0.075562\n#Res1 :: pid 5000529 time 20090324 pred 0.132835 #Res2 pid 5000529 time 20090324 pred 0.132835\n#Res1 :: pid 5000529 time 20110124 pred 0.170343 #Res2 pid 5000529 time 20110124 pred 0.170343\nComparing 99 scores\n&gt;&gt;&gt;&gt;&gt;TEST1: test DLL API batch: total 99 : n_similar 81 : n_bad 0 : n_miss 18\nPASSED\n</code></pre> \u00a0 Another option is running a direct test (note the self explanatory test_data format):</p> <p>Testing a single example directly<pre><code>Linux/Release/DllAPITester --rep /home/Repositories/THIN/thin_jun2017/thin.repository --amconfig /nas1/Products/Pre2D/QA_Versions/dev/pre2d.amconfig --direct_test --test_data \"Glucose:120:20171101;GENDER:1;BYEAR:1988\" --date 20180520\n</code></pre> and get the result :</p> <p>Single test result<pre><code>...\nAlgomarker Pre2D was loaded with config file /nas1/Products/Pre2D/QA_Versions/dev/pre2d.amconfig\nAdding Data: sig Glucose :: vals: 120.000000,  times: 20171101,\nAdding Data: sig GENDER :: vals: 1.000000,  times:\nAdding Data: sig BYEAR :: vals: 1988.000000,  times:\nCreating Request\nBefore Calculate\n...\n...\nShared Messages: 0\nGot 1 responses\nGetting response no. 0\nResponse Messages: 0\nScore 0 Messages: 0\nresp_rc = 0\ni 0 , pid 1 ts 20180520 scr 0.272233\nptr for _scr_type 5288768\n_scr_type Raw\nFinished debug_me() test\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html","title":"Howto Use AlgoMarker","text":""},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#how-to-deploy-algomarker","title":"How to Deploy AlgoMarker","text":"<p>This guide explains how to set up AlgoMarker to run and expose a predictive model API.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#1-obtain-the-model","title":"1. Obtain the Model","text":"<ul> <li>For public models, simply download the directory containing the model and its configuration files. Full List of Models</li> <li>For your own model, you\u2019ll need to create a configuration file.   See: Setup a new AlgoMarker</li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#2-build-the-algomarker-library","title":"2. Build the AlgoMarker Library","text":"<ul> <li>Follow the AlgoMarker Library Setup to build the <code>libdyn_AlgoMarker.so</code> library, or use a pre-built version if available.</li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#3-build-algomarker_server","title":"3. Build AlgoMarker_Server","text":"<ul> <li>Follow the AlgoMarker_Server Setup to build <code>AlgoMarker_Server</code>, or use an existing compiled binary.</li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#4-run-the-server","title":"4. Run the Server","text":"<p>You can run the server either locally or using Docker/Podman.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#local-server","title":"Local Server","text":"<p>Run the following command, adjusting the port as needed: <pre><code>AlgoMarker_Server --algomarker_path $PATH_TO_AM_CONFIG_FILE --port 1234\n# If the AlgoMarker library (`libdyn_AlgoMarker.so`) is not in a \"lib\" directory next to the config file,\n# specify its location with the \"--library_path\" argument.\n</code></pre> Alternatively, you can use the Python wrapper with FastAPI to expose the AlgoMarker model. Note: The Python wrapper is slower, requires a larger setup, and depends on Python and its libraries. In contrast, <code>AlgoMarker_Server</code> only needs glibc and can run in a minimal (distroless) image. However, the Python wrapper allows for easier code modifications and supports legacy ColonFlag models.</p> <p>To use the Python wrapper, run <code>run_server.sh</code> after setting <code>AM_CONFIG</code> and <code>AM_LIB</code> in the script. Clone the repository containing <code>run_server.sh</code> if needed. More details can be found here</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#using-docker","title":"Using Docker","text":"<ol> <li> <p>Create a Base Image    - The recommended base is the chiselled Ubuntu image (~10MB, minimal attack surface).    - Alternatively, use a full Ubuntu image and add the following to your Dockerfile:      <pre><code>apt-get update &amp;&amp; apt-get install libgomp1 -y\n</code></pre>    - To build the chiselled Ubuntu image, run <code>create.sh</code> in Docker/chiselled-ubuntu.</p> </li> <li> <p>Prepare the Application Directory    - Copy the AlgoMarker directory into the <code>data/app</code> folder next to your Dockerfile. Ensure <code>libdyn_AlgoMarker.so</code> is in a <code>lib</code> directory next to the <code>.amconfig</code> file.    - Copy the <code>AlgoMarker_Server</code> binary into <code>data/app</code>.</p> </li> <li> <p>Build the Docker Image    Use the following Dockerfile template:    <pre><code>FROM chiselled-ubuntu:latest\n\nCOPY data /\n\nENTRYPOINT [ \"/app/AlgoMarker_Server\", \"--algomarker_path\", \"/app/PATH_TO_AM_CONFIG_FILE\", \"--port\", \"1234\", \"--no_print\", \"1\" ]\n</code></pre>    Adjust <code>PATH_TO_AM_CONFIG_FILE</code> and the port as needed. You can also specify the path to <code>libdyn_AlgoMarker.so</code> using the <code>--library_path</code> argument.</p> </li> </ol> <p>Build the image with:    <pre><code>docker build -t algomarker_X --no-cache .\n</code></pre></p> <ol> <li>Run a Container    execute this command <code>docker run -id algomarker_X --name algomarker_container_X -p 1234:1234</code> and expose port 1234 to 1234 in your local machine.</li> </ol> <p>[!NOTE] You can replace all the <code>docker</code> commands with <code>podman</code>, it is fully compatible.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Howto%20Use%20AlgoMarker.html#how-to-use-the-deployed-algomarker","title":"How to Use the Deployed AlgoMarker","text":"<p>Once the server is running, it exposes two main API endpoints:</p> <ol> <li> <p>GET <code>/discovery</code>    - No parameters required.    - Returns a JSON specification describing the AlgoMarker (name, inputs, etc.).</p> </li> <li> <p>POST <code>/calculate</code>    - Accepts a JSON request with either a single patient or a batch of patients.    - Request format details: Request json format</p> </li> </ol> <p>Example Response: Example Response<pre><code>{\n  \"type\": \"response\",\n  \"request_id\": \"999ef0f4-1099-4178-8c86-ecbfac6578e2\",\n  \"responses\": [\n    {\n      \"patient_id\": \"1\",\n      \"time\": \"20250806\",\n      \"prediction\": \"0.004082\",\n      \"flag_threshold\": \"USPSTF_50-80$PR_03.000\",\n      \"flag_result\": 0,\n      \"messages\": [\n        \"(320)An outlier was found and ignored in signal: RBC\"\n      ]\n    }\n  ]\n}\n</code></pre></p> Response with Explainability <pre><code>{\n    \"type\": \"response\",\n    \"request_id\": \"999ef0f4-1099-4178-8c86-ecbfac6578e2\",\n    \"responses\": [\n        {\n            \"patient_id\": \"1\",\n            \"time\": \"20230611\",\n            \"messages\": [\n                \"(320)An outlier was found and ignored in signal: RBC\",\n            ],\n            \"prediction\": \"0.004082\",\n            \"explainability_output_field_name_for_your_control\": {\n                \"static_info\": [\n                    {\n                        \"signal\": \"Age\",\n                        \"unit\": \"Year\",\n                        \"value\": \"45\"\n                    },\n                    {\n                        \"signal\": \"GENDER\",\n                        \"unit\": \"\",\n                        \"value\": \"1.000000\"\n                    },\n                    {\n                        \"signal\": \"Hemoglobin\",\n                        \"unit\": \"g/dL\",\n                        \"value\": \"14.500000\"\n                    }\n                ],\n                \"explainer_output\": [\n                    {\n                        \"contributor_name\": \"Age\",\n                        \"contributor_value\": -1.257716417312622,\n                        \"contributor_percentage\": 42.327555760464165,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"Age\",\n                                \"feature_value\": 45.0\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 2,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [\n                            {\n                                \"signal\": \"Age\",\n                                \"unit\": [\n                                    \"Year\"\n                                ],\n                                \"timestamp\": [],\n                                \"value\": [\n                                    \"45.000000\"\n                                ]\n                            }\n                        ]\n                    },\n                    {\n                        \"contributor_name\": \"MCH_Values\",\n                        \"contributor_value\": -0.2684820294380188,\n                        \"contributor_percentage\": 9.035572693121699,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000008.MCH.last.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000019.MCH.last.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000030.MCH.last.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000041.MCH.last.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000052.MCH.last.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000118.MCH.first.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000129.MCH.first.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000140.MCH.first.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000151.MCH.first.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000162.MCH.first.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000173.MCH.last2.win_0_10000\",\n                                \"feature_value\": 29.091323852539063\n                            },\n                            {\n                                \"feature_name\": \"FTR_000184.MCH.last2.win_0_1000\",\n                                \"feature_value\": 29.041757583618164\n                            },\n                            {\n                                \"feature_name\": \"FTR_000195.MCH.last2.win_0_730\",\n                                \"feature_value\": 29.01051139831543\n                            },\n                            {\n                                \"feature_name\": \"FTR_000206.MCH.last2.win_0_360\",\n                                \"feature_value\": 28.922163009643555\n                            },\n                            {\n                                \"feature_name\": \"FTR_000217.MCH.last2.win_0_180\",\n                                \"feature_value\": 28.734722137451172\n                            },\n                            {\n                                \"feature_name\": \"FTR_000228.MCH.avg.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000239.MCH.avg.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000250.MCH.avg.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000261.MCH.avg.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000272.MCH.avg.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000283.MCH.min.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000294.MCH.min.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000305.MCH.min.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000316.MCH.min.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000327.MCH.min.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000338.MCH.max.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000349.MCH.max.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000360.MCH.max.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000371.MCH.max.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000382.MCH.max.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000503.MCH.last2_time.win_0_10000\",\n                                \"feature_value\": 449.85882568359375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000514.MCH.last2_time.win_0_1000\",\n                                \"feature_value\": 315.7171325683594\n                            },\n                            {\n                                \"feature_name\": \"FTR_000525.MCH.last2_time.win_0_730\",\n                                \"feature_value\": 268.54010009765625\n                            },\n                            {\n                                \"feature_name\": \"FTR_000536.MCH.last2_time.win_0_360\",\n                                \"feature_value\": 161.85084533691406\n                            },\n                            {\n                                \"feature_name\": \"FTR_000547.MCH.last2_time.win_0_180\",\n                                \"feature_value\": 85.85099792480469\n                            },\n                            {\n                                \"feature_name\": \"FTR_001148.MCH.last.win_730_10000\",\n                                \"feature_value\": 29.159976959228516\n                            },\n                            {\n                                \"feature_name\": \"FTR_001159.MCH.min.win_730_10000\",\n                                \"feature_value\": 28.687816619873047\n                            },\n                            {\n                                \"feature_name\": \"FTR_001170.MCH.max.win_730_10000\",\n                                \"feature_value\": 29.749162673950195\n                            },\n                            {\n                                \"feature_name\": \"FTR_001208.MCH.Estimate.180\",\n                                \"feature_value\": 28.93360137939453\n                            },\n                            {\n                                \"feature_name\": \"FTR_001208.MCH.Estimate.360\",\n                                \"feature_value\": 28.951967239379883\n                            },\n                            {\n                                \"feature_name\": \"FTR_001208.MCH.Estimate.730\",\n                                \"feature_value\": 28.965957641601563\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    },\n                    {\n                        \"contributor_name\": \"MCV_Values\",\n                        \"contributor_value\": -0.1519976258277893,\n                        \"contributor_percentage\": 5.11537231036717,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000007.MCV.last.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000018.MCV.last.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000029.MCV.last.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000040.MCV.last.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000051.MCV.last.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000117.MCV.first.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000128.MCV.first.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000139.MCV.first.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000150.MCV.first.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000161.MCV.first.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000172.MCV.last2.win_0_10000\",\n                                \"feature_value\": 87.35523986816406\n                            },\n                            {\n                                \"feature_name\": \"FTR_000183.MCV.last2.win_0_1000\",\n                                \"feature_value\": 87.30293273925781\n                            },\n                            {\n                                \"feature_name\": \"FTR_000194.MCV.last2.win_0_730\",\n                                \"feature_value\": 87.2611312866211\n                            },\n                            {\n                                \"feature_name\": \"FTR_000205.MCV.last2.win_0_360\",\n                                \"feature_value\": 87.1921615600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000216.MCV.last2.win_0_180\",\n                                \"feature_value\": 86.85134887695313\n                            },\n                            {\n                                \"feature_name\": \"FTR_000227.MCV.avg.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000238.MCV.avg.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000249.MCV.avg.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000260.MCV.avg.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000271.MCV.avg.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000282.MCV.min.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000293.MCV.min.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000304.MCV.min.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000315.MCV.min.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000326.MCV.min.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000337.MCV.max.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000348.MCV.max.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000359.MCV.max.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000370.MCV.max.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000381.MCV.max.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000502.MCV.last2_time.win_0_10000\",\n                                \"feature_value\": 449.8634948730469\n                            },\n                            {\n                                \"feature_name\": \"FTR_000513.MCV.last2_time.win_0_1000\",\n                                \"feature_value\": 315.7387390136719\n                            },\n                            {\n                                \"feature_name\": \"FTR_000524.MCV.last2_time.win_0_730\",\n                                \"feature_value\": 268.5698547363281\n                            },\n                            {\n                                \"feature_name\": \"FTR_000535.MCV.last2_time.win_0_360\",\n                                \"feature_value\": 161.91400146484375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000546.MCV.last2_time.win_0_180\",\n                                \"feature_value\": 85.87918853759766\n                            },\n                            {\n                                \"feature_name\": \"FTR_001147.MCV.last.win_730_10000\",\n                                \"feature_value\": 87.29901123046875\n                            },\n                            {\n                                \"feature_name\": \"FTR_001158.MCV.min.win_730_10000\",\n                                \"feature_value\": 85.86404418945313\n                            },\n                            {\n                                \"feature_name\": \"FTR_001169.MCV.max.win_730_10000\",\n                                \"feature_value\": 88.5106430053711\n                            },\n                            {\n                                \"feature_name\": \"FTR_001207.MCV.Estimate.180\",\n                                \"feature_value\": 87.37998962402344\n                            },\n                            {\n                                \"feature_name\": \"FTR_001207.MCV.Estimate.360\",\n                                \"feature_value\": 87.38853454589844\n                            },\n                            {\n                                \"feature_name\": \"FTR_001207.MCV.Estimate.730\",\n                                \"feature_value\": 87.39167022705078\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [\n                            {\n                                \"signal\": \"MCV\",\n                                \"unit\": [\n                                    \"fL\"\n                                ],\n                                \"timestamp\": [\n                                    20230611\n                                ],\n                                \"value\": [\n                                    \"82.000000\"\n                                ]\n                            }\n                        ],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    },\n                    {\n                        \"contributor_name\": \"MCH_Trends\",\n                        \"contributor_value\": -0.14473219215869904,\n                        \"contributor_percentage\": 4.8708592685138346,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000063.MCH.slope.win_0_10000\",\n                                \"feature_value\": -0.09105083346366882\n                            },\n                            {\n                                \"feature_name\": \"FTR_000074.MCH.slope.win_0_1000\",\n                                \"feature_value\": -0.03978761285543442\n                            },\n                            {\n                                \"feature_name\": \"FTR_000085.MCH.slope.win_0_730\",\n                                \"feature_value\": -0.008064992725849152\n                            },\n                            {\n                                \"feature_name\": \"FTR_000096.MCH.slope.win_0_360\",\n                                \"feature_value\": 0.014474527910351753\n                            },\n                            {\n                                \"feature_name\": \"FTR_000107.MCH.slope.win_0_180\",\n                                \"feature_value\": -0.00038833924918435514\n                            },\n                            {\n                                \"feature_name\": \"FTR_000393.MCH.std.win_0_10000\",\n                                \"feature_value\": 0.5682898759841919\n                            },\n                            {\n                                \"feature_name\": \"FTR_000404.MCH.std.win_0_1000\",\n                                \"feature_value\": 0.4634387791156769\n                            },\n                            {\n                                \"feature_name\": \"FTR_000415.MCH.std.win_0_730\",\n                                \"feature_value\": 0.436860591173172\n                            },\n                            {\n                                \"feature_name\": \"FTR_000426.MCH.std.win_0_360\",\n                                \"feature_value\": 0.3896379768848419\n                            },\n                            {\n                                \"feature_name\": \"FTR_000437.MCH.std.win_0_180\",\n                                \"feature_value\": 0.35533618927001953\n                            },\n                            {\n                                \"feature_name\": \"FTR_000558.MCH.last_delta.win_0_10000\",\n                                \"feature_value\": -0.06643807888031006\n                            },\n                            {\n                                \"feature_name\": \"FTR_000569.MCH.last_delta.win_0_1000\",\n                                \"feature_value\": -0.03644682839512825\n                            },\n                            {\n                                \"feature_name\": \"FTR_000580.MCH.last_delta.win_0_730\",\n                                \"feature_value\": -0.024617791175842285\n                            },\n                            {\n                                \"feature_name\": \"FTR_000591.MCH.last_delta.win_0_360\",\n                                \"feature_value\": 0.008983400650322437\n                            },\n                            {\n                                \"feature_name\": \"FTR_000602.MCH.last_delta.win_0_180\",\n                                \"feature_value\": 0.0546439066529274\n                            },\n                            {\n                                \"feature_name\": \"FTR_001108.MCH.win_delta.win_0_180_360_10000\",\n                                \"feature_value\": -0.12988980114459991\n                            },\n                            {\n                                \"feature_name\": \"FTR_001128.MCH.win_delta.win_0_180_730_10000\",\n                                \"feature_value\": -0.1698904186487198\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    },\n                    {\n                        \"contributor_name\": \"MCHC-M_Values\",\n                        \"contributor_value\": -0.13522081077098846,\n                        \"contributor_percentage\": 4.550760668340038,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000009.MCHC-M.last.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000020.MCHC-M.last.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000031.MCHC-M.last.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000042.MCHC-M.last.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000053.MCHC-M.last.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000119.MCHC-M.first.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000130.MCHC-M.first.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000141.MCHC-M.first.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000152.MCHC-M.first.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000163.MCHC-M.first.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000174.MCHC-M.last2.win_0_10000\",\n                                \"feature_value\": 33.27936935424805\n                            },\n                            {\n                                \"feature_name\": \"FTR_000185.MCHC-M.last2.win_0_1000\",\n                                \"feature_value\": 33.24119186401367\n                            },\n                            {\n                                \"feature_name\": \"FTR_000196.MCHC-M.last2.win_0_730\",\n                                \"feature_value\": 33.22017288208008\n                            },\n                            {\n                                \"feature_name\": \"FTR_000207.MCHC-M.last2.win_0_360\",\n                                \"feature_value\": 33.142608642578125\n                            },\n                            {\n                                \"feature_name\": \"FTR_000218.MCHC-M.last2.win_0_180\",\n                                \"feature_value\": 33.04962158203125\n                            },\n                            {\n                                \"feature_name\": \"FTR_000229.MCHC-M.avg.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000240.MCHC-M.avg.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000251.MCHC-M.avg.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000262.MCHC-M.avg.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000273.MCHC-M.avg.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000284.MCHC-M.min.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000295.MCHC-M.min.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000306.MCHC-M.min.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000317.MCHC-M.min.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000328.MCHC-M.min.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000339.MCHC-M.max.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000350.MCHC-M.max.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000361.MCHC-M.max.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000372.MCHC-M.max.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000383.MCHC-M.max.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000504.MCHC-M.last2_time.win_0_10000\",\n                                \"feature_value\": 449.85882568359375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000515.MCHC-M.last2_time.win_0_1000\",\n                                \"feature_value\": 315.7171325683594\n                            },\n                            {\n                                \"feature_name\": \"FTR_000526.MCHC-M.last2_time.win_0_730\",\n                                \"feature_value\": 268.54010009765625\n                            },\n                            {\n                                \"feature_name\": \"FTR_000537.MCHC-M.last2_time.win_0_360\",\n                                \"feature_value\": 161.85084533691406\n                            },\n                            {\n                                \"feature_name\": \"FTR_000548.MCHC-M.last2_time.win_0_180\",\n                                \"feature_value\": 85.85099792480469\n                            },\n                            {\n                                \"feature_name\": \"FTR_001149.MCHC-M.last.win_730_10000\",\n                                \"feature_value\": 33.386741638183594\n                            },\n                            {\n                                \"feature_name\": \"FTR_001160.MCHC-M.min.win_730_10000\",\n                                \"feature_value\": 33.00123977661133\n                            },\n                            {\n                                \"feature_name\": \"FTR_001171.MCHC-M.max.win_730_10000\",\n                                \"feature_value\": 33.99413299560547\n                            },\n                            {\n                                \"feature_name\": \"FTR_001209.MCHC-M.Estimate.180\",\n                                \"feature_value\": 33.09005355834961\n                            },\n                            {\n                                \"feature_name\": \"FTR_001209.MCHC-M.Estimate.360\",\n                                \"feature_value\": 33.112545013427734\n                            },\n                            {\n                                \"feature_name\": \"FTR_001209.MCHC-M.Estimate.730\",\n                                \"feature_value\": 33.133628845214844\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    }\n                ]\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Nagative <code>contributor_value</code> means the concept resulted in lower score, positive value means the concept contributed to increas risk score</li> <li>The contribution magnitude is reported as <code>contributor_level</code> and the maximal scale is <code>contributor_level_max</code>. Since <code>contributor_value</code> magnitude is hard to interapt we used those scaling to report contribution magnitude </li> </ul> <p>Additional fields may be included in the response, for example, if explainability is requested.</p> <p>For the full API specification, refer to: AlgoMarker Spec</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Request%20Json%20Format.html","title":"Request Json Format","text":""},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Request%20Json%20Format.html#algomarker-request-format","title":"AlgoMarker Request Format","text":"<p>For complete details, see the AlgoMarker Spec.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Request%20Json%20Format.html#required-fields","title":"Required Fields","text":"<p>A valid request to AlgoMarker should include the following fields:</p> <ul> <li>type: Set to <code>\"request\"</code>.</li> <li>request_id: A unique string identifier for the request.</li> <li>export: Specifies the desired output. For standard predictions, use <code>\"prediction\": \"pred_0\"</code> to request the model prediction and store it in the <code>\"prediction\"</code> field of the response. You can also specify request to recieve explainabiltiy output if applicable for example by specifiying <code>\"explainability_output_field_name_for_your_control\": \"json_attr Tree_iterative_covariance\"</code>.</li> <li>load: Set to <code>1</code> to indicate that patient data is included in the request.</li> <li>flag_threshold (optional): If your model supports configurable thresholds, specify the threshold name here.</li> <li>requests: An array of patient requests, each containing:<ul> <li>patient_id: An integer patient identifier.</li> <li>time: The calculation date in <code>YYYYMMDD</code> format. Any data after this date will be ignored from score calculation.</li> <li>data: Patient data, including:<ul> <li>signals: An array of signal objects, each with:<ul> <li>code: The signal name.</li> <li>data: An array of data points, each with:<ul> <li>timestamp: An array of timestamps (e.g., <code>[20250806]</code>). Can be empty for static values.</li> <li>value: An array of values (e.g., <code>[\"14.5\"]</code> or <code>[82]</code>). You can pass string/float, both OK.</li> </ul> </li> <li>unit (optional): The unit for the signal (e.g., <code>\"fL\"</code>).</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Request%20Json%20Format.html#example-signal-entry","title":"Example Signal Entry","text":"Example Signal Entry<pre><code>{\n    \"type\": \"request\",\n    \"request_id\": \"999ef0f4-1099-4178-8c86-ecbfac6578e2\",\n    \"export\": {\n        \"prediction\": \"pred_0\"\n    },\n    \"flag_threshold\": \"USPSTF_50-80$PR_03.000\",\n    \"load\": 1,\n    \"requests\": [\n        {\n            \"time\": \"20250806\",\n            \"patient_id\": \"1\",\n            \"data\": {\n                \"signals\": [\n                    {\n                        \"code\": \"Hemoglobin\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"14.5\"\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"Hematocrit\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"32\"\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"RBC\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"40000.5\"\n                                ],\n                                \"timestamp\": [\n                                    20230806\n                                ]\n                            },\n                            {\n                                \"value\": [\n                                    \"4.5\"\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"MCV\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    82\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n\n                    {\n                        \"code\": \"GENDER\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"Male\"\n                                ],\n                                \"timestamp\": []\n                            }\n                        ]\n                    },\n                    {\n                        \"code\": \"BDATE\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    19780101\n                                ],\n                                \"timestamp\": []\n                            }\n                        ]\n                    }\n                ]\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Setup%20a%20new%20AlgoMarker.html","title":"Setup a new AlgoMarker","text":""},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Setup%20a%20new%20AlgoMarker.html#algomarker-configuration-guide","title":"AlgoMarker Configuration Guide","text":"<p>A sample packaging script can be found in GastroFlag: https://github.com/Medial-EarlySign/AM_LGI/blob/main/Extended_model/scripts/pack_algomarker.sh</p> <p>The setup script defines variables for the model, repository, reference matrix, deployment path, and more.</p>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/Setup%20a%20new%20AlgoMarker.html#typical-steps","title":"Typical Steps","text":"<ol> <li> <p>Model Adjustments (Optional but Recommended)</p> <ul> <li>Uses tools <code>Flow</code>, <code>adjust_model</code>, and <code>change_model</code> operations to modify the model as needed. For example:<ul> <li><code>Flow --export_production_model</code>: Enables verbose outlier reporting (useful for production, but may slow down predictions and increase memory usage).</li> <li><code>adjust_model</code>: Perform operations such as adding virtual signals (e.g., \"eGFR\") or adding checks (e.g., ensuring a CBC exists on the prediction date and storing the result as an attribute for eligibility filtering).</li> <li><code>change_model</code>: Enable explainability output for AlgoMarker by setting <code>store_as_json=1</code>.</li> </ul> </li> <li>These steps are optional and depend on your production requirements. If you do not need outlier documentation or explainability, you may skip them.</li> </ul> </li> <li> <p>Copy Model Files</p> <ul> <li>Copy the binary model file to the AlgoMarker directory.</li> <li>Document the original model path and its MD5 checksum (recommended).</li> </ul> </li> <li> <p>Build and Copy the Shared Library</p> <ul> <li>Build the <code>libdyn_AlgoMarker.so</code> shared library as described in AlgoMarker Library Setup.</li> <li>Copy the compiled library to the AlgoMarker folder under \"lib\" folder.</li> </ul> </li> <li> <p>Prepare the Repository Configuration</p> <ul> <li>Generate a repository config file containing only the required signals and dictionaries. This defines the signal names, types, and categorical mappings for AlgoMarker.</li> <li>You can copy the config files from the repository used during model training, but it is recommended to clean them up and remove unused signals. A script is available to assist with this from GastroFlag.</li> </ul> </li> <li> <p>Create the AlgoMarker Configuration File (<code>amconfig</code>)</p> <ul> <li>See the Configuration File section for details and a full example.</li> <li>Keep <code>TYPE</code> as <code>MEDIAL_INFRA</code></li> <li><code>NAME</code> is free text to store name for AlgoMarker and will be returned in discovery call request.</li> <li>The <code>FILTER</code> rows allow you to define eligibility rules and warnings. For testing, you may skip these filters.</li> </ul> <p>Example snippet: <pre><code>#################################################################################\n# MedialInfra AlgoMarker config example file\n#################################################################################\nTYPE    MEDIAL_INFRA\nNAME    PRE2D\nREPOSITORY  rep/pre2d.repository\nTIME_UNIT   Date\nMODEL   resources/pre2d.model\n# Eligibility filters and other settings follow...\n</code></pre></p> <ul> <li>Filters can enforce rules such as minimum/maximum number of results, allowed values, age ranges, dictionary validation, and outlier limits.</li> <li>See the full example in Eligibility rules configuration section</li> <li><code>REPOSITORY</code> and <code>MODEL</code> refer to file paths: <code>REPOSITORY</code> is the path to the repository config file (used in Step 4), and <code>MODEL</code> is the path to the model binary (used in Step 2). </li> </ul> <p>It's recommended to use relative paths and place both files in the same folder hierarchy for simplicity.</p> </li> <li> <p>Generate Explainability Config (Optional)</p> <ul> <li>If your model supports explainability, create a configuration file (e.g., <code>resources/explainer.cfg</code>) to control display settings for each group, such as which signals to show and how many values to fetch.</li> </ul> </li> <li> <p>Create Leaflet/Cutoff Threshold Table (Optional)</p> <ul> <li>Generate a cutoff threshold table config file (e.g., from <code>bootstrap_app</code> output) if you want to store thresholds in the AlgoMarker.</li> </ul> </li> <li> <p>Copy Test Script Template (Optional)</p> <ul> <li>Optionally, include a script template for testing AlgoMarker deployments.</li> </ul> </li> </ol>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html","title":"The new DllAPITester","text":""},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#the-dllapitester-is-a-testing-tool-for-algomarkers-that-contains-several-useful-options","title":"The DllAPITester is a testing tool for AlgoMarkers that contains several useful options:","text":"<ul> <li>Testing results on a sample set via the infrastructure and via the AlgoMarker</li> <li>Testing via the AlgoMarker library in the infrastructure or via the .so compiled for it.</li> <li>Generating json examples from data.</li> <li>Testing on json examples.</li> <li>Generating dictionaries for AlgoMarkers.</li> <li>Testing also using the newer json requests and json responses. It is a must use tool whenever packing a new algomarker. In the following explanations we will assume one has an AlgoMarker with a model inside, a repository to test on, and a samples file to work with. \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#general-app-parameters","title":"General App Parameters","text":"<p>The App is in ../MR/Libs/Internal/AlgoMarker/Linux/Release/DllAPITester , to compile simply compile the AlgoMarker directory (smake_rel). Major parameters: \u00a0</p> parameter comment rep repository to use samples samples file model the model file (typically the one in the AlgoMarker wrap) amconfig the algomarker (config) to work with amlib the actual .so to use when calling AM_API calls. Optional. Without it will use the AM_API as is currently in the infrastructure. json_dict A list of dictionaries to load into the AlgoMarker when loading it (prior to actual usage of loading data and getting results) am_res_file (optional) The predictions file of the AlgoMarker as generated by calling the AlgoMarker direct_csv (optional) The full feature matrix generated by running through the infrastructure am_csv (optional) The full feature matrix generated by running through the AlgoMarker single Run tests one by one rather than in batch. Slower. Some modes, such as generating json outputs work only in single mode. out_jsons if given will generate a file with a list of jsons (in a long array) that contain all the data needed to give a prediction. These can be used for direct tests in the AlgoAnalyzer. in_jsons if given will take input data from the given jsons. jreq input json request (could contain also data) jresp get output as a json response create_jreq generate a request for a given samples file (will need also jreq_defs, optionally the --add_data_to_jreq flag) and an output file in --jreq_out)"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#score-compare-testing-results-of-an-algomarker-vs-results-directly-from-the-infrastructure-and-much-more","title":"Score Compare: Testing results of an AlgoMarker vs. results directly from the infrastructure, and much more","text":"<p>This test is currently testing the prediction only (for single prediction models). Not testing other accompanied infomation that could be on top in json responses. It is the major test to be done, to run it: DllAPITester --rep  --samples  --model  --amconfig  This will so a batch run and test the whole samples, producing a summary report. In many cases you would need to add: <ul> <li>--json_dict  : if you require additional dictionaries to be loaded. <li>--single : if you wish to do the test in single mode, one by one, mimicking how it is used by AlgoAnalyzer.</li> <li>optional output files:</li> <li>--amlib  : point to an .so file (typically the one in the AlgoMarker library). If given , the run will use the API from the library, as would happen in real life usage. Without it the API as is currently in the infrastructure will be used. <li>--out_jsons  : generating json examples out of this run. <li>--am_csv  , --direct_csv  : when needing the actual feature matrix via the AlgoMarker or via the infrastructure (or both). <li>--am_res_file : the predictions as generated by the AlgoMarker. \u00a0 An example of a full run example with some options on: <pre><code>DllAPITester --rep /home/Repositories/KPNW/kpnw_apr20/kpnw.repository --samples ./nwp_100.samples --model /nas1/Products/COVID19/QA_Versions/dev_20200608/COVID19-Comp-Flag-2020-05-04.model --amconfig /nas1/Products/COVID19/QA_Versions/dev_20200608/COVID19-Comp-Flag-2020-05-04.amconfig --json_dict /nas1/Work/AlgoMarkers/COVID19/Avi/DIAGNOSIS.txt,/nas1/Work/AlgoMarkers/COVID19/Avi/Drug.txt,/nas1/Work/AlgoMarkers/COVID19/Avi/ADMISSION.txt --single --am_res am.preds --out_jsons n100.jsons\n</code></pre> This example will test the covid predictor on some samples, on the requested repository, but also load some dictionaries prior to that, do the run in single mode, and output both the predictions into a file, and a file containing jsons for all the samples.</li>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#generating-json-outputs","title":"Generating json outputs","text":"<p>To do that simply run a score compare run with the out_jsons file requested. You need to be in single mode for this option to work. You can set up some parameters for the creation (optional):</p> <ul> <li>--accountId  :\u00a0 accountId for output json <li>--calculator  :\u00a0 calculator name for output json <li>--units  : list of units to add for requested signals (example: BMI,kg/m^2,Weight,kg,Height,cm,Pack_Years,pack*years,Smoking_Intensity,cigs/day,Smoking_Quit_Date,date,Smoking_Duration,years) <li>--scoreOnDate : flag, use to have that field in output jsons. \u00a0 The jsons will be created with a request_id field \"req_id\" of the form : req__ , coding the actual time for prediction that it was made for."},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#running-on-input-json-examples","title":"Running on input json examples","text":"<p>Sometimes it is useful to generate a run on a given file of jsons, in order to recreate a bug or a prediction on specific cases. This can be done by simply using the --in_jsons option (if given , then you don't need a repository and samples). Currently this option runs over the jsons in the given file one by one (as in single mode), and prints the score to each. It doesn't compare it to anything. You can generate a preds file for all the given jsons using the --preds_json  option. You can also generate the matrix using the --am_csv option. However this currently will simply write the matrix for the last json given in the file (could be improved in future)."},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#generating-dictionaries-for-algomarkers","title":"Generating Dictionaries for AlgoMarkers","text":"<p>AlgoMarkers with categorial signals may need accompanying dictionaries to adjust them to the actual dictionaries in the AlgoMarker. Those dictionaries can be created in anyway and manner as long as they are in the correct format and meaning at the end. The DllAPITester allows generating such dictionaries for a given AlgoMarker and repository. To do that one must supply a config file. The config file is composed of tab delimited lines with 3 fields:   IN means : the value is an input categorial value OUT means : an output value, a valid value to send to the AlgoMarker. There are 2 possible formates for the output, for the one used by the AlgoAnalyzer use --simple_dict option. Example run: <pre><code>DllAPITester --rep /home/Repositories/KPNW/kpnw_apr20/kpnw.repository --dicts_config ../../../NWP/covid_testing/dev_20200504/full_dicts/dicts.config --out_json_dict ./dict.json --simple_dict\n</code></pre>"},{"location":"Infrastructure%20C%20Library/AlgoMarkers/The%20new%20DllAPITester.html#working-with-input-json-requests-and-output-json-responses","title":"Working with input json requests and output json responses","text":"<p>The newer APIs in the library allow for a new way of generating outputs. It is possible now to load data from a rep+samples or in_jsons (As shown above), and then use a json request (\u2013jreq option) for inputs, and get out a json response\u00a0(\u2013jresp). More than that: requests can contain actual data for the patients and if that is used then one can skip loading the data from a repository and samples or in_jsons. To use these options simply do a usual run, but then add a valid --jreq  and an output --jresp . The DllAPITester contains also options to generate such json requests with/without data. (TBD: detailed description of the json request input options, and the structure of the output json response)."},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html","title":"MedProcessTools Library","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#data-handlers","title":"Data Handlers:","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#medregistry-handling-a-registry-individuals-dated-outcome-a-more-generic-format-of-medcohort","title":"MedRegistry:\u00a0Handling a registry - individuals + (dated) outcome (A more generic format of MedCohort)","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#wrapper-class-for-medregistry-and-additional-labeling-policy-arguments-like-time-window-to-define-labels-from-registry-for-each-sample-time","title":": Wrapper class for MedRegistry and additional labeling policy arguments (like time window) to define labels from registry for each sample time","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#handling-of-creating-medsamples-from-medregistry","title":": Handling of creating MedSamples from MedRegistry","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#medcohort-handling-a-cohort-individuals-dated-outcome-follow-periods","title":"MedCohort: Handling a cohort - individuals + (dated) outcome + follow-periods","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#medsamples-handling-samples-individual-dated-outcome-time-points","title":"MedSamples: Handling samples - individual + (dated) outcome + time-points","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#medfeatures-handling-a-features-matrix-samples-features-information","title":"MedFeatures: Handling a features matrix : samples + features information","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#serializableobject-base-class-for-handling-serializations","title":"SerializableObject:\u00a0 Base class for Handling Serializations","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#data-processors","title":"Data Processors:","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/index.html#analysis-processors","title":"Analysis Processors:","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedBootstrap.html","title":"MedBootstrap","text":"<p>Example Use Function <pre><code>#define TEMP_PATH \"/tmp/\"\nvoid run_bootstrap_on_samples(const string &amp;samples_path, map&lt;string, map&lt;string, float&gt;&gt; &amp;res, const string &amp;inc_file = \"\") {\n    MedSamples smp;\n    if (smp.read_from_file(samples_path) &lt; 0)\n        MTHROW_AND_ERR(\"Couldn't read file %s\", samples_path.c_str());\n    //Example for creating configuraiton file for cohorts\n    ofstream fw(TEMP_PATH \"bootstrap_new.params\");\n    // COHORT_NAME TAB CHOROT_DEFINITION\n    // CHOROT_DEFINITION - is list of paramters with min,max range. each param is seprated by ;\n    fw &lt;&lt; \"Time_Window:0-365\" &lt;&lt; \"\\t\" &lt;&lt; \"Time-Window:0,365\" &lt;&lt; endl;\n    fw &lt;&lt; \"Time_Window:0-365,Age:40-80\" &lt;&lt; \"\\t\" &lt;&lt; \"Time-Window:0,365;Age:40,80\" &lt;&lt; endl;\n    fw.close();\n    map&lt;string, vector&lt;float&gt;&gt; additional_info;\n    MedBootstrap boot(\"sample_ratio=1.0;sample_per_pid=1;loopCnt=500;\" \"filter_cohort=\" + TEMP_PATH + \"bootstrap_new.params\");\n    string addition = \"\";\n    if (!inc_file.empty())\n        addition = \";inc_stats_text=\" + inc_file;\n    boot.roc_Params = ROC_Params(\"working_point_FPR=0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,10;\" + \"working_point_SENS=5,10,20,30,40,50,60,70,80,90;score_resolution=0.0001;score_bins=0\" + addition);\n    if (addition.empty())\n        res = boot.booststrap(smp, additional_info);\n    else\n        res = boot.booststrap(smp, \"/home/Repositories/THIN/thin_mar2017/thin.repository\");\n}\n</code></pre> Example Code for MedBootstrap (you may look at\u00a0$MR_ROOT/Projects/Shared/UnitTestingInfra for the testing process) There is an application which uses this libray - bootstrap_app \u00a0 How to run bootstrap on a custom measurement function: <pre><code>//string bootstrap_params - is the init string for bootstrap parameters\n//string cohort_file - the file with cohorts definitions\n//final_feats - the MedFeature matrix to use in bootstrap, can be also MedSamples (than you can only filter Age,Gender,TimeWindow) in cohorts\n\u00a0\n//the measurement function - has Lazy_Iterator to iterate over label,pred,weight in bootstrap loop\n// can also get additional arguments for the function in \"params\" - can be working points in ROC for example\n//the function returns the measurements\nmap&lt;string, float&gt; calc_acc(Lazy_Iterator *iterator, int thread_num, Measurement_Params *params) {\n    map&lt;string, float&gt; res;\n    float pred_val, label, weight;\n    double total_cnt = 0, sum_prd = 0;\n    while (iterator-&gt;fetch_next_external(thread_num, label, pred_val, weight)) {\n        total_cnt += weight != -1 ? weight : 1;\n        sum_prd += (pred_val == label) * (weight != -1 ? weight : 1);\n        //sum_lbls += label;\n    }\n    total_cnt += weight != -1 ? weight : 1;\n    sum_prd += (pred_val == label) * (weight != -1 ? weight : 1);\n    sum_prd /= total_cnt;\n    res[\"ACCURACY\"] = sum_prd;\n    return res;\n}\n\u00a0\nMedBootstrapResult b;\nb.bootstrap_params.init_from_string(bootstrap_params);\nb.bootstrap_params.parse_cohort_file(cohort_file);\nMeasurementFunctions boot_function = calc_acc;\nb.bootstrap_params.measurements_with_params = { pair&lt;MeasurementFunctions, Measurement_Params *&gt;(calc_acc, NULL) }; //calc_auc doesn't have additional parameters so I passed NULL\nb.bootstrap(final_feats);\nb.write_results_to_text_file(\"/tmp/results.csv\");\n</code></pre> </p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedBootstrap.html#improvment-ideas","title":"Improvment Ideas:","text":"<ul> <li>add support for more complicated conditions in MedBootstrap itself - for example and,or conditions on the ranges</li> <li>add support for calculating bootstrap on multiple predictions on the same samples - it can shortened the running time significantly when comparing/running on same samples with diffrent models\\scores.\u00a0now you need to call bootstrap in a loop for each score and each call will randomize again the bootstrap cohort and samples \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedBootstrap.html#cohorts-file-format","title":"Cohorts file format:","text":"<p>The file format may be in 2 options: 1. COHORT_NAME[TAB]PARAMETERS_DEF - cohort name is string representing cohort\u00a0name. PARAMETER_DEF is in format: \"PARAMETER_NAME:MIN_RANGE,MAX_RANGE;...\"\u00a0the format can repeat itself with \";\" between each parameter. the cohort\u00a0will consist of intersection between all parameters ranges with \"and\" condition.\u00a0there is single tab betwwen the name and the defenition.\u00a0Example Line:\u00a01 year back &amp; age 40-80 Time-Window:0,365;Age:40,80\u00a0will create cohort called \"1 year back &amp; age 40-80\" and will filter out records\u00a0with (Time-Window&gt;=0 and Time-Window&lt;=365) and (Age&gt;=40 and Age&lt;=80)\u00a0 2. MULTI[TAB]PARAMETERS_DEF[TAB]...PARAMETERS_DEF[TAB] - this definition with\u00a0line starting with MULTI keyword will create all the cartesain options for each\u00a0parameter definition with the each parameter definition in the next TABs.\u00a0PARAMETERS_DEF - is same as option 1 format.\u00a0Example Line:\u00a0MULTI Time-Window:0,30;Time-Window:30,180 Age:40,60;Age:60,80;Age:40,80 Gender:1,1;Gender:2,2\u00a0will create 232=12 cohorts for each Time-Window, Age, and Gender option\u00a0 \u00a0 \u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedCohort.html","title":"MedCohort","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedCohort.html#medcohort-is-a-data-structure-with-helpers-to-deal-with-a-cohort-a-list-of-individuals-with-dated-outcomes-and-followup-times","title":"MedCohort is a data structure with helpers to deal with a cohort, a list of individuals with (dated) outcomes and followup times.","text":"<p>MedCohort contatins a vector of basic records (CohortRec), each representing a single period for a specific id (with a corresponding outcome) information. A MedCohort can be sampled to generate MedSamples files according to SamplingParams using one of two fuctions:</p> <ul> <li>int create_sampling_file(SamplingParams &amp;s_params, string out_sample_file)\u00a0:\u00a0Generate samples within cohort times that fit SampleingParams criteria and windows.\u00a0Sample dates are selected randomly for each window of s_params.jump_days in the legal period. </li> <li>int create_sampling_file_sticked(SamplingParams &amp;s_params, string out_sample_file)\u00a0:\u00a0Generate samples within cohort times that fit SampleingParams criteria and windows.\u00a0Sample dates are those with the required signals for each window of s_params.jump_days in the legal period (if existing). A MedCohort can also be used to estimate the age and gender dependent incidence rate. Estimation is done using the following function which according to IncidenceParams:</li> <li>int create_incidence_file(IncidenceParams &amp;i_params, string out_file) :\u00a0Generate an incidence file from cohort + incidence-params.\u00a0Check all patient-years within cohort that fit IncidenceParams and count positive outcomes within the incidence_years_window. IncidenceParams initialization:</li> </ul> Parameter Name Description Default Value incidence_years_window how many years ahead do we consider an outcome? 1 rep Repository configration file None from_year first year to consider in calculating incidence 2007 to_year last year to consider in calculating incidence 2013 gender_mask mask for gender specification (rightmost bit on for male, second for female) 0x3 train_mask mask for TRAIN-value specification (three rightmost bits for TRAIN = 1,2,3) 0x7 from_age minimal age to consider 30 to_age maximal age to consider 90 age_bin binning of ages 5 min_samples_in_bin minimal required samples to estimate incidence per bin 20 <p>SamplingParams initialization:</p> Parameter Name Description Default Value is_continous continous mode of sampling vs. stick to signal (0 = stick) 1 stick_to, stick_to_sigs  comma separated list of signals required at sampling times None take_all in 'stick' mode - take all samples with requrired-signal within each sampling period is selected 0 take_closest <p>in 'stick' mode - take the sample with requrired-signals that is closest to each target sampling-date</p><p>if none of take_all and take_closest is given, a random sample with requrired-signal within each sampling period is selected</p> 0 rep Repository configration file None min_age minimum age for sampling 0 max_age maximum age for sampling 200 gender_mask mask for gender specification (rightmost bit on for male, second for female) 0x3 train_mask mask for TRAIN-value specification (three rightmost bits for TRAIN = 1,2,3) 0x7 min_year first year for sampling 1900 max_year last year for sampling 2100 jump_days days to jump between sampling periods 180 min_days, min_days_from_outcome minimal number of days before outcome for sampling 30 min_case, min_case_years minimal number of years before outcome for cases 0 max_case, max_case_years maximal number of years before outcome for cases 1 min_control, min_control_years minimal number of years before outcome for controls 0 max_control, max_control_years maximal number of years before outcome for controls 10"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedCohort.html#include-file-is-hmrlibsinternalmedutilsmedutilsmedcohorth","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedUtils/MedCohort.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedFeatures.html","title":"MedFeatures","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedFeatures.html#medfeatures-is-a-data-structure-with-helpers-for-holding-features-data-as-a-virtual-matrix","title":"MedFeatures is a data structure with helpers for holding features data as a virtual matrix.","text":"<p>MedFeatures is the data container used by MedModel to for holding the matrix used for learning/predicting. A MedFeatures object contains a vector of samples (id + date + outcome + ...), a\u00a0vector of weights (one per sample) and a vector of floats (one value per sample) for each features. Each feature is identified by it's name (a string) Additional metadata per feature includes a FeatureAttr entry as well as a set of tags (string), which is used by FeatureProcess objects ro decide whether to act on the feature.\u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedFeatures.html#include-file-is-","title":"**Include file is -\u00a0**","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedLabels.html","title":"MedLabels","text":"<p>A class the holds the information on how to label samples and outputs the outcome. It uses the MedRegistryand internal parameters to define the \"labeling\" - for example the relevant time window for the outcome. So you can use the same MedRegistry and just change for example the time window for the outcome. The parameters for the labeling are called LabelParams:</p> <ul> <li>time_from / time_to - the time window defintion for the outcome</li> <li>censor_time_from /\u00a0censor_time_to - time window for the censor registry</li> <li>conflict_method - how to cop with conflicts. If the rules has more the one option:</li> <li>drop - will drop sample. has conflict</li> <li>all - will create a sample for each outcome. In binary usecase, the same sample will appear twice, once as a case and once as control</li> <li>max - take maximal label. If both case/control rules are satisfied take max - which is case</li> <li>last - takes the last matched record</li> <li>label_interaction_mode - please refer to TimeWindowInteraction for more info. Defines the \"rules\" of matching between the patient registry records and the sample time window (defined by time_from, time_to on the MedSample time). If the rule is satsisfied, the registry outcome value is taken into account for labeling the sample. If there is only one matched record (of multiple but with the same vaalue), then this value is selected. Otherwise, uses \"conflict_method\" argument to resolve the conflict</li> <li>censor_interaction_mode - Same format as\u00a0label_interaction_mode but just for censor registry</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedModel.html","title":"MedModel","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedPlot.html","title":"MedPlot","text":"<p>To create general graph please use createHtmlGraph function in MedPlot. to create ROC graphs use plotAUC function. \u00a0 plotAUC** Input:\u00a0**</p> <ul> <li>vector of all models predictions vector(each record is prediction vector result of specific model)</li> <li>vector of the labels</li> <li>vector of the models\\predictors names (same size as the first vector)</li> <li>output direcotry for writing graphs</li> <li> <p>optional - indexes is a mask for selecting rows and may be omitted to select all \u00a0 plotAUC\u00a0Output:** ** the function outputs the following graphs:</p> </li> <li> <p>ROC graph curve</p> </li> <li>PPV graph curve - for each false positive rate - it's PPV value for each model</li> <li>False Positive rate as function of model score - you may see each model scores VS it's false positive rate</li> <li>Label distribution of cases and controls\u00a0 For Each Graph you may search for specific working point in the search button - fill in X value (false positive rate) and it will find you the Y value (Sensitivity or PPV depends on the graph) \u00a0 Example Run: \u00a0 <pre><code>vector&lt;float&gt; preds;\u00a0//from MedModel scores\nvector&lt;float&gt; age_baseline;\u00a0//the age_baseline for each sample\nvector&lt;float&gt; labels;\nplotAUC({age_baseline, preds}, labels, {\"Age_Baseline\" ,\"MedModel\"}, \"~/Auc_Example_Folder\")\n</code></pre> </li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedSamples.html","title":"MedSamples","text":"<p>he MedSamples object is designed to store key fields such as \"label\", \"patient id\", and \"requested prediction time\", along with additional information. Data is stored in a tab-separated file format. The \"pred_0\" field, representing a prediction result for performance analysis, is optional.</p> <ul> <li><code>EVENT_FIELDS</code>: Static field; always set to \"SAMPLE\"</li> <li><code>id</code>: Numeric patient identifier</li> <li><code>time</code> : Requested prediction time; only data prior to this point is used for prediction</li> <li><code>outcome</code>: The label or outcome; can be binary (0/1) or numeric (for regression)</li> <li><code>outcomeTime</code>: Event time if the patient is labeled \"1\", or \"end of followup\" for \"0\". This field is optional; if unused, you can specify a placeholder date like \"19000101\". Useful for filtering by time windows.</li> <li><code>split</code>: Optional field for specifying patient split. Typically, splits are assigned based on patient id in a separate file, so please specify \"-1\" </li> <li><code>pred_0</code> - optional prediction result</li> </ul> <p>Example file <pre><code>EVENT_FIELDS    id  time    outcome outcomeTime split\nSAMPLE  1   20250101    0   20250820    -1\nSAMPLE  1   20250201    0   20250820    -1\nSAMPLE  2   20250101    1   20250820    -1\n</code></pre></p> <p>Explain:</p> <ul> <li>Patient 1 has two prediction points (20250101 and 20250201), both labeled \"0\". Follow-up is documented until 20250820.</li> <li>Patient 2 has one prediction point (20250101) with label \"1\". The event date is 20250820.</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedSamples.html#overview","title":"Overview","text":"<p>MedSamples is a data structure and set of helper functions for managing samples\u2014combinations of individuals, time-points (dates), and associated information such as outcomes, outcome times, splits, and predictions.</p> <p>The data is organized in a three-tier hierarchy:</p> <ol> <li>MedSamples: Represents a collection of MedIdSamples for multiple patients</li> <li>MedIdSamples: Represents a set of samples for a specific patient (id).</li> <li>MedSample:  Represents a single sample.</li> </ol>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedSamples.html#notes","title":"Notes:","text":"<ul> <li>It is not strictly enforced that all samples within a <code>MedIdSample</code> share the same id, but many functions may not work correctly otherwise.</li> <li>Different samples for the same id may have different splits, and these may not match the split parameter in <code>MedIdSample</code>. However, it is strongly recommended to maintain a single split per id.</li> </ul> <p>Samples can be read from and written to CSV or binary files. \u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html","title":"SerializableObject","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#general","title":"General","text":"<p>SerializableObject is a class many classes inherit from. It contains tools for the following:</p> <ul> <li>Serialization via the:<ul> <li>get_size , serialize , deserialize methods.</li> <li>the MedSerialize:: namespace mechanisms including the very useful ADD_SERIALIZATION_FUNCS mechanism</li> </ul> </li> <li>Read/Write objects from/to files: this is a general wrapper on top of the serialization.</li> <li>init_from_string mechanism :<ul> <li>parsing of the init string</li> <li>support the brackets\u00a0{} mechanism for parameters</li> <li>support reading parameters from a file (the pFile option) Together this class provides a powerful and easy way to handle those very needed functionalities. To start using the SerializableObject class, simply inherit from it in the definition.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#serialization","title":"Serialization","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#the-virtual-serialization-methods","title":"The virtual serialization methods","text":"<p>SerializableObject contains the declaration of the three major serialization methods: <pre><code>    // Virtual serialization\n    virtual size_t get_size() { return 0; } ///&lt;Gets bytes sizes for serializations\n    virtual size_t serialize(unsigned char *blob) { return 0; } ///&lt;Serialiazing object to blob memory. return number ob bytes wrote to memory\n    virtual size_t deserialize(unsigned char *blob) { return 0; } ///&lt;Deserialiazing blob to object. returns number of bytes read\n</code></pre> Each inheriting class should implement these methods in order to allow for its serialization. There are 2 main methods to do it:</p> <ul> <li>Directly implementing the methods - recommended only in very complex situations (mainly when using other packages not using this terminology with a need to wrap their internal way to serialize).</li> <li> <p>Using the ADD_SERIALIZATION_FUNCS() macro : this should be the way to go and should always be preffered over any other way. \u00a0 When using the serialzation mechanism of SerializableObject to its full power (using the ADD_SERIALIZATION_FUNCS macro) one automatically gets the following serializations:</p> </li> <li> <p>All needed basic types</p> </li> <li>All supported stl containers (add yours if it is not supported : see below)</li> <li>Recusive on all used classes in variables</li> <li>A pointer to a supported class (but only to single ones, not to arrays allocated like that).</li> <li>Immunity to changes of adding more variables to the serialization - in the sense that you will still be able to read objects serialized before that change.</li> <li>Immuinity to changes of deleting variables from a serialization</li> <li>Immunity to changes of changing the order of variables in the serialization.</li> <li>Correct allocation of derived classes when deserializing a pointer to the base class.</li> <li>Note: not immune to changes in the names of variables. So changing a variable name will break serializations - try to avoid this if possible.</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#add_serialization_funcs","title":"ADD_SERIALIZATION_FUNCS","text":"<p>The ADD_SERIALIZATION_FUNCS() is a macro that adds the needed get_size, serialize and deserialize methods to the class, thus saving us the tedious work of writing it. To use it , simply add ADD_SERIALIZATION_FUNCS to the public side of your class (that inherits from SerializableObject) with the list of the variables you need to serialize. That's it. As simple as that. Any order you like, it is not important. The supported variables are:</p> <ul> <li>Basic types (int, float, double, string, etc...)</li> <li>Stl containers (vector , map , pair, etc..) <li>Other classes that are under SerializableObject and had implemented the serialization methods.</li> <li>T * : pointer to a single allocated (via new) SerializeObject supported class. Note : a single element and not an array (there's no way in c++ to know that size just by seeing the pointer... hence it is recommended to use vector&lt;&gt; when in need of something like that)</li> <li>A recursive combination of the above, for example : vector , map&gt;&gt; , etc... Example : this is all that is needed to be done to serialize some classes : <pre><code>// MedModel serialization line\nADD_SERIALIZATION_FUNCS(rep_processors, generators, feature_processors, predictor, serialize_learning_set, LearningSet)\n\u00a0\n// BasicFeatGenerator serialization line\nADD_SERIALIZATION_FUNCS(generator_type, type, tags, serial_id, win_from, win_to, d_win_from, d_win_to,\n        time_unit_win, time_channel, val_channel, sum_channel, signalName, sets,\n        names, req_signals, in_set_name ,bound_outcomeTime, timeRangeSignalName, timeRangeType)\n</code></pre>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#add_class_name-medserialize_support","title":"ADD_CLASS_NAME , MEDSERIALIZE_SUPPORT","text":"<p>In order to get the full functionality of the serialization process, it is needed to add the following macros for each inheriting class:</p> <ul> <li>ADD_CLASS_NAME(class name) : in the public area of the class : this creates a functions that returns the class name, and is very useful when serializing T * cases, and polymorphic classes.</li> <li>MEDSERIALIZE_SUPPORT(class name) : this should be added in the same h file but outside the class (typically we add it at the bottom of the h file). It is needed in order to connect the class serialization functions to the general recursive serialization methods. Simply add those two simple lines for each new SerializableObject inheritting class you write. Example: <pre><code>class Example : public SerializableObject {\npublic:\n    string name = \"\";\n    vector&lt;int&gt; vec_of_ints = {0,1,2};\n    vector&lt;MedModel *&gt; models;\n\u00a0\n    // ...\n\u00a0\n    // serialization\n    ADD_CLASS_NAME(Example)\n    ADD_SERIALIZATION_FUNCS(name, vec_of_ints, models)\n\u00a0\n}\n\u00a0\n//... later in the same h file\n\u00a0\nMEDSERIALIZE_SUPPORT(Example)\n</code></pre> </li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#polymorphic-classes-support","title":"Polymorphic classes support","text":"<p>If you have a Base class with inheriting classes , all inheriting from SerializableObject, you may have the following issue: Some other class contains an element of : Base * var , but var is allocated dynamically to be one of the derived classes of Base. When serializing var we will use the serialization functions of the derived class, BUT when deserializing if we do nothing var will be newed into a Base element, and hence be Base and not the correct derived class, and we will be using its deserialization function that doesn't match the serialization function we used. To solve that the serilizer of SerializableObject saves the type name before each T * serialization, the name is taken from the derived class, so the derived class name will be used. When deserializing the Base class needs to provide a new_polymorphic function that returns the new to its derived class given its name. So , to summarize:</p> <ul> <li>SerializableObject has a virtual function : void * new_polymorphic(string derived_class_name); by default it returns NULL.</li> <li>Base classes should implement it : it is easy , as it mainly contains lines of the type : if (derived_class_name == string_name_of_derived1) return new derived1; etc...</li> <li>To make implementation of the new_polymorphic function even easier, one can use the\u00a0CONDITIONAL_NEW_CLASS() macro Example : this is the new_polymorphic function of FeatureGenerator <pre><code>//.......................................................................................\nvoid *FeatureGenerator::new_polymorphic(string dname) {\n    CONDITIONAL_NEW_CLASS(dname, BasicFeatGenerator);\n    CONDITIONAL_NEW_CLASS(dname, AgeGenerator);\n    CONDITIONAL_NEW_CLASS(dname, GenderGenerator);\n    CONDITIONAL_NEW_CLASS(dname, SingletonGenerator);\n    CONDITIONAL_NEW_CLASS(dname, BinnedLmEstimates);\n    CONDITIONAL_NEW_CLASS(dname, SmokingGenerator);\n    CONDITIONAL_NEW_CLASS(dname, KpSmokingGenerator);\n    CONDITIONAL_NEW_CLASS(dname, AlcoholGenerator);\n    CONDITIONAL_NEW_CLASS(dname, RangeFeatGenerator);\n    CONDITIONAL_NEW_CLASS(dname, DrugIntakeGenerator);\n    CONDITIONAL_NEW_CLASS(dname, ModelFeatGenerator);\n    return NULL;\n}\n\u00a0\n</code></pre> PreSerialization / PostDeSerialization If you need to run a few commands before the serialization starts you can put them in the pre_serialization method inside your class. This is needed for example when you need to clean some of the serialized variables based on some condition before it starts. In the same manner you can implement in the function post_deserialization operations that are needed to be done after the deserialization. This can be handy and helped convert some models (such as xgb) to use this serialization methods. Example: this is the pre_serialization function in MedModel: <pre><code>// allows for conditional serialization of LearningSet while allowing the use of the ADD_SERIALIZATION_FUNCS macro\nvirtual void pre_serialization() \n{ \n    if (!serialize_learning_set) \n        LearningSet = NULL; /*no need to clear(), as this was given by the user*/ \n}\n</code></pre> </li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#serialization-check-list","title":"Serialization check list","text":"<ol> <li>Inherit from SerializableObject or from a class inheritting from it.</li> <li>add the ADD_CLASS_NAME(class name) macro to your class.</li> <li>add the MEDSERIALIZE_SUPPORT(class name) after your class definition in the same h file.</li> <li> <p>Use the ADD_SERIALIZATION_FUNCS(...) macro to list the variables you need to serialize.</p> </li> <li> <p>if you can't: maybe the pre_serialization() trick can solve your problem? if so - great, implement it, and use the macro.</p> </li> <li> <p>If you still can't due to a complex case: implement the get_size, serialize, and deserialize methods directly.</p> </li> <li>If your class is a Base class , make sure to implement the new_polymorphic method for it.</li> <li>If your class is a derived class, make sure the new_polymorphic method of its base class supports your derived class.</li> <li>Avoid changing variable names , as it will break the serialization backward support.</li> <li>Avoid using a variable names size . This is an unclear bug (may be a compiler bug) , but using it makes the compiler think its type is different. Simply don't use it as a serialized variable.</li> </ol>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#tips-for-writing-an-easily-serialized-class-and-a-correct-one","title":"Tips for writing an easily serialized class and a correct one","text":"<ol> <li>Do not use c style arrays such as int * to different sizes using new or malloc. Use vector&lt;&gt; instead.</li> <li>Give good names to variables (so that later they won't be changed) (don't use size as a varialble -&gt; there's a bug when using it)</li> <li>Use basic types, stl, other serialized classes, recursively if you need.</li> <li>If needed you can use T * for a single element (it's ok to have a vector or map of those of course) , as long as T is a class supported by MedSerialize.</li> <li>If needed use pre_serialization and/or post_deserialization options. Very handy when there's an inner 3rd party class with its own serialization function.</li> <li>Make all the efforts to be able to use the ADD_SERIALIZATION_FUNCS as your serialization implementation. It is the most powerful method.</li> </ol>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#how-to-use-add_serialization_funcs-if-my-class-uses-a-forward-declaration","title":"How to use ADD_SERIALIZATION_FUNCS if my class uses a forward declaration?","text":"<p>Sometimes it happens that we can't use ADD_SERIALIZATION_FUNCS in the h file due to for example forward declarations of classes used, and hence this can only be compiled in the c file. If you need that, you have the ADD_SERIALIZATION_HEADERS() in your declaration (this is to allow virtuality in base classes, otherwise not a must), and you can use the\u00a0ADD_SERIALIZATION_FUNCS_CPP(classname, ...) inside your cpp file. It is the same as the ADD_SERIALIZATION_FUNCS macro but you need to add your class name at the start.</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#my-stl-container-is-not-supported-how-can-i-add-it","title":"My stl container is not supported , how can I add it?","text":"<p>Help others and implement it in SerializableObject_imp.h , see there many examples for stl containers support. \u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#known-bugs","title":"Known BUGS","text":"<ul> <li>DO NOT USE 'size' as a variable ending up in the ADD_SERIALIZATION_FUNCS() list : possibly a compiler bug makes the serializer get the wrong type for it.</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SerializableObject.html#init-from-string","title":"Init from string","text":"<p>The other usage of SerializableObject is the init_from_string parsing you get for free. When you call init_from_string(string init_s) for your SerializableObject class, it will parse init_s to pairs of string &lt;-&gt; string loaded into a map and call your init(map&amp; map) method according to the following rules: <ul> <li>init_s is separated by ;</li> <li>After sepration each part is separated by = : the first element is the variable name, the second is its value (as a string)</li> <li>Should clear whitespaces before and after the variables names and values.</li> <li>If you use var={value} then var will be the variable name and value will be its value BUT you can use ; and = inside value (!!) . This helps when passing parameters to another class via a variable. Value can contain\u00a0{} variables on its own, which is also very useful at times.</li> <li>The pFile=fname is a reserved command: when given , init_from_string will open fname, concatenate its lines to one long string, and feed it to the usual init_from_string() method. This allows to keep parameters in a file.</li> <li>if a param is : var=FILE:fname : the string for the variable will be replaced by a string created by reading the file fname, getting rid of all comment and empty lines, getting rid of each line start/end spaces and eols.</li> <li>if a param is : var=\"LIST:fname\" or var=\"list:fname\" : the variable list will be replaced like when using the \"FILE:\" option, but commas (,) will be added between the string objects in the file (separated by spaces or end-of-lines, or even commas, in the file) \u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/index.html","title":"FeatureGenerator","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/ModelFeatGenerator.html","title":"ModelFeatGenerator","text":"<p>The input model should have been trained on a set of samples different from the samples it is ran on at generation time, it is then used to generate predictions on the current samples at the given time points. The input model can also be used to impute a feature\u00a0 - this functionality is suppose to be moved to a\u00a0separate feature processor. fg_type -\u00a0 \"model\". (required) name - the name to be used in the feature name. Will default to the model file name and then to\u00a0\"ModelPred\" if not given. modelFile - model file, will be used as the feature name if name is not given. impute_existing_feature - use the model to impute a feature in the matrix. Will not work if more than 1 time is given since it is\u00a0 a separate functionality. defaults to 0. n_preds - the number of prediction per sample per time - 1 for binary\u00a0classification which is the default. time_unit_sig - the signal time unit - defaults to the\u00a0global_default_windows_time_unit. time_unit_win\u00a0- the times vector time unit - defaults to the\u00a0global_default_windows_time_unit. times - a vector of times before the sample for which the prediction should be given. Defaults to 0 if no values are given. <pre><code>{\n    \"feat_generator\": \"model\",\n    \"name\": \"$NAME_OF_FEATURE\",\n    //Optional for manipulating prediction time:\n    \"time_unit_win\": \"Hours\", \"times\": \"30, 100, 200\",\n    //Option 1 - using trained model. path to binary MedModel\n    \"file\": \"/nas1/Work/Users/ReutF/model_feature/pre2d_light_S0.model\"\n    //Option 2 - json for learning the model, path to samples if they are different from outer model training. There is a flag to filter and ensure we are using the same patient ids (no leakage from other splits)\n    \"model_json\": \"$PATH_TO_JSON\",\n    \"model_train_samples\": \"$PATH_TO_TRAIN_SAMPLES_IF_DIFFERENT_FROM_CURRENT\"\n}\n</code></pre>  \u00a0 \u00a0 override_predictions cane be used to copy predictions into our feature matrix without running the model (if we have the predictions from a previous run). There is no way to do this using the json file The feature id nember in the feature matrix is constant for all features resulting from running this generator once (FTR_000001.regression_pred_t_100.1,\u00a0FTR_000001.regression_pred_t_200.1,\u00a0\u00a0FTR_000001.regression_pred_t_30.1) \u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html","title":"Unified Smoking Feature Generator","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#background","title":"Background","text":"<p>The purpose of the Unified Smoking Feature Generator is to generate smoking related features based on different types of available smoking information. It was based on THIN and KPSC databases.</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#input-signals","title":"Input Signals","text":"<p>The generator currently require the following signals (it doesn't depend anymore on the THIN smoking_quantity signal):</p> Signal THIN KPSC KPNW Smoking_Status v v v Smoking_Quit_Date v x v Pack_Years x v x Smoking_Intensity [Cigs/Day] v x v <p>Smoking_Duration  [Years]</p> x v v <p>Note:\u00a0 Every repository should have those signals, even if they are not\u00a0 (in that case they should be empty signals) The\u00a0Smoking_Status signal is a categorical signal, with the following values:\u00a0Never, Passive, Former, Current, Never_or_Former.\u00a0 \u00a0 Extraction of the status in THIN is described in the Appendix</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#output-features","title":"Output Features","text":"<p>Boolean features: 1. Current_Smoker 2. Ex_Smoker 3. Never_Smoker 4. Passive_Smoker 5. Unknown_Smoker 6. NLST_Criterion - 1 if age between 55 to 74, pack years &gt; 30, time since quitting &lt; 15 years. **\u00a0features:** 1. Smok_Days_Since_Quitting - For current smokers - 0, For Former smokers, time since quitting, for Never Smokers - time since birth 2. Smok_Years_Since_Quitting - same as previous, but in years 3. Smok_Pack_Years_Max - Maximal report of pack years (pack years if available) if not, it is estimated (and can be corrected with intensity 4. Smok_Pack_Years - the same as\u00a0Smok_Pack_Years_Max 5. Smok_Pack_Years_Last - Last pack years report (without estimation) 6. Smoking_Intensity - Number of pack per day 7. Smoking_Years - Smoking duration.</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#config-example","title":"Config Example","text":"<pre><code>\"model_actions\": [\n    {\n      \"action_type\": \"feat_generator\",\n      \"fg_type\": \"unified_smoking\",\n      \"smoking_features\": \"Current_Smoker,Ex_Smoker,Never_Smoker, Unknown_Smoker,Smoking_Years,Smok_Years_Since_Quitting,Smok_Pack_Years,Smoking_Intensity\"\n    }\n  ]\n</code></pre>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#logic-explanation","title":"Logic Explanation","text":"<p>The most basic information we need to extract is smoking status on different time points The logic is based on the paper:\u00a0Development of an algorithm for determining smoking status and behaviour over the life course from UK electronic primary care records \u00a0https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5217540/pdf/12911_2016_Article_400.pdf The workflow is built\u00a0 from the following methods: 1. genFirstLastSmokingDates - For each status find the first time and last time it appears. This is the input for setting the status at each smoking status report. 2. genSmokingStatus - generate for each point in smoking status vector a corrected smoking status. See Figure 1 3. genSmokingRanges - Build Smoking status ranges\u00a0 4. genLastStatus - Set the\u00a0Boolean\u00a0 smoking status features (take the last status according the the previous method output) 5. calcQuitTime - generates Smok_Days_Since_Quitting/Smok_Years_Since_Quitting.\u00a0Check that last status in the ranges vector - If former smoker, take the delta between sample time to beginning of the \"former smoking\" period, if Current smoker, take 0. if never smoker return time since birth date. 6. calcSmokingIntensity - returns smoking intensity (averages the smoking intensity vector). 7. calcPackYears - Set pack years according to the pack years vector. 8. calcSmokingDuration - Return duration. runs over the ranges vector and integrates the period in which the status is \"Current smoker\" 9. fixPackYearsSmokingIntensity - Fix pack years using smoking intensity\u00a0 and duration. If Intensity is unknown and pack years is known calculate intensity. \u00a0 Example: Taken from THIN, birth date : July 1959, sample date 05/08/2011. Marked in Grey - Input (Raw) Data</p> Smoking Status 19900315 Current 19970227 Never 19970227 Never_or_Former 20060824 Never Smoking Intensity <p> </p> <p>19900315 15.000000</p> 19970227 0.000000 20060824 0.000000 20060824 0.000000 Quit time  Pack years Smoking Status Processed 19590700 UNKNOWN_SMOKER 19900315 CURRENT_SMOKER 19970227 EX_SMOKER 19970227 EX_SMOKER 20060824 EX_SMOKER Smoking Status Ranges 19590700-19781231 UNKNOWN_SMOKER 19790101-19930904 CURRENT_SMOKER 19930905-20110805 EX_SMOKER Intensity Out: 15 Duration Out: 14.684932 Quit time: 17.926027 Pack years: 11.013699"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#appendix-extracting-smoking-status-in-thin","title":"Appendix - Extracting Smoking Status in THIN","text":"<p>In THIN database, smoking status is extracted from Read codes.\u00a0 The mapping from codes to status is taken from\u00a0 \"Development of an algorithm for determining smoking status and behaviour over the life course from UK electronic primary care records: \u00a0https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5217540/pdf/12911_2016_Article_400.pdf I have noticed that there are a lot of \"collisions\" in the smoking status vector when using\u00a0 this mapping (meaning two different status in the same date) - ~10%.\u00a0 After removing non-conclusive\u00a0 Read codes - this was reduced to ~0.5%. When the old THIN smoking feature generator was used in a simple LR model for lung cancer AUC was improved in 1 point. See original and modified mapping in the table below.\u00a0 smoking_readcodes_combined.csv Figure 1 - Logic for setting the smoking status. The code that generates the smoking vectors in THIN: http://bitbucket:7990/projects/MED/repos/gensmoking/browse \u00a0 \u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/Code%20Documnetation.html","title":"Code Documnetation","text":"<p>We will start with _learn \u00a0 General comment: Passive smoker is treated as Never smoker status. NEVER_OR_EX_SMOKER =&gt; it is being resolved by all the smoking information if the patient has no indication of smoking, ex smoking or quit date, it is being used as never, otherwise EX. 1. After reading signals it calls\u00a0getQuitAge to store\u00a0ageAtEx,deltaTime vectors for each patient. It ignores samples and being calculated per patient (uses last sample to ensure we are not using information after last time that will be later filtered to speedup. It has dates for each \"change\" so we can later filter it and use the information that is only available at prediction time).</p> <ol> <li> <p>getQuitAge</p> </li> <li> <p>Calls\u00a0genFirstLastSmokingDates =&gt;\u00a0vector dates:\u00a0vector whenever something changes in the smoking info (either new smoking status information date or quit smoking date value, that refers the past).\u00a0and map&lt;&gt; smokingStatusDates - for each smoking status first and last date of this indication. IT is being calculated by iterating over smoking_status signal + using smoking_quit date signal. When there is quit_date, it results in \"current_smoker\" status in the day before and \"ex_smoker\" status in quit date and it process this information to update\u00a0smokingStatusDates. Also handles Never_or_Ex smoking status. If no indication of current smoking or ex smoking in\u00a0smokingStatusDates after all this logic (means, even no quit date) it will use this information to update never_smoker status, otherwise it will use to update ex smoker. <li> <p>Calls\u00a0genSmokingStatus =&gt; vector\u00a0smokingStatusVec: iterate \"dates\" vector that indicates any change in smoking info and in each date updates the current status by decision tree. return if stopped smoking the stopped age and delta time between last current indication and stopped smoking date. If didn't stop - missing values. It's in order to learn relation between age that stopped smoking and delta time from quit date to last current</p> </li> <li> <p>Learns a linear model from age at stopped smoking to time delta from quit date to last current status =&gt; slope + bias parameter Generate function:</p> </li> <li>calls\u00a0genFirstLastSmokingDates as in _learn, but for each sample, limited till prediction date</li> <li>Calls\u00a0genSmokingStatus as in _learn, but for each sample, limited till prediction date</li> <li> <p>Calls\u00a0genSmokingRanges. generates time ranges for smoking status and returns it in sorted vector by date of\u00a0smokeRanges. Iterates through status and update in each smoking status change:</p> </li> <li> <p>If past state was never/unkown =&gt;and changed to current smoking. set the start smoking time to current date status or to Age of 20 if no previous smoking status (was unknown and no indication of never after age 20). If there is indication of never smoker after age 20, this is the minimal start point for being never smoker till the smoking indication.</p> </li> <li> <p>If status is changed between\u00a0 current_smoker and EX_Smoker - switch the time range in the middle\u00a0</p> </li> <li> <p>If past state was never/unkown =&gt;and changed to EX smoking. calculates age that stopped smoknig indication</p> </li> <li> <p>if past was unknown and current indication is never. end unknown time range and start new time period for never smoker from this indication date. If had Quit_Smoking date, the difference in time between current and EX will be 1 day - no problem and no holes. If the change is due to smoking status - there is a hole in time between the statuses and it will use the linear slope from the age to determine the age the patient really stopped smoking.\u00a0</p> </li> <li>Calls\u00a0genLastStatus - fetches last smoking status from\u00a0smokeRanges for smoking_statuts features</li> <li>Calls\u00a0calcQuitTime - Measure the number of days/years that stopped smoking. If current it's 0, if Never - it's like quit on his birth date, if ex smoker - using the ex date</li> <li>Calls\u00a0calcSmokingIntensity - Smoking intensity valid values are 0-200 otherwise ignored. above 140 value is trimmed to 140 cigars/day.\u00a0 calculates average number in all time window, 0 value for never smoker</li> <li>Calls\u00a0calcPackYears - takes values &gt; 0. calculates max and last pack years in the time till the sample. If never smokers - it's 0. If has pack_years &gt;0, changes never_smoker status to 0. if not current_smoker, turns on the ex_smoker (MEANS you need to load PACK_YEARS only for smokers and non zero values)</li> <li> <p>Calls\u00a0calcSmokingDuration - 0 for never smoker. if not unknown smoker. Calculate smoking_duration and\u00a0smokingDurationBeforeLastPackYears.\u00a0</p> </li> <li> <p>Calls\u00a0getLastSmokingDuration -that return the last time and value of intensity before sample time. Calculates this again for last time before last pack_years date indication.\u00a0</p> </li> <li> <p>Sums the current_smoker statuts time ranges - instead of using full time period, uses the\u00a0lastDurationDateBeforTestDate as starting time point for calculation, since till this time you have the smoking intensity provided by the user. It does this also for last_pack_years - till the last pack years information</p> </li> <li>Calls\u00a0fixPackYearsSmokingIntensity: if no last pack years and has smoking duration + intensity =&gt; calculate pack years. If last pack years exists, validate it when smoking duration exists and not zero. When smoking intensity exists - add the delta from last, max pack years date multiply by smoking intensity(average). If smoking intensity is missing =&gt; complete the intensity from pack years + duration You can set \"debug_file\" argument to debug smoking generation \u00a0</li>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureProcessor/index.html","title":"FeatureProcessor","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureProcessor/FeatureSelector.html","title":"FeatureSelector","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureProcessor/FeatureEncoder/index.html","title":"FeatureEncoder","text":"<p>It's FeatureProcessor, which gets existing matrix (MedFeatures) and creates new features Existing subclasses:</p> <ul> <li>FeaturePCA - encoding based on PCA</li> <li>AutoEncoder - To be implemented in the future</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/FeatureProcessor/FeatureEncoder/FeaturePCA.html","title":"FeaturePCA","text":"<p>example json configuration: <pre><code>.......         \n       \"process\":{\n            \"process_set\":\"3\",\n            \"fp_type\":\"pca\",\n            \"duplicate\":\"no\",\n            \"pca_top\":\"10\", //Taking top 10 PCA dim\n            \"subsample_count\":\"100000\"\n        },\n        \"process\":{\n            \"process_set\":\"4\",\n            \"fp_type\":\"tags_selector\",\n            \"duplicate\":\"no\",\n            \"selected_tags\":\"pca_encoder\" //feature selection of the PCA features\n        }\n......\n</code></pre></p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedRegistry/index.html","title":"MedRegistry","text":"<p>Code documentation Methods in MedRegistry:</p> <ul> <li>reading the object:\u00a0read_text_file or\u00a0read_from_file to read in text format or in binary format.</li> <li>writing the object: write_text_file\u00a0or\u00a0write_from_file\u00a0to writein text format or in binary format.</li> <li>create_registry\u00a0- an option to create registry records by implementing private function\u00a0**\u00a0to fetch each patient registry records</li> <li>calc_signal_stats\u00a0- a method to create\u00a0contingency table with other signal splited by gender and age groups.</li> <li>create_incidence_file - a method to calc the incidence (also with kaplan meier)\u00a0 \u00a0 A class that holds all registry records on all patients using MedRegistryRecord. very similar to MedCohort, but a more generic class to hold multiple periods for outcome on same patients. for example pregnancy, influenza, kidney stones, sofa scores...\u00a0 Each record consist of those fields in MedRegistryRecord:</li> </ul> Parameter name description pid patient id start_date the start date of the outcome\u00a0 end_date the end date of the outcome \u00a0registry_value <p>\u00a0the registry value. 0 for controls, 1 for cases or other value in more complex cases.</p><p>For example in diabetes it may mark the states from 0-2. 0 - no diabetes, 1- pre, 2- diabetes.</p><p>for each period we create record, or value for SOFA Score</p> <p>Example records for cancer from MedCohort: A patient who is in the cohort from 01.01.2000 till 01.01.2016 and got cancer in 01.01.2012 will be presented by 2 MedRegistryRecords. one period for control outcome period and one period for the case period:</p> <ol> <li>control period:\u00a0start_date=01.01.2000,\u00a0end_date=01.01.2012,\u00a0registry_value=0</li> <li> <p>case period:\u00a0start_date=01.01.2012,\u00a0end_date=01.01.2016,\u00a0registry_value=1 a patient who is always control will create 1 record with the start,end dates of the control period \u00a0 It has several ways to be initialized:</p> </li> <li> <p>by reading from disk - binary format or text format</p> </li> <li>by creating registry using create_registry method. need to implement get_registry_records to handle single patient records.the class have also the ability to create contingency table with other signal:for each Gender,Age_bin - the 4 stats number of the registry with the appearances or not appearances of the signal value</li> </ol>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedRegistry/MedSamplingStrategy.html","title":"MedSamplingStrategy","text":"<p>A Class that controls how to create MedSamplesfrom MedRegistryby sampling methods. it has several subclasses, each has it's own parameters and logic to sample the samples form registry:  Code is documented here. do_sample aruments in MedSamplingRegistry:</p> <ul> <li>const vector&lt;MedRegistryRecord&gt; &amp;registry - the registry for the labeling the samples by the sampler (already initialized with sampling params)</li> <li>MedSamples\u00a0&amp;samples - the samples\u00a0result</li> <li>const vector&lt;\u00a0MedRegistryRecord\u00a0&gt; *censor_registry - optional arg for specifying censoring times for the sampling</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedRegistry/MedSamplingStrategy.html#important-samplers","title":"Important Samplers:","text":"<p>**** -\u00a0can be used by specifying\u00a0MedSamplingRegistry::make_sampler(\"time_window\"). from the updated documentition reffer to doxygen Used to sample for each registry record randomly withing a specific time window. It can define diffrent time windows for cases/controls. Parameters:</p> Parmeter Name Type Description Default Value sample_count int how many samples to sample from each registry record. 1 minimal_time_case int the minimal time to give prediciton before the case outcomeTime 0 maximal_time_case int the maximal time to give predicition before the case outcomeTime 0 minimal_time_control int the minimal time to give prediciton before the control outcomeTime (which marks the last time we know the patient is control) 0 maximal_time_control int the maximal time to give prediciton before the control outcomeTime (which marks the last time we know the patient is control) 0 take_max bool If True will take maximal time window for case/control 0 <p>MedSamplingYearly - can be used by specifying\u00a0MedSamplingRegistry::make_sampler(\"yearly\").\u00a0from the updated documentition reffer to\u00a0doxygen\u00a0 Used to sample from year to year by jumping periodically between sample times for each patient. for sampling in ICU (more generic sampler for sampling in Fixed time, please reffer to MedSamplingFixedTime) The arguments:\u00a0time_from, time_to,\u00a0conflict_method,\u00a0outcome_interaction_mode,\u00a0censor_interaction_mode - are common in almost all samplers.</p> Parmeter Name Type Description Default Value start_year int the start year to sample from 0 - Must be provided end_year int The end year to sample from 0 - Must be provided prediction_month_day int the prediction date for the first year to start sampling 101 - mean 01/01 day_jump int the period of days to jump between each sampling date 0 - Must be provided, the common value should be 365 to jump yearly between sample times back_random_duration int random time to sample backward from the prediction date - adds ability to sample in random times in the year 0 <p>MedSamplingDates - Provides way to list all sampling options for each patient (or general options list) in a text file and sample randomly from those options MedSamplingStick - can sample on sticked signal times (fetches the signal times and uses MedSamplingDates to do the sampling)</p> Parmeter Name Type Description Default Value signal_list string a comma \",\" delimeted list with signals to list all possible sampling times for patient to stick to \"\" Must be provided take_count int how many samples to sample for each patient (inherited from MedSamplingDates). 0 - means take all samples 1 sample_with_filters bool Whether to use the filters as constraints and sample only when valid (may cause bais) or sample totally random and filter later True"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedRegistry/TimeWindowInteraction.html","title":"TimeWindowInteraction","text":"<p>This class defines how two time windows interact (boolean result - Yes or No). Motivation - it is being used to\u00a0decide:</p> <ul> <li>How to label samples based on registry - given outcome registry time range and prediction time range. return True/False if we should label the sample based on the registry record outcome value.</li> <li>Whether or not to censor the sample\u00a0- given outcome registry time range and prediction time range. return True/False if we should censor the sample the object is being initialized by medial::sampling::init_time_window_mode\u00a0function.</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedRegistry/TimeWindowInteraction.html#the-initialization-string-format","title":"the initialization string format","text":"<p>the init strings define the Rules for interaction between sample time window and the registry time window. this is abstraction of the time windows definitions:  \u00a0 The init string has format of \u201clabel_value:Interaction_string|label_value:Interaction_string;\u2026\u201d Can also use\u00a0label_value\u00a0for all labels by specifying \u201call\u201d or just the numeric value: \"0\" for controls and \"1\" for cases. We can specify diffrent rules for cases/controls \u00a0 \u00a0Interaction_string\u00a0has format of \u201ccondition,condition\u201d. The first\u00a0condition\u00a0is for sample from time window interaction with [registry start, registry end] The first\u00a0condition\u00a0is for Sample to time window\u00a0interaction\u00a0with [registry start, registry end] \u00a0 Condition\u00a0is enum with those options:</p> <ul> <li>\u201cbefore_start\u201d \u2013 condition for time to be before registry start\u2022\u201cafter_start\u201d \u2013 condition for time to be after registry start\u2022\u201cwithin\u201d \u2013 condition for time to be after registry start and before registry end</li> <li>\u201cbefore_end\u201d \u2013 condition for time to be before registry end\u2022\u201call\u201d \u2013 no condition, always true</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/MedRegistry/TimeWindowInteraction.html#examples","title":"Examples:","text":"<p>First example -\u00a0Diseases that occurs once and forever like cancer full init string: \u201c0:within,within|1:before_start,after_start\u201d Explain controls rule \"within,within\": samples should by within registry start to end time (which registry defines time range we mark the patient as sure control). also the from time window\u00a0of sample/prediction and the end time window of sample/prediction Explain cases rule \"before_start,after_start\": sample should start before start_time of registry and finish after start_time of registry. in the registry there meaning for end time only start time = outcome time. there is no end_time for cancer and it's not being used Second example for vaccination registry (each vaccination holds for X time) full init string: \u201c0:within,within|1:all,within\u201d Explain controls rule \"within,within\": sample time window should be within all time range of unvaccinated period to be counted. can also specify less\u00a0strict rule by \"before_end,after_start\" and conlifct_method=\"max\"\u00a0 to include controls as patients with some interseciton with sure unvaccinated period and no intersection with vaccination period. Explain\u00a0cases rule \"all,within\": sample time window should finish within registry vaccination period (never mind if started already vaccinated or not vaccinated). can also provide more\u00a0strict rule by providing 3rd argument for intersection rate like \"all,within,0.5-1.0\" to count only samples with at least 50%-100% intersection. \u00a0 \u00a0 \u00a0 \u00a0</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/index.html","title":"RepProcessor","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/index.html#repprocessor-applies-in-place-processing-to-the-information-held-for-a-single-patient-id-a-dynamicpidrec","title":"RepProcessor applies in-place processing to the information held for a single patient-id (a DynamicPidRec)","text":"<p>The main functionalities of a RepProcessor include learning\u00a0the processing parameters from (a subset of) a repository, and applying to a single patient Id (DynamicPidRec) at selected time-points. A processor may act differently at different time points (e.g. a predictor that looks at a range of times, must not consider points after the sample-time) - this is achieved by generating different versions of the affected signals in the DynamicPidRec (a version per time point). If the\u00a0DynamicPidRec already has different versions in the required signals (i.e. it has been processed by other rep_processors), the correct version should be used when learning and applying the processor. RepProcessor is a virtual class. See below for the list of implemeted children classes</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/index.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolsrepprocessh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/RepProcess.h","text":"<p>*RepProcessorTypes* RepBasicRangeCleaner</p> Name Value Class Note multi, multi_processor REP_PROCESS_MULTI RepMultiProcessor A container for a set of processors that can be learned simultanously (e.g. cleaneds of different signals) basic_cln, basic_outlier_cleaner REP_PROCESS_BASIC_OUTLIER_CLEANER RepBasicOutlierCleaner Outliers cleaning (removing and trimming) working on single-values nbrs_cln, nbrs_outlier_cleaner REP_PROCESS_NBRS_OUTLIER_CLEANER RepNbrsOutlierCleaner Outliers cleaning (removing and trimming) working on values and their neighborhoods configured_outlier_cleaner, conf_cln REP_PROCESS_CONFIGURED_OUTLIER_CLEANER RepConfiguredOutlierCleaner Uses configuration file for learning borders from statistics , or just set border according to hard coded values rulebased_outlier_cleaner, rule_cln REP_PROCESS_RULEBASED_OUTLIER_CLEANER RepRuleBasedOutlierCleaner Uses set of coded rules about relation among signals taken simulatneously, and if rule is not met all measurements are removed. aggregation_period REP_PROCESS_AGGREGATION_PERIOD RepAggregationPeriod Creates a virtual signal containing the 'treatment period' for the input signal basic_range_cleaner, range_cln REP_PROCESS_BASIC_RANGE_CLEANER RepBasicRangeCleaner Creates a virtual signal containing only instances of the signal that fall within some instance of the range signal."},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/MedValueCleaner.html","title":"MedValueCleaner","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepAggregationPeriod.html","title":"RepAggregationPeriod","text":"<p>The rep processor will only work on\u00a0categorical signals. The rep processor assumes that the input signal has no time range - it only considers the first time channel. It is a logical rep\u00a0processor to run before\u00a0RepBasicRangeCleaner. rp_type - \"aggregation_period\". (required) input_name - the name of the (categorical) signal to process. (required) The\u00a0obvious choice for this is one of the drug signals, ie \"DRUG_PRESCRIBED\".\u00a0 output_name - the name of the resulting virtual signal.(required) sets - the set of values that will be considered as a signal. (required) ie\u00a0\"ATC_C03C_\" or\u00a0\"ATCC03C,ATC_N02A_\" note: [\u00a0\"ATCC03C\",\"ATC_N02A____\"]\u00a0(cross product) is not supported. period - the length of the window to be considered a treatment period - defaults to 0. time_unit_sig - the signal time unit - defaults to the\u00a0global_default_windows_time_unit. time_unit_win\u00a0- the period time unit - defaults to the\u00a0global_default_windows_time_unit. <pre><code>    {\n      \"action_type\": \"rep_processor\",\n      \"rp_type\":\"aggregation_period\",\n      \"input_name\":\"DRUG_PRESCRIBED\",\n      \"output_name\":\"drugs_sets_period\",\n      \"sets\": [\"ATC_C03C____\"],\n      \"period\":\"43200\"\n      },\n</code></pre>  \u00a0 (Code that tests this rep processor:\u00a0U:\\ReutF\\MR\\Projects\\Shared\\check_medication_period_rep_processor)</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepBasicOutlierCleaner.html","title":"RepBasicOutlierCleaner","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepBasicOutlierCleaner.html#repbasicoutliercleaner-is-a-point-wise-cleaner-of-outliers","title":"RepBasicOutlierCleaner\u00a0is a point-wise cleaner of outliers","text":"<p>Each value of a given signal is considered with respect to two ranges. First, the values is compared to the 'Remove' range, and if outside, it is deleted. Otherwise, it is compare to the 'Trim' range and clipped if outside (i.e. - assigned the upper limit if larger, and the lower limit if smaller). The 'Remove' and 'Trim' ranges are learnt from data, using the methods of MedValueCleaner.</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepBasicOutlierCleaner.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolsrepprocessh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/RepProcess.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepBasicOutlierCleaner.html#outliersamplefilter-initialization","title":"OutlierSampleFilter initialization:","text":"<p>see\u00a0MedValueCleaner.</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepBasicRangeCleaner.html","title":"RepBasicRangeCleaner","text":"<p>Does not support range input signals, if a signal has more than one time channel, choose the relevent one. Should be run after\u00a0RepAggregationPeriod \u00a0 rp_type - \"range_cln\". (required) signal_name\u00a0- the name of the signal to process. (required) ie \"Albumin\".\u00a0 ranges_sig_name - the name of the ranges signal to use. (can be the output signal of the\u00a0RepAggregationPeriod processor) output_name - the name of the resulting virtual signal. defaults to a combination of the input and ranges signal. time_channel - the time channel of the signal that will be used to\u00a0decide\u00a0if to keep or discard the signal instance. defaults to 0. output_type - the signal type of the output signal, should be the same as the input signal, defaults to 3 (2 time + value channels). \u00a0 <pre><code>{\n      \"action_type\": \"rep_processor\",\n      \"rp_type\":\"range_cln\",\n      \"signal_name\":\"Albumin\",\n      \"ranges_sig_name\":\"drugs_sets_period\",\n      \"time_channel\":\"0\"\n},\n</code></pre>  (Code that tests this rep processor:\u00a0U:\\ReutF\\MR\\Projects\\Shared\\check_medication_period_rep_processor)</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepConfiguredOutlierCleaner.html","title":"RepConfiguredOutlierCleaner","text":"<p>RepConfiguredOutlierCleaner is a subClass of basicOutlierCleaner. It implements the learn method to define the borders on a signal values, and inherit apply from its parent so that \u00a0values that lie outside those values are removed. As the basic cleaner also has a trimming threshold these thresoholds are set to +-1e98, so no values are trimmed. This cleaner gets in its parameters a name of a csv file that details the way each signal is treated. Another parameter is clean_method which is either \"learned\", \"confirmed\" or \"logical\".\u00a0** \"learned\" means that the borders are determined by calculation of distribution as described in the configuration file for this signal. \"confirmed\" means that learning was done already on part of THIN and bounds \u00a0were set and confirmed by the author (Coby). \"logical\" means that the thresholds are predetermined to the thresholds given in the configuration file. Those thresholds were set by understanding the nature of the signal (for example signal must be positive or percentage must not exceed 100). note: even when confirmed or learned are chosen, values that are outside the logical bounds are removed first. Format of the configuration file: name,logicalL,logicalH,low bound,low dist,high bound,high dist name:name of signal. logicalL: Lower bound for the logical option logicalH: Higher bound for the logical option low bound: The lower bound that was calculated for the confirmed option ( may be set to none- meaning stay with the logical bound). low dist: The probability distribution used in confirmed or should be used in learned for calculation of lower bound. May be norm, lognorm or manual ( that means do not use learned. Value was chosen manually for confirmed mode. high bound: see low bound above. high dist : see low dist above.**</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepConfiguredOutlierCleaner.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolsrepprocessh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/RepProcess.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepConfiguredOutlierCleaner.html#repbasicoutliercleaner","title":"RepBasicOutlierCleaner","text":"<ul> <li>**Description:\u00a0**\u00a0A child class of RepBasicOutlierCleaner</li> <li>Inherits from: RepBasicOutlierCleaner</li> <li>Generate new dynamic-version:\u00a0No</li> <li>Members:<ul> <li>string confFileName; \u00a0 The file that holds the cleaning parameters for each signal.</li> <li>string cleanMethod; \u00a0 \u00a0// \"logical\" \"confirmed\" or \"learned\" as explained above.</li> <li>map outlierParams; \u00a0 \u00a0 \u00a0 \u00a0 holds the parameter that were read from confFile. <li>**Implemented methods: \u00a0\u00a0**<ul> <li>Constructors :<ul> <li>inheritted</li> </ul> </li> <li>void init_defaults()\u00a0: init cleaning parameters to default values</li> <li>virtual int init(map&amp; mapper)\u00a0: init cleaning parameters according to map <li>int Learn(MedPidRepository&amp; rep, vector&amp; ids, vector&amp; prev_processor)\u00a0:\u00a0Learn the thresholds for removal of values according to the description above. <li>apply \u00a0is inheritted from basic cleaner.</li>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepMultiProcessor.html","title":"RepMultiProcessor","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepMultiProcessor.html#repmultiprocessor-is-an-container-of-processors-that-enables-initializing-learning-and-applying-a-set-of-processors-in-parallel-eg-applying-cleaners-to-a-set-of-signals","title":"RepMultiProcessor is an container of processors that enables initializing, learning and applying a set of processors in parallel (e.g. applying cleaners to a set of signals).","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepMultiProcessor.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolsrepprocessh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/RepProcess.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepNbrsOutlierCleaner.html","title":"RepNbrsOutlierCleaner","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepNbrsOutlierCleaner.html#repprocessor-applies-in-place-processing-to-the-information-held-for-a-single-patient-id-a-dynamicpidrec","title":"RepProcessor\u00a0applies in-place processing to the information held for a single patient-id (a DynamicPidRec)","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/RepProcessor/RepRulebasedOutlierCleaner.html","title":"RepRulebasedOutlierCleaner","text":"<p>RepRulebasedOutlier cleaner is a subclass of RepProcessor and MedValueCleaner. This cleaner applies the following rules drawn from Coby's imagination: 1. BMI=Weight/Height^2*1e4 2. MCH=Hemoglobin/RBC*10 3. MCV=Hematocrit/RBC*10 4. MCHC-M=MCH/MCV*100 5. Eosinophils#+Monocytes#+Basophils#+Lymphocytes#+Neutrophils#&lt;=WBC 6. MPV=Platelets_Hematocrit/Platelets 7. UrineAlbumin&lt;=UrineTotalProtein 8. UrineAlbumin_over_Creatinine=UrineAlbumin/UrineCreatinine 9. LDL+HDL&lt;=Cholesterol 10. NonHDLCholesterol+HDL=Cholesterol 11. HDL_over_nonHDL=HDL/NonHDLCholesterol 12. HDL_over_Cholesterol=HDL/Cholesterol 13. HDL_over_LDL=HDL/LDL 14. HDL_over_LDL=1/LDL_over_HDL 15. Cholesterol_over_HDL=Cholesterol/HDL 16. -----canceled rule 16 17. Cholesterol_over_HDL=1/HDL_over_Cholestrol 18. LDL_over_HDL=LDL/HDL 19. Albumin&lt;=Protein_Total 20. FreeT4&lt;=T4*1000 21. NRBC&lt;=RBC 22. CHADS2&lt;=CHADS2_VASC All rules are checked to within 10% tolerance (#define TOLERANCE (0.1)) Rules are checked only if all signals needed for rule implementation exist for the same date. If a rule is checked for a certain date and found to be false (outside the tolerance), all values \u00a0of the signals that are included in the rule will be removed for that date. The cleaner has 2 additional parameters: consideredRules: a string of comma separated integers stating the rules you want to apply to the data. If 0 is included in the list, all rules will be applied. If the string is empty (default), no rule will be applied so the cleaner will actualy do nothing. addRequiredSignals: When set to \"0\" only rules that all the participating signals are in the list of this cleaner will be applied. **\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 When set to \"1\" any rule that includes even one of the named signals will be applied. Signals that are in the rule and are not in the list will be loaded by the cleaner. **\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 : the additional signals that are loaded by this cleaner because \u00a0addRequiredSignals was set to \"1\", may not go through the preprocessing by cleaners that preceed this cleaner, if they are not in the signals list for that cleaner. , - Description:\u00a0**\u00a0A child class of RepProcessor used for point-wise cleaning of outliers according to predefined rules - Inherits from: SampleFilter RepProcessor - Generate new dynamic-version:\u00a0No - Members:** - vector  signalNames; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Names of signals that should be cleaned - vector  signalIds; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Ids of signals to clean - int time_channel = 0; - int val_channel = 0; - MedDictionarySections myDict; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0keeping it will enable us to get ids at apply stage - bool addRequiredSignals=false; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 a flag stating if we want to load signals that are not in the cleaned signal list (see explanation above) - vector consideredRules; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 only rules in this list will be considered in this cleaner (see explanation above) <ul> <li>**Implemented methods: \u00a0\u00a0**<ul> <li>Constructors :<ul> <li>*RepRuleBasedOutlierCleaner()\u00a0*</li> </ul> </li> <li>void init_defaults()\u00a0: init cleaning parameters to default values</li> <li>int init(void processor_params)\u00a0*: init cleaning parameters according to input params</li> <li>virtual int init(map&amp; mapper)\u00a0: init cleaning parameters according to map <li>void\u00a0set_signal_ids(MedDictionarySections&amp; dict)\u00a0: set signalId (actually keep the dictionary for further use in apply).</li> <li>int apply(PidDynamicRec&amp; rec, vector&amp; time_points)\u00a0: apply outliers-cleaning to signal in dynamic-rec at time-points"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/index.html","title":"SampleFilter","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/index.html#samplefilter-filters-a-set-of-samples","title":"SampleFilter filters a set of samples","text":"<p>SampleFIlter takes a MedSamples object and generates a new one (optionally in-place) which contains a subset of the original samples. SampleFilters may (optinally) require a MedRepository to apply the filtering SampleFilter is a virtual class. See below for the list of implemeted children classes</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/index.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":"Name Value Class Note train SMPL_FILTER_TRN BasicTrainFilter Generate a training set test SMPL_FILTER_TST BasicTestFilter Generate a test set outliers SMPL_FILTER_OUTLIERS OutlierSampleFilter Remove outlying outcomes match SMPL_FILTER_MATCH MatchingSampleFilter Perform matching required SMPL_FILTER_REQ_SIGNAL RequiredSignalFilter OBSOLETE, replaced by 'basic' basic SMPL_FILTER_BASIC BasicSampleFilter A range of filtering options"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html","title":"BasicSampleFilter","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#basicsamplefilter-applies-one-or-more-basic-filters-to-the-set","title":"BasicSampleFilter applies one (or more) basic filters to the set","text":"<p>BasicSampleFilter filtering options include:</p> <ul> <li>Allowed time (date) range</li> <li>A list of filters per signal, each specified using BasicFilteringParams<ul> <li>Allowed value-range of specific signals within a time-window</li> <li>Requirement on availability of specific signals - number of values within a time-window</li> </ul> </li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#basicfilteringparams-initializing","title":"BasicFilteringParams initializing:","text":"Parameter Name Description Default Value sig name of signal used for filtering None min_val minimal allowed value -1e10 max_val maximal allowed value 1e10 min_Nvals minimal required number of values 1 win_from start of time window to check requirements 0 win_to <p>end of time window to check requirements</p><p>e.g. (win_from=30,win_to=60) means that we check the signal one to two months before sample date</p> 2^30 time_ch time-channel to consider 0 val_ch value-channer to consider 0"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#basicsamplefilter-initializing","title":"BasicSampleFilter initializing:","text":"Parameter Name Description Default Value min_sample_time minimal allowed time (date) 0 max_sample_time maximal allowed time (date) 2^30 win_time_unit time-unit name (e.g. \"date\") Days bfilter <p>BasicFilterParams: list of plus-separated filters.</p><p>Each filter is defined by a \":=\"-separated initializer of BaiscFilteringParams</p> None <p>**</p>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicTestFilter.html","title":"BasicTestFilter","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicTestFilter.html#basictestfilter-is-a-dummy-filter-that-keeps-all-samples","title":"BasicTestFilter is a dummy filter that keeps all samples","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicTestFilter.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicTrainFilter.html","title":"BasicTrainFilter","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicTrainFilter.html#basictrainfilter-filters-out-all-cases-outcome-0-that-occur-after-outcometime","title":"BasicTrainFilter filters out all cases (outcome\u00a0\u2260 0) that occur after outcomeTime","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/BasicTrainFilter.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/MatchingSampleFilter.html","title":"MatchingSampleFilter","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/MatchingSampleFilter.html#matchingsamplefilter-matches-cases-a-control-according-to-a-combination-of-matching-criteria","title":"MatchingSampleFilter matches cases a control according to a combination of matching criteria.","text":"<p>MatchingSampleFilter can use the following matching criteria:</p> <ul> <li>Age</li> <li>Time (e.g. calendar year)</li> <li>Values of signals</li> <li>Gender The optimal matching ratio is determined given a relative cost of losing a single cases versus losing a single controls sample.</li> </ul>"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/MatchingSampleFilter.html#matchingsamplefilter-initialization","title":"MatchingSampleFilter initialization:","text":"Parameter Name Description Default Value priceRatio the relative cost of losing a single controls sample 100.0 match_to_prior Given directly the prior to match to in each bin. If &lt; 0 won't be used -1 maxRatio the maximal allowed control/case matching ratio 10.0 verbose a verbositry flag (set to &gt;0 to allow logging) 0 strata <p>Definition of a matching strata, possibly more than one, separated by a colon</p><p>Each stratum is comma-separated and can be one of:</p><ul><li>\"age\" or \"age,1\" which 1 stands for bin_size in age</li><li>\"time,time-unit-name,time-resolution\" (e.g. \"time:year,1\")</li><li>\"signal,signal-name,resolution,TimeWindow (in days , takes the last)\" (e.g. \"signal,WBC,0.5,365\")</li><li>\"gender\"</li></ul><p> </p> None"},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/OutlierSampleFilter.html","title":"OutlierSampleFilter","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/OutlierSampleFilter.html#outliersamplefilter-is-used-to-filter-samples-with-outlying-values-of-outcome-relevant-to-regression-problems-outliers-detection-is-done-using-medvaluecleaner","title":"OutlierSampleFilter is used to filter samples with outlying values of outcome (relevant to regression problems). Outliers detection is done using MedValueCleaner.","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/OutlierSampleFilter.html#include-file-is-mr_rootlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0$MR_ROOT/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":""},{"location":"Infrastructure%20C%20Library/MedProcessTools%20Library/SampleFilter/OutlierSampleFilter.html#outliersamplefilter-initialization","title":"OutlierSampleFilter initialization:","text":"<p>see MedValueCleaner.</p>"},{"location":"Installation/index.html","title":"Installation","text":""},{"location":"Installation/index.html#overview","title":"Overview","text":"<p>There are four components that can be installed. You may install all of them or just the ones you need, depending on your requirements.</p>"},{"location":"Installation/index.html#preliminary-steps-to-build-the-tools","title":"Preliminary Steps to Build the Tools","text":"<p>These are the preliminary installation steps required:</p>"},{"location":"Installation/index.html#1-install-compiler-and-build-tools-ubuntu","title":"1. Install Compiler and Build Tools (Ubuntu)","text":"<p>To install the essential compiler and build tools, run:</p> <pre><code>sudo apt install binutils gcc g++ cmake make -y\n</code></pre>"},{"location":"Installation/index.html#2-install-openmp-support-ubuntu","title":"2. Install OpenMP Support (Ubuntu)","text":"<p>To enable OpenMP (used for parallel processing), install the following package:</p> <pre><code>sudo apt install libgomp1 -y\n</code></pre> <p>NOTE: This is required step.</p>"},{"location":"Installation/index.html#3-install-boost-libraries-ubuntu","title":"3. Install Boost Libraries (Ubuntu)","text":"<p>To install the required Boost components, on Ubuntu 24.04, use:</p> <pre><code>sudo apt install libboost-system1.83-dev libboost-filesystem1.83-dev libboost-regex1.83-dev libboost-program-options1.83-dev -y\n</code></pre> <p>Note: On Ubuntu 22.04, Boost version 1.74 is available and is also compatible.</p> <p>If you want to compile the AlgoMarker library or compile it against a different Boost library, please download and compile Boost manually and follow the Boost Compilation Steps. This project has been tested with Boost versions 1.67 through 1.85 and should work with other versions as well.</p>"},{"location":"Installation/index.html#boost-compilation-steps","title":"Boost Compilation Steps","text":"<p>Example installation steps for version 1.85.0:</p> Boost Compilation<pre><code># Ensure you have wget to download the file and bzip2 to extract the \"bz2\" file. Setup in Ubuntu:\nsudo apt install bzip2 wget -y\n\n# Download the Boost Library\nwget https://archives.boost.io/release/1.85.0/source/boost_1_85_0.tar.bz2\n\n# Extract\ntar -xjf boost_1_85_0.tar.bz2\nrm -f boost_1_85_0.tar.bz2\n\n# Setup Boost Install directory to current directory\nWORK_BUILD_FOLDER=$(realpath .)\ncd boost_1_85_0\n\n# Configure to current system\n./bootstrap.sh \n./b2 --clean\n\n# Generate static libs\n./b2 cxxflags=\"-march=x86-64\" link=static variant=release linkflags=-static-libstdc++ -j8 cxxflags=\"-fPIC\" --stagedir=\"${WORK_BUILD_FOLDER}/Boost\" --with-program_options --with-system --with-regex --with-filesystem\n\nmkdir -p ${WORK_BUILD_FOLDER}/Boost/include\n\n# Generate symbolic link to headers inside Boost/include path\nln -sf ${WORK_BUILD_FOLDER}/boost_1_85_0/boost  ${WORK_BUILD_FOLDER}/Boost/include\n\n# Generate shared/dynamic libs for tools, not needed for AlgoMarker\n./b2 cxxflags=\"-march=x86-64\" link=shared variant=release linkflags=-static-libstdc++ -j8 cxxflags=\"-fPIC\" --stagedir=\"${WORK_BUILD_FOLDER}/Boost\" --with-program_options --with-system --with-regex --with-filesystem\n</code></pre>"},{"location":"Installation/index.html#1-algomarker-library","title":"1. AlgoMarker Library","text":""},{"location":"Installation/index.html#description","title":"Description","text":"<p>The AlgoMarker shared library is used for deploying the AlgoMarker model. This library works with your final binary model output to access the model, apply it, and retrieve results. It provides a C-level API.</p> <p>The Git repository is available at MR_LIBS Git Repository.</p>"},{"location":"Installation/index.html#installation_1","title":"Installation","text":"<ol> <li>Clone the Git repository:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_LIBS.git\n</code></pre></li> <li>Compile the Boost library. Refer to Install Boost Libraries. You must compile the Boost library since the <code>-fPIC</code> flag is needed, and it is not included in Ubuntu packages.</li> <li>Edit <code>Internal/AlgoMarker/CMakeLists.txt</code> to include the following line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    This should point to your Boost compiled home directory (<code>WORK_BUILD_FOLDER</code>) from the compilation step. Ensure the compiled libraries are in <code>/libs</code> and the headers are in <code>/include</code>.</li> <li>Execute:    <pre><code>Internal/AlgoMarker/full_build.sh\n</code></pre></li> </ol>"},{"location":"Installation/index.html#2-algomarker-wrapper","title":"2. AlgoMarker Wrapper","text":""},{"location":"Installation/index.html#description_1","title":"Description","text":"<p>The AlgoMarker Wrapper provides a REST API for the AlgoMarker C++ Library. There are two wrappers available:</p> <ol> <li>C++ Native Wrapper: Minimal dependencies, very fast, and efficient. Uses Boost Beast. Can be installed in a minimal Ubuntu Chiselled Docker image with just glibc.</li> <li>Python Wrapper: Built with FastAPI, more flexible for changes, and supports the old AlgoMarker API. It is slower and has more dependencies but is friendlier for testing.</li> </ol>"},{"location":"Installation/index.html#installation_2","title":"Installation","text":"<p>C++ Native Wrapper:</p> <ol> <li>Set up Boost Libraries. No need to compile.</li> <li>Clone the repository:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_Tools.git\n</code></pre></li> <li>If you compiled the Boost library, edit <code>AlgoMarker_python_API/ServerHandler/CMakeLists.txt</code> to include the following line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    This should point to your Boost compiled home directory (<code>WORK_BUILD_FOLDER</code>) from the compilation step. Ensure the compiled libraries are in <code>/libs</code> and the headers are in <code>/include</code>.</li> <li>Compile the wrapper:    <pre><code>AlgoMarker_python_API/ServerHandler/compile.sh\n</code></pre></li> <li> <p>Execute the server:    <pre><code>AlgoMarker_python_API/ServerHandler/Linux/Release/AlgoMarker_Server --algomarker_path $AM_CONFIG --library_path $AM_LIB --port 1234\n</code></pre></p> <ul> <li><code>AM_CONFIG</code>: Path to the AlgoMarker configuration file.</li> <li><code>AM_LIB</code>: Path to the AlgoMarker shared library.    Refer to AlgoMarker Library for compilation steps.</li> </ul> </li> </ol> <p>Python Wrapper:</p> <ol> <li>Clone the repository:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_Tools.git\n</code></pre></li> <li>Edit <code>AlgoMarker_python_API/run_server.sh</code> and update the following:</li> </ol> <ul> <li><code>AM_CONFIG</code>: Path to the AlgoMarker configuration file.</li> <li><code>AM_LIB</code>: Path to the AlgoMarker shared library. Refer to AlgoMarker Library for compilation steps.</li> <li>If using the old ColonFlag, follow the steps in the ColonFlag setup page to compile the ICU library. Add the ICU library path to <code>LD_LIBRARY_PATH</code> in the script before calling <code>uvicorn</code>. 3. Run the Server <code>AlgoMarker_python_API/run_server.sh</code></li> </ul> <p>For more details follow: Python AlgoMarker API Wrapper</p>"},{"location":"Installation/index.html#3-mes-tools-to-train-and-test-models","title":"3. MES Tools to Train and Test Models","text":""},{"location":"Installation/index.html#description_2","title":"Description","text":"<p>Executables for training and testing models, along with other tools developed by MES for command-line use.</p>"},{"location":"Installation/index.html#installation_3","title":"Installation","text":"<ol> <li>Set up Boost Libraries. No need to compile.</li> <li>Clone the repositories:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_Tools.git\ngit clone git@github.com:Medial-EarlySign/MR_LIBS.git\n</code></pre></li> <li>Navigate to the <code>MR_Tools</code> directory:    <pre><code>cd MR_Tools\n</code></pre></li> <li>Edit <code>All_Tools/CMakeLists.txt</code> to set <code>LIBS_PATH</code> to the MR_LIBS cloned directory. The default structure is:    <pre><code>Root Directory\n\u251c\u2500\u2500 MR_LIBS\n\u2514\u2500\u2500 MR_Tools\n</code></pre>    With this structure, no edits are needed.</li> <li>Execute:    <pre><code>AllTools/full_build.sh\n</code></pre></li> </ol>"},{"location":"Installation/index.html#4-python-wrapperpython-api-for-mes-infrastructure","title":"4. Python Wrapper/Python API for MES Infrastructure","text":""},{"location":"Installation/index.html#description_3","title":"Description","text":"<p>A Python API Wrapper for the MES Infrastructure.</p>"},{"location":"Installation/index.html#installation_4","title":"Installation","text":"<ol> <li>Install the required libraries:    <pre><code>sudo apt install python3-dev swig -y\n</code></pre></li> <li>Compile the Boost library.</li> <li>Edit <code>Internal/MedPyExport/generate_binding/CMakeLists.txt</code> to include the following line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    This should point to your Boost compiled home directory (<code>WORK_BUILD_FOLDER</code>) from the compilation step. Ensure the compiled libraries are in <code>/libs</code> and the headers are in <code>/include</code>.</li> <li>Execute:    <pre><code>Internal/MedPyExport/generate_binding/make-simple.sh\n</code></pre></li> </ol>"},{"location":"Installation/index.html#code-to-load-all-tools-into-the-environment","title":"Code to Load All Tools into the Environment","text":"Start-Up Script<pre><code>#!/bin/bash\n\n# Path to Boost library\nLD_PATH=${HOME}/Documents/MES/Boost/lib\n# Path to Git Repo clones\nMR_LIBS=${HOME}/Documents/MES/MR_LIBS\nMR_TOOLS=${HOME}/Documents/MES/MR_Tools\nMR_SCRIPTS=${HOME}/Documents/MES/MR_Scripts\n\nexport PATH=$PATH:${MR_TOOLS}/AllTools/Linux/Release:${MR_SCRIPTS}/Python-scripts:${MR_SCRIPTS}/Bash-Scripts:${MR_SCRIPTS}/Perl-scripts\nif [ ! -z \"$LD_LIBRARY_PATH\" ]; then\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${LD_PATH}\nelse\n    export LD_LIBRARY_PATH=${LD_PATH}\nfi\nexport PYTHONPATH=${MR_LIBS}/Internal/MedPyExport/generate_binding/Release/medial-python312:${MR_TOOLS}/RepoLoadUtils/common\n</code></pre> <p>Please edit <code>LD_PATH</code>, <code>MR_LIBS</code>, <code>MR_TOOLS</code>, and <code>MR_SCRIPTS</code> paths as needed.</p>"},{"location":"Installation/Doxygen.html","title":"Doxygen","text":"<p>Automatic HTML Documentation with C++.</p>"},{"location":"Installation/Doxygen.html#creating-comments-in-libs","title":"Creating Comments in Libs","text":"<ul> <li>You can create local documentation for your code for MR_LIBS by runnig this:</li> </ul> <pre><code>MR_LIBS/document_code_user.sh\n</code></pre> <p>The documentation will be created in /home/$USER/html/libs/html, please edit the script to change the desired location if needed.  To access, you can just open the index.html file or host the directory with <code>python -m http.server -d /home/$USER/html/libs/html 8000</code> on port 8000</p> <p>The build process of this repository is being executed by runnnig this script: <pre><code>$MR_ROOT/Libs/document_code_server.sh\n</code></pre> the documentation will be availabe in\u00a0https://Medial-EarlySign.github.io/MR_LIBS</p>"},{"location":"Installation/Doxygen.html#general-use-of-doxygen-tool-for-other-projects","title":"General Use of Doxygen tool for other projects:","text":"<ol> <li>Create a Doxygen configuration file by running (on Linux) to create default config file:</li> </ol> <pre><code>doxygen -g\n</code></pre> <ol> <li>Edit the following lines in the created file Doxyfile:</li> <li>PROJECT_NAME - write project name</li> <li>OUTPUT_DIRECTORY - the output html directory. If empty, docs will be written to html/ in the project directory. For public use, change to /var/www/html/${YOUR_DOCUMENTATION_ROOT_NAME, e.g., \"Libs\"}</li> <li>JAVADOC_AUTOBRIEF = YES</li> <li>OPTIMIZE_OUTPUT_FOR_C = YES</li> <li>QUIET = YES</li> <li>RECURSIVE= YES</li> <li>GENERATE_LATEX = NO</li> <li>The following command will generate html documentation from comments in the code (see next section). Re-run the command if you want newly-added Doxygen comments to be incorporated.</li> </ol> <pre><code>sudo doxygen Doxyfile\n</code></pre> <p>If OUTPUT_DIRECTORY was empty, simply view html/index.html in the project directory with any browser. The public documentation (e.g., for Libs) look for Creating Lib documentation section</p>"},{"location":"Installation/Doxygen.html#how-to-create-documentation-in-code","title":"How to create documentation in code","text":"<ol> <li>An example of documenting class members:</li> </ol> <pre><code>string predictor; ///&lt;the predictor type - same as in the json file: qrf,lightgbm...\n</code></pre> <p>The \"///\" initiates a Doxygen comment and the \"&lt;\" specifies that the comment comes after the element declaration. A second option is to add a comment before the member declaration:</p> <pre><code>\u00a0///the predictor type - same as in the json file: qrf,lightgbm...\nstring predictor;\n</code></pre> <ol> <li>To document a class or a file, put this section before the class declaration:</li> </ol> <p><pre><code>/** ImportanceFeatureSelector(importance_selector) - selector which uses feature importance method for sepcific\n* model to rank the feature importance and select them\n* \n* To Use this selector specify \"importance_selector\"\n*/\n</code></pre> The first paragraph, up to a dot or an empty line, is used for a brief description, which appears at the head of the documentation page. The rest of the comment is placed in a detailed description section that appears further down on the documentation page.</p> <ol> <li>For functions, the following syntax is better: <pre><code>/// &lt;summary&gt;\n/// Compares features created by current instance are compatible to features\n/// &lt;/summary&gt;\n/// &lt;returns&gt;\n/// Null if compatible, otherwise the difference\n/// &lt;/returns&gt;\n</code></pre></li> </ol> <p>For more details, see (external network): https://www.stack.nl/~dimitri/doxygen/manual/docblocks.html</p>"},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html","title":"Howto Debug Program in Linux","text":""},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html#step-1-compile-your-program-with-debug-symbols-g-flag","title":"Step 1 - compile your program with debug symbols \"-g\" flag","text":"<p>In order to debug program properly, you need to compile the program with debug symbols. to do that you can use \"smake_dbg\" instead of \"smake_rel\" and the new exe will be saved under Linux/Debug. If the debug mode is very slow you can directly edit CMakeList.txt and add \"-g\" flag to the CXX_FLAGS so it will compile with optimizations as in release but you can still debug it</p>"},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html#step-2-open-debugger","title":"Step 2 - open debugger","text":"<p>After the compilation has been finished. run</p> <pre><code>gdb EXE_PATH\n</code></pre> <p>or better tool (wrapper of gdb) with \"gui\":</p> <pre><code>cgdb EXE_PATH\n</code></pre> <p>in order to jump between code and command line in the gui - use ESC key to focus on code window (you can than scroll up and down) and \"i\"\u00a0 to return the command prompt</p>"},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html#step-3-define-breakpoints-and-usefull-debug-stuff","title":"Step 3 - define breakpoints and usefull debug stuff:","text":"<p>Some usefull commands used to debug (you may shortend the commands, for example instead of \"print\" you may write just \"p\" and \"c\" instead of \"continue\"). remember you have history for your commands even after you close the debugger, so using arrows UP, arrows DOWN to visit history may be usefull.</p> <ul> <li>\"catch th\" - sets breakpoint when first exception is thrown</li> <li>\"b FILE_NAME.cpp:LINE_NUMBER\" - sets breakpoint in line with file. for example: \"b MedModel.cpp:100\" will set breakpoint at line 100 in MedModel.cpp.<ul> <li>hitting just \"b LINENUM\" will set breakpoint in line of current file(of stach trace)</li> <li>Conditional breakpoint - you may specify \"if CONDITION\" in the end of the command to set conditional breakpoint. example:\u00a0\"b MedModel.cpp:100 if pid==5000001\"</li> </ul> </li> <li>\"i b\" - prints all breakpoints you have</li> <li>\"d #NUM_BREAKPOINT\" - deletes breakpoint by number, without number will delete all</li> <li>\"bt\" - prints backtrace of your program to see all program call. example: <pre><code>bt\n#0  medial::io::ProgramArgs_base::parse_parameters (this=this@entry=0x7fffffffd400, argc=argc@entry=1, argv=argv@entry=0x7fffffffd888) at /nas1/UsersData/alon/MR/Libs/Internal/MedUtils/MedUtils/MedUtils.cpp:\n360\n#1  0x00000000004db6d4 in main (argc=1, argv=0x7fffffffd888) at /nas1/UsersData/alon/MR/Tools/Flow/Flow/Flow.cpp:396\n</code></pre></li> <li>\"u\" - go up in the stacktrace (who called the function), \"d\" - do down in the stacktrace (the function the current function called). \"sel #NUM\" - jump to current NUM stack in backtrace. for example \"sel 1\" will jump to first item in stacktrace</li> <li>\"i th\" - info threads prints inforamtion about threads. example: <pre><code>i thr\n  Id   Target Id         Frame\n* 1    Thread 0x7ffff7fcb000 (LWP 14541) \"Flow\" main (argc=1, argv=0x7fffffffd888) at /nas1/UsersData/alon/MR/Tools/Flow/Flow/Flow.cpp:395\n</code></pre></li> <li>\"thr #THREAD_NUM\" - go to different thread for debugging different stack trace in other thread</li> <li>\"r PROGRAM_ARGS\" - to start running the program. you may start the program again at anytime.</li> <li>\"i args\" - prints function arguments</li> <li>\"i locals\" - prints current function scope with all variables</li> <li>\"p VAR_NAME\" - prints variable value. example: <pre><code>p argv[0]\n$2 = 0x7fffffffdc1e \"/server/Linux/alon/bins/Flow\"\n</code></pre></li> <li>\"n\" - next step advance program by 1 command</li> <li>\"s\" - step into - advance program by 1 command but step into if calling new function</li> <li>\"c\" - continue program run</li> <li>\"q\" - to quit debugger</li> <li>stop jump between threads when debugging multiple threads program - \"set scheduler-locking on\"</li> </ul> <p>Special prints of STL library:</p> <p>to access vector (we have old debugger so we can't just access it with\u00a0[]). If we have for example \"vector vec\" variable, to access i index we should use \"vec._M_impl._M_start[i]\" to access this position"},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html#bonus-section","title":"Bonus Section","text":"<p>Linux creates crash dump for crashed programs (configuration of which program to run when program crashes can be found here: /proc/sys/kernel/core_pattern). currently uses with abrt tool. the dumps are located in different places (depend on the node, we have for some reason different configuration path for dumps in each node - no good reason). The dumps are deleted when new crash dump is created and when we reach the limit quota for dumps, so don't worry, it won't fill up all our storage space. there is command \"show_crashes.sh\" in BASH_SCRIPTS - that locate those dump location in the current node. to open a crash dump, see backtrace and print variables in the stack use gdb/cgdb to open the dump:</p> <pre><code>sudo cgdb CRASHED_PROGRAM_BIN_PATH CRASH_DUMP_PATH\n# example:\n# $&gt; show_crashes.sh\n# /home/tmp/abrt/ccpp-2019-02-21-09:54:17-13058/coredump  /server/UsersData/alon/MR/Projects/Shared/But_Why/Linux/Release/TryExplain --base_config /server/Work/Users/alon/But_Why/configs/explain_base.cfg\n# $&gt; sudo cgdb /server/UsersData/alon/MR/Projects/Shared/But_Why/Linux/Release/TryExplain /home/tmp/abrt/ccpp-2019-02-21-09:54:17-13058/coredump\n</code></pre>"},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html#change-path-to-different-source-code-folder","title":"Change path to different source code folder","text":"<p>For example: <pre><code>set substitute-path /home/alon-internal/MR_ROOT /nas1/UsersData/Alon/MR\n</code></pre></p>"},{"location":"Installation/Howto%20Debug%20Program%20in%20Linux.html#profiler","title":"Profiler","text":"<p>how to debug speed. compile program with -pg flags. g adds debug symbols and p adds gmon.out output for th profiler. after program ends gmon.out output will apear in your current directory. run : <pre><code>gprof PATH_TO_BIN_APP PATH_TO_GMON.OUT | less\n</code></pre></p> <p>set substitute-path /home/foo /tmp/debug/home/foo </p>"},{"location":"Medial%20Tools/index.html","title":"Medial Tools","text":"<p>This section provides a list of applications, tools, and executables built on the AlgoMedical library framework:</p> <ul> <li>Guide for Common Actions</li> <li>Using the Flow App: A versatile application with multiple switches, each triggering a specific action. Below are some of the key functionalities:<ul> <li>Load New Repository</li> <li>Train a model</li> <li>Apply a model to generate predictions.</li> <li>Extract feature matrices from the model pipeline.</li> <li>Print specific patient data or signal distributions.</li> <li>Feature Importance with Shapley Values Analysis</li> <li>more...</li> </ul> </li> <li>Bootstrap App: A tool for bootstrap analysis of prediction and outcome files.<ul> <li>Bootstrap Legend - The bootsrap output file result legend</li> <li>Extending Bootstrap:<ul> <li>Using Harrell C Statistics.</li> </ul> </li> <li>Utility Tools for Processing Bootstrap Results.</li> <li>Versions.</li> </ul> </li> <li>Optimizer: A tool for hyperparameter optimization.</li> <li>TestModelExternal: Compares repositories or samples by building a propensity model to identify differences.</li> <li>Change Model: Modifies a model without retraining, such as enabling verbose mode for outliers or limiting memory batch size.<ul> <li>How to Limit Memory Usage in Predict.</li> </ul> </li> <li>Adjust Model: Adds components like rep_processors or post_processors to a model. Some components may require training with MedSamples and a repository.</li> <li>Iterative Feature Selector: Iteratively selects features or groups of features in a top-down or bottom-up manner and generates a report.</li> <li>GANs for Imputing Matrices:<ul> <li>Training Masked GAN.</li> </ul> </li> <li>Fairness Extraction: Calculates fairness metrics.</li> <li>Model Signals Importance: Evaluates the importance of signals in an existing model and measures the impact of including or excluding them on performance.</li> <li>Simulator: Simulates performance by controlling variables such as target population age distribution and key covariates. It also evaluates the impact of signal availability and existence.</li> </ul>"},{"location":"Medial%20Tools/index.html#examples-of-simple-applications","title":"Examples of Simple Applications","text":"<p>To learn how to create your own applications, clone the MR_Tools repository. Navigate to the <code>MedProcessUtils</code> directory and explore the following examples:</p> <ul> <li><code>learn</code> - Application.</li> <li><code>getMatrix</code> - Application.</li> <li><code>predict</code> - Application.</li> </ul>"},{"location":"Medial%20Tools/index.html#retired-applications","title":"Retired Applications","text":"<ul> <li>Action Outcome Effect: A tool for estimating the average treatment or action effect on outcomes. It also supported feature selection, model selection, training, and bootstrap analysis with recovery and step-skipping capabilities.</li> <li>Signals Dependencies: Identifies statistical dependencies between registry and signal values within a time window. Useful for selecting categorical signals.</li> <li>Create Registry:<ul> <li>Create Membership Registry Example Command.</li> </ul> </li> <li>Find Required Signals.</li> <li>Compare AUCs.</li> </ul>"},{"location":"Medial%20Tools/Compare%20AUC%27s.html","title":"Compare AUC's","text":"<p>As a part of the Lung cancer paper, we were asked to check the statistical significance of the AUC's difference (our model vs. a baseline model). We used a non-parmetric empirical method describe by Delong. The method is decribed In the attached bwloe pdf (Comparing Two ROC Cureves - Paired Design - page 547-6) The tool can be found in\u00a0$MR_ROOT/Tools/MedProcessUtils/CompareRocs/. Should run in\u00a0python 3\u00a0(use the command: source /opt/medial/python36/enable). \u00a0 There are two modes for running the Tool: 1.  Using 2 prediction files (one for each model) Example: Running Example <pre><code>python /server/UsersData/ron-internal/MR/Tools/CompareRocs/compareROCs.py --preds_file_1 /server/Work/Users/Ron/Projects/LungCancer/results/model_27/model_27_test.preds --preds_file_2 /server/Work/Users/Ron/Projects/LungCancer/results/Tammemagi_nsclc_ever_smokers_smoking_intensity/Tammemagi_nsclc_ever_smokers_smoking_intensity_non_linear_test.preds\nsys:1: DtypeWarning: Columns (1,2,3,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\npreds are equal\nmodel 1: AUC: 0.8647144227120102, Variance: 5.427313956814398e-06, estimated 95% CI: [0.8601482898518649, 0.8692805555721554]\nmodel 2: AUC: 0.823383955909251, Variance: 8.174414229417143e-06, estimated 95% CI: [0.817780133133348, 0.8289877786851539]\nZ - score: 23.081440151744086, p-value: 0.0\n</code></pre> \u00a0 \u00a0 \u00a0 2.  Using 2 bootstrap .Raw files + the requested cohort string Example: Running Example <pre><code>python /server/UsersData/ron-internal/MR/Tools/CompareRocs/compareROCs.py --preds_file_1 /server/Work/Users/Ron/Projects/LungCancer/results/model_27/paper/p_value_bs_out_1.Raw --preds_file_2 /server/Work/Users/Ron/Projects/LungCancer/results/model_27/paper/p_value_bs_out_2.Raw --cohort_string Time-Window:270.000-365.000,Age:55.000-80.000,Lung_Cancer_Type.category_set_NotNonSmallCell.win_-3650_3650:-0.500-0.500,NLST_Criterion_min_age_55_max_age_80_pack_years_30:0.500-15.000\npreds are equal\nmodel 1: AUC: 0.8011430555038992, Variance: 0.00025990328389302294, estimated 95% CI: [0.7695448837940765, 0.8327412272137219]\nmodel 2: AUC: 0.7436638110941679, Variance: 0.0003739639445801397, estimated 95% CI: [0.7057610422045595, 0.7815665799837763]\nZ - score: 4.470422572292469, p-value: 3.903260102799955e-06\n</code></pre> Comparing_Two_ROC_Curves-Paired_Design.pdf</p>"},{"location":"Medial%20Tools/Fairness%20extraction.html","title":"Fairness extraction","text":"<p>In order to test fairness we need to compare Sensitivity, Specificity at specific score cutoff among different groups and compare those values. We have tool in SCRIPTS git repository, so it is already part of the PATH and we can use it directly. \u00a0 Example usage: <pre><code>#!/bin/bash\nFAIRNESS_BASE_COHORT=\"Age:40,89;Time-Window:90,365;Ex_or_Current_Smoker:0.5,1.5\"\nFAIRNESS_GROUPS=(\"Gender:1,1\" \"Gender:2,2\")\nOUTDIR=/tmp\necho \"MULTI;${FAIRNESS_BASE_COHORT}\" | sed 's|;|\\t|g' &gt; ${OUTDIR}/cohorts\nfor fr_grp in \"${FAIRNESS_GROUPS[@]}\"; do\n    echo \"MULTI;${FAIRNESS_BASE_COHORT};${fr_grp}\" | sed 's|;|\\t|g' &gt;&gt; ${OUTDIR}/cohorts\ndone\nPREDS_FILE=/nas1/Work/Users/Eitan/Lung/outputs/models2023/EX3/model_63/Test_Kit/bootstrap/TimeWindow.alt/result_win_90_365.preds\nbootstrap_app --use_censor 0 --sample_per_pid 0 --input ${PREDS_FILE} --rep /nas1/Work/Repositories/KP/kp.repository  --json_model /nas1/Work/Users/Alon/LungCancer/configs/analysis/bootstrap/bootstrap.json --output ${OUTDIR}/bt.fairness.by_pr --cohorts_file ${OUTDIR}/cohorts --working_points_fpr 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99 --working_points_pr 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99\nfairness_extraction.py --bt_report ${OUTDIR}/bt.fairness.by_pr.pivot_txt --output ${OUTDIR} --bt_cohort \"${FAIRNESS_BASE_COHORT}\" --cutoffs_pr 3 5\n</code></pre> \u00a0 fairness_extraction.py inputs:</p> <ul> <li>bt_report - accepts bootstrap output file (that we run with many PR or FPR cutoffs and on cohorts with the groups we want to compare)</li> <li>bt_cohort - the basic bootstrap cohort we used to filter/select as baseline, on top of that we will filter the \"groups\" to compare. If not given will extract from the bootstrap file the shortest cohort description and will use this.</li> <li>output - directory where we will output files:</li> <li>cutoffs_pr - and array of cutoffs to inspect based on PR if exists or FPR if not, based on the baseline cohort. The code before the script calls the bootstrap and we also define the \"BASE_COHORT\" group and the groups to compare \"Gender:1,1\" and \"Gender:2,2\"</li> </ul>"},{"location":"Medial%20Tools/Find%20Required%20Signals.html","title":"Find Required Signals","text":"<p>Find Required Signals is a tool for finding minimal requirement signals. The idea is to find combinations of signals that for a given model achieve a certain performance requirement. The tool can be found in\u00a0$MR_ROOT/Tools/MedProcessUtils/findRequiredSignals. \u00a0 Algorithm Overview:\u00a0 The algorithm runs over combinations of signals of growing sizes, starting from combinations of size one. At each combination size, the model runs over different combination sets. For each combination, rep-processors that delete the signals that are not part of the current combination are added, and performance is evaluated. The number of possible combinations at combination size\u00a0k\u00a0is generally\u00a0number of signals\u00a0over\u00a0k.\u00a0Since this number grows very fast, we limit ourselves with a parameter provided by the user\u00a0max_num_tests_per_iter,\u00a0and we search only over\u00a0n\u00a0signals, where\u00a0n\u00a0is the largest integer so that\u00a0n\u00a0over\u00a0k\u00a0is smaller than\u00a0max_num_tests_per_iter.\u00a0The\u00a0n chosen signals\u00a0are the best\u00a0n\u00a0signals from the previous stage. \u00a0</p>"},{"location":"Medial%20Tools/Find%20Required%20Signals.html#parameters","title":"Parameters:","text":"<ul> <li>Input parameters<ul> <li>model_file::\u00a0 *.model file.</li> <li>samples_file:: samples file.</li> <li>rep_file::\u00a0repository file.</li> <li>bootstrap_json: bootstrap json file</li> </ul> </li> <li>Output parameters<ul> <li>out_file:: output file</li> <li>evaluations_to_print:0: Number of evaluations to print in each stage (0 - Only combinations that pass the required performance. 1 - The best result, 2 - The two best results. etc. -1 means print all/</li> </ul> </li> <li>Algorithm parameters<ul> <li>req:\"BDATE,BYEAR,GENDER\": comma-separated list of required signals (signals that are always used).</li> <li>max_num_tests_per_iter:: maximal allowed tests to be done in a iteration. Determines how quick the algorithm finishes. Large number of tests - means that more signals will be checked, but running time will grow.\u00a0Recommendation: evaluate it\u00a0 with a short dummy run</li> <li>required_ratio: : required performance compared to the full model (either that or required_abs should be entered).</li> <li>required_abs:: absolute required performance\u00a0(either that or required_ratio should be entered).</li> <li>num_iterations_to_continue:0 :Number of iterations to perform after we get to the required performance.</li> <li>maximal_num_of_signals:666:Maximal number of signals allowed in a required signal set</li> <li>bootstrap_params:sample_per_pid:1: Parameters for bootstrap. e.g. sample_per_id=1 ('/' separated) file\")</li> <li>cohort_params:Age:45,120/Time-Window:0,365: Parameters for defining the bootstrap cohort. e.g.\u00a0Age:50,75/Time-Window:0,365</li> <li>msr_params:AUC:\u00a0Define the performance measurement. e.g. AUC or SENS,FPR,0.2 for Sensitivity at FPR=20%</li> <li>skip_supersets: true : Whether to skip supersets of acceptable combinations (for example: if signals X,Y are good enough, whether to run on X,Y,Z)</li> <li>delete_signals: : Signals which exists, but never checked (meaning that they are forced to be deleted always).</li> </ul> </li> </ul>"},{"location":"Medial%20Tools/Find%20Required%20Signals.html#example-running-example","title":"Example: Running Example","text":"<p>Running Example<pre><code>$MR_ROOT/Tools/MedProcessUtils/Linux/Release/findRequiredSignals --model_file example_model.model --samples_file example_samples.samples --rep_file /server/Work/CancerData/Repositories/KP/kp.repository --out_file required_sigs_out.txt --msr_params AUC --max_num_tests_per_iter 1000 --required_ratio 0.95 --evaluations_to_print 0 --num_iterations_to_continue 2 --cohort_params Age:45,80/Time-Window:120,365\n</code></pre> Output example:</p> <p>Output example<pre><code>Required Performance0.817788\nSignals Performance\nSmoking_Status 0.82436\nICD9_Diagnosis,Pack_Years 0.823099\nWBC,Pack_Years 0.818223\nSmoking_Quit_Date,Pack_Years 0.817865\nSmoking_Quit_Date,WBC,ICD9_Diagnosis 0.821482\n</code></pre> </p>"},{"location":"Medial%20Tools/Iterative%20Feature%20Selector.html","title":"Iterative Feature Selector","text":"<p>Iterative Selector is a tool for\u00a0finding\u00a0important signals (or features), by building predictors iteratielvy (by a greedy bottom up approach or vice versa). The tool can be found in $MR_ROOT/Tools/MedProcessUtils/IterativeSelector.\u00a0This is an envelope for running the FeatureProcessor iterativeFeatureSelector. For brevity we will refer to signals or features as signals. There are two methods for\u00a0finding\u00a0 the important signals: 1. Bottom up - Start with a low number of signals (some required signals, or none). In each iteration run on the remaining signals, and add those who improve the selected metric the most (i.e., AUC). 2. Top down - Start with all the signals. In each iteration run on the remaining signals, and remove those who least decrease the selected metric. \u00a0 Parameters:</p> <ul> <li>Input parameters (either inCsv, inBin or inSamples+inModel+config should be given):<ul> <li>inCsv: input matrix as csv file.</li> <li>inBin: input matrix as bin file.</li> <li>inSamples: input samples for generating matrix</li> <li>inModel: Bin model file for generating matrix from samples</li> <li>inJson: Json model file for generating matrix from samples (either inModel or inJson should be given with inSamples)</li> <li>rep: Repository config file</li> <li>inpatient: :indicate that relevant data is in-patient data.</li> </ul> </li> <li>Output parameters<ul> <li>out: : output file.</li> </ul> </li> <li>Selector parameters:<ul> <li>predictor: : predictor type (i.e, qrf)</li> <li>predictor_params: : default predictor parameters. in the form of\u00a0 \"param1=value1;param2=value2 \" .There is an option to provide a file with \"number of features\" ranges, and a predictor parameters\u00a0 string (will be explained later). If the current number of features is not covered in the file (or the file doesn't exist), then this value is used.</li> <li>predictor_params_file:\"\":input predictor parameters file - if exists, overrides predictor parameters. The file\u00a0 has one line per number or features range. each line's fomat is \"min_number_of_features\u00a0\u00a0max_number_of_features predictors parameter string\". Tab\u00a0\u00a0delimited. see example below.</li> <li>nfolds: : if given, replace given splits with id-index%nfolds</li> <li>folds: :comma-separated list of folds to actually take for test. If not given - take all.</li> <li>mode: top2bottom : directions of selection (top2bottom/bottom2top).</li> <li>rate:\"50:1,100:2,500:5,5000:10\": instruction on rate of selection - comma separated pairs : #-bound:step. Determines how many signals to add or to drop. For example, in the default setting, when working with bottom up, when number of signals is between 1 to 50, the program adds 1 signal each iteration. When\u00a0number of signals is 51 to100, the program adds 2 signals each iterations, etc. It works symmetrically in top2bottome.</li> <li>univariate_nfeatures: : number of features (not signals!) to select in an initial univariate selector stage.</li> <li>univariate_params:\"method=mi\": univariate selector parameters.</li> <li>required: comma-separated list of required features.</li> <li>work_on_ftrs:false:if true - work on features, if false, work on signals.</li> <li>verbose: Verbosity flag</li> <li>Performance evealuation is done by MedBootstrap with nbootstrap=0 (written as \"loopcnt:0\"), looking at the observed measurement:<ul> <li>bootstrap_params: Parameters for bootstrap. e.g. sample_per_id=1 ('/' separated)</li> <li>cohort_params: Parameters for defining the booststrap cohort. e.g. Age:50,75/Time-Window:0,365</li> <li>msr_params: Define the performance measurement. e.g. AUC or SENS,FPR,0.2 for Sensitivity at FPR=20%</li> </ul> </li> <li>selector_params: iterativeFeatureSelection initialization string (semicolon-separated. Json files format) that replaces all the above parameters Example of running line: Running Example Running Example<pre><code>/nas1/UsersData/yaron/MR/Tools/MedProcessUtils/Linux/Release//iterativeSelector --inSamples samples --inJson simple_model.json --out outReport --predictor qrf --predictor_params_file params_iterative_seletcor --nfolds 5 --folds \"0,2,4\" --mode top2bottom --verbose 1 --msr_params AUC --cohort_params \"Age:0,200\" --required \"Age,Gender\"\n</code></pre> Example for predictor parmeters file: (Can be found at /nas1/Work/Users/yaron/Examples/iterativeSelector/params_iterative_seletcor). 0 50 ntrees = 200 ; min_node = 300 51 150 ntrees = 200 ; min_node = 200 151 200 ntrees = 200 ; min_node = 100 201 100000 ntrees = 200 ; min_node = 50 \u00a0 Output Example: File can be found at\u00a0/nas1/Work/Users/yaron/Examples/iterativeSelector/outReport</li> </ul> </li> </ul> <p>Output example</p> <pre><code>Removing family RBC with AUC_Obs = 0.747491\nRemoving family RDW with AUC_Obs = 0.747476\nRemoving family INR with AUC_Obs = 0.749214\nRemoving family PDW with AUC_Obs = 0.751048\nRemoving family MCV with AUC_Obs = 0.750944\nRemoving family Hematocrit with AUC_Obs = 0.751496\nRemoving family WBC with AUC_Obs = 0.751073\nRemoving family MCHC-M with AUC_Obs = 0.750825\nRemoving family Hemoglobin with AUC_Obs = 0.752080\nRemoving family Platelets with AUC_Obs = 0.741643\nRemoving family MCH with AUC_Obs = 0.688449\n</code></pre>"},{"location":"Medial%20Tools/Optimizer.html","title":"Optimizer","text":""},{"location":"Medial%20Tools/Optimizer.html#goal","title":"Goal","text":"<p>The goal is to optimize hyper parameters. the options are main focused on MedPredictor parameters, but we can also specify weights and different samples for training and combineing all those options. Please use <code>Optimizer --help</code> compiles using AllTools</p>"},{"location":"Medial%20Tools/Optimizer.html#lightgbm_modeloptions","title":"lightgbm_model.options","text":"<pre><code>verbose=0\nsilent=2\nnum_threads=15\nnum_trees=200\nmetric=auc\nobjective=binary\nlearning_rate=0.01,0.03,0.05\nlambda_l2=0\nmetric_freq=1000\nmin_data_in_leaf=100,500,1000,2000\nfeature_fraction=0.8,1\nbagging_fraction=0.8\nbagging_freq=5\nmax_bin=250\nboosting_type=gbdt\nmax_depth=0,5,6,7\nmin_data_in_bin=50\n</code></pre>"},{"location":"Medial%20Tools/SignalsDependencies.html","title":"SignalsDependencies","text":"<p>The Code exists in: $MR_ROOT/Tools/SignalsDependencies\u00a0and is basically using library functions in MedRegistry object - method called \"calc_signal_stats\" after loading the registry. This tool will allow you to discover relevant categorical signals (for example: readcodes or drugs) that has statstical connection to you outcome within a defined time-window. The tool will create MedSamples based on the signal time points and than label those samples based on the registry and \"labeling_params\" parameter which defines the rules for the labeling - either case, control or excluded (if can't determine for example). It will create contingency\u00a0table from samples within time-window for each gender and age group:</p> <ul> <li>Signal value doesn't exists (the patient didn't have certain readcode value in the time window) &amp; the registry outcome is false - will be calcluated based on the incidence rate of the outcome in this age bin</li> <li>Signal value doesn't exists (the patient didn't have certain readcode value in the time window) &amp; the registry outcome is true\u00a0- will be calcluated based on the incidence rate of the outcome in this age bin</li> <li>Signal value exists (the patient certain readcode value in the time window) &amp; the registry outcome is false</li> <li> <p>Signal value exists (the patient certain readcode value in the time window) &amp; the registry outcome is true It will allow you the sort and filter the results using fdr (false detection rate, minimal count for signal existence, minimal coutn for positive registry in siganl existence.. and more) It will also allow you to create and look at specific tables of Male,Female and all age-group for certain readcode value to see the connection between the specific signal and the registry \u00a0 The registry format is tab-delimited: [PID, Start_Date, End_Date, RegistryValue]\u00a0 Start_date - is the outcome registry start time for the outcome to be labeled (in cancer it's the first time the patient got cancer) End_date - is the outcome registry finish time (where after it the outcome value isn't valid anymore) - for example it may be censoring date. for control it's the last time we know it's still control for more details reffer to\u00a0MedRegistry\u00a0 \u00a0 Explain on labeling_params and inc_labeling_params can be given in\u00a0TimeWindowInteraction. Those arguments are LabelParams objet that defines how to label sampels. \u00a0 Important parameters for the tool (that most be supplied, don't use default ones unless you know what you are doing):</p> </li> <li> <p>global_rep - Repository path</p> </li> <li>registry_path - the path to the MedRegistry file</li> <li>labeling_params - the parameters to control how to label the samples created by the signal time points</li> <li>test_from_window, test_to_window - to control the time window</li> <li>test_main_signal - the signal to test If you are using default parameters, you are at high risk of a problem.</li> </ul>"},{"location":"Medial%20Tools/SignalsDependencies.html#hirarchy-filtering-parameters","title":"Hirarchy Filtering parameters:","text":"<p>The filtering happens in this method: medial::contingency_tables::filterHirarchy The filtering happens in this order:</p> <ol> <li>float filter_child_count_ratio (default value is 0.05)  If child ratio count is similar to the parent, keep only parent code. For example child has 10,000 samples and parent has 10,100 samples. The additional 100 samples out of 10,100 are little ~1% which is less than default value of 5%, so the child is eliminated.</li> <li>Those are used together  float filter_child_pval_diff (default value is 1e-10 )  float filter_child_lift_ratio (default value is 0.05) When both p_value difference between parent and child is below filter_child_pval_diff AND diff in average lift is below filter_child_lift_ratio  , will remove parent. The parent \"behaves\" differently from at least 2 children, so aggregation of those children into the parent category might be unreasonable.  3 .float filter_child_removed_ratio (default value is 1) Only when node has child that pass the above filters and at least 1 child eliminated.  If the aggregated sum of removed samples due to filtered children is high, consider removal of parent code. For example: if parent has 10,000 samples, and removed children with 8,000 codes, than remove the parent, since aggregation of the children below the parent is unreasonable.</li> </ol>"},{"location":"Medial%20Tools/SignalsDependencies.html#examples","title":"Examples","text":""},{"location":"Medial%20Tools/SignalsDependencies.html#labeling_params-parameter-examples","title":"labeling_params parameter examples:","text":"<p>since this parameter is tricky, here are some examples:</p> <ul> <li> <p>Outcome which can happen several times for a specific period (For example Flu).\u00a0labeling_params=\"label_interaction_mode=0:after_start|1:before_start,after_start;conflict_method=all\"Explaination - cases has the settings of \"before_start,after_start\",\u00a0 which means the from_time_window from the signal time should happen before registry start time records (tha patient starts as control) AND the to_time_window from the signal time should happen after the start time of the same registry record (the patient turned into case). Controls has the settings of \"before_end,after_start\" - which means you should have some overlap with control period of non pregnancy - start time window of signal is before end of control period and end time window of signal is after the start. conflict_method=all - means if we have sample which is also control and also case be those settings - treat it (for counting prupose) also as control and as case.</p> </li> <li> <p>outcome which occours once (for example cancer)-\u00a0\u00a0labeling_params=\"label_interaction_mode=0:after_start,before_end|1:before_start,after_start;conflict_method=max\"Explanation - controls are the ones who the from_time_window of the sample occours after_start of registry control period AND before_end of the same registry control period - means the whole time window is inside the control period. Cases are those where the from_time_window is before_start of specific case time period and the to_time_window is after_start of that same case period. the end_period for cases in this registry is not use since there is no \"due\" date for cancer.</p> </li> </ul>"},{"location":"Medial%20Tools/SignalsDependencies.html#run-examples","title":"Run Examples:","text":"<p>Program Help Program Help<pre><code>SignalsDependencies --help\n##     ## ######## ########  ####    ###    ##\n###   ### ##       ##     ##  ##    ## ##   ##\n#### #### ##       ##     ##  ##   ##   ##  ##\n## ### ## ######   ##     ##  ##  ##     ## ##\n##     ## ##       ##     ##  ##  ######### ##\n##     ## ##       ##     ##  ##  ##     ## ##\n##     ## ######## ########  #### ##     ## ########\n\u00a0\nProgram General Options:\n  -h [ --help ]                         help &amp; exit\n  --help_module arg                     help on specific module\n  --base_config arg                     config file with all arguments - in CMD we override those settings\n  --debug                               set debuging verbose\n --version                              prints version information of the program\nGlobal Options:\n  --global_rep arg                      repository path to fetch signal\\registry\n  --global_stats_path arg               location to save or load stats dictionary for fast load in the second time for more filtering\n  --global_override arg (=0)            whather or not override stats file if already exists\n  --global_age_bin arg (=5)             age bin size (default is 5 years)\nRegistry Options:\n  --registry_path arg                   location to load registry txt file\n  --registry_init_cmd arg               An init command for registry creation on the fly\n  --registry_save arg                   location to export registry txt file created by this tool on the fly\n  --registry_filter_train arg (=1)      if True will filter TRAIN==1\n  --labeling_params arg (=conflict_method=all;label_interaction_mode=0:all,before_end|1:before_start,after_start;censor_interaction_mode=all:within,all)\n                                         the labeling params\n  --sub_sample_pids arg (=0)             down-sample pids till this number to speedup calculation. If 0 no sub sampling\nCensoring Registry Options:\n  --censoring_registry_path arg         location to load registry txt file for censoring\n  --censoring_registry_type arg (=keep_alive)\n                                        censoring registry default path\n  --censoring_registry_args arg         An init command for registry creation on the fly\nTest Signal Options:\n  --test_main_signal arg                main signal to look for it's values when creating test signal\n  --test_from_window arg (=0)           min time before the registry to look for signal (if negative means search forward)\n  --test_to_window arg (=365)           max time before the registry to look for signal (if negative means search forward)\n  --test_hierarchy arg                  Hierarchy type for test signal. free string to filter regex on parents to propage up. to cancel pass \"None\"\n  --inc_labeling_params arg (=conflict_method=all;label_interaction_mode=all:before_end,after_start)\n                                        params for outcome registry interaction with age bin\nFiltering Options:\n  --filter_min_age arg (=20)            minimal age for filtering population in stats table nodes\n  --filter_max_age arg (=90)            maximal age for filtering population in stats table nodes\n  --filter_gender arg (=3)              filter by gender - 1 for male, 2 - for female, 3 - take both\n  --filter_positive_cnt arg (=0)        minimal positive count in registry for signal value to keep signal value from filering out\n  --filter_total_cnt arg (=100)         minimal count for signal value to keep signal value from filering out to remove small redandent signal values\n  --filter_pval_fdr arg (=0.0500000007) filter p value in FDR to filter signal values\n  --filter_min_ppv arg (=0)             filter minimal PPV for signal value and registry values\n  --filter_positive_lift arg (=1)       should be &gt;= 1. filtering of lift &gt; 1. from which value? for example at least lift of 1.5\n  --filter_negative_lift arg (=1)       should be &lt;= 1. filtering of lift &lt; 1. from which value? for example at tops lift of 0.8\n  --filter_child_pval_diff arg (=1.00000001e-10)\n                                        p value diff to consider similar in hirarchy filters\n  --filter_child_lift_ratio arg (=0.0500000007)\n                                        lift change ratio in child comapred to parent to consider similar\n  --filter_child_count_ratio arg (=0.0500000007)\n                                        count similarty to consider similar in hirarchy filters\n  --filter_child_removed_ratio arg (=1) If child removed ratio is beyond this and has other child taken - remove parent\n  --filter_stats_test arg (=mcnemar)    which statistical test to use: (mcnemar,chi-square,fisher)\n  --filter_chi_smooth arg (=0)          number of balls to add and smooth each window\n  --filter_chi_at_least arg (=0)        diff in ratio for bin to cancel - will get change of at least greater than\n  --filter_chi_minimal arg (=0)         the minimal number of observations in bin to keep row\n  --filter_fisher_smooth arg (=0)       fisher bandwith for division to happend. in ratio change from total_count of no_sig to with_sig [0-1]\nOutput Options:\n  --output_value arg (=-1)              if exists will print stats table for only selected signal value else will print all sorted list\n  --output_full_path arg                will only treat if output_value &lt;&gt; NULL. if exist will use to run all and print out all pids data with corersponding\n                                        signal in output_value\n  --output_debug arg (=0)               If true &amp; output_value!=-1 will output intersetions into file for debuging\n</code></pre> \u00a0 Example Usage can be found here: <pre><code>SignalsDependencies --base_config /nas1/Work/Users/Alon/UnitTesting/examples/signalDependency/pregnancy.cfg\n</code></pre> \u00a0 The File config file content (where all parameters may be override with command arguments) is: Config File Example \u00a0Expand source Config File Example<pre><code>#The Repository path:\nglobal_rep = /home/Repositories/THIN/thin_jun2017/thin.repository\n#A param which controls if to override \"global_stats_path\" file or use it if exists and save running time:\nglobal_override = 0\n#the bining of age\nglobal_age_bin = 5\n#the output binary dictionary of the stats to save for faster load in next time (when we want to play with the filtering params)\n#global_stats_path = /server/Work/Users/Alon/Models/outputs/dicts/preg_rc_after.dict\nglobal_stats_path = /server/Work/Users/Alon/Models/outputs/dicts/preg_rc_before.new.dict\n#registry args:\n#each line in registry format looks like: [PID, Start_Date, End_Date, RegistryValue]\n#registry can be create on the fly by specifying \"registry_init_cmd\" or with command:\n#created by $MR_ROOT/Tools/MedProcessUtils/Linux/Release/create_registry --rep $rep_thin --registry_type binary --registry_censor_init \"max_repo_date=20170101;start_buffer_duration=365;end_buffer_duration=365;duration=1095;signal_list=RC\" --conflicts_method none --registry_init \"max_repo_date=20170101;start_buffer_duration=365;end_buffer_duration=365;config_signals_rules=/server/Work/Users/Alon/Models/configs/registry.pregnancy.cfg\"  --registry_save /server/Work/Users/Alon/Models/registry/pregnancy.reg\nregistry_path = /server/Work/Users/Alon/Models/registry/pregnancy.new.reg\n#,Hemoglobin,Glucose,LDL,ALT,Creatinine\ncensoring_registry_args = max_repo_date=20170101;start_buffer_duration=365;end_buffer_duration=365;duration=1095;signal_list=RC\n#the signal to fetch and test compared to the registry\\samples file\nregistry_filter_train = 1\nlabeling_params = label_interaction_mode=0:before_end,after_start|1:before_start,after_start;censor_interaction_mode=all:within,all;conflict_method=all\ninc_labeling_params = label_interaction_mode=all:before_end,after_start;censor_interaction_mode=all:within,all;conflict_method=all\n#test signal\ntest_main_signal = RC\n#the time window parameters in days:\ntest_from_window = 30\ntest_to_window = 720\n#some filtering parameters of the output - age- ranges, minimal count of positives, minimal total count for signal...\nfilter_min_age = 15\nfilter_max_age = 50\n#To filter by gender - 1 for only males, 2 -only females, 3-males and females(default)\nfilter_gender = 2\nfilter_positive_cnt = 0\nfilter_total_cnt = 1000\nfilter_pval_fdr = 0.05\nfilter_min_ppv = 0\nfilter_chi_smooth = 10\nfilter_chi_minimal = 10\nfilter_chi_at_least = 0\n#whater to bring general stats on all the values, or print specific tables of male,gender with all age-groups for specific readcode\\drug value\noutput_value = -1\n</code></pre> The output for all values (the example for the pregnancy in time window before the pregnancy read code of at least 1 month till 2 years): Output example <pre><code>read [267598] records on both male and female stats.\nFilter male stats - using only females\nusing mcnemar statistical test\nAfter Filter Age [15 - 50] have 0 keys in males and 137519 keys in females\ncreating scores vector with size 137519\nAfter total filter has 18095 signal values\nAfter positive filter has 18095 signal values\nAfter positive ratio filter has 18095 signal values\nAfter Hirarchy filter has 11964 signal values\nAfter lifts has 11964 signal values\nAfterFDR has 6908 signal values\nSignal Chi-Square scores (6908 results):\nIndex   Signal_Value    Chi-Square_Score        pValue  dof     Count_allBins   Positives_allBins       LiftProb_allBins        Signal_Name\nStatRow: 1      287376  196.492 0       7       10324   1434    4.4591  GROUP: IVF      G_8C84.\nStatRow: 2      122633  224.836 0       7       3896    847     3.5023  Fertility_counselling_#1        6778.11\nStatRow: 3      122614  3430.6  0       7       34254   9975    3.3106  Pre-pregnancy_counselling       676..00\nStatRow: 4      106282  1099.49 0       7       30692   5717    3.2144  Fertility_problem       1AZ2.00\nStatRow: 5      287372  485.476 0       7       24642   3745    3.1663  GROUP: Treatment_for_infertility        G_8C8..\nStatRow: 6      142172  139.051 0       7       7288    1022    3.1596  IVF     8C84.11\nStatRow: 7      270286  771.086 0       7       11988   2828    3.1565  GROUP: Fertility_counselling_#1 G_6778.\nStatRow: 8      122884  747.336 0       7       11399   2762    3.1528  Pre-conception_advice   67IJ.00\nStatRow: 9      270272  3432.34 0       7       39358   10753   3.1124  GROUP: Pre-pregnancy_counselling        G_676..\nStatRow: 10     292418  478.115 0       7       11690   2244    3.1101  GROUP: Seen_in_fertility_clinic_#1      G_9N07.\nStatRow: 11     262820  147.48  0       7       19118   1796    3.0742  GROUP: Hepatitis_C_antibody_test        G_43X2.\nStatRow: 12     268007  1049.69 0       7       13455   3485    3.0688  GROUP: Planning_to_start_family G_6125.\nStatRow: 13     289114  459.583 0       7       10729   2171    3.0278  GROUP: Referral_to_fertility_clinic     G_8HTB.\nStatRow: 14     122632  547.773 0       7       8122    1988    2.9820  Procreat/fertility_counselling  6778.00\nStatRow: 15     342228  111.969 0       7       2130    460     2.7558  GROUP: Advice_relating_to_pregnancy_and_fertility       G_ZG9..\nStatRow: 16     120165  124.06  0       6       1852    462     2.6691  Planning_to_start_family        6125.11\nStatRow: 17     311447  771.139 0       7       19060   3776    2.6687  GROUP: Subfertility     G_K5Byz\nStatRow: 18     255323  2462.54 0       7       86948   14995   2.6330  GROUP: Genitourinary_symptoms_NOS       G_1AZ..\nStatRow: 19     142164  286.2   0       7       8664    1608    2.6292  Treatment_for_infertility       8C8..00\nStatRow: 20     262814  180.03  0       7       27468   2508    2.5821  GROUP: Hepatitis_antibody_test  G_43X..\nStatRow: 21     264801  138.169 0       7       6613    892     2.5780  GROUP: Urine_sex_hormone_titre  G_46J..\nStatRow: 22     292409  68976.8 0       7       4769351 359266  2.4232  GROUP: Encounter_administration G_9N...\nStatRow: 23     254586  5232.19 0       7       119552  22218   2.4079  GROUP: Last_menstrual_period_-1st_day   G_1513.\nStatRow: 24     106283  1515.82 0       7       57061   9796    2.3924  Infertility_problem     1AZ2.11\nStatRow: 25     170163  225.844 0       7       8639    1390    2.2657  Missed_miscarriage      L02..11\nStatRow: 26     270530  571.406 0       7       18561   3127    2.2606  GROUP: Pre-conception_advice    G_67IJ.\nStatRow: 27     254603  487.864 0       7       19694   2774    2.2502  GROUP: Missed_period    G_151I.\nStatRow: 28     311873  194.39  0       7       11567   1667    2.2349  GROUP: Bleeding_in_early_pregnancy      G_L10y.\nStatRow: 29     122760  280.372 0       7       15156   2331    2.2160  Pregnancy_advice        67A..00\nStatRow: 30     311416  1845.72 0       7       78378   13105   2.1946  GROUP: Infertility_-_female     G_K5B..\nStatRow: 31     254592  1212.24 0       7       43897   6499    2.1937  GROUP: Menstrual_period_late    G_1517.\nStatRow: 32     254233  98.3707 0       7       5035    111     2.1780  GROUP: H/O:_pneumonia   G_14B2.\nStatRow: 33     170257  99.7106 0       7       4515    673     2.1447  Inevitable_miscarriage_complete L045.11\nStatRow: 34     292563  311.94  0       7       15052   2266    2.1305  GROUP: Seen_by_health_visitor   G_9N23.\nStatRow: 35     311867  954.495 0       7       60058   8672    2.0825  GROUP: Haemorrhage_in_early_pregnancy   G_L10..\nStatRow: 36     40006   137.015 0       7       30239   192     2.0744  Heart_Disease   Heart_Disease\nStatRow: 37     306897  223.302 0       7       19487   196     2.0243  GROUP: Cerebrovascular_disease  G_G6...\nStatRow: 38     261609  137.577 0       7       24348   2370    2.0054  GROUP: HIV_antibody/antigen_(Duo)       G_43d5.\nStatRow: 39     267999  38651.2 0       7       2959607 274901  2.0043  GROUP: Family_planning  G_61...\nStatRow: 40     264551  8138.74 0       7       397242  52412   1.9975  GROUP: Urine_pregnancy_test     G_465..\n</code></pre> We can see the results are reasonable for pre pregnancy - we see some pre pregnancy tests\\vaccinations consults, fertility and IVF treatments... the resutls for the after pregnancy are also reasonables.. you may try it out yourself \u00a0 Example run for printing out specific value 106283 (which is \"Infertility_problem\"): <pre><code>SignalsDependencies --base_config /nas1/Work/Users/Alon/UnitTesting/examples/signalDependency/pregnancy.cfg --output_value 106283\n</code></pre> And the output (there are some gender problems with the pregancny readcode which needs to be eliminated. Some patients are seen as Males with pregnancy readcode - but small amount. for example there are 39 males in the age 20-25 which will be pregnant in THIN out of almost 3M males who are in THIN in those years): <pre><code>read [267598] records on both male and female stats.\nAfter Filter Age [15 - 50] have 130079 keys in males and 137519 keys in females\nusing mcnemar statistical test\nTest Signal:106283 - Infertility_problem\nStats for Males\n              [NO_SIG&amp;REG_FALSE, NO_SIG&amp;REG_TRUE, SIG&amp;REG_FALSE, SIG&amp;REG_TRUE]\nAge_Bin: 15: [1031209,          9,         80,          0] score=    0.00070 [ 0.001%,  0.000%] indep_ratio=0.001%\nAge_Bin: 20: [895861,         10,        875,          0] score=    0.00977 [ 0.001%,  0.000%] indep_ratio=0.001%\nAge_Bin: 25: [884222,         17,       2739,          0] score=    0.05266 [ 0.002%,  0.000%] indep_ratio=0.002%\nAge_Bin: 30: [889268,         28,       4275,          0] score=    0.13460 [ 0.003%,  0.000%] indep_ratio=0.003%\nAge_Bin: 35: [867361,         25,       3201,          0] score=    0.09226 [ 0.003%,  0.000%] indep_ratio=0.003%\nAge_Bin: 40: [823505,         16,       1631,          0] score=    0.03169 [ 0.002%,  0.000%] indep_ratio=0.002%\nAge_Bin: 45: [768280,         13,        597,          1] score=    0.97005 [ 0.002%,  0.167%] indep_ratio=0.002%\nAge_Bin: 50: [701062,          9,        230,          0] score=    0.00295 [ 0.001%,  0.000%] indep_ratio=0.001%\nStats for Females\n              [NO_SIG&amp;REG_FALSE, NO_SIG&amp;REG_TRUE, SIG&amp;REG_FALSE, SIG&amp;REG_TRUE]\nAge_Bin: 15: [1068203,      42935,        764,        162] score=   80.54961 [ 3.864%, 17.495%] indep_ratio=3.875%\nAge_Bin: 20: [1200618,     103881,       5911,       1052] score=  154.07719 [ 7.963%, 15.108%] indep_ratio=8.001%\nAge_Bin: 25: [1304870,     158141,      12256,       2489] score=  196.26870 [10.809%, 16.880%] indep_ratio=10.870%\nAge_Bin: 30: [1251426,     165653,      14471,       3471] score=  338.85030 [11.690%, 19.346%] indep_ratio=11.785%\nAge_Bin: 35: [1120124,      92800,       9884,       2144] score=  488.71722 [ 7.651%, 17.825%] indep_ratio=7.751%\nAge_Bin: 40: [1028137,      25450,       3439,        450] score=  233.07304 [ 2.416%, 11.571%] indep_ratio=2.449%\nAge_Bin: 45: [944238,       2424,        486,         26] score=   22.31869 [ 0.256%,  5.078%] indep_ratio=0.259%\nAge_Bin: 50: [857528,        179,         54,          2] score=    1.96521 [ 0.021%,  3.571%] indep_ratio=0.021%\n</code></pre> We may see that in Females young females 15-20 the\u00a0Infertility_problem readcode has percentage of 17.495% in the time before pregnancy compared to 3.864% in the females 15-20 that don't have Infertility_problem. We see that when you have Infertility_problem you are more probable to get pregnant in the future. this population wants to get pregnant in the first place and eventually sucessed\u00a0\u00a0</p>"},{"location":"Medial%20Tools/Simulator.html","title":"Simulator","text":""},{"location":"Medial%20Tools/Simulator.html#simulator-overview","title":"Simulator Overview","text":"<p>The simulator code is located in the Tools git repository, specifically under the MR_TOOLS repo at: <code>AlgoMarker_python_API/PopulationAnalyzer</code>.</p>"},{"location":"Medial%20Tools/Simulator.html#running-the-server","title":"Running the Server","text":"<p>To start the server, execute <code>./ui.py</code> from this directory, or use the full path to <code>ui.py</code>. The server default port is 3764.</p>"},{"location":"Medial%20Tools/Simulator.html#adding-a-new-algomarker","title":"Adding a New AlgoMarker","text":"<p>To add an additional AlgoMarker, copy an existing model file (e.g., <code>LungFlag.py</code>) into the \"algomarkers\" folder within this project. The filename you choose will be used in the application with a <code>.py</code> extension. The keyword <code>_SLASH_</code> in filenames will be displayed as <code>/</code> in the UI.</p> <p>In the Python config file for the AlgoMarker, define the following fields:</p> <ul> <li>am_regions: Dictionary mapping region names (strings) to <code>ReferenceInfo</code> objects.</li> <li>sample_per_pid: Numeric parameter for bootstrap assessment.</li> <li>default_region: (Optional) String specifying the default region.</li> <li>additional_info: String for descriptive text near the model selection.</li> <li>optional_signals: (Optional) List of <code>InputSignal</code> objects describing input options (e.g., limiting history, selecting recent signals).</li> <li>model_path: String specifying the model's path.</li> <li>orderdinal: (Optional) Integer for ordering this AlgoMarker among others.</li> </ul>"},{"location":"Medial%20Tools/Simulator.html#example-configuration","title":"Example Configuration","text":"Click to expand <pre><code>from models import *\n\nlung_cohorts = [\n    CohortInfo(cohort_name='Ever Smokers Age 50-80', bt_filter=lambda df: (df['age']&gt;=50) &amp; (df['age']&lt;=80)),\n    CohortInfo(cohort_name='Ever Smokers Age 45-80', bt_filter=lambda df: (df['age']&gt;=45) &amp; (df['age']&lt;=80)),\n    CohortInfo(cohort_name='Ever Smokers Age 40-90', bt_filter=lambda df: (df['age']&gt;=40) &amp; (df['age']&lt;=90)),\n    CohortInfo(cohort_name='Ever Smokers Age 55-74', bt_filter=lambda df: (df['age']&gt;=55) &amp; (df['age']&lt;=74)),\n]\n\nus_lung_cohorts = [\n    CohortInfo(cohort_name='Ever Smokers Age 50-80', bt_filter=lambda df: (df['age']&gt;=50) &amp; (df['age']&lt;=80)),\n    CohortInfo(cohort_name='Ever Smokers Age 55-74', bt_filter=lambda df: (df['age']&gt;=55) &amp; (df['age']&lt;=74)),\n    CohortInfo(cohort_name='Ever Smokers Age 45-90', bt_filter=lambda df: (df['age']&gt;=45) &amp; (df['age']&lt;=90)),\n    CohortInfo(\n        cohort_name='USPSTF Age 50-80 (20 pack years, less then 15 years quit)',\n        bt_filter=lambda df: (df['age']&gt;=50) &amp; (df['age']&lt;=80) &amp;\n                             (df['smoking.smok_pack_years']&gt;=20) &amp;\n                             (df['smoking.smok_days_since_quitting']&lt;=15*365)\n    ),\n]\n\nam_regions = {\n    'US-KP': ReferenceInfo(\n        matrix_path='/nas1/Work/Users/eitan/Lung/outputs/models2023/EX3/model_63/reference_matrices/reference_features_kp.final.matrix',\n        control_weight=20,\n        cohort_options=us_lung_cohorts,\n        default_cohort='USPSTF Age 50-80 (20 pack years, less then 15 years quit)',\n        repository_path='/nas1/Work/CancerData/Repositories/KP/kp.repository',\n        model_cv_path='/nas1/Work/Users/eitan/Lung/outputs/models2023/EX3/model_63/results',\n        model_cv_format='CV_MODEL_%d.medmdl'\n    ),\n    'UK-THIN': ReferenceInfo(\n        matrix_path='/nas1/Work/Users/eitan/Lung/outputs/models2023/EX3/model_63/reference_matrices/reference_features_thin.final.matrix',\n        cohort_options=lung_cohorts,\n        default_cohort='Ever Smokers Age 55-74',\n        repository_path='/nas1/Work/CancerData/Repositories/THIN/thin_2021.lung/thin.repository'\n    ),\n}\n\nsample_per_pid = 0\ndefault_region = 'UK-THIN'\nadditional_info = 'Time Window 90-365'\n\noptional_signals = [\n    InputSignalsExistence(\n        signal_name='Smoking',\n        list_raw_signals=['Smoking_Duration', 'Smoking_Intensity', 'Pack_Years', 'Smoking_Quit_Date'],\n        tooltip_str='If true will include Smoking_Duration, Smoking_Intensity, Pack_Years, Smoking_Quit_Date in the inputs and not only status'\n    ),\n    InputSignalsExistence(\n        signal_name='Labs',\n        list_raw_signals=[\n            \"Albumin\", \"ALKP\", \"ALT\", \"Cholesterol\", \"Triglycerides\", \"LDL\", \"HDL\", \"Creatinine\",\n            \"Glucose\", \"Urea\", \"Eosinophils%\", \"Hematocrit\", \"Hemoglobin\", \"MCHC-M\", \"MCH\",\n            \"Neutrophils#\", \"Neutrophils%\", \"Platelets\", \"RBC\", \"WBC\", \"RDW\", \"Protein_Total\",\n            \"Lymphocytes%\", \"Basophils%\", \"Monocytes%\", \"Lymphocytes#\", \"Basophils#\", \"Monocytes#\",\n            \"Eosinophils#\", \"MCV\"\n        ]\n    ),\n    InputSignalsExistence(\n        signal_name='BMI',\n        list_raw_signals=['BMI', 'Weight', 'Height']\n    ),\n    InputSignalsExistence(\n        signal_name='Spirometry',\n        list_raw_signals=['Fev1']\n    ),\n]\n\nmodel_path = '/earlysign/AlgoMarkers/LungFlag/lungflag.model'\norderdinal = 1\n</code></pre>"},{"location":"Medial%20Tools/TestModelExternal.html","title":"TestModelExternal","text":"<p>A tool to test difference between repositories when applying a model. It builds a propensity model to discriminate between repositories to reveal differences and do simplr compare of the feature matrices. The goal is to discover that the model is transferable and can well behave on the second repository. \u00a0 Uses example:</p> <ul> <li>Estimate model performance/transferability on different repository</li> <li>Test model on different years and the differences in different year. You can pass train_rep==test_rep and just change the samples to test for differences</li> </ul> <p>Part of Tools git repository. Can be found under AllTools solution.</p>"},{"location":"Medial%20Tools/TestModelExternal.html#mode-1-comparing-the-model-when-the-2-repositories-are-available-in-the-same-network","title":"Mode 1 - Comparing the model when the 2 repositories are available in the same network:","text":"<ul> <li>model_path - Must be given in any mode. The path to the binary MedModel that we want to test for.</li> <li>rep_test - repository for testing and comparing with. In the propensity model it will be labeled as 1.</li> <li>samples_test - path to MedSamples to be applied in the test repository</li> <li>output - direcotry to output files</li> <li>rep_trained - repository path of the trained model (or just the reference repository to test with)</li> <li>samples_train - path to similar way (same logic of creation as test_samples) created MedSample on the training repository</li> <li>predictor_type,\u00a0predictor_args - parameters for the propensity model to discriminate difference of repositories by the model view</li> <li> <p>calibration_init_str - calibration args. Will be used in the propensity model as Post Processor \u00a0 Less important (optional)</p> </li> <li> <p>smaller_model_feat_size\u00a0 - if given &gt; 0, will create additional smaller propensity model based on top X group of features. X=smaller_model_feat_size.\u00a0</p> </li> <li>additional_importance_to_rank - path fo shaply report created by \"Flow --shap_val_request\" to rank the difference combained with the importance of each feature to the model</li> <li>features_subset_file - file to filter features from the MedModel</li> <li>fix_train_res - if &gt;0, will also set the feature resulotion in the train to match the test.</li> <li>sub_sample_train - sub sampling on train samples. Integer number to limit the maximal number of samples to sub sample to. 0 -no subsampling.</li> <li>sub_sample_test - sub sampling on test samples.\u00a0Integer number to limit the maximal number of samples to sub sample to.0 -no subsampling.</li> <li>train_ratio - the train/test ratio - the test are used to report propensity model performance</li> <li>bt_params - bootstrap parameters for the propensity model</li> <li>binning_shap_params - for the shapley report analysis on the propensity model</li> <li>group_shap_params - grouping args for the shapley on the propensity model</li> <li>shap_auc_threshold - if the AUC is smaller than that, will skip shapley analysis to save time</li> <li>print_mat - if &gt; 0 will print propensity matrix. 0 - labels are for train samples, 1 - labels are for test samples,</li> </ul>"},{"location":"Medial%20Tools/TestModelExternal.html#mode-2-compare-when-the-repositories-are-in-different-networks","title":"Mode 2 - Compare when the repositories are in different networks","text":"<p>To use this mode, rep_trained and/or samples_train should be empty.</p> <ul> <li>model_path - Must be given in any mode. The path to the binary MedModel that we want to test for.</li> <li>rep_test - repository for testing and comparing with. In the propensity model it will be labeled as 1.</li> <li>samples_test - path to MedSamples to be applied in the test repository</li> <li>output - direcotry to output files</li> <li>strata_json_model - json to be used for creating strats and collecting stats</li> <li>strata_settings - the strata setting for collecting stats When comparing to different repository. Add this argument:</li> <li>strata_train_features_moments - will create the file for the current test_samples and compare it to the file path in this path Less important (optional):</li> <li>features_subset_file - file to filter features from the MedModel</li> <li>sub_sample_test - sub sampling on test samples.\u00a0Integer number to limit the maximal number of samples to sub sample to.0 -no subsampling.</li> </ul>"},{"location":"Medial%20Tools/action_outcome_effect.html","title":"action_outcome_effect","text":"<p>The tool can be found\u00a0at $MR_ROOT/Tools/action_outcome_effect The tool check treatment/action effect on outcome. the main input is MedSamples+json to create matrix or MedFeatures(the matrix itself) with list of confounders.</p>"},{"location":"Medial%20Tools/action_outcome_effect.html#disclaimers","title":"Disclaimers:","text":"<ol> <li>We need to list all covariates that effect action\\treatment. because it's human based decision it should be doable to list all. If we miss covariate or confounder we may have missleading results</li> <li>the matching process may be prone to weighting errors that effect matching or predictor with poor results. we may look at the second_weighted_auc and would like it to be as close as it can to 0.5 (0.5 is perfect match, no more information in the covariates left)</li> <li>We need strong ignorabilty (so except to 1st point for listing all covariates) we need all the population to have probability for action\\treatment that is not pure 0 or 1. we have defined cutoff probability to drop patients who have no dilemas. so the final population in which we show results is different from the requested original (probablity no patients who are very healthy). we may look at the covariated distribution in the new population after drop</li> <li>the action\\treatment may effect indirectly on outcome through other covariates. For example taking statins will lower your LDL value and that's what lower your risk for stroke\\MI. It's important to understand that if you have 2 patient with dilema for treatment the treatment effect may occour indirectly by the treatment and we also measure that. We are not testing for direct treatment effect only \u00a0</li> </ol>"},{"location":"Medial%20Tools/action_outcome_effect.html#what-the-tool-does","title":"What the tool does?","text":"<ol> <li>Selects a model thats when using it's prediction score on cross validation sqeeze all the information in the covariates:If we do inverse probabilty reweighting (similar method like matching to match populations) and try to learn validation model with cross validation we reach low AUC.It selects the model that the secondry validaiton model after the matching achieves the worst AUC.</li> <li>Trains a model for predicting the action\\treatment and calibrates the scores to probabilty\u00a0</li> <li>Matches or Reweight with the model score to cancel the confounders - it drops patients who are only treated\\only untreated because we can't measure treatment effect on them.It also show comperasion of the populations before and after the matching for each of the covariates</li> <li>writes the stats (number of cases,contols with the original outcome) for each of the given groups to compare.</li> <li>Does all the process in bootstrap manner to ahve mean, std, CI for each measured number on each group \u00a0</li> </ol>"},{"location":"Medial%20Tools/action_outcome_effect.html#app-help","title":"App Help","text":"<p>get help from the app</p> <p><pre><code>$&gt; $MR_ROOT/Tools/action_outcome_effect/Linux/Release/action_outcome_effect --h\n##     ## ######## ########  ####    ###    ##\n###   ### ##       ##     ##  ##    ## ##   ##\n#### #### ##       ##     ##  ##   ##   ##  ##\n## ### ## ######   ##     ##  ##  ##     ## ##\n##     ## ##       ##     ##  ##  ######### ##\n##     ## ##       ##     ##  ##  ##     ## ##\n##     ## ######## ########  #### ##     ## ########Program General Options:\n  -h [ --help ]         help &amp; exit\n  --base_config arg     config file with all arguments - in CMD we override those settings\n  --debug               set debuging verbose\nProgram options:\n  --rep arg                                                                     the repository if needed for age\\gender cohorts\n  --input arg                                                                   the location to read input\n  --input_type arg (=samples)                                                   the input type (samples,samples_bin,features,medmat_csv,features_csv)\n  --down_sample_max_cnt arg (=0)                                                the maximal size of input to downsample to. if 0 won't do\n  --change_action_prior arg (=-1)                                               If &gt; 0 will change prior of action in the sample to be the given number\n  --output arg                                                                  the location to write output\n  --json_model arg                                                              json model for creating features for the filtering of cohorts in bootstrap\n  --confounders_file arg                                                        the file with the list of confounders [TAB] binSettings init line\n  --patient_groups_file arg                                                     each sample and belongness - same order as input\n  --patient_action arg                                                          each sample and belongness - same order as input\n  --probabilty_bin_init arg                                                     the init line for probabilty binning (only in bin squezzing method)\n  --features_bin_init arg                                                       the init line for features. if not empty and probabilty_bin_init not empty will\n                                                                                so bin splitting(only in bin squezzing method)\n  --model_type arg (=xgb)                                                       the model type to learn\n  --models_selection_file arg                                                   the model selection file with all params\n  --method_price_ratio arg (=-1)                                                if -1 will do reweight. if 0 - no matching. otherwise matching with this price\n                                                                                ratio\n  --pairwise_matching_caliper arg (=-1)                                         If &gt; 0 will do pairwise matching using caliper for std on prctiles\n  --nFolds_train arg (=5)                                                       0 means use all. no train\\test. other number splits the matrix\n  --nFolds_validation arg (=5)                                                  0 means use all. no train\\test. other number splits the matrix\n  --min_probablity_cutoff arg (=0.00499999989)                                  the minimal probabilty cutoff in reweighting\n  --model_validation_type arg (=qrf)                                            the predictor type of the validation\n  --model_validation_init arg (=ntrees=100;maxq=500;spread=0.0001;min_node=50;ntry=4;get_only_this_categ=1;n_categ=2;type=categorical_entropy;learn_nthreads=1;predict_nthreads=1)\n                                                                                the predictor init line of the validation predictor\n  --model_prob_clibrator arg (=caliberation_type=binning;min_preds_in_bin=200;min_prob_res=0.005)\n                                                                                the init line for model probability calibrator\n  --bootstrap_subsample arg (=1)                                                bootstrap subsample param in each loop if nbotstrap&gt;1\n  --nbootstrap arg (=1)                                                         bootstrap loop count\n</code></pre> example Run with config file with all arguments in /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/base_config_example.cfg:</p> example run<pre><code>Linux/Release/action_outcome_effect --base_config /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/base_config_example.cfg\nWARNING: header line contains unused fields [EVENT_FIELDS,]\n[date]=2, [id]=1, [outcome]=3, [outcome_date]=4, [split]=5,\nread [635795] samples for [635795] patient IDs. Skipped [0] records\nSamples has 635795 records. for uniq_pids = [ 0=621655 1=14140 ] all = [ 0=621655 1=14140 ]\nPrinting by prediction time...\nYear    Count_0 Count_1 ratio\n2007    131494  2398    0.017910\n2008    137281  2661    0.019015\n2009    137154  2791    0.019944\n2010    121066  3011    0.024267\n2011    94660   3279    0.033480\nAdding new features using /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json\ninit model from json file [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json], stripping comments and displaying first 5 lines:\n{\n       \"processes\":{\n        \"process\":{\n            \"process_set\":\"0\",\nmodel_json_version [1]\nUSING DEPRECATED MODEL JSON VERSION 1, PLEASE UPGRADE TO model_json_version: 2\ninit model from json file [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json], stripping comments and displaying first 5 lines:\n{\n       \"processes\":{\n        \"process\":{\n            \"process_set\":\"0\",\nAdding new rep_processor set [0]\nAdding new feature_processor set [0]\nAdding new feature_processor set [1]\nfp_type [imputer] acting on [7] features\nAdding new feature_processor set [2]\nNOTE: no [predictor] node found in file\nMedRepository: read config file /home/Repositories/THIN/thin_jun2017/thin.repository\nMedRepository: reading signals: BP,BYEAR,Cholesterol,DM_Registry,Drug,GENDER,HDL,LDL,Smoking_quantity,Triglycerides,\nRead 10 signals, 635795 pids :: data  1.552GB :: idx  0.051GB :: tot  1.603GB\nRead data time 5.216681 seconds\nadding virtual signals from rep type 0\nMedModel::get_required_signal_names 10 signalNames 0 virtual_signals\nMedModel::get_required_signal_names 10 signalNames 0 virtual_signals after erasing\nMedModel::learn() : learn rep processors time 777.803 ms\nMedModel::learn() : learn feature generators 4.993 ms\nMedModel::learn() : generating learn matrix time 1310.35 ms\nMedModel::learn() : feature processing learn and apply time 156.1 ms\nprinting stats for outcome:\nSamples has 635795 records. for uniq_pids = [ 0=621655 1=14140 ] all = [ 0=621655 1=14140 ]\nDone reading 635795 lines from [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_groups.list]\nHas 120 groups\nDone reading 635795 lines from [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_action.list]\nno_outcome_no_action: 534437, no_outcome_yes_action: 87218, yes_outcome_no_action:11515, yes_outcome_yes_action:2625\noutcome_prior=2.22%(14140 / 635795). action_prior=14.13%(89843 / 635795).\nno_action_prob_outcome=2.109%, yes_action_prob_outcome=2.922%\nDone reading 11 Confounders in [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/confounders.list]\nI have 1 models in model selection.\nStarting bootstrap loop 1\nprinting stats for learning action:\nSamples has 635788 records. for uniq_pids = [ 0=344975 1=56611 ] all = [ 0=546381 1=89407 ]\nscores hist: HISTOGRAM[0.0%:0.000, 10.0%:0.008, 20.0%:0.020, 30.0%:0.036, 40.0%:0.057, 50.0%:0.083, 60.0%:0.116, 70.0%:0.162, 80.0%:0.235, 90.0%:0.366, 100.0%:0.997]\nProb hist: HISTOGRAM[0.0%:0.000, 10.0%:0.005, 20.0%:0.020, 30.0%:0.037, 40.0%:0.060, 50.0%:0.085, 60.0%:0.119, 70.0%:0.169, 80.0%:0.244, 90.0%:0.361, 100.0%:0.886]\nTRAIN_CV_AUC = 0.817, TRAIN_AUC_BINNED=0.824\nWarning: matching group has very small counts - skipping group=0.019608 [199, 4]\nWarning: matching group has very small counts - skipping group=0.009926 [398, 4]\nWarning: matching group has very small counts - skipping group=0.009877 [401, 4]\nWarning: matching group has very small counts - skipping group=0.009756 [399, 4]\nWarning: matching group has very small counts - skipping group=0.006633 [597, 4]\nWarning: matching group has very small counts - skipping group=0.004963 [401, 2]\nWarning: matching group has very small counts - skipping group=0.004926 [399, 2]\nWarning: matching group has very small counts - skipping group=0.003731 [802, 3]\nWarning: matching group has very small counts - skipping group=0.003317 [1206, 3]\nWarning: matching group has very small counts - skipping group=0.003306 [603, 2]\nWarning: matching group has very small counts - skipping group=0.003300 [603, 2]\nWarning: matching group has very small counts - skipping group=0.002982 [1003, 3]\nWarning: matching group has very small counts - skipping group=0.002481 [1608, 4]\nWarning: matching group has very small counts - skipping group=0.002475 [806, 2]\nWarning: matching group has very small counts - skipping group=0.001990 [1002, 2]\nWarning: matching group has very small counts - skipping group=0.001658 [604, 1]\nWarning: matching group has very small counts - skipping group=0.001657 [1207, 2]\nWarning: matching group has very small counts - skipping group=0.001656 [602, 1]\nWarning: matching group has very small counts - skipping group=0.001653 [1203, 2]\nWarning: matching group has very small counts - skipping group=0.001650 [1812, 3]\nWarning: matching group has very small counts - skipping group=0.001325 [3015, 4]\nWarning: matching group has very small counts - skipping group=0.001244 [2410, 3]\nWarning: matching group has very small counts - skipping group=0.001243 [2412, 3]\nWarning: matching group has very small counts - skipping group=0.001242 [2409, 3]\nWarning: matching group has very small counts - skipping group=0.001241 [805, 1]\nWarning: matching group has very small counts - skipping group=0.000902 [2217, 2]\nAfter Matching has 88530.0 controls and 82542.0 cases with 48.250% percentage\nSECOND_WEIGHTED_AUC = 0.499\nComparing populations - original population has 635795 sampels, matched has 171072 samples. Features distributaions:\nBAD feature Age :: original mean=53.255[30.000 - 78.000],std=14.132. matched mean=59.597[40.000 - 78.000],std=11.555. mean_diff_ratio=11.908%\nBAD feature FTR_000003.DM_Registry.ever_DM_Registry_Diabetic.win_0_100000 :: original mean=0.050[0.000 - 1.000],std=0.219. matched mean=0.110[0.000 - 1.000],std=0.313. mean_diff_ratio=119.001%\nBAD feature FTR_000004.Current_Smoker :: original mean=0.244[0.000 - 1.000],std=0.429. matched mean=0.282[0.000 - 1.000],std=0.449. mean_diff_ratio=15.259%\nGOOD feature FTR_000005.BP.last.win_0_1095 :: original mean=80.974[64.000 - 100.000],std=10.374. matched mean=82.565[67.000 - 100.000],std=10.551. mean_diff_ratio=1.964%\nGOOD feature FTR_000006.BP.last.win_0_1096.t0v1 :: original mean=134.039[108.000 - 166.000],std=17.669. matched mean=139.302[112.000 - 172.000],std=17.973. mean_diff_ratio=3.926%\nBAD feature FTR_000007.LDL.last.win_0_1095 :: original mean=128.172[73.359 - 189.189],std=36.009. matched mean=145.561[84.942 - 204.633],std=37.765. mean_diff_ratio=13.567%\nBAD feature FTR_000008.Cholesterol.last.win_0_1095 :: original mean=209.340[146.718 - 277.992],std=39.961. matched mean=231.132[166.023 - 297.297],std=41.690. mean_diff_ratio=10.410%\nGOOD feature FTR_000009.HDL.last.win_0_1095 :: original mean=55.937[34.749 - 84.942],std=16.492. matched mean=53.698[30.888 - 84.942],std=16.333. mean_diff_ratio=4.003%\nBAD feature FTR_000010.Triglycerides.last.win_0_1095 :: original mean=130.621[53.100 - 274.350],std=96.524. matched mean=164.286[61.950 - 336.300],std=116.033. mean_diff_ratio=25.773%\nBAD feature FTR_000013.Drug.category_set_BloodPressureDrugs.win_0_1095 :: original mean=0.108[0.000 - 1.000],std=0.310. matched mean=0.166[0.000 - 1.000],std=0.372. mean_diff_ratio=54.031%\nGOOD feature Gender :: original mean=1.550[1.000 - 2.000],std=0.497. matched mean=1.484[1.000 - 2.000],std=0.500. mean_diff_ratio=4.258%\npredictor AUC with CV to diffrentiate between populations is 0.751\nProcessed 1 out of 5(20.00%) time elapsed: 3.7 Minutes, estimate time to finish 14.7 Minutes\nStarting bootstrap loop 2\n...\n\u00a0\n</code></pre> <p>you may see that after the matching the secondry model doesn't achieves good results \"SECOND_WEIGHTED_AUC = 0.499\". but you can see that the population is very different from the original requested one. a predictor can seperated them with AUC=0.751. you can see that the \"DM_Registry\" has more than doubled it's value from 0.05 to 0.11!! the GOOD/BAD keywords only shows you where the populations are differs. you need to remember that those diffrences are unavoidable - to induce causality you have to look on population with strong ignorabilty! If you have for example very healty patients with LDL = 70, low BMI and without statins (and you never see treated patients with those covariates in the data), you just can't induce treatment effect on them so you have to drop them. the same thing happend on \"very sick people\". inducing treatment effect only works on grey zones when we have dilemas \u00a0 It also output the results to /tmp/LDL.txt (if nbootstrap==1 all the numbers are without STD, and CI):</p> <p>head of output<pre><code>head  /tmp/LDL.txt\nmean_incidence_precantage=2.13% chi-square=4649.937     DOF=119 prob=0.000\nrisk_factor     controls_count  cases_count     case percentage lift    chi-square      chi-prob\nLDL_Group=0.00, LDL_Delta=-1, Treated_Group=0   54.0    5.0     8.47%   3.97    11.37   0.00\nLDL_Group=0.00, LDL_Delta=-1, Treated_Group=1   26.6    2.4     8.41%   3.94    3.37    0.07\nLDL_Group=0.00, LDL_Delta=-1, Treated_Group=2   47.9    0.0     0.00%   0.00    1.02    0.31\nLDL_Group=0.00, LDL_Delta=-2, Treated_Group=0   6.0     3.0     33.33%  15.63   41.97   0.00\n</code></pre> We may see each risk_factor and it's stats - number of controls,cases</p>"},{"location":"Medial%20Tools/action_outcome_effect.html#input-arguments","title":"Input Arguments:","text":"<p><code>cat base_config_example.cfg</code></p> <pre><code>rep = /home/Repositories/THIN/thin_jun2017/thin.repository\n#input = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_2/final_matrix.bin\n#input_type = features\ninput = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/random_ldl_test_no_statins.samples\ninput_type = samples\njson_model = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json\noutput = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/output/results.txt\npatient_action =  /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_action.list\nconfounders_file = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/confounders.list\npatient_groups_file = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_groups.list\n#features_bin_init = split_method=partition_mover;binCnt=100;min_bin_count=100;min_res_value=0\n#probabilty_bin_init = split_method=iterative_merge;min_bin_count=100;binCnt=500\nmodel_type = xgb\nmodels_selection_file = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/xgb_model_selection.cfg\nnFolds_train = 5\nnFolds_validation = 4\nmin_probablity_cutoff = 0.005\nmodel_validation_type = qrf\nmodel_validation_init = ntrees=100;maxq=500;spread=0.0001;min_node=50;ntry=4;get_only_this_categ=1;n_categ=2;type=categorical_entropy;learn_nthreads=20;predict_nthreads=20\nmodel_prob_clibrator = caliberation_type=binning;min_preds_in_bin=200;min_prob_res=0.005\n#method_price_ratio = -1\nmethod_price_ratio = 6.2\nnbootstrap = 5\n</code></pre> <p>the input argument is the main data file and it can be MedSamples with json to create matrix or the MedFeatures itself.</p> <ul> <li>patient_action - a file with same number of lines as the samples in the input, each line is correspond to the same sample in the input. it may be 0/1 for [no treatment, treatment] mark for each sample</li> <li>confounders_file - a file list with all the confounders search name (searching contains in the column names) in the matrix of input. each line consist of the the confounder search name</li> <li>example: $&gt; head confounder.list</li> </ul> <pre><code>Age\nGender\nLDL.last\n</code></pre> <p>here we have 3 confounders: Age,Gender and LDL.last</p> <ul> <li>patient_groups_file -\u00a0a file with same number of lines as the samples in the input, each line is correspond to the same sample in the input. it will be the risk group name for the sample for later split.\u00a0 you may write for example in line: \"Age=20-40;Gender=Male\" to mark the sample as belong to that group in the output results</li> <li>models_selection_file - a file with initialization of the parameters of the model. you may provide more than one option for parameter with \",\" and than the tool will select the best option over all available options.\u00a0 you may also provide only one option and than the tool will just use this. for example: $&gt; head xgb_model_selection.cfg</li> </ul> <p><pre><code>max_depth=5,6,7\nnum_round=200\neta=0.1,0.3\n</code></pre> here we fixed num_round to be 200 and checks all the options of max_depth = 5 or 6 or 7 with eta = 0.1 or 0.3 - we have 6 model options \u00a0</p>"},{"location":"Medial%20Tools/adjust_model.html","title":"adjust_model","text":"<p>Template for adding pre processors json:</p>"},{"location":"Medial%20Tools/adjust_model.html#adding-rep_processors-json-template","title":"adding rep_processors json template","text":"<p><pre><code>{ \"pre_processors\" : [\n  {\n    \"action_type\":\"rp_set\",\n    \"members\":[\n      \u00a0// Add your blocks in here\n    ]\n  }\n] }\n</code></pre> Caution: When adding \"pre_processros\" with \"rp_set\" - all the rep processors are added in the END of the existing model rep proceossors.\u00a0 When using single \"rep_processor\" in action type, the single rep_processor is added in the begining. \u00a0 Template for adding post processors json:</p>"},{"location":"Medial%20Tools/adjust_model.html#json-for-adding-post_processors","title":"Json for adding post_processors","text":"<p><pre><code>{ \"post_processors\": [\n         // Add your blocks in here\n] }\n</code></pre> \u00a0 change_model_info examples can be shown in change_model</p>"},{"location":"Medial%20Tools/model_signals_importance.html","title":"model_signals_importance","text":""},{"location":"Medial%20Tools/model_signals_importance.html#goal","title":"Goal","text":"<p>Compare model performance with\\without passing group os signals to input the model.</p> <ul> <li>--skip_list\u00a0 comma deliimited list that contorls which signals to skip - mainly passed Age,Gender. We always have those values and don't/can't check those</li> <li>--no_filtering_of_existence - If passed, will not filter each cohort to a one that we have the signal exists in certain time window controlled by\u00a0--time_windows</li> <li>--features_groups - 2 columns tab delimeted or ButWhy grouping format like \"BY_SIGNAL\". It mapps feature to a group name/signal</li> <li>--group2signal - can map the \"groups\" of\u00a0features_groups into list of signals to exclude when handling this group. Tab delimeted, 2 columns: group name as in\u00a0features_groups and list of signal, comma delimited</li> </ul> <ol> <li>features_groups - Taking but why group names and maping feature to names, just to count how many features are effected, but we are going to use only the \"group names\" of this\u00a0</li> <li>if name matches in group2signal =&gt; take the mapped value, list of signals to use, otherwise it assume the name of the group is already a valid signal name So groups are determined by\u00a0features_groups values.\u00a0\u00a0group2signal, help to remap those groups to lists of signals.</li> </ol>"},{"location":"Medial%20Tools/GANs%20for%20imputing%20matrices/index.html","title":"GANs for imputing matrices","text":""},{"location":"Medial%20Tools/GANs%20for%20imputing%20matrices/index.html#general-idea","title":"General Idea","text":"<p>Given a feature matrix with missing values, we can use GAN bases ideas to impute the missing values. Formally we have a matrix\u00a0(Eqaution) , generated by our model, with missing values. Assume we also have a binary masks matrix\u00a0(Eqaution) in which\u00a0(Eqaution) \u00a0iff (Eqaution) is not missing. The patterns of the missing value matrices are far from being random, as some features always exist, some always exist when others exist, and some have very correlated mask bits. Our goal is to train a GAN with a Generator G, and a discriminator D. The generator G is a function (which we will train as a deep learning network) that gets an input vector (Eqaution) , and a mask vector (Eqaution) stating which channels are known, and generates (Eqaution)\u00a0, a complete imputed vector, in which the masked variables are exactly as in the input, and the rest are imputed. So:</p> <ul> <li>(Eqaution)</li> <li>(Eqaution)\u00a0is an imputed value if\u00a0(Eqaution) The discriminator D\u00a0is another (deep network) function that gets a vector (Eqaution) and a mask (Eqaution) and tries to estimate if the vector confined to the lit up channels is a real sample or one that went through imputing. The reason we do this with a mask vector is that we may not have enough examples in which\u00a0(Eqaution)\u00a0is given to us on all channels... we may only have a large\u00a0(Eqaution)\u00a0matrix that has many missing values in it. Hence we don't have full real examples to feed into the discriminator learning, and we need to confine D to answer the question of real vs. fake confined to only on some of the channels. This creates a problem on its own - the problem of making sure that the population of masks given for real samples is the same as the population of masks given for fake samples. While training, we do competition rounds in which G is trained to generate values for channels that are hidden from it, and D is trying to tell real from fake. The loss functions are arranged in away that will push G to generate examples that D finds hard to discriminate. \u00a0 \u00a0</li> </ul>"},{"location":"Medial%20Tools/GANs%20for%20imputing%20matrices/TrainingMaskedGAN.html","title":"TrainingMaskedGAN","text":"<p>Please clone this:\u00a0https://github.com/Medial-EarlySign/MR_Projects/tree/main/ButWhy \u00a0 There is a python script called \"channels_GAN\\train_channels_GAN.py\" Example running command from directory with \"train_channels_GAN.py\": \u00a0 Train scripts, generate 3 files <pre><code>train_channels_GAN.py --rep ${TRAIN_REP} --get_mat --gpu \"0\" --gen_nhiddens 400,400,400 --disc_nhiddens 100,100,100 \\\n--gen_noise 0.01 --disc_noise 0.2 --gen_keep_prob 0.5 --disc_keep_prob 0.5 --gen_learning_rate 0.001 \\\n--disc_learning_rate 0.001 --max_auc_batch 1000 --disc_global_p 0.5 --gen_global_p 0.5 --cross_entropy_weight 0.5 \\\n--batch_size 1000 --csvs_freq 5000 --batch_num 100000 --n_dsteps 5 --n_gsteps 3 --round 1 --nout 50000 \\\n--work_output_dir ${output_directory} --samples ${SAMPLES_PATH} --model ${WORK_PATH}/base_model.bin --sub_sample 0\n\u00a0\n#disc_nhiddens  - discriminator hidden layers\n#gen_nhiddens  - generator hidden layers\n#model - to generate matrix with missing values to learn the GAN. this is trained model path, Or when no \"get_mat\", you can specify matrix directly in \"data\", \"validation_data\" argument.\n</code></pre> \u00a0 Example output files: W:\\Users\\Alon\\But_Why\\outputs\\GAN\\crc_gan_model.txt there are 2 more files with additional suffix, when used, please specify the shortest file path without suffix \u00a0</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html","title":"Guide for Common Actions","text":""},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#common-actions","title":"Common Actions","text":""},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#1-match-medsamples-by-year-or-criteria","title":"1. Match MedSamples by Year or Criteria","text":"<p>See: Using Flow To Prepare Samples and Get Incidences</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#2-train-a-model-from-json","title":"2. Train a Model from JSON","text":"<p>refer to Flow</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#3-calculate-model-score-on-samples","title":"3. Calculate Model Score on Samples","text":"<p>refer to Flow</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#4-create-feature-matrix-for-samples","title":"4. Create Feature Matrix for Samples","text":"<p>refer to Flow</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#5-adjust-model","title":"5. Adjust Model","text":"<p>Add or retrain rep_processor or post_processor components. Useful for calibration or explainability. See: adjust_model Can be compiled in AllTools.</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#6-change-model","title":"6. Change Model","text":"<p>Remove or modify model components (e.g., enable debug logs, test normalization). See: change_model Can be compiled in AllTools.</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#7-simplify-model-remove-signals","title":"7. Simplify Model / Remove Signals","text":"<p>Iteratively add or remove signals to simplify the model. See: Iterative Feature Selector Can be compiled in AllTools.</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#8-analyze-feature-importance-model-behavior","title":"8. Analyze Feature Importance &amp; Model Behavior","text":"<p>\"But Why\" graphs for feature validation. See: Feature Importance</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#9-bootstrap-performance-analysis","title":"9. Bootstrap Performance Analysis","text":"<p>See: bootstrap_app</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#10-compareestimate-model-performance-on-different-repository","title":"10. Compare/Estimate Model Performance on Different Repository","text":"<p>Test model on external repository. See: TestModelExternal Can be compiled in AllTools.</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#11-create-and-load-repository-from-files","title":"11. Create and Load Repository from Files","text":"<p>See: Load new repository</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#12-create-random-splits-for-traintestall-patients","title":"12. Create Random Splits for Train/Test/All Patients","text":"<p>Please refer Using Splits</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#13-filter-traintest-by-train-signal","title":"13. Filter Train/Test by TRAIN Signal","text":"<pre><code>FilterSamples --rep $REPOSITORY_PATH --samples $INPUT_SAMPLES_PATH --output $OUTPUT_SAMPLES_PATH --filter_train $FILTER_TRAIN_VAL\n</code></pre> <ul> <li>TRAIN == 1: Training set (70%)</li> <li>TRAIN == 2: Test set (20%)</li> <li>TRAIN == 3: Validation set (10%)</li> </ul>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#14-print-model-info","title":"14. Print Model Info","text":"<p>Please refer to Flow Model Info</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#15-filter-samples-by-bt-cohort","title":"15. Filter Samples by BT Cohort","text":"<p>Include <code>json_mat</code> even if not required by definition. <pre><code>FilterSamples --filter_train 0 --rep ${REP_PATH} --filter_by_bt_cohort \"Time-Window:90,730;Age:50,80;Suspected:0,0;Ex_or_Current:1,1\" --samples ${INPUT} --output ${OUTPUT} --json_mat ${JSON}\n</code></pre></p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#16-check-model-compatibility-with-repository-suggest-adjustments","title":"16. Check Model Compatibility with Repository / Suggest Adjustments","text":"<p>When applying a model to a different repository, some modifications may be necessary. For instance, MedModel strictly checks for the presence of required signals. If a signal is missing but not critical, you can explicitly mark it as acceptable by adding a rep processor for an \"empty\" signal. For further guidance, see Flow fit_model_to_rep.</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/index.html#fixing-missing-dictionary-definitions","title":"Fixing Missing Dictionary Definitions","text":"<p>When training on one repository and testing on another, missing diagnoses may occur. To find missing codes: <pre><code>Flow --print_model_signals --f_model $MODEL_NAME --rep $REP --transform_rep 1 --output_dict_path $PATH\n</code></pre> To resolve:</p> <ul> <li>Add missing codes to the target dictionary, matching SECTION codes as needed.</li> <li>For example:</li> <li>Add <code>ICD9_CODE:786.09</code> if missing.</li> <li>Add <code>ICD9_CODE:420-429.99</code> for range codes.</li> <li>For named codes (e.g., <code>MALIGNANT_NEOPLASM_OF_LIP_ORAL_CAVITY_AND_PHARYNX</code>), find the equivalent numeric code (e.g., <code>140-149</code>) and add accordingly.</li> </ul> <p>Can be compiled in AllTools</p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/AlgoMarker%20common%20actions.html","title":"AlgoMarker common actions","text":"<p>How to list all Rules outliers <pre><code>Flow --print_model_info --f_model $MODEL_PATH 2&gt;&amp;1\u00a0 | egrep \"RepRuleBasedOutlierCleaner|BasicOutlierCleaner\"\n</code></pre> How to list all Outliers bounds: <pre><code>Flow --print_model_info --f_model $MODEL_PATH 2&gt;&amp;1 | grep BasicOutlierCleaner | grep -v \"FeatureBasicOutlierCleaner\"\n</code></pre> \u00a0 Print model used signals + categories (some of the signals can turned to be virtual if not exists - for example BMI): <pre><code>Flow --print_model_sig --f_model $MODEL_PATH \n</code></pre></p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/Calibrate%20model%2C%20and%20calibration%20test.html","title":"Calibrate model, and calibration test","text":"<p>****: Calibrate a model on one repository, and check calibration on another repository.</p> <p><pre><code>FilterSamples --filter_train 0 --rep $REP --filter_by_cohort \"Time-Window:0,365\" --samples $SAMPLES --output $OUTPUT --json_mat $JSON\n</code></pre> Comments:</p> <ul> <li>Time Window should be 0 to horizon. Thus, 0,365 means calibrated risk for outcome within 1 year. And 0,730 would mean calibrated risk for outcome within 2 years.</li> <li>json_mat is required even though it has no effect (to be removed)</li> <li>filter_train default is 1 =&gt; take just train. As we set '0' - all samples are taken.</li> </ul> <p>Standard predict for the samples generated previously. <pre><code>Flow --get_model_preds --rep $REP --f_samples $INPUT --f_model $MODEL --f_preds $PREDS_FOR_CALIBRATION \n</code></pre> </p> <p><pre><code>adjust_model --postProcessors $JSON --rep $REP --samples $PREDS_FOR_CALIBRATION --inModel $MODEL --skip_model_apply 1 --out $OUTPUT \n</code></pre> Comments:</p> <ul> <li>The OUTPUT is a model with calibration</li> <li>The terminal output is 'staircase graph' - bins and calibrated risk (printed on screen and reachable through Flow --print_model_info)\u00a0 Output format example: <pre><code>Succesfully added 1 post_processors\nCreated 44 bins for mapping prediction scores to probabilities\nRange: [0.7224, 2147483648.0000] =&gt; 1.0000 | 10.77%(107736.000000 / 1000000.000000)\nRange: [0.6419, 0.7224] =&gt; 0.2500 | 9.98%(99850.000000 / 1000000.000000)\nRange: [0.4393, 0.6419] =&gt; 0.1123 | 11.70%(116964.000000 / 1000000.000000)\nRange: [0.4373, 0.4393] =&gt; 0.1000 | 4.30%(43022.000000 / 1000000.000000)\nRange: [0.4284, 0.4373] =&gt; 0.0889 | 7.88%(78793.000000 / 1000000.000000)\n...\n</code></pre> \u00a0 The required JSON is: <pre><code>{\n    \"post_processors\": [\n        {\n            \"action_type\":\"post_processor\",\n            \"pp_type\":\"calibrator\",\n            \"calibration_type\":\"isotonic_regression\",\n            \"use_p\":\"0.25\"\n        }\n    ]\n} \n</code></pre> </li> </ul> <p>Run the program: <pre><code>TestCalibration --rep ${REP} --tests_file ${TEST} --output ${OUT_PATH}\n</code></pre> The test file has 3 TAB tokens in each line: samples_path, optional model_path to apply on samples and optional split to filter from samples. \u00a0</p> <p>File with expected risk and actual in validation, for each bin of the calibrated model, e.g., plus some KPIs: <pre><code>probabilty_of_model     Validation_probabilty    cases   total_observations     Diff\n0.0000%                 0.0000%                  0       11                     0.0000%\n0.0046%                 0.0062%                  92      1493236                0.0016%\n0.0103%                 0.0096%                  513     5329513                0.0007%\n0.0158%                 0.0184%                  398     2166035                0.0026%\n\u00a0\n9.9270%                 7.6923%                  2       26                     2.2347%\n10.8825%                2.0000%                  1       50                     8.8825%\ntot_diff(L2)=0.001983 prior=0.002349 prior_loss(L2)=0.003845 R2=0.484125 Num_Bins=108 Calibration_Index=0.000890\n</code></pre> \u00a0 And a graph  </p>"},{"location":"Medial%20Tools/Guide%20for%20common%20actions/DevOps.html","title":"DevOps","text":"<ol> <li>Compile all tools</li> <li>Compile python: Build the python extention</li> <li>Pack all tools to external environment (Please Clone MR_Scripts git repository and see script under Bash-scripts): <pre><code>full_pack_all.sh\n</code></pre> In the remote machine, unzip the mes_full.tar with \"tar -xvf mes_full.tar\" and run \"medial_earlysign_setup.sh\" to setup environment. The environment is similar to what we have in the docker:\u00a0Ubuntu docker (medial_dev). We can also install a docker if possible. In my opinion docker is 2nd priority. This is based on those actions. If you don't want to to do all of them:</li> </ol> Command Description output path Where to extract <pre>full_build.x86-64.sh</pre> <pre>Builds all our tools for X86_64 computers: Flow, bootstrap_app, etc.</pre> /nas1/UsersData/git/MR/Tools/AllTools/Linux/Release/bin_apps.x86-64.tar.bz2 <p>#copy to file to /earlysign/bins</p><p>cd /earlysign/bins</p><p>tar -xvf bin_apps.x86-64.tar.bz2</p> <pre>pack_etl.sh</pre> Packs all ETL Infra code /server/Linux/${USER%-*}/ETL.tar.bz2. USER might be: git if run from \"<pre>full_pack_all.sh\" or your username without \"-internal\"</pre> #copy to file to /earlysign/scriptscd /earlysign/scriptstar -xvf ETL.tar.bz2 pack_libs.sh Packs all \"External libs\" needed for \"full_build.x86-64.sh\". For example, xgboost, lightGBM &amp; boost.There are not suppose to be \"updates\" in here. /server/Linux/${USER%-*}/libs.tar.bz2. USER might be: git if run from \"<pre>full_pack_all.sh\" or your username without \"-internal\"</pre> <p>#copy to file to /earlysign/libs</p><p>cd /earlysign/libs</p><p>tar -xvf libs.tar.bz2</p> pack_scripts.sh Packs all our helper scripts like: AutoValidation kits, bootstrap_format.py, plot.pt, paste.pl, etc. /server/Linux/${USER%-*}/scripts.tar.bz2\u00a0 USER might be: git if run from \"<pre>full_pack_all.sh\" or your username without \"-internal\"</pre> <p>#copy to file to /earlysign/scripts</p><p>cd /earlysign/scripts</p><p>tar -xvf scripts.tar.bz2</p> build_py_wrapper.sh Builds the python wrapper for our infrastructure. /server/Linux/${USER%-*}/PY.tar.bz2 USER might be: git if run from \"<pre>full_pack_all.sh\" or your username without \"-internal\"</pre> <p>#copy to file to /earlysign/libs</p><p>cd /earlysign/libs</p><p>tar -xvf PY.tar.bz2</p> Additional helper scripts: A setup script that setup/update the environment: /nas1/UsersData/git/MR/Projects/Scripts/Bash-Scripts/usefull:medial_earlysign_setup.sh, medial_earlysign_resetup.sh /nas1/UsersData/${USER%-*}/MR/Projects/Scripts/Bash-Scripts/usefull USER might be: git if run from \"full_pack_all.sh\" or your username without \"-internal\" <ol> <li>Jupyter is down in the internal?\u00a0 <pre><code>sudo systemctl restart jupyter\n</code></pre> </li> </ol>"},{"location":"Medial%20Tools/Model%20Checklist/index.html","title":"Model Checklist","text":"<ul> <li>Samples distribution over time - #controls, #cases for each year, each month</li> <li>Bootstrap on validation set (AND prefered future time set). On future time set also asset performance on same patients<ul> <li>Performance in different years, months - AUC and other measurements</li> <li>Performance in different time windows</li> <li>Performance on different age groups, sex, important comorbidities (diabetes, COPD, CVD)</li> <li>Minimal membership period, with\\without important lab tests if relevant</li> </ul> </li> <li>Calibration assesment - on same samples as bootstrap</li> <li>ButWhy analysis<ul> <li>Global feature importance with\\without grouping of signals</li> <li>Single features contribution analysis - for the important features, mean score, outcome, and shapley value contribution for each feature value bin</li> </ul> </li> <li>Coverage/Lift on risk groups in different PR cutoffs. For examle how many COPD patients with hospital admission we have (prevalence), and how much out of them are captured in top x,y,z PR cutoffs (coverage)</li> <li>Print matrix - mean feature value and CI/STD for each feature - look for outliers and unreasonable numbers - can be done on large test/train matrtix</li> <li>Matrix differences over the years - take several years and compare<ul> <li>Check score distribution on several years</li> <li>Build propensity model to differentite between different years and see which features changes\u00a0</li> </ul> </li> <li>Fairness /Bias analysis<ul> <li>wihtout matching:different sex, age groups, insurance, race, socio-demographic info?</li> <li>with matching of important features \"clinical\", or accepted for explanations</li> </ul> </li> <li>External Validation on different datasets</li> <li>Compare to simple baseline model - compare not only performance, but also the flagged patients to understand who the model flags - ButWhy on the population differences\u00a0</li> <li> <p>Sensitivity analysis to noise:</p> <ul> <li>Noise in lab values</li> <li>Shifting of dates</li> <li>Missing values - removing lab values \u00a0 Check the model has cleaners on all signals \u00a0 Applying on new dataset without labels for validation:</li> </ul> </li> <li> <p>Test matrix difference from training repository matrix - compare feature moments with TestModelExternal and\\or try to train propensity model.</p> </li> <li>Also compare score distribution + score distribution after matching important factors.</li> <li>Test ButWhy importance on the test set - compare with training repo</li> <li>Stats of outliers from cleaners \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/index.html#test-kit-for-validation-of-models-under-some-stage-development-external-with-labels-silent_run","title":"Test Kit for validation of models Under some stage: Development, External with labels, Silent_Run:","text":"<p>In this Tools git repository:\u00a0https://github.com/Medial-EarlySign/MR_Tools\u00a0under for example: $MR_ROOT/Tools/AutoValidation. In Windows: U:\\Alon\\MR\\Tools\\AutoValidation \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html","title":"AutoTest","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#motivation","title":"Motivation","text":"<p>The goal is to execute generic tests collected from our group as common knowledge on certain use cases  For example, we have knowledge on how we should test a new model, for example - run feature importance, make sure we have all cleaners for all features etc. Second thing we achieve is standardize way to validate/document a new model. Suppose to be faster to move from one project to another when needed.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#location","title":"Location","text":"<p>Under tools repository:\u00a0https://github.com/Medial-EarlySign/MR_Tools\u00a0 In Linux: $MR_ROOT/Tools/AutoValidation\\kits. In Windows: %MR_ROOT%\\Tools\\AutoValidation (for example\u00a0U:\\Alon\\MR\\Tools\\AutoValidation\\kits) Or environment variable: \"$AUTOTEST_LIB\"</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#use-cases","title":"Use cases","text":"<p>There are 4 use cases:</p> <ul> <li>Development - testing of new model</li> <li>External Silent Run\u00a0-\u00a0auto tests for existin model on dataset WITHOUT labeling</li> <li>External_validation_after_SR - autotest for dataset after \"External_Silent_Run\" when we received the labeling.</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#how-to-create-a-new-testkit-for-you-algomarker","title":"How to create a new TestKit for you AlgoMarker","text":"<ol> <li>Create empty directory to hold your TestKit</li> <li>cd into the directory</li> <li>Execute: $AUTOTEST_LIB/generate_tests.sh</li> <li>It will ask you which Kit to create:\u00a0Development ,\u00a0External_retrospective ,\u00a0External Silent Run,\u00a0External_validation_after_SR - please type in the desired TestKit</li> <li>Please configure \"configs/env.sh\" and other settings if needed in configs folder</li> <li>Execute ./run.sh to run all tests,. There is \"run.specific.sh\" - to run a specific test by number, if not providing an argument (number) the scripts output the name of all tests with their number that you can choose.</li> </ol>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#how-each-test-is-being-executed-how-to-write-new-test","title":"How each test is being executed/ How to write new test:","text":"<p>Each test in Tests is given 3 arguments:</p> <ol> <li>CFG_PATH_OF_CURRENT_KIT - path to config folder of current kit</li> <li>SCRIPT_DIR_OF_INFRA - path of AutoTest infrastructure</li> <li>OVERRIDE - binary 0/1 if to override previous results You need to respect those arguments when writing a new test \u00a0 The tests are being executed one after another, there is a unified log of all tests and a specific log for each test. Output of each test has to be manually verified (one should make sure that test results make sense) There is also a file that show the status of all the Tests called \"tests_status.log\". Each test has status:</li> </ol> <ul> <li>FINISHED_NEED_VERIFY - test was ended successfully without errors. Please go over output and approve/disapprove the test.\u00a0</li> <li>FINISHED_FAILED - Test ended\u00a0successfully without errors, but failed. The outputs seems wrong or raised a problem. need to rerun. When you want to rerun, erase this status row for the test</li> <li>FINISHED_VERIFIED - If test ended correctly and verified</li> <li>STOPPED_NEED_VERIFY - test crashed, has errors. please verify if that's OK or not. It can be OK for example, if you want to skip the test</li> <li>STOPPED_FAILED -\u00a0test crashed, has errors. You marked this test as failure, Needs to rerun.\u00a0When you want to rerun, erase this status row for the test</li> <li>STOPPED_VERIFIED - test crashed, has errors. You marked this test as OK.\u00a0 After test finished for the first time, If has \"*_NEED_VERIFY\" status. In next time you run run.sh, it will ask you to decide on the status: Approved, Not approved, Rerun test, Skip decision for now.</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#configuration","title":"configuration","text":"<p>Configuration through parameters exposed in \"config\" folder. The main parameters are supposed to be defined under configs/env.sh. All The test are suppose to use/reuse arguments defined in env.sh. When we need external files, like bootstrap json.  It's good practice to put those files in config folder and reuse the same files in all scripts when needed. </p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#extension","title":"Extension","text":"<p>In some cases we want to add more tests, override exiting test.  We can do that be coping the desires test template from Test/Templates - Either \"TEMPLATE_TEST.sh\" for shell scripts or \"TEMPATE_TEST.py\" for python scripts. The tests are going to be execute based on the base template folder, for example: $AUTOTEST_LIB/kits/Development/Tests for Development. To override existing test in our use case, we can put a new test in our \"Tests\" folder with the same numeric prefix of the test we want to replace/override in the base template folder. Base Template: We have in each Template a row with definition of required arguments from \"env.sh\", pay attention to that. Make use of existing parameters and add new ones to env.sh as you wish/need.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/index.html#testing-the-testkit","title":"Testing the TestKit","text":"<p>There is also test unit to run the TestKit for LGI on \"Development\", \"External_Silent_Run\" and \"External_validation_after_SR\" to test the TestKit. Please refer to\u00a0$MR_ROOT/Tools/AutoValidation/test_kit and run the test of the desired TestKit. \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html","title":"Development kit","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#motivation","title":"Motivation","text":"<p>This test kit is used to test a NEW developed model before wrapping it to AlgoMarker. It will make sure the model has cleaners, imputers, bootstrap results and some other test. This will support the validation of a model on the same dataset used to train the model. External validation tests is in different kit.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#configuration","title":"Configuration","text":"<p>We will call our test kit code folder as TEST_KIT. The main configuration file can be found under current TEST_KIT/config/env.sh Parameters:</p> <ul> <li>REPOSITORY_PATH - A path to the repository use to develop the mode - train (&amp; test)</li> <li>TRAIN_SAMPLES_BEFORE_MATCHING - Path to raw train samples before matching</li> <li>TEST_SAMPLES/PREDS_CV - path to test samples that we can apply the model on without over-fitting (a different set of patient). Or use PREDS_CV to prediction files collection in cross validation manner (fir example by cv or Optimizer)</li> <li>MODEL_PATH - Path of the model</li> <li>WORK_DIR - Output of the KIT</li> <li>CALIBRATED_MODEL - path to calibrated model if applicable calibration tests. It's by default referenced to the same MODEL_PATH. \"CALIBRATED_MODEL=${MODEL_PATH}\"</li> <li>EXPLAINABLE_MODEL - path to model with explainability if applicable (recommended to create one). It's by default referenced to the same MODEL_PATH.</li> <li>TEST_SAMPLES_CALIBRATION - set of test samples to test model calibration. It's by default referenced to the same TEST_SAMPLES</li> <li>BT_JSON - bootstrap json path. It is reference by default to the current config folder /bootstrap/bootstrap.json. We will use this json to generate features for bootstrap analysis.<ul> <li>bootstrap/bootstrap.json - a bootstrap json file to run bootstrap_app</li> </ul> </li> <li>BT_COHORT - bootstrap cohorts definition file for bootstrap analysis. It is reference by default to the current config folder /bootstrap/bt.params<ul> <li>bootstrap/bt.params - a bootstrap cohorts file for bootstrap_app</li> </ul> </li> <li>NOISER_JSON - a json that defines rep processors to \"noise\" the inputs and test model sensitivity to noise. It references a file in current folder named: noiser_base.json. Please refer to Noiser page for more details.</li> <li>TIME_NOISES - vector of values to control the noise in dates. The unit is days, so \"30\" means uniform randomly shifting dates between 0-30 days backward of all signals in NOISER_JSON.</li> <li>VAL_NOISES - vector of values to control the noise in signal values. The unit is standard deviation multiply by 10 - so \"1\" means \"0.1\" * signal std. This will control normal random noise standard deviation based on signal std.</li> <li>DROP_NOISES - vector of values to control the probability to \"drop\" tests/values. The unit is probability multiply by 10 - so \"1\" mean \"0.1\"/10% chances to drop a lab test value in certain date.</li> <li>BT_JSON_CALIBRATION - bootstrap json to control calibration cohorts if they are different than regular cohorts. By default it is the same as regular bootstrap: BT_JSON_CALIBRATION=${BT_JSON}</li> <li>BT_COHORT_CALIBRATION - bootstrap cohort definitions for calibration. By default it is the same as regular bootstrap</li> <li>EXPLAIN_JSON - bootstrap json to control explainability cohorts if they are different than regular cohorts. By default it is the same as regular bootstrap: EXPLAIN_JSON=${BT_JSON}</li> <li>EXPLAIN_COHORT - bootstrap cohort definitions for explainability. By default it is the same as regular bootstrap</li> <li>BT_JSON_FAIRNESS - bootstrap json to control fairness cohorts if they are different than regular cohorts. By default it is the same as regular bootstrap: BT_JSON_FAIRNESS=${BT_JSON}</li> <li>FAIRNESS_BT_PREFIX - A single cohort to filter samples to test fairness. The syntax is the same as bootstrap cohort. bootstrap_app<ul> <li>\"configs/fairness_groups.cfg\" - more info in Test_12 - fairness</li> </ul> </li> <li>FAIRNESS_MATCHING_PARAMS - Control How to do matching between groups if fairness isn't met. The default is to do the matching by age - 10 years bin.</li> <li>BASELINE_MODEL_PATH - A path to baseline model to compare with</li> <li>BASELINE_COMPARE_TOP - compare top X% to compare our model with baseline model. van diagram and more... We have 2 additional files:</li> <li>coverage_groups.py - which defines with python pandas special high risk groups by rules - for example old patients, etc. That we want to see that the model flags/selects them more than random as sanity test. more into in Test_09 - coverage</li> <li>feat_resolution.tsv - controls feature resolution when plotting to html the graphs. more info in Test_10 - test matrix features</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html","title":"Test_01 - test_train_samples_over_years","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#overview","title":"Overview","text":"<p>The goal is to test the samples distribution and some properties of the trained samples. \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#inputs","title":"Inputs:","text":"<ul> <li>WORK_DIR\u00a0 -output work directory</li> <li>TRAIN_SAMPLES_BEFORE_MATCHING - The training samples</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#outputs","title":"Outputs","text":"<p>The output is located under WORK_DIR/01.test_train_samples_over_years.log</p> <ul> <li>Prints the distribution of cases/controls in each year in the sample - Do we see something weird here? is it unbalanced? Is there any pattern?\u00a0</li> <li>Prints the distribution of cases/controls in each month in the samples - Do we see something weird here? is it unbalanced? Is there any pattern?\u00a0</li> <li>Creates a folder - samples_stats.train<ul> <li>stats.txt - contains a table how many \"distinct\" outcomes a patient has in the samples - prints a histogram of that. If each patient is suppose to be either case/control please check that. Otherwise you can see how many controls turned into cases</li> <li>cases_controls_id_histogram.html: x axis - how many times a patient was repeated in the samples as case/control. y axis - patients count. We can see how many of the patients repeats themselves in the samples. Does that seems OK?</li> </ul> </li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html","title":"Test_02 - test samples","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#overview","title":"Overview","text":"<p>Very similar to previous test, but for test samples. The output will be under WORKDIR/02.test_test_samples_over_years.log and samples_stats.test. \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#inputs","title":"Inputs:","text":"<ul> <li>WORK_DIR</li> <li>TEST_SAMPLES \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html","title":"Test_03 - test cleaners","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#overview","title":"Overview","text":"<p>Tests that the model has cleaners for all used signals.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH - path for model</li> <li>REPOSITORY_PATH - repository path</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#output","title":"Output","text":"<p>$WORK_DIR/03.test_cleaners.log - describes which signals lacks cleaners if some are missing. If some are missing the test will fail.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html","title":"Test_04 - test imputers","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#overview","title":"Overview","text":"<p>Tests that the model has imputers</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH - path for model</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#output","title":"Output","text":"<p>$WORK_DIR/04.test_imputers.log - Test if model has imputers. If model lacks imputers the test will fail</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html","title":"Test_05 - But why","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#overview","title":"Overview","text":"<p>Tests model feature importance with shapley</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH - path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES - test samples path</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#output","title":"Output","text":"<p>$WORK_DIR/ButWhy:</p> <ul> <li>Global.html - global signals importance in the model</li> <li>Global.ungrouped.html - Global features importance in the mode</li> <li>single_features directory - Each of the most important feature ButWhy analysis</li> <li>For each one of the importance feature it plots stratification of the feature value and how the model respond to each value<ul> <li>Mean outcome for each feature value - Which is probability to be a case. We can see the relation between feature value and the outcome</li> <li>Mean score for each feature value - we can see the relation between feature value and the score (suppose to be similar to outcome graph)</li> <li>Mean+Confidence interval of Shapley value for each feature value. We want it to behave differently then outcome in some cases. For example: Flu complications - Age is risk factor, young and old individual are at risk - U shape graph of risk as function of age.When we use BMI, BMI for little children is low - so outcome relation to BMI acts also as U shape, but model shapley values does not - It \"understand\" that low BMI is not reasonTo increase score (it's due to younger age which we already take into account by using age).</li> </ul> </li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html","title":"Test_06 - bootstrap results","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#overview","title":"Overview","text":"<p>Tests that the model performance with bootstrap.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH - path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>PREDS_CV / TEST_SAMPLES - Test samples</li> <li>BT_JSON - json for bootstrap analysis</li> <li>BT_COHORT - bootstrap cohort definitions</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#output","title":"Output","text":"<p>${WORK_DIR}/bootstrap - bt.pivot_txt is the bootstrap output file</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html","title":"Test_07 - feature importance","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#overview","title":"Overview","text":"<p>Tests that the model feature importance in different way - how each signal impact model performance if we will not \"send\" it to the model. We will test each signal in a group that the signal exists, otherwise important signals but very rare will get low importance and we don't want necessarily this to happen.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH - path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>BT_JSON_FAIRNESS\u00a0- json for bootstrap analysis</li> <li>FAIRNESS_BT_PREFIX\u00a0- bootstrap cohort definition to focus on feature importance \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#depends","title":"Depends:","text":"<p>Test_06 -\u00a0 predictions file results</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#output","title":"Output","text":"<p>${WORK_DIR}/ButWhy/feature_importance.sorted_final.tsv - file with sorted signals importance. For each signal - how many of the samples has this signal (feature are not missing values) and how important is it (impact on AUC if you don't pass it to the model). \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html","title":"Test_08 - calibration","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#overview","title":"Overview","text":"<p>Tests that the model calibration performance if applicable. For calibration information please refer to Calibrate model, and calibration test</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>CALIBRATED_MODEL\u00a0- path for calibrated model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES_CALIBRATION - test samples for calibration</li> <li>BT_JSON_CALIBRATION - json for bootstrap analysis</li> <li>BT_COHORT_CALIBRATION\u00a0- bootstrap cohort definition to focus on calibration \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#output","title":"Output","text":"<p>${WORK_DIR}/calibration</p> <ul> <li>bt.pivot_txt - bootstrap results of calibration measurement</li> <li>*.html\u00a0 - calibration graph result for each cohort - for each score bin, plots observed mean outcome, expected confidence internal, etc.</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html","title":"Test_09 - coverage","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#overview","title":"Overview","text":"<p>Tests that the model flags expected high risk population. The high risk population is defined by rule based like age, and other features. \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH\u00a0- path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> <li>configs/coverage_groups.py - rules with pandas to define the groups \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#output","title":"Output","text":"<p>$WORK_DIR}/09.test_coverage.log We defined a \"risk\" group for undiagnosed CKD by taking patients with low egfr level &lt; 65. We defined the group like this in coverage_groups.py <pre><code># Usage with getcol to search for columns inside \"df\" DataFrame. The DataFrame is created based on model feature matrix.\neGFR=getcol(df, 'eGFR_CKD_EPI.last.win_1_360')\n# Define Groups and store in dictionary \"cohort_f\" - The key is the name of the cohort, Value is the filter with pandas on DataFrame \"df\". \ncohort_f['eGFR&lt;65'] = df[eGFR]&lt;65\n</code></pre> Example output: <pre><code>Cohort eGFR&lt;65 :: Has 19426 patient in cohort out of 605636 (3.2%)\nAnalyze cutoff 1.0% with score 0.51042 =&gt; covered 2246 (11.6%), total_flag 6057 (1.0%), 37.1% has condition in flagged. Lift=11.6\nAnalyze cutoff 3.0% with score 0.28844 =&gt; covered 5906 (30.4%), total_flag 18170 (3.0%), 32.5% has condition in flagged. Lift=10.1\nAnalyze cutoff 5.0% with score 0.19112 =&gt; covered 8761 (45.1%), total_flag 30282 (5.0%), 28.9% has condition in flagged. Lift=9.0\nAnalyze cutoff 10.0% with score 0.09499 =&gt; covered 13650 (70.3%), total_flag 60564 (10.0%), 22.5% has condition in flagged. Lift=7.0\n</code></pre> We can see that there are 19426 that meets the criteria eGFR&lt;65 out of 605636 which is 3.2%. We can see results for several cutoffs - But in all cutoffs the \"lift\" is high - for example 11.6 in cutoff of 1% (which is 0.51042) - so patients in this group are getting flagged 11.6 more times then random patient. This captures 2246 patients out of all 19426 patients with eGFR&lt;65, which is 11.6%. Cutoff of 1% which is score&gt;=0.51042 flags 6057 which is indeed 1% out of population 605636. The probability to have eGFR&lt;65 in the flagged population is 37.1% (2246 / 6057) - so most patients that are getting flagged are with last eGFR&gt;65 (not trivial) but many of them 37.1% with eGFR&lt;65. \u00a0 \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html","title":"Test_10 - test matrix features","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#overview","title":"Overview","text":"<p>Tests of model features - plots important features. We want to make sure there are no outliers, \"junk\" values in the matrix that the model can use.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH\u00a0- path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> <li>config/feat_resolution.tsv - defines resolution for features that we are going to print [Optional].</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#depends","title":"Depends","text":"<p>test_05 - Will use the feature importance to select which features to plot.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#output","title":"Output","text":"<ul> <li>$WORK_DIR}/outputs/features_stats.tsv - a table with stats on each feature divided into cases/controls. Mean,Std, missing values percentage. Please go over to see the values are OK</li> <li>$WORK_DIR}/outputs/graphs - graph for each feature - please have a look and see the distribution. \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html","title":"Test_11 - test matrix over years","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#overview","title":"Overview","text":"<p>Tests of model features over the years. What changes in the features between different years? We want to make sure there are no biases or something important that is time sensitive. It takes the test samples and compares the most recent prediction date samples to the least recent prediction date samples and creates propensity model that tries to differentiate between samples.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH\u00a0- path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#output","title":"Output","text":"<ul> <li>$WORK_DIR/compare_years<ul> <li>Global.html - the most important features in the propensity model that are different between the years</li> <li>features_diff - A directory that compares each of the important features in the propensity model - least recent to most recent on the same graph.</li> <li>single_features - But why single feature analysis directory for each of the important features in the propensity model analysis</li> <li>compare_rep.txt - text file that compare the average value of each feature</li> <li>test_propensity.bootstrap.pivot_txt - the propensity model performance Please go over on the most important different features and decide if that's OK</li> </ul> </li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html","title":"Test_12 - fairness","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#overview","title":"Overview","text":"<p>Compare sensitive groups in the same cutoff and check that the performance is similar. \\ A common definition is to have similar sensitivity for the same cutoff. In other words: no matter if you are a case from this group or that group (black/white male/female) - you will have similar probability to be captured by the model - which is the sensitivity</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH\u00a0- path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> <li>BT_JSON_FAIRNESS - bootstrap json features to filter the cohort for testing</li> <li>FAIRNESS_BT_PREFIX - bootstrap cohort definition to define the cohort for testing for fairness</li> <li>config/fairness_groups.cfg - defines the group that we want to compare: Each line defines 2 or more groups to compare one with each other. Each line consist of 2 tokens tab delimited. First token is bootstrap filter definition for each group separated by \"|\".\u00a0 The second token is the \"pretty\" names to give each of the filters separated by \"|\". For example: <pre><code>Gender:1,1|Gender:2,2  [TAB]   Males|Females\n</code></pre></li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#output","title":"Output","text":"<p>$WORK_DIR/fairness</p> <ul> <li>fairness_report.tsv - a summary table that compare sensitivity/ specificity AUC in the same cutoff 5%,10%</li> <li>fairness_report.* - result that compare statistical chi square between the groups to see if the different in sensitivity is statistically significant. There are 2 files. 1 for 5% PR cutoff and 1 for 10% PR cutoff</li> <li>Graph_fairness - plots the sensitivity as function of score threshold in the different groups (with confidence intervals) that we can compare not just 5,10%</li> <li>graph_matched - If the model is not \"fair\" we tried to do matching by strata (The default in the config is age). Here we have graphs results after matching \u00a0 What to look for? please search for low chi square in fairness report and similar sensitivity. Have a look on the graph Graph_fairness. If needed see the matched resutls.</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html","title":"Test_13 - model explainability","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#overview","title":"Overview","text":"<p>The goal is to \"feel/taste\" the data or what the model does. We will want to see real data examples of high risk patients report + analyze most common reason for getting flagged. It will do that analysis on top 1000 patients.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>EXPLAINABLE_MODEL\u00a0- path for model with explainability</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> <li>EXPLAIN_JSON - json for bootstrap filtering</li> <li>EXPLAIN_COHORT - optional filter of samples to focus on explainability samples</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#output","title":"Output","text":"<p>$WORK_DIR/ButWhy/explainer_examples</p> <ul> <li>group_stats*.tsv - Summary table of most common reasons. For example: <pre><code>\u00a0Group    Frequency    Percentage    leading_feature_1    feature_frequency_1    leading_feature_2    feature_frequency_2    leading_feature_3\nSmoking    996    99.7    Smoking.Smoking_Years    992    Smoking.Smok_Pack_Years_Max    693    Smoking.Never_Smoker\nICD9_Diagnosis.ICD9_CODE:496    537    53.8    ICD9_Diagnosis.category_dep_set_ICD9_CODE:496.win_0_10950    537    ICD9_Diagnosis.category_dep_set_ICD9_CODE:496.win_0_365    537    \nBMI    405    40.5    BMI.max.win_0_1095    390    BMI.last.win_0_180    358    BMI.max.win_0_180\nWBC    282    28.2    WBC.last.win_0_1095    226    WBC.last.win_0_180    198    WBC.min.win_0_180\nPlatelets    271    27.1    Platelets.last_delta.win_0_1095    193    Platelets.slope.win_0_1095    192    Platelets.min.win_0_180\nAge    200    20    111    1    113    1    116\n</code></pre> We can see the in LungFlag most important risk factor that repeats itself is Smoking - which appears in 99.7% of the times in top 3 reasons - The leading feature inside is Smoking.Smoking_Years After it we can see COPD diagnosis that appears 53.8% of the times in top 3 and than BMI - 40.5% and then WBC 28.2%</li> <li>test_report.*.tsv - report example of high risk patients each several grouped rows described the same patient but with different risk factor from most important to least important. Example: <pre><code>pid    time    outcome    pred_0    Tree_iterative_covariance    Code_Description    Specific_Feature_Inside_Group(optional)...                        \n100192    20100913    1    0.445575    Smoking:=1.51313(27.38%)        Smoking.Smoking_Years(40.13972):=0.94721(64.44%)        Smoking.Never_Smoker( 0):=0.22725(15.46%)                \n100192    20100913    1    0.445575    WBC:=0.70804(12.81%)        WBC.min.win_0_180(12.6):=0.151(27.51%)        WBC.last.win_0_1095(17.5):=0.11922(21.72%)        WBC.last.win_0_180(17.5):=0.11428(20.82%)        \n100192    20100913    1    0.445575    Platelets:=0.48661(8.81%)        Platelets.min.win_0_180(395):=0.16023(36.41%)        Platelets.last_delta.win_0_1095(35):=0.0843(19.15%)                \n100192    20100913    1    0.445575    BMI:=0.43598(7.89%)        BMI.last.win_0_180(25.53):=0.06451(21.86%)        BMI.max.win_0_1095(26.95):=0.04568(15.48%)        BMI.range_width.win_365_730(1.77):=0.03247(11.00%)        BMI.max.win_0_180(26.24):=0.03156(10.69%)\n100192    20100913    1    0.445575    Neutrophils#:=0.42125(7.62%)        Neutrophils#.range_width.win_365_730(21.49):=0.06566(20.60%)        Neutrophils#.std.win_365_730(6.09858):=0.0565(17.73%)        Neutrophils#.range_width.win_0_1095(24.1):=0.051(16.01%)        \n100192    20100913    1    0.445575    ICD9_Diagnosis.ICD9_CODE:496:=0.29496(5.34%)    496|Chronic_airway_obstruction_not_elsewhere_classified|ICD9_CODE:496|ICD9_DESC:496:Chronic_airway_obstruction_not_elsewhere_classified    ICD9_Diagnosis.category_dep_set_ICD9_CODE:496.win_0_10950( 1):=0.33176(93.77%)    496|Chronic_airway_obstruction_not_elsewhere_classified|ICD9_CODE:496|ICD9_DESC:496:Chronic_airway_obstruction_not_elsewhere_classified    ICD9_Diagnosis.category_dep_set_ICD9_CODE:496.win_0_365( 0):=-0.02205(6.23%)    496|Chronic_airway_obstruction_not_elsewhere_classified|ICD9_CODE:496|ICD9_DESC:496:Chronic_airway_obstruction_not_elsewhere_classified            \n100192    20100913    1    0.445575    Triglycerides:=0.23074(4.18%)        Triglycerides.range_width.win_0_1095( 0):=0.08213(60.18%)        Triglycerides.max.win_0_1095(69):=0.04217(30.90%)                \n100192    20100913    1    0.445575    Hematocrit:=0.19547(3.54%)        Hematocrit.min.win_0_1095(37.9):=0.0481(45.39%)        Hematocrit.slope.win_0_1095(-0.62271):=0.0296(27.94%)                \n100192    20100913    1    0.445575    Neutrophils%:=0.16415(2.97%)        Neutrophils%.range_width.win_0_1095(50.2):=0.08764(37.97%)        Neutrophils%.min.win_0_1095(30.8):=0.06082(26.36%)                \n100192    20100913    1    0.445575    ICD9_Diagnosis.ICD9_CODE:786:=-0.1153(2.09%)    786|Symptoms_involving_respiratory_system_and_other_chest_symptoms|ICD9_CODE:786|ICD9_DESC:786:Symptoms_involving_respiratory_system_and_other_chest_symptoms    ICD9_Diagnosis.category_dep_set_ICD9_CODE:7866.win_0_365( 0):=-0.03598(72.11%)    786.6|Swelling_mass_or_lump_in_chest|ICD9_CODE:7866|ICD9_CODE:786.6|ICD9_DESC:786.6:Swelling_mass_or_lump_in_chest    ICD9_Diagnosis.category_dep_set_ICD9_CODE:7866.win_0_10950( 0):=-0.0075(15.03%)    786.6|Swelling_mass_or_lump_in_chest|ICD9_CODE:7866|ICD9_CODE:786.6|ICD9_DESC:786.6:Swelling_mass_or_lump_in_chest            \nSCORES    100192    20100913    ###                                    \nPID_VIEWER_URL    =HYPERLINK(\"http://node-04:8196/pid,100192,20100913,prediction_time,Smoking_Status&amp;Smoking_Duration&amp;P_White&amp;P_Red&amp;Platelets&amp;P_Diabetes&amp;P_Cholesterol&amp;P_Liver&amp;P_Renal&amp;ICD9_Diagnosis&amp;Race&amp;Lung_Cancer_Location\",\"Open Viewer\")                                            \n</code></pre> We can see a single patient 100192 that recieved score 0.445575 on time 20100913 and is indeed a case (outcome is 1). The main reason is Smoking, shap value of 1.51 (27.38% of the shap values sum in absolute). the main feature is Smoking_Years which is 40.13 and Never_smoker is 0 so he is current or past smoker.  Then WBC with 0.708 of shap value (12.81%), the minimum WBC was 12.6 which is quite high and last value was 17.5.  We can see for example that \"ICD9_Diagnosis.ICD9_CODE:786\" is negative - protective with low value of -0.11. The feature ICD9_Diagnosis.category_dep_set_ICD9_CODE:7866.win_0_10950\u00a0 has value of \"0\" So patient doesn't have this diagnosis in the past 10 years.</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html","title":"Test_14 - noise sensitivity analysis","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#overview","title":"Overview","text":"<p>Tests the model sensitivity for noise - noise in missing values, shifting dates backward and shifting the values (see that there is no resolution problem for example).</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH\u00a0- path for model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> <li>TRAIN_SAMPLES_BEFORE_MATCHING - the training samples</li> <li>BT_JSON - bootstrap json to filter cohort</li> <li>BT_COHORT - bootstrap cohort to filter cohort</li> <li>NOISER_JSON - Path to noiser json config</li> <li>TIME_NOISES, VAL_NOISES, DROP_NOISES - parameters to control the noise</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#depends","title":"Depends:","text":"<p>test_06</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#output","title":"Output","text":"<p>$WORK_DIR/test_noiser/results</p> <ul> <li>time_analysis.csv - How the noise in time effect the model in different noise levels</li> <li>value_analysis.csv - How the noise in values effect the model in different noise levels</li> <li>drop_analysis.csv - How dropping tests effect the model in different drop levels</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html","title":"Test_15 - compare to baseline model","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#overview","title":"Overview","text":"<p>The goal is to compare our model to baseline model. We will compare performance and correlation, which patient get flagged and how are they different? what baseline model misses that we capture?</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#input","title":"Input","text":"<ul> <li>WORK_DIR - output work directory</li> <li>MODEL_PATH\u00a0- path for model</li> <li>BASELINE_MODEL_PATH - path to baseline model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES\u00a0- test samples</li> <li>BT_JSON - bootstrap json to filter cohort</li> <li>BT_COHORT - bootstrap cohort to filter cohort</li> <li>BASELINE_COMPARE_TOP - Which top percentage to take for comparing (default is 5%)</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#output","title":"Output","text":"<ul> <li>$WORK_DIR/ButWhy.baseline - But why for baseline model. Global.html, Global.ungrouped.html, single_features directory</li> <li>$WORK_DIR/bootstrap/bt_baseline_compare.tsv - compares bootstrap result with baseline</li> <li>$WORK_DIR/compare_to_baseline/correlation.txt - correlation of scores</li> <li>$WORK_DIR/compare_to_baseline - result of training propensity model that tries to differentiate which top\u00a0 \"BASELINE_COMPARE_TOP\" our model flags compares to baseline flag in top BASELINE_COMPARE_TOP.<ul> <li>Global.html - most important signals in that propensity model . Describe which features our model uses more/different than baseline</li> <li>single_features - a directory with butwhy analysis of each important feature in that model</li> <li>compare_rep.txt - a text table that compares each of the features mean, std, etc in both samples - the one our model flags compared to baseline</li> </ul> </li> <li>$WORK_DIR/compare_to_baseline/baseline_coverage_by_mes_$BASELINE_COMPARE_TOP.html - How much intersection/coverage of baseline flagged individuals are covered by our model in different cutoffs</li> <li>$WORK_DIR/compare_to_baseline/compare_Age_top_flagged.html - comparing flagged age by both models</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html","title":"External Silent Run","text":"<p>External Silent Run Goal When we have a new dataset, with no labels, before we give scores we want to:</p> <ul> <li>Make sure data is OK for running the model</li> <li>Estimate performances</li> </ul> <p>This WIKI Goal In the silent run, the new dataset is loaded into a repository, and several tests are performed, to compare the dataset against the expected. After setting the right environment parameters(see next), the whole process run in one-click. However, to check the outcomes we need to look inside relatively many different output files. The main goal of this WIKI is to help in reading the results.</p> <p>Environment Parameters All parameters are set in one file: configs/env.sh **What is the dataset? (fix per dataset)\u00a0**</p> <ul> <li>AWS_REGION=...</li> <li>AWS_ACCESS_KEY_ID=...</li> <li>AWS_SECRET_ACCESS_KEY=...</li> <li>AWS_INPUT_PATH=...input dataset raw data, as organized by the AlgoAnalyzer, in\u00a0\"file_api\" format:\u00a0</li> </ul> ID Date Signal Value Unit <ul> <li>AWS_OUTPUT_PATH=... samples file with scores, generated by the AlgoAnalyzer Or if it's not in AWS, you can define the path to the input dataset and AlgoMarker output directly, without all those \"AWS_*\" parameters:</li> <li>SILENCE_RUN_INPUT_FILES_PATH=... copy of input dataset raw data\u00a0</li> <li> <p>SILENCE_RUN_OUTPUT_FILES_PATH=... copy of AlgoAnalyzer output score - \"samples\" When you use AWS_ parameters, the\u00a0SILENCE_RUN_INPUT_FILES_PATH,\u00a0SILENCE_RUN_OUTPUT_FILES_PATH are set automatically to be downloaded inside the $WORK_DIR/data \u00a0 What is the model? (fix per AlgoMarker)*</p> </li> <li> <p>ALGOMARKER_PATH=... model directory</p> </li> <li>REFERENCE_MATRIX=... full path to reference matrix</li> <li>CMP_FEATURE_RES=...\u00a0<ul> <li>List, comma separated, of important features for the model, to be used in several tests</li> <li>To get important feature, check butwhy of original model</li> <li>Format is: 'Feature_Name:Resolution,Feature_Name:Resolution,...\"<ul> <li>For instance: \"\"Age:1,FTR_000074.MCH.slope.win_0_1000:0.01\".\u00a0</li> <li>Feature_Name: full name or string that only one feature contains.</li> <li>Resolution - just for plotting value distribution.</li> </ul> </li> </ul> </li> <li>SCORE_MIN_RANGE=0.1\u00a0Minimal score range for testing the sex ratio\u00a0of flagged males in those score threshold range</li> <li>SCORE_MAX_RANGE=0.2 Maximal score range for testing the sex ratio of flagged males in those score threshold range</li> <li>FILTER_LAST_DATE=0, In some cases, the AlgoAnalyzer was executed more than once on the input data and there are duplications. This flag is used to filter duplications.</li> </ul> <p>Where shall we put the output?</p> <ul> <li>WORK_DIR=... output directory</li> </ul> <p>To run the whole process, simply execute run.sh from the External_Silent_run root.\u00a0 \u00a0 \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/TODO.html","title":"TODO","text":"<p>env file -\u00a0remove\u00a0FILTER_LAST_DATE, we don't want to support it = we want to assume data has no duplications General - prepare focused output, with just the important features General -\u00a0printing the missing value rate for each feature/important feature. There is a \"mask\" of imputations in MedFeatures, we want to print this matrix also in (TestModelExternal), than we can count the stats of missing values or plot the graphs without imputations. Test 1 - improve ETL to mark with WARNING any evidence for possible concern Test 1 - parse output file to get all signal value distribution effort in one dataframe, see code in wiki\u00a0 General - arranged output from test by directory (all output from test 'n' are in 'directory n') Test 2 - parse\u00a0compare_rep.txt with the code from the wiki, or save as tsv from the start Test 2 - improve the logic to mark feature status in table 21, or simply drop the table and keep just t22 Test 2 - add t-test Test 2 - table 22, what is AUC, and why do we need it? Test 2 - bring to feature importance columns, the value from the reference Test 3 - bug in calculation of correlation between Test_Run and Test_Run.Original Test 3 - consider checking score distribution differences (also?) after matching on Age Test 5 - print also Male ratio for the whole population Test 6 - coverage calculation and missing values (as reference comes after imputation, and currently we report test before imputation) Test 7 - check KLD computation Test 9 - add a reference \u00a0 \u00a0 Rerun on THIN with LungFlag</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%201%20-%20Generate%20Repository.html","title":"Test 1 - Generate Repository","text":"<p>Overview The main goal of this 'test' is to load the inputs from the AlgoAnalyzer into repository for evaluation. Next, tests that are part of the ETL are performed. Parameters (see env.sh) Inputs:\u00a0</p> <ul> <li>WORK_DIR - output folder path to process and load the repository</li> <li> <p>SILENCE_RUN_INPUT_FILES_PATH - The path to the input data files in \"file_api\" format Outputs:</p> </li> <li> <p>Repository in directory\u00a0${WORK_DIR}/rep</p> </li> <li> <p>ETL tests results in ${WORK_DIR}/ETL (see also\u00a0ETL_WORK_DIR) Test Results Review Full ETL log including tests -\u00a0WORK_DIR/01.generate_repository.log The following is a recipe for checking the log. a. high level review of signal values distribution See example: <pre><code>Done testing nulls in signal Hemoglobin\nunit:g/L || KLD (127)= 0.005790, KLD_to_Uniform=0.740022, entory_p=4.105875, grp_cnt=8444, group_counts=1/4\nunit:g/l || KLD (127)= 0.010441, KLD_to_Uniform=0.740022, entory_p=4.105875, grp_cnt=6995, group_counts=2/4\nThere are issues with low range, please have a look (more than factor 3)\n       q  value_0  reference     ratio1    ratio2      ratio\n0  0.001   80.908        7.2  11.237223  0.088990  11.237223\n1  0.010  102.000        9.0  11.333333  0.088235  11.333333\n2  0.100  126.000       11.2  11.250000  0.088889  11.250000\nThere are issues with the median, please have a look (more than factor 2)\n     q  value_0  reference     ratio1    ratio2      ratio\n3  0.5    145.0       13.3  10.902255  0.091724  10.902255\nThere are issues with high range, please have a look (more than factor 3)\n       q  value_0  reference     ratio1    ratio2      ratio\n4  0.900    163.0  15.300000  10.653595  0.093865  10.653595\n5  0.990    178.0  16.799999  10.595239  0.094382  10.595239\n6  0.999    189.0  17.900000  10.558659  0.094709  10.558659\nDone testing values of signal Hemoglobin\n</code></pre> Output would be list of features where some percentiles are far from the reference: q - is the quantile, value_0 - is the quantile in current dataset, reference - the quantile in the reference dataset. ratio1 = value_0 / reference, ratio2 = 1 / ratio1, ratio = max(ratio1, ratio2). What may be the reason?</p> </li> <li> <p>If unit transformation is wrong, we expect several 'issues' for the same signal with different percentiles (q) ,like in the example above =&gt; This would be a RED FLAG.</p> </li> <li>If we see 'issues' here and there, we still need to understand them. For instance:<ul> <li>Are 0 allowed? if the answer is not the same for the two datasets, then we are likely to see alerts on low values</li> <li>Are kid allowed? different age range in the reference compared to the dataset may affect the range ... please add here more possible reasons that you happened to meet/explore b. Deep dive into important features The important features of this model are defined in env.sh For every\u00a0SIGNAL the detailed output includes:</li> </ul> </li> <li>A specific test log in ETL/outputs/test.$SIGNAL.log and logs from processing instructions (if were, for example, dropping lines without dates, before testing) in\u00a0ETL/signal_processings_log/$SIGNAL.log</li> <li>Distribution of day, month, year and value, in\u00a0ETL/signal_processings_log/SIGNAL/batches/, If we have more than 1 batch, and aggregated report will appear in\u00a0ETL/signal_processings_log/SIGNAL It is recommended to manually check the logs and charts of all important features. What we might see?</li> <li>Example 1:The following graphs are monthly distribution of Hemoglobin samples, from dataset prepared in the middle of 2023. The monthly samples looks suspicious, however from the yearly graph we see that samples are just since last year. Hence, more samples on the first months of the year is expected. </li> <li>Example 2:On the right we see normal distribution of a lab measurement.On the left we unclear 'vibrations'. It is not likely to affect the model, but one might check with the dataset owner the reason, to make sure it does not hide a bigger problem. **</li> <li>... please add here examples/issues that you happened to meet/explore in the charts and/or detailed logs </li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010_12%20-%20butwhy%20%28TBD%29.html","title":"Test 10/12 - butwhy (TBD)","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Sample%20Dates.html","title":"Test 11 - Sample Dates","text":"<p>Simple straight forward value counts on sample dates. See results in\u00a0samples_stats, and make sure the distribution match your understanding of the dataset expected nature.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20Features%20and%20Flagged%20%28TBD%29.html","title":"Test 13 - Features and Flagged (TBD)","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%202%20-%20Compare%20Repository%20with%20Reference%20Matrix.html","title":"Test 2 - Compare Repository with Reference Matrix","text":"<p>Overview The main goal of this 'test' is to understand the differences between the current dataset, and the dataset that was used to train the model. It would be important, at later phase, in estimating model performance with the new dataset.</p> <p>Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run. In particular, this test uses:</p> <ul> <li>REFERENCE_MATRIX=... full path to reference matrix (feature matrix from model train)</li> <li>CMP_FEATURE_RES=... list of important features for the model (based on butwhy of original model)</li> </ul> <p>What is actually done? The test builds 2 propensity models to separate and predict if data point is from reference or the new dataset:</p> <ul> <li>\"Over fitted model\" - a model trained and tested on the full data, that uses all model features =&gt; output in directory 'compare'</li> <li>\"modest_model\" - using standard separation to train and test, a model that uses just the important features =&gt; output in directory 'compare.no_overfitting'</li> </ul> <p>Test Results Review Each directory (compare, compare.no_overfitting) includes 4 major outputs:\u00a0</p> <ul> <li>compare_rep.txt - file that compares mean, std for each feature in reference matrix and in \"loaded repository\" based on input files</li> <li>test_propensity.bootstrap.pivot_txt - how good is the separation between reference and input data</li> <li>shapley_report.tsv - butwhy of the model that separates the input data reference matrix</li> <li> <p>features_diff - graphs comparisons of top 10 most different features. \u00a0 How to read\u00a0compare_rep.txt During the ETL we tested each signal against its reference. Here we test every feature. First, look in the file just in the compare directory. It includes 2 tables in txt format. Run the following code to turn it into readable dataframes. <pre><code>f = os.path.join(DIR, 'compare/compare_rep.txt')\nt2 = pd.read_csv(f, sep= '\\r')\nt2 = t2[t2[t2.columns[0]].map(lambda x: x[0:3])=='MAN']\ncut = t2.index.min()\nt21 = pd.read_csv(f, nrows=cut)\nt21['status'] = t21.index.map(lambda x: x[0].split(' ')[0])\nt21['feature'] = t21.index.map(lambda x: x[0].split('::')[0].rstrip().split(' ')[-1])\nt21['TRAIN mean'] = t21.index.map(lambda x: x[0].rstrip().split('mean=')[1])\nt21['TRAIN std'] = t21.index.map(lambda x: x[1].rstrip().split('=')[1])\nt21['TRAIN miss_cnt'] = t21.index.map(lambda x: x[2].rstrip().split('=')[1].split(\"|\")[0])\nt21['TEST mean'] = t21.index.map(lambda x: x[2].rstrip().split('mean=')[1])\nt21['TEST std'] = t21[t21.columns[0]].map(lambda x: x.rstrip().split('=')[1])\nt21['TEST miss_cnt'] = t21[t21.columns[1]].map(lambda x: x.split('|')[0].split('=')[1].rstrip())\nt21['mean_diff_ratio'] = t21[t21.columns[1]].map(lambda x: x.split('mean_diff_ratio=')[1].split('|')[0].rstrip())\nt21['IMP'] = t21[t21.columns[1]].map(lambda x: x.split(' ')[-1])\ncols = ['status', 'feature', 'TRAIN mean', 'TRAIN std', 'TRAIN miss_cnt', 'TEST mean', 'TEST std', 'TEST miss_cnt', 'mean_diff_ratio', 'IMP'] \nt21 = t21[t21.status=='BAD'][cols].reset_index(drop=True)\n\u00a0\nt22 = pd.read_csv(f, skiprows=cut+1, sep='\\t')\n</code></pre> The first dataframe, t21, should be ignored. t21 shows moments, range and missing values count, for every feature, comparing the reference to the tested dataset. A general status is calculated - however, logic is unclear and need debug.  The second dataframe, t22, shows the same information (without range), plus Mann Whitney test result.</p> </li> <li> <p>_1 is for the new dataset</p> </li> <li>_2 is for the reference</li> <li> <p>The Mann-Whitney U Test assesses whether two sampled groups are likely to derive from the same population, but note test limitations - if median and shape are the same for both samples, P_value would be high even for different std/scale.\u00a0  In table t22:</p> </li> <li> <p>We need to make sure we don't see low P_value for any important feature to the model, or proxy for such features, i.e.. we may list\u00a0MCH.min.win_0_180 as important feature, and we don't want it or MCH.min.win_0_360 to have low P-value.</p> </li> <li>We need to understand the reasons for the low P-value when happened, in order to better understand the new data set. For instance, in the table above, we see that in the tested dataset RDW was not given close to sample point, probably because it is not part of the panel. As RDW is not an important signal, we can ignore it.</li> </ul> <p>How to read\u00a0test_propensity.bootstrap.pivot_txt If we looked in the compare directory - we are likely to see AUC=1, i.e., perfect separation between the two dataset. However, it is not a problem, as it might use features that are not important to the model. Therefore, look in the file just in the compare.no_overfitting directory:</p> <ul> <li>Watch the\u00a0AUC_Mean.\u00a0</li> <li>We would like it to be close to 0.5.</li> <li>Higher AUC would reduce sample size when we match the samples to estimate performance, and smaller matched sample means bad accuracy.</li> <li>However often AUC is greater than 0.5, and we don't have a clear definition of what is bad:</li> <li> <p>0.99 is bad, for sure, but what about 0.8? 0.7? To see if we have statistically significant difference in one of the important feature, look at\u00a0compare_rep.txt in the compare.no_overfitting directory. However, we may see very high AUC without any significant difference in any specific feature (recall Mann Whitney limitations mentioned above) ... \u00a0 How to read\u00a0shapley_report.tsv and\u00a0features_diff To better understand the results of the separation model we have two more outputs:</p> </li> <li> <p>shapley_report.tsv is a standard butwhy report,\u00a0look in the file just in the compare.no_overfitting directory. Here you can see the feature importance in the separation model that uses just the important features of the model we want to apply to the dataset.\u00a0</p> </li> <li>Graphical representation of the differences for every feature can be found in the feature_diff directory:</li> <li>Look for anomalies in the graphs for the features with highest imprtance in the shapley report.</li> <li> <p>Always look at Age, as some other differences might be proxy to difference in Age distribution. Good separation, no matter what feature/features were used, would hurt the accuracy of model performance estimate by matching (ass later). However, we don't have a good measure to say by how much ...\u00a0\u00a0 \u00a0 Example AUC_Mean is 0.98 - very high. However no significant different seen in\u00a0compare_rep.txt\u00a0Mann Whitney test. In shapely_report one feature has very high importance. Next we look at the difference dataset-reference for this feature - comparing controls to controls.  We see that the reference has several dominant values - probably due to imputations. So how come the feature was important for the model?</p> </li> <li> <p>In this case we use as reference a dataset different from the one we use for model training and feature importance.\u00a0</p> </li> <li>However, the reference dataset has many missing values for the relevant signal.</li> <li>Lesson learned is that we need to use as reference the original dataset (test samples only). \u00a0 \u00a0</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%203%20%26%204%20-%20Compare%20Score%20Distribution.html","title":"Test 3 &amp; 4 - Compare Score Distribution","text":"<p>Overview One more test comparing the reference and the new dataset, this time from score distribution aspect.\u00a0</p> <p>Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run. In particular, this test uses:</p> <ul> <li>REFERENCE_MATRIX=... full path to reference matrix (feature matrix from model train)</li> <li>ALGOMARKER_PATH=... path to the algomarker model directory</li> </ul> <p>What is actually done? Statistics comparing two sets of scores:</p> <ul> <li>Moments</li> <li>Cutoffs</li> <li>Statistical test (KLD) Side check - make sure Algomarker scores are the same as expected using our model.</li> </ul> <p>Test Results Review Test 3:</p> <ul> <li>Raw view - Numerical (compare/score_dist.tsv) and Graphical (compare/score_dist.html) histogram of the two distributions.</li> <li>Statistical analysis - compare Mean, STD, and PR in cutoff (0.5-5%, every 0.5%), for the two distributions, see\u00a0compare/compare_score.txt.</li> <li>Check test scores - are they the same when we re-run the model?<ul> <li>In general it should be exactly the same. Thus, statistics for Test_Run and Test_Run.Original in compare/compare_score.txt should be exactly the same.</li> <li>However, in LGI, due to historical reasons, we would see minor differences.</li> </ul> </li> <li>Another check for test_scores is presented in the test log - 03.compare_scores.log. It prints Pearson and Spearman correlations between AlgoMarker scores and our infrastructure calculation. Test 4:</li> <li>04.calc_score_kld.log: Kullback-Leibler Divergence diff,\u00a0KLD_to_Uniform,\u00a0entory_p (no detailed description here as it seems that\u00a0\u00a0Test 4 benefit over Test 3 is doubtful). How to read the results? See example next, left to right/down: Statistical summary, histograms, and\u00a0  First and simple, Test_Run and Test_Run.Original are very close. That's OK. However, we see significant differences between the test run (Orange) and the reference (Blue). It looks like the Age distribution can explain the difference, as the new dataset (Green line) is older than the reference (Blue line). However, the importance of Age might limit our ability to recognize other issues.</li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%205%20-%20Sex%20Ratio.html","title":"Test 5 - Sex Ratio","text":"<p>Overview Here we compare score distribution to check sex fairness, and similarity to reference\u00a0\u00a0</p> <p>Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run. In particular, this test uses:</p> <ul> <li>SCORE_MIN_RANGE, SCORE_MAX_RANGE - range of score to inspect, will jump with 0.01 between scores</li> <li>Assumes test 02 completed and we have \"$WORK_DIR/compare/rep_propensity_non_norm.matrix\",\"$WORK_DIR/compare/rep_propensity.matrix\", \"$WORK_DIR/compare/test.preds\", \"$WORK_DIR/compare/reference.preds\"\u00a0Alon - we have not discussed those in test 2, what are they and are they important to mention at all?</li> </ul> <p>What is actually done? Compare results (reference vs new dataset) by sex and positive rate. \u00a0 How to read the results? The following is example of results for Test 5 (see\u00a005.sex_ratio.log) For each score cutoff in range, we see:</p> <ul> <li>The flagged males ratio in this cohort VS reference. . For instance, we can see here:<ul> <li>Male ratio going down when cutoff goes up, both for reference and test.</li> <li>However, the values are higher for the tested dataset, probably because of overall bigger share for males (need to verify).</li> </ul> </li> <li>Positivity rate, same as we saw in Test 3. </li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%206%20-%20Special%20Groups.html","title":"Test 6 - Special Groups","text":"<p>Overview Here we compare score distribution over special groups checking model ability to highlight high risk populations\u00a0\u00a0</p> <p>Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run. In particular, this test uses:</p> <ul> <li>configs/coverage_groups.py - definition of risk groups</li> <li>Assumes test 02 completed</li> </ul> <p>What is actually done? Compare results (reference vs new dataset) in special group, where rate of positive is expected to be high. \u00a0 How to read the results? Important: In the reference, we don't have missing values (-65336), but in the new dataset feature matrix we have. Until fixed, be careful in setting the risk groups.\u00a0 The following is example of results for Test 6 (see\u00a006.coverage.log) First we see that the size of the cohort is small, especially in the reference (just 104 patients). Moreover, the prevalence is much smaller in the reference - 0.3% compared to 1.1%. The difference is high, and probably resulted from the age distribution differences. \u00a0 \u00a0 \u00a0 Next, for every positive rate 1, 3, 5, 10% (numbers below are example for 3%):</p> <ul> <li>The test calculates cutoff\u00a0in the tested dataset (0.1611)</li> <li>How many flagged? 1113, that are 29% of the cohort (and 11% of the flagged), so lift is 9.7 (29 / 3)</li> <li>In the reference, for the same 3% cutoff, just 1.7% would be flagged, including 44.2% of the cohort (who would be 9% of the flagged) =&gt; lift of 26.8 (44.2 / 1.7)\u00a0 Conclusions:</li> <li>Difference looks significant. Note that the large difference in the the 'lift per cutoff value' is influenced by the difference in incidence rate, but the differences between the tested dataset and the reference are evident from any aspect.</li> <li>If the model flag less from this cohort, then, from what cohort it flags more? more investigation is needed before we can come up with applicable lesson. </li> </ul>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%207%20-%20Compare%20Important%20Feature.html","title":"Test 7 - Compare Important Feature","text":"<p>Overview One more test exploring the differences between the current dataset, and the dataset that was used to train the model, from the important features aspect (and see also Test 2). \u00a0 Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run. In particular, this test uses:</p> <ul> <li>CMP_FEATURE_RES=... list of important features for the model, with resolution\u00a0</li> </ul> <p>What is actually done? Several test and graphs per important feature. \u00a0 Test Results Review In the log file 07.feature_analysis.log, for every feature, we get something like the following (but note that we need to check the the similarity measure (KLD).\u00a0:  In features_stat.tsv, we get similar results, without separation male/female and without similarity measures, but with missing value rate in the test.  Last, we have graphical histogram (with the defined resolution above) in the directory features_graphs. \u00a0 How to read the results? This test repeats part of Test 2, in a slightly difference way, with separation male/female. Thus, when we see differences in a specific feature in Test 2, we can use the output here to further investigate the gaps.</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%208%20-%20Estimate%20Performances.html","title":"Test 8 - Estimate Performances","text":""},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%208%20-%20Estimate%20Performances.html#overview","title":"Overview","text":"<p>Estimate performance on tested dataset based on matching important factors. \u00a0 Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run. In particular, this test uses:</p> <ul> <li>REFERENCE_MATRIX=... reference matrix to compare with</li> <li>CMP_FEATURE_RES=... list of important features</li> </ul> <p>What is actually done? Sample the reference cohort based on the distribution of the important features in the tested dataset, and calculate predicted AUC. Bootstrap the matched reference to estimate accuracy. \u00a0 Test Results Review compare.no_overfitting/summary_table.estimated_performance.tsv gives the bottom line, for example:  Detailed bootstrap analysis can be found in\u00a0compare.no_overfitting/bt_reference.estimated.pivot_txt **How to read the results? In the example above, we estimate expected drop in AUC, and maybe more significant, larger confidence interval for the tested data set (column D). To decrease the\u00a0confidence interval we could decrease the number of important features. That would yield better separation in Test 3 and smaller confidence interval here. However, it is just number games. In the example here, the important features covers ~80% of feature importance in the original model. We don't have yet thumb rule for the right number though. More work regarding model performance is needed ... \u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%209%20-%20Lab%20Frequency.html","title":"Test 9 - Lab Frequency","text":"<p>Overview One of the major difference between dataset, and health systems, is rate of lab samples. Here we explore lab frequency for the signal behind the important features. \u00a0 Parameters (see env.sh) All parameters are included in env.sh and described in\u00a0External Silent Run.</p> <ul> <li>CMP_FEATURE_RES=... list of important features</li> </ul> <p>What is actually done? For every signal relevant to an important feature, create value counts table - how many patients have n labs from this type. \u00a0 Test Results Review Results, separate signal.tsv per relevant signal, are located in\u00a0signals_cnt\u00a0 \u00a0 How to read the results It is difficult to learn from the results without a reference.\u00a0</p>"},{"location":"Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Tips%20and%20Scripts.html","title":"Tips and Scripts","text":"<p>How to see html graphs ? The External Silent Run generates several graphs in html format;</p> <ul> <li>In the docker, run files_folder_server.py, and open the html from the browser</li> <li>If you downloaded the results to directory DIR in our environment,\u00a0run /nas1/Temp/LGI/fix_plotly.sh from DIR \u00a0</li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html","title":"Using the Flow App","text":""},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#overview","title":"Overview","text":"<p>The Flow App is a versatile tool with multiple switches, each designed to perform a specific action. Below are its key functionalities:</p> <ul> <li>Load New Repository: Converts raw ETL output files into an efficient, binary, and indexed format compatible with the AlgoMedical library framework.</li> <li>Train a model.</li> <li>Apply a model to generate predictions.</li> <li>Extract feature matrices from the model pipeline.</li> <li>Print specific patient data or signal distributions.</li> <li>Feature Importance with Shapley Values Analysis.</li> <li>Prepare Samples and Get Incidences Using Flow.</li> <li>Fit MedModel to Repository: Adjusts an existing model to fit a new repository. For instance, if a non-critical signal is missing, the \"fit\" operation generates a virtual empty signal to bypass errors, ensuring compatibility. The suggested changes can later be reviewed and validated or corrected. More information inside</li> </ul> <p>GitHub Repository: Flow App Code</p> <p>The Flow App is also compiled as part of AllTools. Refer to the Setup Instructions.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#flow-app-options","title":"Flow App Options","text":""},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#general-switches","title":"General Switches","text":"<ul> <li><code>--help</code>: Displays the full help menu.</li> <li><code>--help_</code>: Searches the help menu and displays only the relevant sections matching the search term.</li> <li><code>--rep</code>: Specifies the path to the repository.</li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#creating-repositories","title":"Creating Repositories","text":"<p>To create a repository using a convert configuration file, use the <code>--convert_conf</code> option:</p> <pre><code>Flow --rep_create --convert_conf ./ICU.convert_config\n</code></pre> <p>To create a by-pid transposed version of a repository, use the following command. This allows faster access to specific patient IDs (pids) and reduces memory consumption significantly:</p> <pre><code>Flow --rep_create_pids --rep ./ICU.repository\n</code></pre> <p>For more details on creating repositories, convert configuration files, and required inputs, refer to Load a New Repository.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#printing-pids-and-signals","title":"Printing PIDs and Signals","text":"<ul> <li>Print all records for all signals for a specific pid using the default API:</li> </ul> <pre><code>Flow --rep ./ICU.repository --printall --pid 200001\n</code></pre> <ul> <li>Print all records for all signals for a specific pid using the by-pid API (faster but requires a by-pid repository):</li> </ul> <pre><code>Flow --rep ./ICU.repository --pid_printall --pid 200001\n</code></pre> <ul> <li>Print all records for a specific signal and pid using the default API:</li> </ul> <pre><code>Flow --rep ./ICU.repository --print --pid 200001 --sig Sepsis\n</code></pre> <ul> <li>Print all records for a specific signal and pid using the by-pid API (faster but requires a by-pid repository):</li> </ul> <pre><code>Flow --rep ./ICU.repository --pid_print --pid 200001 --sig Sepsis\n</code></pre> <ul> <li>Print general statistics for a signal, such as sample counts, gender distribution, average samples per person, and more. This works only for <code>SDateVal</code> type signals and repositories containing <code>GENDER</code> and <code>BYEAR</code> signals:</li> </ul> <pre><code>Flow --rep ./thin.repository --describe --sig Creatinine\n</code></pre>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#training-a-model","title":"Training a Model","text":"<p>To train a model, you need the following inputs:</p> <ul> <li><code>REPOSITORY_PATH</code>: Path to the data repository.</li> <li><code>PATH_TO_TRAIN_SAMPLES</code>: Path to MedSamples, a TSV file defining labels for each patient and point in time.</li> <li><code>PATH_TO_JSON_WITH_MODEL_INSTRUCTIONS</code>: Path to the JSON file defining the model architecture. See Model JSON Format.</li> <li><code>PATH_TO_OUTPUT_TO_STORE_MODEL</code>: Path to save the trained model.</li> </ul> <p>Example command:</p> <pre><code>Flow --simple_train --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_json $PATH_TO_JSON_WITH_MODEL_INSTRUCTIONS --f_model $PATH_TO_OUTPUT_TO_STORE_MODEL\n</code></pre> <p>For cross-validation, use the <code>--train_test</code> mode switch. However, this is deprecated. Use the Optimizer instead.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#predictingapplying-a-model","title":"Predicting/Applying a Model","text":"<p>To apply a model, you need the following inputs:</p> <ul> <li><code>REPOSITORY_PATH</code>: Path to the data repository.</li> <li><code>PATH_TO_TRAIN_SAMPLES</code>: Path to MedSamples, defining requested prediction times for each patient. The outcome column is not used during testing.</li> <li><code>PATH_TO_TRAINED_MODEL_BINARY_FILE</code>: Path to the stored model.</li> <li><code>OUTPUT_PATH_TO_STORE_SAMPLES</code>: Path to save the predictions. The output will include a <code>pred_0</code> column in the MedSamples file for each requested prediction date.</li> </ul> <p>Example command:</p> <pre><code>Flow --get_model_preds --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_model $PATH_TO_TRAINED_MODEL_BINARY_FILE --f_preds $OUTPUT_PATH_TO_STORE_SAMPLES\n</code></pre> <p>Pre-processors can be added to the beginning of the model pipeline to manipulate raw signals before they are fed into the model. This allows you to perform operations that don't require training or storage in the model itself, such as simulating the removal or limitation of a specific signal. For more details, see Using Pre Processors</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#creating-a-feature-matrix-for-samples","title":"Creating a Feature Matrix for Samples","text":"<p>To create a feature matrix, use the same inputs as for predicting/applying a model. The output will be a CSV file containing the feature matrix:</p> <pre><code>Flow --get_mat --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_model $PATH_TO_TRAINED_MODEL_BINARY_FILE --f_matrix $OUTPUT_PATH_TO_STORE_MATRIX\n</code></pre> <p>To inspect the training matrix directly from the model JSON, use the following command. The inputs are the same as for model training, but the output is a matrix instead of a model:</p> <pre><code>Flow --get_json_mat --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_json $PATH_TO_JSON_WITH_MODEL_INSTRUCTIONS --f_matrix $OUTPUT_PATH_TO_STORE_MATRIX\n</code></pre>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/index.html#print-trained-model-information","title":"Print trained model information","text":"<p>To inspect model pipeline:</p> <p><pre><code>Flow --print_model_info --f_model $MODEL \n</code></pre> You can add <code>--print_json_format 1 --f_output $OUTPUT_JSON</code> and set <code>OUTPUT_JSON</code> to output path with a more detailed information about the model. It is not exactly a json format, but this is textual.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html","title":"Fitting a MedModel to a Repository","text":"<p>The Flow app provides an option called <code>fit_model_to_rep</code>. You can use it as follows:</p> <pre><code>Flow --fit_model_to_rep [other arguments]\n# See all arguments with: Flow --help_ fit_model_to_rep\n</code></pre>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#overview","title":"Overview","text":"<p>The purpose of this tool is to take a repository path and a model path as input, and output a new, adjusted model that fits the repository at a specified output path.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#key-considerations","title":"Key Considerations","text":"<ol> <li>Signal names may change over time (e.g., GENDER \u2194 SEX, DIAGNOSIS \u2194 ICD9_Diagnosis, RC). These changes can cause compatibility issues.</li> <li>Sometimes, models require signals that do not exist in the repository. Rather than treating missing signals as empty (which can be misleading), MedModel enforces strict checks to avoid confusion.</li> <li>This tool outputs list of \"suggested\" changes in the model for your review and creates the new adjusted model.</li> </ol>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#workflow","title":"Workflow","text":"<ol> <li>The tool reads both the model and the repository, checking for required signals that are missing from the repository.</li> <li>For each missing signal, it attempts to resolve the issue by adding or removing a <code>rep_processor</code> in the model, following a set priority. It starts by search an alternative signal name (for example, if SEX signal is missing and we have GENDER it will use GENDER as SEX, otherwise it will create an emoty signal to mark this signal as missing).</li> <li>It validates all categorical codes used by the model to ensure they are present in the repository.</li> <li>The adjusted model is saved to the specified output file.</li> <li>A log of all transformations made to the model is printed to a file or the screen.</li> <li>A log of any missing codes for each signal is also printed to a file or the screen.</li> <li>If all codes are present and all adjustments are successful, the process completes without errors.</li> </ol>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#example-usage","title":"Example Usage","text":"<pre><code>Flow --fit_model_to_rep \\\n  --f_model /nas1/Work/Users/Eitan/Lung/outputs/models2023/EX3/model_63/config_params/exported_full_model.final.medmdl \\\n  --rep /nas1/Work/CancerData/Repositories/THIN/thin_2021.lung2/thin.repository \\\n  --f_output /tmp/1.mdl \\\n  --log_action_file_path /tmp/actions.log \\\n  --log_missing_categories_path /tmp/categ.log \\\n  --cleaner_verbose -1 \\\n  --allow_virtual_rep 0\n</code></pre>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#argument-descriptions","title":"Argument Descriptions","text":"<ul> <li><code>log_action_file_path</code>: Logs all changes made to the model. If not provided, output is printed to the screen.</li> <li><code>log_missing_categories_path</code>: Logs all missing codes for signals. If not provided, output is printed to the screen.</li> <li><code>cleaner_verbose</code>: Controls verbosity of outlier reporting. Use <code>1</code> for production (verbose), <code>-1</code> for validation (no verbose, faster).</li> <li><code>allow_virtual_rep</code>: Allows use only a repository definition without data (like we have in AlgoMAkrer) for testing model fitting. Some adjustments may be limited, because we don't use actual data.</li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#sample-output","title":"Sample Output","text":"<pre><code>Signal BUN, median value is 34.200001 in repository\nwrite_to_file [/tmp/1.mdl] with crc32 [273944850]\nread_binary_data_alloc [/tmp/1.mdl] with crc32 [273944850]\nread_from_file [/tmp/1.mdl] with crc32 [273944850] and size [193041186]\ncategorical signal ICD9_Diagnosis is OK\ncategorical signal Smoking_Status is OK\nAll categorical signals are OK!\nAll OK - Model can be applied on repository\n</code></pre> <p>In this example, the model uses Urea, but the repository has BUN. The tool finds the median value of BUN, writes the adjusted model, and verifies it. The output confirms that all categorical signals are valid and the model can be applied.</p> <p>If all codes exist, the <code>/tmp/categ.log</code> file will be empty. If codes are missing, the file will list the signal name and missing categorical value (tab-delimited).</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#example-action-log","title":"Example Action Log","text":"<pre><code>REMOVED_RENAME_FROM_TO  SEX     GENDER\nCONVERT_SIGNAL_TO_FROM_FACTOR   Urea    BUN     0.467290\nEMPTY_SIGNAL    RDW\n</code></pre> <ul> <li>The first line indicates that the model used \"SEX\", but the repository has \"GENDER\". The processor converting GENDER to SEX was removed, so the model now accepts GENDER input signal.</li> <li>The second line shows that a virtual signal \"Urea\" was created from \"BUN\" by multiplying by 0.467290.</li> <li>The third line indicates that \"RDW\" was missing from the repository, so an empty virtual signal was</li> </ul> <p>This is valid transformation of some breaking changes we had in our signals/repositories during the years + empty RDW signal which is acceptable in that case.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html","title":"Split Files","text":"<p>A split file contains a list of pids split into several splits. Keeping the split in a file allows using the exact same split over several runs, or between stages such as creating a matrix with completions and modeling it. Pids not appearing in the split file should always be assumed to not be used. \u00a0</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html#format","title":"Format","text":"<p>Comment lines start with # . File is tab or space delimited. first line: NSPLITS  Followed by tupples:"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html#the-medsplit-class","title":"The MedSplit Class","text":"<p>Defined in MedFeat/MedOutcome.h , this class contains the basic tools to create splits, and read/write them to files.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html#create-random-splits-for-patients","title":"Create Random Splits for Patients","text":"<p>Inputs:</p> <ul> <li>REPOSITORY_PATH - you will need data to retreive all patients</li> <li>SPLIT_NUMBER - fill in the nubmer of splits you want to create</li> <li>OUTPUT_PATH - output file to store the output split file in format  <pre><code>Flow --create_splits \"nsplits=$SPLIT_NUMBER\" --rep $REPOSITORY_PATH --f_split $OUTPUT_PATH\n</code></pre>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html","title":"Using Flow To Prepare Samples and Get Incidences","text":"<p>The Flow app contains useful tools to choose samples for a problem given a cohort, to filter the samples, and to match them (say for calendar year). These are powerful and useful tools needed to be done before preparing samples files for training/validation , or when needing to match populations on some parameters. It also contains options to estimate incidences and get incidence files that can be used later for bootstrap analysis. At the moment these are written and verified for the date case... and should be generalized and tested for in patient modes and other time scales.</p>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#cohort-files","title":"Cohort Files","text":"<p>Cohort files contain one line per pid , and define the time the patient entered the cohort, the time it left it, and if it has an outcome also the the oucome time and its value. This is a full definition of all the patients we want to use for training and testing and the times they are eligible for using. The format of a cohort file is:</p> <ul> <li>tab delimited</li> <li>lines starting with '#' are comments</li> <li>the following 5 fields in each line:<ul> <li>pid : (only one like per pid is supported at the moment)</li> <li>entry date to cohort</li> <li>end date in cohort</li> <li>outcome date : in binary cases for 0 outcomes (controls) it will be the same as end date, and for 1 outcomes it will be the actual event date which must be &lt;= the end date.</li> <li>outcome : typically a 0/1 control/case outcome, but can be also a regression value, or a multicategory value, but currently only a single value (should be generalized to an outcome vector in the future). Example for a few lines of a cohort file: Cohort file example <pre><code># note in the following sample: pid 5000009 entered the cohort on 20060505, left on 20091023 , but got a 1.0 (case) outcome at 20091023\n5000009 20060505        20091023        20060605        1.000000\n# in the following sample: pid 5000014 is a control (0.0 outcome) that entered the cohort on 20150107 and left it on 20160929\n5000014 20150107        20160929        20160929        0.000000\n5000017 20110826        20160819        20160819        0.000000\n5000020 20140731        20161117        20161117        0.000000\n5000025 20141125        20160916        20160916        0.000000\n5000027 20060914        20161115        20161115        0.000000\n5000040 20080422        20160223        20160223        0.000000\n5000042 20060522        20140910        20140910        0.000000\n5000043 20080613        20100609        20100609        0.000000\n5000044 20110114        20160510        20150109        1.000000\n5000059 20120501        20120515        20120515        0.000000\n5000061 20140408        20160425        20160425        0.000000\n5000073 20100219        20160422        20160422        0.000000\n5000077 20090915        20111109        20111109        0.000000\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#creating-a-sample-file-given-a-cohort-file","title":"Creating a Sample file given a cohort file","text":"<p>The Flow line to use is: Flow --rep  --seed  --cohort_fname  --cohort_sampling  --out_samples  All parameters are self explanatory except cohort_sampling , which we explain below: cohort_sampling contains a rich list of parameters to decide how to create samples (for control and cases) using the cohort: which dates to sample, continous or on-test, frequency of sampling, time window relative to end date and/or outcome date, and more. Follows is a description of the options and their defaults <ul> <li>min_control_years : (0) : minimal number of years for sampling before outcome for controls , controls are always those for which outcome == 0 , can be a float number (0.5 year etc)</li> <li>max_control_years : (10) : maximal number of years for sampling before outcome for controls</li> <li>min_case_years : (0) : minimal number of years for sampling before outcome for cases , cases are always those for which outcome\u00a0!= 0</li> <li>max_case_years : (1) : maximal number of years for sampling before outcome for cases</li> <li>is_continous : (1) : continous mode of sampling vs. stick to (0 = stick) (stick is on test)</li> <li>min_days_from_outcome : (30) : minimal number of days before outcome to sample</li> <li>jump_days : (180) : days to jump between sampling periods, meaning a sample will be randomly selected each jump_days period</li> <li>min_year : (1900) : min year for sampling</li> <li>max_year : (2100) : max year for sampling</li> <li>gender_mask : (3) : mask for gender specification (rightmost bit on for male, second for female) : 1 - only males , 2 - only females , 3 - both</li> <li>int train_mask : (7) : mask for TRAIN-value specification (three rightmost bits for TRAIN = 1,2,3)</li> <li>min_age : (0) : minimum age for sampling</li> <li>max_age : (200) : maximum age for sampling</li> <li>stick_to_sigs : : a list of signals (, separated) : only use time points with at least one of the given signals</li> <li>take_closest \u00a0: (0) : flag: take the sample with stick signals that is closest to each target sampling-date</li> <li>take_all : (0) : flag: take all samples with stick signal within each sampling period\u00a0</li> <li>max_samples_per_id : (2^31-1) : maximal number of samples per id</li> <li>max_samples_per_id_method : ('last') : 'last' or 'rand'. 'last' picks the last\u00a0\u00a0max_samples_per_id samples, while 'rand' chooses them randomly \u00a0 Creating samples from cohort examples <pre><code># choose continous sampling, a random sample in each 180 days window,\u00a0\n# for controls choose from 1 year before end of cohort up to 10 years before\n# for cases choose from outcome date up to 2 years before\n# choose only for cases with TRAIN=1 , and in ages 35-90 at the time of sampling\nSAMPLING_PARAMS1=\"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=1;min_age=35;max_age=90\"\n\u00a0\n# same as the above but on-test case and taking only samples at the days either a Glucose or HbA1C test was made\nSAMPLING_PARAMS1=\"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=1;min_age=35;max_age=90;is_continous=0;stick_to_sigs=Glucose,HbA1C\"\n\u00a0\n# actual command line : replace SAMPLING_PARAMS with one of the options above\nFlow --rep /home/Repositories/THIN/thin_mar2017/thin.repository --seed 123 --cohort_fname ./pre2d.cohort --cohort_sampling ${SAMPLING_PARAMS} --out_samples ./temp.samples\n</code></pre> </li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#filter-and-match-options","title":"Filter and Match options","text":"<p>This Flow option allows one to start from a samples file (typically one that was created using the cohort_sampling method explained above) , apply filters and/or matching procedures on it and create a new samples file. The Flow line to use is: Flow --rep  --seed  --filter_and_match --in_samples  --out_samples \u00a0--filter_params  --match_params  This run will run first the filtering (if given), and then the matching (if given) Again : all params are obvious besides filter and match params, explained below:"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#filter-params","title":"Filter params","text":"<p>Filtering options allow for taking samples only within a defined dates range, and also filter using ranges of signals in a window before the time point (say take only time points with Creatinine values below 1.1 in the 2 years before the time point, etc...)</p> <ul> <li>min_sample_time : (0) :minimal allowed time (should always be given in the samples' time-unit, in our case typically date)</li> <li>max_sample_time : ((1&lt;&lt;30)) : maximal allowed time (should always be given in the samples' time-unit : in our case typically date)</li> <li>win_time_unit: (\"Days\") : \u00a0///&lt; time unit to be used in bfilter windows</li> <li>bfilter : : a filter that checks conditions on a signal. Several filters can be defined. Parameters for filters (',' separated rather than ';') are:<ul> <li>sig_name : : Name of signal to filter by</li> <li>win_from : (0) : Time window for deciding on filtering - start (relative to sample time, and going backwards)</li> <li>win_to : ((1&lt;&lt;30)) : Time window for deciding on filtering - end</li> <li>min_val : (-1e10) : Allowed values range for signal - minimum</li> <li>max_val : (1e10) : Allowed values range for signal - maximum</li> <li>min_Nvals (1) : Required number of instances of signal within time window</li> <li>time_channel (0) : \u00a0signal time-channel to consider</li> <li>val_channel (0) : signal value channel to consider</li> </ul> </li> <li>min_bfilter : how many bfilters need to pass in order to take sample (default : all) , this allows for example to define a choice of one out of several signals to be before the sample and not all of them. \u00a0 Filter params examples <pre><code># take only samples in the range 20070101 - 20150101\nFILTER1=\"min_sample_time=20070101;max_sample_time=20150101\"\n\n# take only samples that have at least 1 Creatinine test up to 2 years before\n# note that we use , and not = when stating the bfilter params. The = is kept for the upper level.\nFILTER2=\"bfilter=sig,Creatinine,win_from,0,win_to,730,min_Nvals,1\"\n\u00a0\n# same as FILTER2 but also force all Creatinine tests to be below 0.9\nFILTER3=\"bfilter=sig,Creatinine,win_from,0,win_to,730,min_Nvals,1,min_val=0,max_val=0.9\"\n\n# combined: range 20070101-20101201 , At least one Glucose in last 2Y, all Glucose tests in last 5Y below 100, all HbA1C tests in last 5Y below 5.7\nFILTER4=\"min_sample_time=20070101;max_sample_time=20101201;bfilter=sig,Creatinine,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,Creatinine,win_from,0,win_to,1825,min_val,0,max_val,100;bfilter=sig,HbA1C,win_from,0,win_to,1825,min_val,0,max_val,5.7\"\n</code></pre> </li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#matching-params","title":"Matching params","text":"<p>The matching option allows to take a MedSamples , and apply matching methods that make sure that the ratio between case and control is kept in various stratas. For example: one could match for years, which means that in each calendar year we will seek to have the same ratio of cases to controls, this when given as a training samples file will make the predictor \"blind\" to the calendar year, and learn features not correlated with it. Another example : we could match for Gender, or Age, or a value of some signal , or a combination of those, which will define stratification bins, and we will seek to find a subset of the input samples that has the same ratio od cases vs controls in all those bins. The algorithm used is trying to find an optimal solution in the sense of leaving the maximal number of samples possible after the match. However it does so in a weighted manner between cases and controls, we can give a weight W signaling that we are willing to seek the best solution under the assumption that we allow to throw W controls instead of a single case. This helps keeping the cases in the matched samples when we have a low proportion of cases.</p> <ul> <li>priceRatio : (100.0) : the weight ratio: how many controls are we willing to lose for each case (good guess for this number would be at the order of n_controls/n_cases in the input samples)</li> <li>maxRatio : (10.0) : if the optimal ratio found by the matching algorithm is larger than maxRatio, we will sample less (enriching cases vs. control even more)</li> <li>verbose: (0) : get more prints from algorithm (recommended...)</li> <li>match_to_prior - specify the target prior directly</li> <li>strata : : add stratification criterias (':' delimited between ), strata parameters (',' delimited) are:<ul> <li>type: one of time , age , gender , signal</li> <li>signalName :\u00a0<ul> <li>for time type: year , month, days , etc</li> <li>for age : none</li> <li>for gender : none</li> <li>for signal : the signal name (Creatinine, Glucose, etc)</li> </ul> </li> <li>resolution : size of bin to stratify with Match params examples <pre><code># change price and max ratio, run verbose\n# add matching and stratification by years\nMATCH1=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=time,year,1\"\n# add matching and stratification by years to certain prior - in this example 0.1 which is 10%\nMATCH1=\"match_to_prior=0.1;maxRatio=4.5;verbose=1;strata=time,year,1\"\n# same with gender stratification\nMATCH2=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=gender\"\n\u00a0\n# same with age in bins of 5 years\nMATCH3=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=age,5\"\n\u00a0\n# same : combining all 3 previous stratas, matching for all of them together\nMATCH4=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=age,5:time,year,1:gender\"\n\u00a0\n# same : match by Glucose values in bin of 10 and HbA1C values in bins of 1.0\nMATCH5=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=signal,Glucose,10:signal,HbA1C,1.0\"\n\u00a0\n</code></pre> more details in here: MatchingSampleFilter</li> </ul> </li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#get-incidence-for-a-cohort","title":"Get Incidence for a cohort","text":"<p>In order to get an incidence file for a cohort one can use the following Flow line: Flow --rep  --cohort_incidence \"from_year=2007;to_year=2014;from_age=40;to_age=80;age_bin=40;incidence_days_win=1825\" --cohort_fname  --cohort_incidence  --out_incidence \u00a0--censor_reg  --use_kaplan_meir 1\u00a0 The\u00a0cohort_fname and\u00a0censor_reg are MedRegistry Objects. there is simple convert command from MedCohort to MedRegistry/ convert command: <pre><code>cat &lt;cohort file&gt; | awk '{ if ($NF &gt; 0) { print $1 \"\\t\" $2 \"\\t\" $4 \"\\t\" \"0\";  print $1 \"\\t\" $4 \"\\t\" $3 \"\\t\" \"1\" } else { print $1 \"\\t\" $2 \"\\t\" $3  \"\\t\" \"0\" } }' #Creates MedRegistry from MedCohort\ncat &lt;cohort file&gt; | awk '{ if ($NF &gt; 0) { print $1 \"\\t\" $2 \"\\t\" $4 \"\\t\" \"1\" } else { print $1 \"\\t\" $2 \"\\t\" $3  \"\\t\" \"1\" } }' #creates Censor registry from MedCohort\n</code></pre>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#incidence-parameters-provided-in-cohort_incidence-argument-with","title":"Incidence parameters (provided in cohort_incidence argument with \";\")","text":"<ul> <li>age_bin : size of incidence age bins (typical: 5)</li> <li>min_samples_in_bin : too small bins will be unified with bins near them</li> <li>from_year : year to start collecting numbers from</li> <li>to_year : year to end collection</li> <li>start_date : date in year to test : mmyy without trailing 0 , for example 508 is may 8th , 1201 is December 1st</li> <li>gender_mask : as usual 2 bits</li> <li>train_mask : as usual 3 bits</li> <li>from_age : ages to start counting</li> <li>to_age : ages to end counting</li> <li>incidence_years_window : how many years a head to calculate incidence for</li> <li> <p>incidence_days_win : if given trumps years : how many days ahead to calculate incidence for \u00a0 Incidence params example <pre><code># calculate incidence on years 2007 to 2010 (averaged over years) , for 1 year ahead (annual) only on TRAIN=1, age bins of 5 and test date in each year : June 2nd\nINC_PARAMS=\"train_mask=1;age_bin=5;start_date=602;incidence_years_window=1;from_year=2007;to_year=2010\"\n</code></pre> \u00a0 Control\u00a0 Sampling Args directly by\u00a0 providing\u00a0\"\u2013sampler_params\":</p> </li> <li> <p>start_year,end_year or start_time,end_time as full time/date</p> </li> <li>prediction_month_day as prediction date</li> <li>time_jump/day_jump - jump between prediction dates</li> <li>time_jump_unit - the jump time unit. for example Day, Year</li> <li>time_range_unit - the time range unit - Date on Minutes \u00a0</li> </ul>"},{"location":"Medial%20Tools/Using%20the%20Flow%20App/Using%20Pre%20Processors.html","title":"Using Pre Processors","text":"<p>Given a trained model , one may need to apply some additional rep processors before the model is applied.\u00a0 A classical example is : the model was trained without history limits on the signals, and one needs to test the results when limiting the signals (or some of them) to say only 1 year of history.</p> <p>The way to do that is add a pre processor at apply time to the model.</p> <p>This can be done using the Flow --get_model_preds option and adding the pre processors using the --f_pre_json parameter</p> <p>Example:</p> <p>A pre processor json file that limits histrory:</p> <pre><code>{\n        \"pre_processors\" : [ {\"rp_type\" : \"history_limit\" , \"signal\" : \"ref:signals\", \"win_from\" : \"0\" , \"win_to\" : \"365\"} ] ,\n        \"signals\" : [\"Hemoglobin\", \"MCV\", \"MCH\"]\n}\n</code></pre> <p>A Flow get prediction example using pre processors</p> <pre><code>Flow --get_model_preds --rep myrep.repository --f_samples test.samples --f_model trained.model --f_preds out.preds --f_pre_json pre.json\n</code></pre>"},{"location":"Medial%20Tools/bootstrap_app/index.html","title":"bootstrap_app","text":"<p>This is a user manual page. For list of frozen versions and updates see\u00a0* ** this is a bootstrap application for running bootstrap (which uses MedBootstrap Library). The code is availbe under: $MR_ROOT/Tools/bootstrap_app I have also created alias for the bootstrap_app named \"bootstrap\" so you can simply type \"bootstrap --help\" or run bootstrap without providing the full path in linux* \u00a0 <pre><code>./bootstrap_app --help\n</code></pre> Program options:</p> <ul> <li>--help help &amp; exit</li> <li>--h help &amp; continue</li> <li>--base_config arg config file with all arguments - in CMD we override those settings</li> <li>--rep arg the repository if needed for age\\gender cohorts</li> <li>--input arg the location to read input</li> <li>--input_type arg (=samples) the input type (samples,samples_bin,features,medmat_csv,features_csv)</li> <li>--output arg the location to write output</li> <li>--use_censor arg (=1) if true will use repository censor</li> <li>--incidence_file arg the location to load incidence file</li> <li>--json_model arg json model for creating features for the filtering of cohorts in bootstrap</li> <li>--registry_path arg\u00a0the registry from_to path to calc\u00a0accurate incidence by samplings (slower)</li> <li>--incidence_sampling_args arg\u00a0(=start_year=2007;end_year=2014;day_jump=365;conflict_method=all;use_allowed=1) \u00a0the init argument string for theMedSamplingAge for the sampling. has default value</li> <li>--nbootstrap arg (=500) bootstrap loop count</li> <li>--sample_per_pid arg (=1) num of samples to take for each patient. 0 - means take all samples for patient</li> <li>--sample_pid_label sample on each pid and label</li> <li>--sample_seed arg (=0) seed for bootstrap sampling</li> <li>--whitelist_ids_file arg file with whitelist of ids to take</li> <li>--blacklist_ids_file arg file with blacklist of ids to remove</li> <li>--sample_min_year arg (=-1) used for filtering samples before that year. when sample_min_year &lt; 0 will not filter anything</li> <li>--sample_max_year arg (=-1) used for filtering samples after that year. when sample_max_year &lt; 0 will not filter anything</li> <li>--cohorts_file arg cohorts definition file</li> <li>--do_autosim to do auto simulation - requires min_time,max_time</li> <li>--min_time arg min_time for autosim</li> <li>--max_time arg max_time for autosim</li> <li>--score_resolution arg (=9.99999975e-05) score bin resolution rounding for speed up. put 0 to no rounding</li> <li>--score_bins arg (=0) score bin count for speed up. put 0 to no use</li> <li>--max_diff_working_point arg (=0.0500000007) the maximal diff in calculated working point to requested working point to put missing value</li> <li>--force_score_working_points force using scores as working points</li> <li>--working_points_sens arg sens working point - list with \",\" between each one. in percentage 0-100</li> <li>--working_points_fpr arg fpr working point - list with \",\" between each one. in percentage 0-100</li> <li>--working_points_pr arg pr working point - list with \",\" between each one. in percentage 0-100</li> <li>--output_raw flag to output bootstrap filtering of label,score to output</li> <li>--use_splits flag to perform split-wise analysis in addition to full_data</li> <li>--sim_time_window\u00a0flag to treat cases as controls which\u00a0are not in time window and not censor them</li> <li>--debug set debuging verbose \u00a0 example run: \u00a0 <pre><code>bootstrap_app --base_config /nas1/Work/Users/Alon/UnitTesting/examples/bootstrap_app/bootstrap_example.cfg\n</code></pre> \u00a0 \u00a0 the\u00a0bootstrap_example.cfg contains all program arguments in \"ini\" file format - parameter_name = parameter_value you may not provide this file or override all parameter with command arguments. The\u00a0bootstrap_example.cfg content is: \u00a0 bootstrap_example.cfg<pre><code>#The repository path:\nrep = /home/Repositories/THIN/thin_jun2017/thin.repository\n#The MedSampels input or if provding other input type we can use input_type paramter\ninput = /server/Work/Users/Alon/UnitTesting/examples/bootstrap_app/validation_samples.preds\n#where to write output path:\noutput = /tmp/bootstrap_test\n#the json model to create additional features for cohort filtering by cohorts_file\njson_model = /server/Work/Users/Alon/UnitTesting/examples/bootstrap_app/model_stats.json\n#the json model content (without the leading \"#\" before each line):\n#{\n#   \"processes\":{\n#      \"process\": {\n#         \"process_set\": \"0\",\n#         \"rp_type\": \"basic_cln\",\n#         \"take_log\": \"0\",\n#         \"range\": \"range_min=0.001;range_max=100000\",\n#         \"signal\": [\"Glucose\",\"HbA1C\",\"BMI\"]\n#      },\n#      \"process\":{\n#         \"process_set\":\"0\",\n#         \"fg_type\":\"basic\",\n#         \"type\":[\"last\",\"max\",\"last_time\"],\n#         \"window\":[\"win_from=0;win_to=10000\"],\n#         \"time_unit\":\"Days\",\n#         \"signal\":[\"Glucose\",\"HbA1C\",\"BMI\"]\n#      }\n#   }\n#}\n#cohorts defintion file:\ncohorts_file = /server/Work/Users/Alon/UnitTesting/examples/bootstrap_app/bootstrap_new.params\n#The file has those lines (without the leading \"#\" before each line):\n#TimeWindow 0-365 &amp; age 40-80 Time-Window:0,365;Age:40,80\n#TimeWindow 0-365 All Ages Time-Window:0,365\n#TimeWindow 0-365 with glucose &lt; 110 Time-Window:0,365;Glucose.last.win_0_10000:0,110\n##This line will create all the options for time windows: 0-30,30-180,180-365 with ages 40-60,60-80,40-80 for both male,females\n#MULTI Time-Window:0,30;Time-Window:30,180;Time-Window:180,365 Age:40,60;Age:60,80;Age:40,80 Gender:1,1;Gender:2,2\n</code></pre> For exact format of cohorts_file please reffer to MedBootstrap wiki page or doxygen. \u00a0</li> </ul>"},{"location":"Medial%20Tools/bootstrap_app/index.html#fixing-incidence-with-incidence-file","title":"Fixing incidence with incidence file","text":"<p>It calculates the average incidence in your cohort based on sex, age group and count of patients in this group.\u00a0Then to calculate ppv: it multiplies the sensitivity by the incidence\u00a0- sensitivity average incidence. it divides it with sensitivity average incidence + fpr * (1 - average incidence).\u00a0Equivalent to give this weight for cases: average incidence * total cohort / total casesWeight for control as (1- average incidence)*total cohort / total controls .\u00a0Sanity test: if model is random, results in sensitivity = fpr, results in ppv = incidenceThe bootstrap program also assesses what would happen to the incidence by other filters of the bootstrap cohort besides (sex, age). For example, taking anemic patients would cause bias toward cases and increase the incidence driven from global population SEER.\u00a0It assumes that the effect on stratified outcome,age,sex within input samples and the SEER samples(Or generated samples yearly) is the similar - by measuring lift in odds ratio between after applying the bootstrap filters and before the filters. An example for Incidence file format may be seen here (The file can be created via Flow App): \u00a0 <pre><code>head /nas1/Work/Users/Alon/UnitTesting/examples/bootstrap_app/pre2d_incidence_thin.new_format\n</code></pre> <pre><code>AGE_BIN 3\nAGE_MIN 21\nAGE_MAX 90\nOUTCOME_VALUE   0.0\nOUTCOME_VALUE   1.0\nSTATS_ROW       MALE    21      1.0     5242\nSTATS_ROW       MALE    21      0.0     94758\nSTATS_ROW       FEMALE  21      1.0     5242\nSTATS_ROW       FEMALE  21      0.0     94758\nSTATS_ROW       MALE    24      1.0     5338\n</code></pre> \u00a0 the registry_path is a text format of\u00a0MedRegistry. look for the code documentation for more info in the write_text_file method. \u00a0</p>"},{"location":"Medial%20Tools/bootstrap_app/Bootstrap%20legend.html","title":"Bootstrap legend","text":"<p>The bootstrap result file is tab delimited file with 3 columns:</p> <ol> <li>Cohort - The name of the \"filter\" used to select and defined the cohort which the results are reffered to. For example \"All\" - means no filters, \"Males\" - only males, \"Age:45-80\", etc..\u00a0The name is configured in bootstrap cohort configuration file (--cohort_file argument) that maps name of cohort to set of filter rules, as described under:\u00a0bootstrap_app</li> <li>Measurement - The name of the measurement. For example \"AUC_Mean\" - which is the mean value of the AUC measured in the bootstrap analysis process.The name is divided into 2 sections - The metric that was measured in each bootstrap experiment and the statistical calculation over this measure from all experiments. For each metric we extract the following statistical values which appears in the suffix:</li> </ol> <ul> <li>_Mean - mean value of the metric from all bootstrap\u00a0experiments</li> <li>_Std - Standard Deviation of the metric from all bootstrap\u00a0experiments</li> <li>_CI.Lower.95 - The lower confidence interval of the metric\u00a0from all bootstrap\u00a0experiments. lowest 2.5% percentile.\u00a0</li> <li>_CI.Upper.95 - The higher confidence interval of the metric\u00a0from all bootstrap\u00a0experiments. highest 2.5% percentile.</li> <li>_Obs - The observed metric value when there is no bootstrap randomization - the calculated metric on the all data where each record is taken exactly once The metrics are sometimes simple as \"AUC\", \"RMSE\", but in some cases have more complicated names that can be divided into 2 parts by \"@\" delimeter. For example \"SENS@FPR_10.000\"- The first token is the measured metric and the second part is the cutoff defined by the second metric. The second metric can be \"SCORE\" to define cutoff by score, \"FPR\" to define cutoff by False Positive rate, \"SENS\" - by sensitivity and \"PR\" - positive rate.</li> <li>The First token can be one of the following:\u00a0<ul> <li>SENS - Sensitivity/Recall/True Positive Rate (Y axis in the ROC curve)</li> <li>FPR - False Positive Rate (X axis in the ROC curve)</li> <li>PR - Positive Rate</li> <li>PPV - Positive Predictive Values / Precision</li> <li>SCORE - the score of the model</li> <li>LIFT - the lift</li> <li>OR - Odds Ratio</li> <li>RR - Relative Risk</li> <li>NPV - Negative Predictive Value</li> <li>SPEC = Specificity := 1 - FPR Additional Measures: NNEG - number of negatives (controls in the cohort), NPOS - number of positive/cases in the cohort</li> </ul> </li> </ul> <ol> <li>Value - The numeric value that relates to the cohort and measurement</li> </ol>"},{"location":"Medial%20Tools/bootstrap_app/Utility%20tools%20to%20process%20bootstrap%20results.html","title":"Utility tools to process bootstrap results","text":""},{"location":"Medial%20Tools/bootstrap_app/Utility%20tools%20to%20process%20bootstrap%20results.html#process-bootstrap-result-to-create-text-table-excel-like","title":"Process bootstrap result to create text table (Excel like):","text":"<p>You can use bootstrap_format.py to process the results into nice table. The script is under \"MR_SCRIPTS\" git repo and suppose to be in your PATH under $MR_ROOT/Projects/Scripts/Python-scripts, so you can just call it. <pre><code>bootstrap_format.py --report_path $BT_REPORT_PATH_1 $BT_REPORT_PATH_2 $BT_REPORT_PATH_3 ... $BT_REPORT_PATH_N \\\n  --report_name $NAME_FOR_1 $NAME_FOR_2 $NAME_FOR_3 ... $NAME_FOR_N \\\n  --cohorts_list $REGEX_TO_FILTER_COHORTS_FORM_BOOTSTRAP \\\n  --measure_regex $REGEX_TO_FILTER_MEASUREMENTS\n  --table_format $TABLE_FORMAT\n#Full example:\nbootstrap_format.py --report_path /tmp/bt_baseline.pivot_txt /tmp/bt_full.pivot_txt --report_name baseline MES_Full --cohorts_list . --measure_regex \"AUC|OR@PR\" --table_format \"cm,r\"\n</code></pre></p> <ul> <li>You can specify 1 or multiple bootstrap file results and give each file a name.</li> <li>You can filter the cohorts using regex \"\u2013cohorts_list\" argument or pass \"\u2013cohorts_list\u00a0.\" to keep all cohorts (. will match all characters in regex)</li> <li>You can specify which measurements to extract with\u00a0\"\u2013measure_regex\". For example \"AUC|SENS@FPR\" - to extract both AUC and SENS@FPR</li> <li> <p>The final output is a table and you can control the rows/columns with \"table_format\" argument. There are 3 dimensions that are being projected into 2D table. Here are the letters that describe each one of them</p> <ul> <li>r - \"report\". The bootstrap report file. In the example it's either baseline or MES_Full\u00a0</li> <li>c - \"cohort\". The bootstrap files can contain multiple cohorts</li> <li>m - \"measurement\" - The bootstrap results is based on multiple measurements, like \"AUC\", \"SENS@FPR_05\", etc. In order to control how to project those 3 dimensions into row/cols (2D), you need to specify those 3 characters and put \",\" to separate rows and cols. The first token will control \"rows\" of the table and the second token will controls the \"columns\".As you can see, one of the tokens will have 2 characters - it will expend all possible combinations of those values (Cartesian multiplier) and use a delimiter of \"$\" between the 2 tokens. In the example, The rows will be based on \"cohort\" X \"Measurement\" and the columns will be the 2 reports - so we will see side by side the baseline VS MES_Full in this example Additional arguments:</li> </ul> </li> <li> <p>--break_cols \"breaks\" Cohort filters into columns by comma - each filter condition is separated by comma.</p> </li> <li>--break_mes \"breaks\" measurement values (e.g. 8.4[7.8 - 9.2]) to 3 columns - Mean, Min and Max</li> <li>--output_path to save the results in csv Full output example without \"--break_cols\":</li> </ul> <p><pre><code>Cohort$Measurements     baseline        MES_Full\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000$AUC        0.807[0.801 - 0.814]    0.816[0.809 - 0.822]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000$OR@PR_3    8.4[7.8 - 9.2]  9.5[8.7 - 10.3]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000$OR@PR_5    8.1[7.6 - 8.7]  8.8[8.2 - 9.4]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000$OR@PR_10   7.6[7.2 - 8.2]  8.3[7.7 - 8.8]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000$AUC        0.812[0.805 - 0.818]    0.821[0.815 - 0.827]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000$OR@PR_3    9.2[8.5 - 9.9]  10.1[9.4 - 11.0]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000$OR@PR_5    8.4[7.8 - 9.0]  9.4[8.7 - 10.1]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000$OR@PR_10   8.0[7.5 - 8.6]  8.7[8.1 - 9.2]\n</code></pre> With\u00a0\"\u2013break_cols\" (which is the default):</p> <p><pre><code>Cohort  Age     Ex_or_Current   Suspected       Time-Window     Measurements    baseline        MES_Full\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000    50-80   1       0       90-360  AUC     0.807[0.801 - 0.814]    0.816[0.809 - 0.822]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000    50-80   1       0       90-360  OR@PR_3 8.4[7.8 - 9.2]  9.5[8.7 - 10.3]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000    50-80   1       0       90-360  OR@PR_5 8.1[7.6 - 8.7]  8.8[8.2 - 9.4]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000    50-80   1       0       90-360  OR@PR_10        7.6[7.2 - 8.2]  8.3[7.7 - 8.8]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000    50-80   1       0-1     90-360  AUC     0.812[0.805 - 0.818]    0.821[0.815 - 0.827]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000    50-80   1       0-1     90-360  OR@PR_3 9.2[8.5 - 9.9]  10.1[9.4 - 11.0]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000    50-80   1       0-1     90-360  OR@PR_5 8.4[7.8 - 9.0]  9.4[8.7 - 10.1]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-1.000,Ex_or_Current:1.000-1.000    50-80   1       0-1     90-360  OR@PR_10        8.0[7.5 - 8.6]  8.7[8.1 - 9.2]\n</code></pre> \u00a0 How to generate graphs like ROC from bootstrap results file: Please use plt_bt.py in scripts. <pre><code>plt_bt.py --input $BT_REPORT_PATH_2 $BT_REPORT_PATH_3 ... $BT_REPORT_PATH_N \\        \n  --names NAMES  $NAME_FOR_1 $NAME_FOR_2 $NAME_FOR_3 ... $NAME_FOR_N \\     \n  --output $OUTPUT_PATH \\\n  --measure $MEASURE\n  --filter_cohorts $REGEX_TO_FILTER_COHORTS_FORM_BOOTSTRAP\n  --show_ci 1\n  --add_y_eq_x 1  #If true will add y=x graph\n</code></pre> measure for ROC for example is SENS@FPR</p>"},{"location":"Medial%20Tools/bootstrap_app/Versions.html","title":"Versions","text":""},{"location":"Medial%20Tools/bootstrap_app/Versions.html#frozen-versions","title":"Frozen Versions","text":""},{"location":"Medial%20Tools/bootstrap_app/Versions.html#frozen-versions-of-the-bootstrap-app-can-be-found-here-serverworkfrozentools","title":"Frozen versions of the bootstrap app can be found here:\u00a0/server/Work/FrozenTools","text":"Version Date Windows/Linux Git Branch Name Git Tag Name Changes bootstrap_app_1.0.2 21 Jan 2019 Linux bootstrap_app_1.0 bootstrap_app_1.0.2 - bugfix in incidence calculation with kaplan-meier bootstrap_app_1.0 19 Dec 2018 Linux bootstrap_app_1.0 191218_01 - The MedRegistry format has changed, and the info is now passed by two inputs: registry and censor. See MedRegistry and MedSamplingStrategy for more info."},{"location":"Medial%20Tools/bootstrap_app/Versions.html#releasing-a-new-version","title":"Releasing a new version","text":"<p>Before releasing a new version, the output of the new version should be compared to the outputs of the old one. Below is a benchmark test to compare to. If the outputs have changed, explain why. Test: 1. Open a folder for the outputs of the tested version. E.g: /server/Work/FrozenVersions/test_benchmark/bootstrap_app_1.1 2. Run the following command (change the output to the folder you created in step 1) tal@node-01:/server/Work/FrozenTools/test_benchmark$\u00a0../bootstrap_app_1.0\u00a0--input pre2d_continuous.preds --output new_version_name/Bootstrap.pre2d_continuous --rep /home/Repositories/THIN/thin_jun2017/thin.repository --registry_path pre2d.MedRegistry --censoring_registry_path pre2d.MedRegistry.censor --cohorts_file pre2d_main_cohorts.params --incidence_sampling_args \"start_year=2007;end_year=2014;conflict_method=all;outcome_interaction_mode=0:after_start,before_end|1:before_start,after_start;censor_interaction_mode=all:within,all\" --sample_min_year 2007 --sample_max_year 2015 --sim_time_window --do_kaplan_meir 1 --output_raw TRUE --debug | tee bootstrap_app_1.0/Bootstrap.pre2d_continuous.log Compare Results:</p> Version Cohort NPOS_Obs NNEG_Obs AUC_Mean AUC_CI.Lower.95 \u00a0AUC_CI.Upper.95 Incidence (when supplying --registry_path) sim_time_window kaplan meier bootstrap_app_1.0 TimeWindow_0_365_Age_40_80 9954 127834 0.850361 0.846586  0.854049 3.2927% 1 1 bootstrap_app_1.0 TimeWindow_0_365_Age_40_80 18406 118188 0.868676 0.866065 0.871808 3.2927% 0 1 <p>**It is also strongly recommend to:\u00a0**</p> <ul> <li>Add subcohorts which use the json file and make sure everything works (to do this, uncomment the second line in pre2d_main_cohorts.params and run the command above)</li> <li>Repeat test on other problems which are based on registries of different character (cancer, flu...) Freeze: Exit to branch and/or add tag at the repositories: Libs, Tools and Scripts. Compile and add the executable file to /server/Work/FrozenTools, and update this page. \u00a0 \u00a0</li> </ul>"},{"location":"Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html","title":"Extending bootstrap","text":"<p>The main idea of the bootstrap infrastructure is the ability to extend it to measure some statistical measurement in bootstrap analysis process (repeated experiment with replacement)\u00a0 - to assest the confidence interval, std, etc. In order to do that, you need to write function that calculates that measurement/s.</p>"},{"location":"Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#function-input-you-just-need-to-keep-that-signature-and-cant-change-that","title":"Function input - you just need to keep that signature and can't change that.","text":"<ul> <li>Lazy_Iterator *iterator - this is iterator that allows you to \"fetch\" the data for calculating the measurements. (It's lazy iterator to allow it to be efficient without allocating memory for each bootstrap experiment, but with doing the randomization process on the fly)</li> <li>int thread_num - the thread num for parallelism - will be used by the \"Lazy_Iterator\"</li> <li>Measurement_Params function_params - optional pointer to function parameters (can be null if not given). You can pass parameters to your function in this way \u00a0 How to use the iterator?* You need to call \"fetch_next\" with thread_num, y, pred, w. IT will return true if you didn't reached to end of input.\u00a0 If you want to use if outside of \"bootstrap.cpp\" in the infrastructure, you will need to use \"fetch_next_external\" instead. The difference is that \"fetch_next\" is optimized and not exists outside of bootstrap.cpp, so you can't referred to it (The optimization is just slightly speedup). You can also pass \"ret_preds_order\" to fetch_next when you have multiple predictions and your measurement function is more complicated (for example in multi label outcomes). Arguments meanings: y - it's the label/outcome pred - the prediction/score w - weight, if no weights than it's \"-1\". If your measurment doesn't support weights, please check that \"w==-1\" and throw error if you received weights \u00a0 function_params - if the function has parameters, please check it is not \"null\" (if null please use default parameters), cast this object to your parameter object. Measurement_Params is simple class that you need to extend if you want to specify parameters. For example class \"ROC_Params\". Example peace of used code in \"calc_roc_measures_with_inc\": <pre><code>map&lt;string, float&gt; calc_roc_measures_with_inc(Lazy_Iterator *iterator, int thread_num, Measurement_Params *function_params) {\n    ....\n    ROC_Params default_params; //default ctor\n    ROC_Params *params = &amp;default_params\n    if (function_params != NULL) \n        params = (ROC_Params *)function_params; \n    //Now use \"params\"\n\n    //Example usage of fetching \"max_diff_working_point\" parameters into max_diff_in_wp\n    float max_diff_in_wp = params-&gt;max_diff_working_point;\n    ....\n}\n</code></pre> </li> </ul>"},{"location":"Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#function-output","title":"Function output","text":"<p>The function returns \"map\" from string to float. the key of the map is the name of the measurement and the float is the corresponding value. In that way, you can calculate multiple measurements in a single function and give them names.\u00a0 When used, the infrastructure will append suffix for each measurement: \"_Mean\", \"_Std\", \"_CI.Lower.95\", \"_CI.Upper.95\", \"_Obs\" as in this page:\u00a0Bootstrap legend</p>"},{"location":"Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#here-is-a-simple-example-of-function-that-counts-how-many-cases-and-how-many-controls-exists","title":"Here is a simple example of function that counts how many cases and how many controls exists:","text":"<p><pre><code>map&lt;string, float&gt; calc_npos_nneg(Lazy_Iterator *iterator, int thread_num, Measurement_Params *function_params) {\n    map&lt;string, float&gt; res;\n    map&lt;float, int&gt; cnts;\n    float y, w, pred;\n    while (iterator-&gt;fetch_next(thread_num, y, pred, w))\n        cnts[y] += w != -1 ? w : 1;\n    res[\"NPOS\"] = (float)cnts[(float)1.0];\n    res[\"NNEG\"] = (float)cnts[(float)0];\n    return res;\n}\n</code></pre> This function iterates through the data and \"counts\" how many of each outcome we see in the data and stores it in \"cnt\" variable. It also supports weights. \u00a0</p>"},{"location":"Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#how-to-use-the-custom-function","title":"How to use the custom function?","text":"<p>Expand source <pre><code>#include &lt;MedStat/MedStat/MedBootstrap.h&gt;\n\u00a0\n//Let's assume that you wrote custom function like calc_npos_nneg and you called it: \"MY_CUSTOMIZED_MEASURMENT_FUNCTION\"\nmap&lt;string, float&gt; MY_CUSTOMIZED_MEASURMENT_FUNCTION(Lazy_Iterator *iterator, int thread_num, Measurement_Params *function_params) {\n....\n}\n\u00a0\n//Let's assume you have \"arguments\" for you function in class Custom_Measurement_Parameters: \nclass Custom_Measurement_Parameters : public Measurement_Params {\npublic:\n    int min_bin_size = 0;\n    float cases_weight = 1;\n    float controls_weight = 1;\n};\n//Helper function to generate matrix for filtering... Will be only used/needed if you want to generate cohorts and do filtering with bootstrap\u00a0\nvoid get_mat(const string &amp;json_model, const string &amp;rep_path, MedSamples &amp;samples, MedModel &amp;mdl) {\n\n    if (!json_model.empty())\n        mdl.init_from_json_file(json_model);\n    else\n        MLOG(\"No json for bootstrap - can only use Time-Window,Age,Gender filters\\n\");\n    bool need_age = true, need_gender = true;\n    for (FeatureGenerator *generator : mdl.generators) {\n        if (generator-&gt;generator_type == FTR_GEN_AGE)\n            need_age = false;\n        if (generator-&gt;generator_type == FTR_GEN_GENDER)\n            need_gender = false;\n    }\n    if (need_age)\n        mdl.add_age();\n    if (need_gender)\n        mdl.add_gender();\n    vector&lt;int&gt; pids_to_take;\n    samples.get_ids(pids_to_take);\n\u00a0\n    MedPidRepository rep;\n    mdl.load_repository(rep_path, pids_to_take, rep, true);\n\n    if (mdl.learn(rep, &amp;samples, MedModelStage::MED_MDL_LEARN_REP_PROCESSORS,\n        MedModelStage::MED_MDL_APPLY_FTR_PROCESSORS) &lt; 0)\n        MTHROW_AND_ERR(\"Error creating features for filtering\\n\");\n}\n\u00a0\nint main(int argc, char *argv[]) {\n    //String argument for controlling bootstrap parameters, as in the example. To see full parameters, refer to \"MedBootstrap::init\".\n    // It has default - so you can keep that empty, skip the call for \"bt.bootstrap_params.init_from_string\" if you want the default. \n    //You can also change the parameters programically by accessing them: \"bt.bootstrap_params.loopCnt=500;\"\n    string bt_params = \"loopcnt=500;sample_per_pid=1\"; \n    MedBootstrapResult bt;\n    bt.bootstrap_params.init_from_string(bt_params);\n\u00a0\n    //Your custom mearument parameters:\n    Custom_Measurement_Parameters custom_args;\n\u00a0\n    string json_model; //optional argument to generate features for filtering cohorts in the bootstrap\n    string cohorts_file; // path to bootstrap cohorts definitions. If not specify (no need for json_model, rep_path), will not filter the samples and will create \"All\" cohort with all samples and no filtering.\n    string rep_path; // path to repository in case you need to filter and you specified \"cohorts_file\" that is not just \"Time-Window\".\n\u00a0\n    MedSamples  preds; // Your MedSamples with predictions\n    preds.read_from_file(\"YOUR_SAMPLES_PATH\");\n\u00a0\n    if (!cohorts_file.empty())\n        bt.bootstrap_params.parse_cohort_file(cohorts_file);\n    // The measurement functions. The default is vector with single element of \"calc_roc_measures_with_inc\"\n    vector&lt;pair&lt;MeasurementFunctions, Measurement_Params *&gt;&gt; &amp;cl_m = bt.bootstrap_params.measurements_with_params; \n    cl_m.clear(); //clear and remove the default\n    cl_m.push_back(pair&lt;MeasurementFunctions, Measurement_Params *&gt;(MY_CUSTOMIZED_MEASURMENT_FUNCTION, &amp;custom_args)); // Add your function and arguments\n\n    //create matrix for json, and rep\n    MedModel mdl;\n    get_mat(json_model, rep_path, smps_preds, mdl);\n    MedFeatures &amp;bt_features_matrix = mdl.features;\n\n    //Prepare and arrange the features for bootstrap analysis\n    vector&lt;float&gt; preds_v, labelsOrig;\n    vector&lt;int&gt; pidsOrig, preds_order;\n    map&lt;string, vector&lt;float&gt;&gt; bt_data;\n    bt.bootstrap_params.prepare_bootstrap(bt_features_matrix, preds_v, labelsOrig, pidsOrig, bt_data, preds_order);\n    //run the bootstrap analysis\n    bt.bootstrap(preds, bt_data);\n\n    //Now you can access results in bt.bootstrap_results. It's map of map. The first index is the \"cohort name\", second index is \"measurement name\" and the value is the value\n    for (auto &amp;it_cohort : bt.bootstrap_results) {\n        const string &amp;cohort_name = it_cohort.first;\n        for (auto &amp;it_measurment : it_cohort.second) {\n            const string &amp;measurement_name = it_measurment.first;\n            float value = it_measurment.second;\n            fprintf(stdout, \"%s %s %f\\n\", cohort_name .c_str(), measurement_name.c_str(), value);\n        }\n    }\n}\n</code></pre> \u00a0 \u00a0 When executing bootstrap, you can append multiple measurement functions and the bootstrap process will be applied on all functions.</p>"},{"location":"Medial%20Tools/bootstrap_app/Extending%20bootstrap/Using%20Harrell%20C%20Statistics.html","title":"Using Harrell C Statistics","text":"<p>activate: <pre><code>bootstrap_app --measurement_type\"calc_harrell_c_statistic\"\n</code></pre> Encoding samplers for harrell's\u00a0 \u00a0 //Case/Control =&gt; effect outcome/y sign. positive is case, negative controls. Can't handle event in time zero.  //Time to event =&gt; abs value of outcome/y  //Score =&gt; the prediction \u00a0 \u00a0</p>"},{"location":"Medial%20Tools/change_model/index.html","title":"change_model","text":"<p>In some cases we need to manipulate Existing MedModel without need for relearn. For example:</p> <ul> <li>In production models with use special flags in the BasicCleaner to store in the MedSamples information about the cleaning process</li> <li>Normalization, Imputation - we might want to turn it off for some use cases to create matrixes</li> <li>ButWhy - we might want to turn it off or change some arguments for the apply without realearn - change grouping arguments for example... A new app in Tools repository called change_model can handle all of those in a very simple manner. Example for use-case #1 - how to disable the attributes storage in existing model: <pre><code>change_model --model_path /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/crc/base_model.bin --output_path $NEW_CHANGE_MODEL_PATH --interactive_change 0 --search_object_name RepBasicOutlierCleaner --object_command \"nrem_attr=;nrem_suff=;ntrim_attr=;ntrim_suff=\"\nread_binary_data_alloc [/server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/crc/base_model.bin] with crc32 [1219618941]\nread_from_file [/server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/crc/base_model.bin] with crc32 [1219618941] and size [5131504]\nFound object as RepProcessor\nFound object as RepProcessor - touched 20 objects - succesed in 20\n</code></pre> The above command opens the MedModel in model_path argument, looks for \"RepBasicOutlierCleaner\" object and passes into all objects the init string command that changes those objects and than stores it in the NEW_CHANGE_MODEL_PATH location. You can also passed \"DELETE\" in the object_command to delete a certain objects or interactively navigate trough MedModel object to change/delete certain elements by index numbers. Be carefull to change only things that won't require relearn!!! \u00a0</li> </ul>"},{"location":"Medial%20Tools/change_model/index.html#json-examples-of-changemodelinfo-to-pass-as-json","title":"JSON examples of ChangeModelInfo to pass as json","text":"<p>General template: <pre><code>{ \n  \"changes\": [\n       //Write your change block in here\n  ]\n}\n</code></pre> All templates can be seen here:\u00a0/server/Work/Users/Alon/UnitTesting/examples/ChangeModelInfo/\u00a0(symbolic link to git repository /server/UsersData/alon/MR/Projects/Shared/Projects/configs/UnitTesting/examples/ChangeModelInfo) The json_query_whitelist and json_query_blacklist are lists of conditions to filter the json by regex. If multiple items are presented it does AND condition. \u00a0 Remove Normalizers and Imputers. Remove Only Age+Gender Normalizers and not all of them. <pre><code>{ \n  \"changes\": [\n       {\n            \"change_name\":\"Remove Normalizers\",\n            \"object_type_name\":\"FeatureNormalizer\",\n            \"json_query_whitelist\": [ \"Age|Gender\" ],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"DELETE\",\n            \"verbose_level\":\"2\"\n       },\n       {\n            \"change_name\":\"Remove Imputers\",\n            \"object_type_name\":\"FeatureImputer\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"DELETE\",\n            \"verbose_level\":\"2\"\n       }\n  ]\n}\n</code></pre> Remove attributes from cleaners, remove attribute check and change max_data in memory that used to split model apply into blocks <pre><code>{ \n  \"changes\": [\n       {\n            \"change_name\":\"Remove attributes from cleaners\",\n            \"object_type_name\":\"RepBasicOutlierCleaner\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"nrem_attr=;ntrim_attr=;nrem_suff=;ntrim_suff=\",\n            \"verbose_level\":\"1\"\n       },\n       {\n            \"change_name\":\"Remove attributes Req\",\n            \"object_type_name\":\"RepCheckReq\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"DELETE\",\n            \"verbose_level\":\"1\"\n       },\n       {\n            \"change_name\":\"MedeModel Memory\",\n            \"object_type_name\":\"MedModel\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"max_data_in_mem=0\",\n            \"verbose_level\":\"1\"\n       }\n  ]\n}\n</code></pre></p>"},{"location":"Medial%20Tools/change_model/How%20to%20limit%20memory%20usage%20in%20predict.html","title":"How to limit memory usage in predict","text":"<p>If 'bad alloc' occur in predict, you may need to limit memory usage Use this flag: <pre><code>Flow --get_model_preds ... --change_model_file ${LIMIT_MEMORY_JSON}\n</code></pre> Where LIMIT_MEMORY_JSON is: <pre><code>{\n  \"changes\": [\n    {\n        \"change_name\":\"Decrease mem size\",\n        \"object_type_name\":\"MedModel\",\n        \"change_command\":\"max_data_in_mem=1000000000\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"Medial%20Tools/create_registry/index.html","title":"create_registry","text":"<p>A tool to create MedRegistry and if provided sampling startegy parameters to create MedSamples. The program steps:</p> <ul> <li>Creates or load from text file MedRegistry. Can provide file path or config file to generate MedRegistry</li> <li>Creates or load from text file\u00a0MedRegistry\u00a0for censoring (a time periods where that marks the patients \"Membership\" period, it's also in format of MedRegistry) - OPTIONAL, if not given, assume the patient has full membership</li> <li>Creates\u00a0MedLabel from registry and censor_registry with problem definition arguments - time window argument + labeling policy arguments. This object knows how to \"label\" the outcome for a sample in a given time or decide to exclude it based on MedRegistry and the LabelParams.</li> <li>Creates MedSamples from MedSamplingStrategy\u00a0arguments of how to sample + additional filtering arguments you may provide to restrict sampling (Age, Years...)</li> </ul>"},{"location":"Medial%20Tools/create_registry/index.html#create-samples-from-medregistry","title":"Create samples from MedRegistry","text":"<pre><code>create_registry --rep $REP_PATH --registry_load $PATH_TO_MED_REGISTRY_FILE --registry_active_periods_complete_controls_sig MEMBERSHIP \\\n --labeling_params $LABELING_PARAMS --sampler_type $SAMPER_TYPE --sampler_args $SAMPLER_ARGS --samples_save $OUTPUT_PATH_FOR_MEDSAMPLES \\\n--filtering_params $OPTIONAL_FILTERING_PARAMS_LIKE_AGE\n#Can also provide additional MedRegistry for Membership if it is not a signal, by passing \"--censor_load\".\n</code></pre> <ul> <li>LABELING_PARAMS - defines how to label the sample - is it case/control or other outcome value?- The initialization text for MedLabels</li> <li>SAMPER_TYPE - type of sampler. The options are in here (code documentation make_sampler) or can look for informaiton here:\u00a0MedSamplingStrategy</li> <li>SAMPLER_ARGS - for the specific Sampler, the arguments for it.\u00a0MedSamplingStrategy\u00a0or browse the arguments of the specifc sampler</li> <li>OPTIONAL_FILTERING_PARAMS_LIKE_AGE - parameters to filter the samples like age. FilterParams. For example \"min_age=0;max_age=90;min_time=20120101;max_time=20180101\" The SAMPLER_TYPE, SAMPLER_ARGS are independent of the labeing - they define just when to give score or try to give score. \u00a0 Example: We have Influenza MedRegistry where the patient is marked and control when he has Membership and not influenza. In influenza events we have a 1 day time window of influenza. We want to see which patient will have flu within 1 year. The censor registry is the membership signal - we require no gaps in the whole year. The labeling params: \"time_from=0;time_to=365;censor_time_from=0;censor_time_to=365;conflict_method=max;censor_interaction_mode=all:within:within;label_interaction_mode=0:within,within|1:before_start,after_start\"\u00a0 The sampling: <code>--sampler_type yearly --sampler_args \"start_year=2016;end_year=2018;prediction_month_day=901;day_jump=365\"</code> Which predicts in 1st of September in each year from 2016 to 2017, is can give score within the specific time for the patient (has a match for a certain registry record)</li> </ul>"},{"location":"Medial%20Tools/create_registry/index.html#create-medregistry-from-command","title":"Create MedRegistry from command","text":"<p><pre><code>create_registry --rep $REP_PATH --registry_type $REGISTRY_TYPE --registry_init  $REGISTRY_ARGS --registry_save $OUTPUT_PATH_FOR_MEDREGISTRY \n# --use_active_for_reg_outcome can pass this argument with censor registry, whether with \"--registry_active_periods_complete_controls_sig\" or \"--registry_active_periods_complete_controls\"\n</code></pre> Needs to explain and give some examples on how to create MedRegistry</p> <ul> <li>REGISTRY_TYPE : either \"binary\" for binary problems , \"categories\" (for outcome with more than case/controls states). and \"keep_alive\" that is mostly used to generate membership period based on patients activity</li> <li>REGISTRY_ARGS : the arguments for the registry type. For example \"binary\" has those options:<ul> <li>max_repo_date - the maximal repoistory date to cut registry date. in format YYYYMMDD</li> <li>start_buffer_duration - buffer duration from first \"rule\" start. Mostly, set to 0</li> <li>end_buffer_duration - buffer duration from last \"rule\" end duration. In case we want to \"trim\" the last period. Mostly, set to 0</li> <li>allow_prediciton_in_case - if true will continue to process rules even if in \"case\" time period</li> <li>seperate_cases - will allow more than one time period of case. Might be useful fot influenza events that may occur several times.</li> <li>config_signals_rules - file path to registry rules to define the MedRegistry records<ul> <li>Tab delimted file with 2 columns: RegistrySignalTypeand it's arguments Example\u00a0config_signals_rules\u00a0 file for CKD from 2 to 3 and up: <pre><code>#Definition of controls- if CKD_State is less than 2, mark it as 0 for at least 1 year if not contradicted or contiuned:\nrange   signalName=CKD_State;duration_flag=365;take_only_first=0;outcome_value=0;min_value=0;max_value=2\n#Definition of cases - if CKD_State is 3 and more, mark it as 1 for at least 1 year if not contradicted or contiuned:\nrange   signalName=CKD_State;duration_flag=365;take_only_first=0;outcome_value=1;min_value=3;max_value=9\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"Medial%20Tools/create_registry/Create%20Membership%20registry%20example%20command.html","title":"Create Membership registry example command","text":"<p><pre><code>create_registry --rep ${REP_PATH} --registry_type keep_alive --registry_init \"duration=${CONNECT_BUFFER};max_repo_date=${MAX_REP_DATE};secondry_start_buffer_duration=0;start_buffer_duration=0;end_buffer_duration=0;signal_list=${SIGNAL_LIST}\" --registry_save $OUTPUT_PATH\n</code></pre> Parameters explained:</p> <ul> <li>REP_PATH - repository path</li> <li>SIGNAL_LIST - list of signals with comma. Those list will be used to calculate membership, if there is event of either one of them, the membership signal will continue. Example: Hemoglobin,DIAGNOSIS,Drug,Glucose</li> <li>CONNECT_BUFFER - how many days to take from each \"event\" of signal appearance to consider the patient as a member - going forward. Example values: 365 or 730</li> <li>MAX_REP_DATE - maximal time of repository.</li> </ul>"},{"location":"Models/index.html","title":"Index","text":"<p>Available models:</p> Model Name Model description Contact Details for Usage LGI/Colon-Flag Detects colon cancer using age, sex, and CBCs Roche LungFlag Detects lung cancer using age, sex, smoking infromation, and common blood tests Roche GastroFlag Detects gastric cancer using age, sex, and common blood tests Roche AAA Predicts AAA events Geisinger/TBD FluComplications Predicts flu followed by complications such as pneumonia, hospitalization, or death TBD Pred2D Predicts progression from prediabetes to diabetes Planned to be open source FastProgressors Predicts rapid decline in eGFR Planned to be open source Mortatlity Predicts mortality using CMS claims data TBD Unplanned COPD Admission Prediction Model Predicts COPD hospitalization using CMS claims data TBD <p>Instructions for using an existing model can be found here</p>"},{"location":"Models/AAA.html","title":"AAA","text":""},{"location":"Models/AAA.html#overview","title":"Overview","text":"<p>Developed and implemented at Geisinger Health System (Pennsylvania), this model identifies cases of Abdominal Aortic Aneurysm (AAA). Beyond standard criteria, it has been specifically adapted to enhance fairness and to detect AAA in females, even though this is not part of the USPSTF guidelines.</p>"},{"location":"Models/AAA.html#deployments","title":"Deployments","text":"<ul> <li>Geisinger Health System (since 2022)</li> </ul>"},{"location":"Models/AAA.html#intended-usage","title":"Intended Usage","text":"<p>The model is designed to assist in referring patients for ultrasound (US) screening for AAA. While USPSTF guidelines recommend screening for males aged 65 and older who are current or former smokers, Geisinger has broadened the criteria to also include females aged 65 and older with a history of smoking.</p>"},{"location":"Models/AAA.html#publications","title":"Publications","text":"<p>Selected publication:</p> Manuscript Population Year Development and validation of a machine-learning prediction model to improve abdominal aortic aneurysm screening Geisinger - US, prospective usage 2024"},{"location":"Models/AAA.html#contact","title":"Contact","text":"<p>For further information, please contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/COPDCMS.html","title":"Unplanned COPD Admission Prediction Model","text":""},{"location":"Models/COPDCMS.html#overview","title":"Overview","text":"<p>This model predicts the risk of unplanned hospital admission due to COPD within 30 days, using claims data from the CMS Health AI Challenge. It is designed for Medicare and Medicaid populations and targets patients with a history of COPD.</p> <p>Eligibility Criteria:</p> <ul> <li>Patient must have a prior COPD diagnosis before the prediction date</li> <li>Prediction date must not fall within an inpatient admission</li> </ul> <p>The model provides explainability outputs and has undergone fairness testing.</p>"},{"location":"Models/COPDCMS.html#publications","title":"Publications","text":"<p>Presentation slides with detailed results are available upon request.</p>"},{"location":"Models/COPDCMS.html#contact","title":"Contact","text":"<p>For more information, contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/ColonFlag.html","title":"LGI/Colon-Flag","text":""},{"location":"Models/ColonFlag.html#overview","title":"Overview","text":"<p>The LGI/Colon-Flag model was developed to detect colon and rectal cancer using data from Maccabi Healthcare Services (MHS), the second largest HMO in Israel. First version published in 2015, with the final version completed in 2018.</p> <p>The model has undergone external validation at numerous sites across various regions. A partial list of related publications is provided below.</p> <p>This model is effective at detecting gastric cancer, although our specialized GastroFlag model offers superior performance in this area. It also performs well in identifying lower gastrointestinal (GI) disorders and pre-cancerous conditions.</p>"},{"location":"Models/ColonFlag.html#model-inputs","title":"Model Inputs","text":"<p>The model uses the following signals:</p> <ul> <li>Birth year (for age calculation)</li> <li>Sex</li> <li>CBC panel: Hemoglobin, Hematocrit, RBC, MCH, MCV, MCHC, Platelets, RDW, WBC, MPV, Lymphocytes (absolute and %), Monocytes (absolute and %), Eosinophils (absolute and %), Basophils (absolute and %), Neutrophils (absolute and %)</li> </ul>"},{"location":"Models/ColonFlag.html#deployments","title":"Deployments","text":"<ul> <li>Maccabi Healthcare Services - Israel (since 2015)</li> <li>Geisinger Health System - US (since 2018)</li> <li>List of more sites...</li> </ul> <p>Note: partial list - there are more deployments</p>"},{"location":"Models/ColonFlag.html#intended-usage","title":"Intended Usage","text":"<ul> <li>Not for screening exclusion: This tool is not intended to rule out patients from screening. Its sensitivity is insufficient for use as a primary screening tool.</li> <li>Purpose: The model is designed to help increase compliance and improve the yield of colonoscopies among patients who are non-adherent to screening recommendations.</li> <li>Professional use only: The tool is intended for interpretation and use by healthcare professionals, not patients.</li> </ul> <p>The model is designed to calculate a score when new CBC data is available. Requests to calculate a score without CBC data will be rejected. A score can still be generated even if some other input signals are missing.</p> <p>For further information, please see the \"Contact Details for Usage\" section and request the User Guide.</p>"},{"location":"Models/ColonFlag.html#list-of-publications","title":"List of Publications","text":"<ul> <li>Partial list of publications.</li> </ul> Manuscript Population Year Model Study Type Research Organization Development and validation of a predictive model for detection of colorectal cancer in primary care by analysis of complete blood counts: a binational retrospective study MHS Israel, THIN - UK 2016 Old ColonFlag Retrospective, Outcomes were retrieved after scoring + External validation MES Performance analysis of a machine learning flagging system used to identify a group of individuals at a high risk for colorectal cancer MHS Israel 2017 Old ColonFlag Prospective Observational MES Evaluation of a prediction model for colorectal cancer: retrospective analysis of 2.5 million patient records UK - CPRD 2017 Old ColonFlag Retrospective, External Validation Oxford Early Colorectal Cancer Detected by Machine Learning Model Using Gender, Age, and Complete Blood Count Data US - Kaiser Permanente North West 2017 ColonFlag ??? Retrospective, External Validation MES Computer-Assisted Flagging of Individuals at High Risk of Colorectal Cancer in a Large Health Maintenance Organization Using the ColonFlag Test MHS Israel 2018 Old ColonFlag Prospective Interventional MHS Prediction of findings at screening colonoscopy using a machine learning algorithm based on complete blood counts (ColonFlag) Canada 2018 ColonFlag Retrospective, External Validation MES Potential roles of artificial intelligence learning and faecal immunochemical testing for prioritisation of colonoscopy in anaemia UK - gastroenterology clinic in Plymouth, Royal London Hospital 2019 ColonFlag Prospective Observational Barts Validation of an Algorithm to Identify Patients at Risk for Colorectal Cancer Based on Laboratory Test and Demographic Data in Diverse, Community-Based Population US - Kaiser Permanente North West 2020 ColonFlag Retrospective, External Validation University of Washington + KP Collaboration to Improve Colorectal Cancer Screening Using Machine Learning US - Geisinger Health System 2022 ColonFlag Prospective Interventional Geisinger Diagnostic application of the ColonFlag AI tool in combination with faecal immunochemical test in patients on an urgent lower gastrointestinal cancer pathway UK - Barts Health - Urgency pathway 2024 ColonFlag Prospective Interventional Barts Machine Learning-Guided Cancer Screening: The Benefits of Proactive Care Geisinger 2024 ColonFlag Prospective Interventional Geisinger <p>Less important publication:</p> Manuscript Population Year Model Study Type Research Organization Comment Use of ColonFlag score for prioritisation of endoscopy in colorectal cancer UK - Barts Health - Urgency pathway 2021 ColonFlag Prospective Interventional Barts There is a new publication from 2024 with more data Predicting the presence of colon cancer in members of a health maintenance organisation by evaluating analytes from standard laboratory records MHS Israel 2017 ColonFlag variations with all labs Retrospective MES Different model Development and Validation of a Colorectal Cancer Prediction Model: A Nationwide Cohort-Based Study Clalit - Israel 2024 Clalit Model Retrospective, Developent and internal validation Clalit Not our model and no comparison to ColonFlag"},{"location":"Models/ColonFlag.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>Roche Navify Algosuit - Details to be announced.</p>"},{"location":"Models/FastProgressors.html","title":"FastProgressor","text":""},{"location":"Models/FastProgressors.html#overview","title":"Overview","text":"<p>This model is designed to screen patients after a creatinine test to identify those at risk of rapid eGFR decline. The definition of fast deterioration aligns with KDIGO guidelines:</p> <ul> <li>A yearly average drop of more than 5 points in eGFR</li> <li>The observation period is at least 18 months</li> <li>Excludes acute conditions (e.g., AKI)</li> <li>Excludes patients who begin SGLT2 drug treatment, as they are already being managed</li> </ul> <p>This model is not the KFRE, which predicts ESRD (end-stage renal disease). Instead, it targets otherwise healthy patients to help slow, stop, or prevent kidney function decline earlier in the disease process.</p> <p>The model was developed using the UK THIN (The Health Improvement Network) dataset and externally validated at a US site. However, there are no peer-reviewed publications due to competing interests.</p>"},{"location":"Models/FastProgressors.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the \"/discovery\" API, including signal names and required units.</p>"},{"location":"Models/FastProgressors.html#intended-usage","title":"Intended Usage","text":"<p>Refer patients identified as high risk for rapid deterioration to the endocrinology track.</p>"},{"location":"Models/FastProgressors.html#list-of-publications","title":"List of Publications","text":""},{"location":"Models/FastProgressors.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>The model may be released as open source upon request. please contact : alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/FluComplications.html","title":"FluComplications","text":""},{"location":"Models/FluComplications.html#overview","title":"Overview","text":"<p>This model was developed using data from Kaiser Permanente Northwest. Its primary goal is to identify unvaccinated patients who would most benefit from receiving a flu shot by predicting those at higher risk for influenza or influenza-like illness followed within three months by a complication. Complications may include pneumonia (the most common), hospitalization, or death.</p> <p>The model is especially sensitive to severe complications such as hospitalization and death, but also performs well in predicting all types of complications. In the related publication, we also analyzed the additional complications observed after flu compared to patients who did not contract the flu. Since the model's risk factors often identify frail patients who may experience complications regardless of flu, we specifically examined the \"additional\" complications following influenza.</p> <p>The model was compared to a linear model based on the World Health Organization (WHO) list of approximately 10 risk factors (e.g., asthma, immunosuppression, pregnancy, age, etc.). Our model outperforms the WHO-based model and also incorporates hospitalization/admission history, medications, and other diagnosis codes. Some variants included blood and spirometry tests, but these were ultimately excluded from the final public model for simplicity, as they are less common or less impactful.</p> <p>The model assumes good vaccine efficacy. While ideally the model would target \"who will benefit most from a flu shot,\" this is difficult to determine due to varying vaccine efficacy from year to year. Instead, the model focuses on predicting who is likely to get sick and experience complications, highlighting those for whom vaccination would be especially important.</p>"},{"location":"Models/FluComplications.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the \"/discovery\" API, including signal names and required units.</p>"},{"location":"Models/FluComplications.html#deployments","title":"Deployments","text":"<ul> <li>MHS (Maccabi Health Services), 2019</li> <li>Kaiser Permanente Northwest, 2019</li> <li>Geisinger, 2019 and 2021\u20132025</li> </ul>"},{"location":"Models/FluComplications.html#intended-usage","title":"Intended Usage","text":"<p>The model is intended to increase flu shot adherence among populations at higher risk for flu complications.</p>"},{"location":"Models/FluComplications.html#publications","title":"Publications","text":"<p>Partial list:</p> Manuscript Population Year Prediction of Influenza Complications: Development and Validation of a Machine Learning Prediction Model to Improve and Expand the Identification of Vaccine-Hesitant Patients at Risk of Severe Influenza Complications Geisinger - US 2022"},{"location":"Models/FluComplications.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>please contact : alon (dot) medial at gmail (dot) com for more details</p>"},{"location":"Models/GastroFlag.html","title":"GastroFlag","text":""},{"location":"Models/GastroFlag.html#overview","title":"Overview","text":"<p>The GastroFlag model was created to identify gastric cancer using data from Maccabi Healthcare Services (MHS), Israel\u2019s second largest HMO.</p> <p>This model is effective not only in detecting colon and rectal cancer but also performs well in recognizing lower gastrointestinal (GI) disorders and pre-cancerous conditions.</p> <p>The finalized model has been validated across eight different sites:</p> <ul> <li>THIN (UK dataset)</li> <li>Japan</li> <li>Taiwan</li> <li>United States</li> <li>Latvia</li> <li>Korea (2 sites)</li> <li>MHS</li> </ul> <p>A publication may be available in the future.</p>"},{"location":"Models/GastroFlag.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the \"/discovery\" API, including signal names and required units.</p> <ul> <li>Birth year (for age calculation)</li> <li>Sex</li> <li>CBC panel: Hemoglobin, Hematocrit, RBC, MCH, MCV, MCHC, Platelets, RDW, WBC, MPV, Lymphocytes (absolute and %), Monocytes (absolute and %), Eosinophils (absolute and %), Basophils (absolute and %), Neutrophils (absolute and %)</li> <li>Iron panel: Ferritin, Iron_Fe</li> <li>Basic Metabolic Panel (BMP): Glucose, Sodium, Creatinine, Urea, Potassium</li> <li>Comprehensive Metabolic Panel (CMP) additions: Albumin, Protein_Total, ALT, AST, ALKP, Bilirubin</li> </ul>"},{"location":"Models/GastroFlag.html#intended-usage","title":"Intended Usage","text":"<ul> <li>Not for screening exclusion: This tool is not intended to rule out patients from screening. Its sensitivity is insufficient for use as a primary screening tool.</li> <li>Purpose: The model is designed to help increase compliance and improve the yield of colonoscopies among patients who are non-adherent to screening recommendations.</li> <li>Professional use only: The tool is intended for interpretation and use by healthcare professionals, not patients.</li> </ul> <p>The model is designed to calculate a score when new CBC data is available. Requests to calculate a score without CBC data will be rejected. A score can still be generated even if some other input signals are missing.</p> <p>For further information, please see the \"Contact Details for Usage\" section and request the User Guide.</p>"},{"location":"Models/GastroFlag.html#list-of-publications","title":"List of Publications","text":""},{"location":"Models/GastroFlag.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>Roche Navify Algosuit - Details to be announced.</p>"},{"location":"Models/LungFlag.html","title":"LungFlag","text":""},{"location":"Models/LungFlag.html#overview","title":"Overview","text":"<p>The LungFlag model was developed at Kaiser Permanente Southern California (KPSC) to identify all types of lung cancer, including Squamous, Adenocarcinoma, Small Cell, and Non-Small Cell variants.</p> <p>It has been validated at approximately 10 different sites.</p>"},{"location":"Models/LungFlag.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the <code>/discovery</code> API, including signal names and required units.</p> <ul> <li>Birth year (for age calculation)</li> <li>Sex</li> <li>Measurements: BMI, Weight, Height</li> <li>Smoking Information: Smoking Status (mandatory), Smoking duration (years), smoking intensity (ciggarets per day), Pack years, Smoking quit date (if applicable)</li> <li>Diagnosis signal: ICD10/ICD9</li> <li>Spirometry test: Fev1</li> <li>CBC panel: Hemoglobin, Hematocrit, RBC, MCH, MCV, MCHC, Platelets, RDW, WBC, Lymphocytes (absolute and %), Monocytes (absolute and %), Eosinophils (absolute and %), Basophils (absolute and %), Neutrophils (absolute and %)</li> <li>Lipid panel: Cholesterol, Triglycerides, LDL, HDL, NonHDLCholesterol</li> <li>Basic Metabolic Panel (BMP): Glucose, Creatinine, Urea</li> <li>Comprehensive Metabolic Panel (CMP) additions: Albumin, Protein_Total, ALT, ALKP</li> </ul>"},{"location":"Models/LungFlag.html#deployments","title":"Deployments","text":"<ul> <li>Geisinger Health System (since 2022)</li> </ul>"},{"location":"Models/LungFlag.html#intended-usage","title":"Intended Usage","text":"<p>For further information, please see the \"Contact Details for Usage\" section and request the User Guide.</p>"},{"location":"Models/LungFlag.html#list-of-publications","title":"List of Publications","text":"<ul> <li>Partial list of publications.</li> </ul> Manuscript Population Year Machine Learning for Early Lung Cancer Identification Using Routine Clinical and Laboratory Data KPSC - US 2021 1561P Targeted screening methodologies to select high risk individuals: LungFlag performance in Estonia Lung Cancer Screening Pilot Estonia 2024 Validation of LungFlag\u2122 Prediction Model Using Electronic Medical Records (EMR) On Taiwan Data National Taiwan University Hospital - Taiwan 2024 Validation of LungFlag Lean machine-learning model to identify individuals with lung cancer using multinational data KPSC - US, Geisinger - US, THIN - UK, additional site in US 2025 Maximizing Lung Cancer Screening in High-Risk Population Leveraging ML-Developed Risk-Prediction Algorithms: Danish Retrospective Validation of LungFlag Southern Denmark 2025 <p>Validation on CPRD, Canada are in progress</p>"},{"location":"Models/LungFlag.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>Roche Navify Algosuit - Details to be announced.</p>"},{"location":"Models/MortatlityCMS.html","title":"Mortality Model","text":""},{"location":"Models/MortatlityCMS.html#overview","title":"Overview","text":"<p>This model was created using claims data from the CMS Health AI Challenge. It focuses on Medicare and Medicaid populations, with the goal of predicting all-cause mortality within one year, as defined by CMS.</p> <p>The model provides explainability outputs and has undergone fairness testing.</p>"},{"location":"Models/MortatlityCMS.html#publications","title":"Publications","text":"<p>Slides detailing the results are available upon request from the author.</p>"},{"location":"Models/MortatlityCMS.html#contact-information","title":"Contact Information","text":"<p>For inquiries, please contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/Pred2D.html","title":"Pre2D","text":""},{"location":"Models/Pred2D.html#overview","title":"Overview","text":"<p>Pre2D is a predictive model developed using data from THIN (The Health Improvement Network) in the UK. Its purpose is to identify patients in the pre-diabetes stage who are likely to develop diabetes within the next two years.</p> <p>The criteria for defining diabetes are as follows:</p> <ul> <li>Two consecutive fasting glucose tests above 125 mg/dL</li> <li>HbA1C above 6.5</li> <li>A single glucose measurement above 200 mg/dL</li> <li>A diabetes diagnosis code with at least one abnormal test (glucose or HbA1C)</li> <li>Evidence of treatment with any glucose-lowering agent other than metformin</li> </ul>"},{"location":"Models/Pred2D.html#model-inputs","title":"Model Inputs","text":"<p>The model utilizes the following input signals:</p> <ul> <li>Birth year (used to calculate age)</li> <li>Sex</li> <li>Measurements: BMI</li> <li>Diabetes screening: Glucose, HbA1C</li> <li>Lipid panel: Triglycerides, HDL</li> <li>WBC</li> <li>ALT</li> <li>Prescriptions (optional): NDC, RX NORM, which may be converted to ATC codes in the future</li> </ul>"},{"location":"Models/Pred2D.html#intended-usage","title":"Intended Usage","text":"<p>For the user guide, please contact the author.</p>"},{"location":"Models/Pred2D.html#publications","title":"Publications","text":"<p>Partial list of publications:</p> Manuscript Population Year Prediction of progression from pre-diabetes to diabetes: Development and validation of a machine learning model THIN - UK, MHS - Israel, AppleTree - Canada 2020"},{"location":"Models/Pred2D.html#contact-information","title":"Contact Information","text":"<p>The model may be released as open source upon request. For inquiries, please contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Python/index.html","title":"Python","text":""},{"location":"Python/index.html#quick-start","title":"Quick Start","text":"<p>We provide three libraries for use:</p> <ol> <li>MedPython: A Python library that integrates with our C library.</li> <li>ETL Library: A pure Python utility designed to assist in creating Data Repositories.</li> <li>AlgoMarker API Wrapper: A pure Python wrapper for utilizing the AlgoMarker library (limited to predict/apply for implementation setting. Much lighter as opposed to MedPython).</li> </ol> <p>Note: These libraries are not currently available as PyPi packages. To use them, you need to set the <code>PYTHONPATH</code> environment variable to their installation paths. For more information: Setup</p>"},{"location":"Python/index.html#pages","title":"Pages","text":"<ul> <li>MedPython<ul> <li>Examples: Usage examples for MedPython.</li> <li>Python Binding Troubleshooting: Guidance for troubleshooting Python bindings in MedPython.</li> <li>Extend and Develop: Instructions for exposing additional C++ APIs to Python.</li> </ul> </li> <li>ETL Library: Refer to the ETL Tutorial for more details.</li> <li>Python AlgoMarker API Wrapper: Documentation for the pure Python wrapper of the AlgoMarker library.</li> </ul>"},{"location":"Python/index.html#setup","title":"Setup","text":"<ol> <li>Clone the Git Repositories:<ul> <li>MR_LIBS</li> <li>MR_Tools</li> </ul> </li> <li>Set Up MedPython:    Follow the instructions in Setup MedPython.</li> <li>Configure Environment Variables:    Ensure Python recognizes the libraries by setting the <code>PYTHONPATH</code> environment variable. Replace <code>${MR_LIBS}</code> with the path to the cloned <code>MR_LIBS</code> repository and <code>${MR_TOOLS}</code> with the path to the cloned <code>MR_Tools</code> repository.</li> </ol> <pre><code>export PYTHONPATH=${MR_LIBS}/Internal/MedPyExport/generate_binding/Release/medial-python312:${MR_TOOLS}/RepoLoadUtils/common\n</code></pre> <p>The Python AlgoMarker API Wrapper does not require installation. Simply run the script directly when needed.</p>"},{"location":"Python/Examples.html","title":"Examples","text":""},{"location":"Python/Examples.html#import-medials-api-in-python","title":"Import Medial's API in Python","text":"<pre><code>import med\n# Optional set to use stdout, usually I'm not running this:\nmed.logger_use_stdout()\n</code></pre>"},{"location":"Python/Examples.html#which-medpython-module-am-i-using","title":"Which MedPython module am I using","text":"<pre><code>print(med.__file__) # Shows file path\nif hasattr(med.Global, 'version_info'):\n    print(med.Global.version_info) # Shows compilation date and git commit hash when compliled \"version\"\n</code></pre>"},{"location":"Python/Examples.html#inspect-available-functions","title":"Inspect Available Functions","text":"<p>Documentation is a work in progress. To inspect available methods:</p> <pre><code>help(med)\nhelp(med.PidRepository)\n</code></pre>"},{"location":"Python/Examples.html#load-a-repository","title":"Load a Repository","text":"<pre><code>rep = med.PidRepository()\nrep.read_all(\n    '/nas1/Work/CancerData/Repositories/THIN/thin_2021/thin.repository',\n    [],\n    ['GENDER', 'DEATH', 'BDATE', 'Albumin']\n)\nprint(med.cerr())\n</code></pre>"},{"location":"Python/Examples.html#iterating-over-a-signal","title":"Iterating over a Signal","text":"<pre><code>## This is not the proper way to work with Python, since Python loops are very slow, however, it's nice as a poc.\n## Don't do this, using python loops is slow - use get_sig to retrieve dataframe\n\nsigname = 'Albumin'\nrep = med.PidRepository()\nrep.read_all('/home/Repositories/THIN/thin_jun2017/thin.repository',[],[signame])\nprint(med.cerr())\nfor pid in rep.pids[20:30]:\n  usv = rep.uget(pid, rep.sig_id(signame))\n  if len(usv)&gt;0:\n    print('\\n\\n')\n    for rec in usv:\n      print(\"Patient {} had {}={:.2} at {}\".format(pid, signame, rec.val(), rec.date()))\n</code></pre>"},{"location":"Python/Examples.html#useful-functions-to-fix-signal-types","title":"Useful functions to fix signal types","text":"<pre><code>def fix_ts_m1900(df, col):\n    import datetime as dt\n    df[col] = dt.datetime(1900,1,1)+pd.TimedeltaIndex(df[col], unit='m')\ndef fix_date_ymd(df, col): df[col] = pd.to_datetime(df[col], format='%Y%m%d')\n</code></pre>"},{"location":"Python/Examples.html#load-some-signals","title":"Load some signals","text":"<p>When using \"get_sig\" function, there is no need to call \"read_all\" before, \"init\" is enough. get_sig, loads the signal automatically from disk if needed and not loaded</p> <pre><code>albumin = rep.get_sig('Albumin')\nalbumin = albumin[albumin['date'] % 100 != 0]\nalbumin.rename(columns={'val': 'Albumin'}, inplace=True)\nfix_date_ymd(albumin, 'date')\ngender = rep.get_sig('GENDER')\ngender.rename(columns={'val': 'Gender'}, inplace=True)\nbdate = rep.get_sig('BDATE')\nbdate['val'] = bdate['val'] + 1\nfix_date_ymd(bdate, 'val')\nbdate.rename(columns={'val': 'BDate'}, inplace=True)\n\u00a0\nmortality = rep.get_sig('DEATH')\nmortality = mortality[(mortality['val'] % 100 &lt;= 31) &amp; (mortality['val'] % 100 &gt; 0)]\nfix_date_ymd(mortality, 'val')\nmortality.rename(columns={'val': 'MortDate'}, inplace=True)\n\u00a0\n</code></pre>"},{"location":"Python/Examples.html#using-lookup-table-in-python","title":"Using Lookup table in python","text":"<ul> <li>When using \"get_sig\" function, there is no need to call \"read_all\" before, \"init\" is enough. get_sig, loads the signal automatically from disk if needed and not loaded.</li> <li>We can use dictionaries to query specific categorical codes and their hierarchies. <ul> <li>For example, by defining \"ICD10_CODE:J00-J99\", we'll capture all codes within this group based on the dictionary's definition of ICD10. </li> <li>This method relies on predefined parent-child pairs for hierarchy and does not use regular expressions. </li> <li>It is not limited to ICD10, ICD9 or specific known code system, but you will need to define the dictionaries correctly and their hierarchies.</li> </ul> </li> </ul> <pre><code># readmissions is data frame with readmitted patients\nrep = med.PidRepository()\nrep.read_all(FLAGS.rep, readmissions.pid.values.astype('int32'), ['ADMISSION','DIAGNOSIS_IP','DIAGNOSIS_OP'])\nadmissions = rep.get_sig('ADMISSION').rename(columns = {'time0':'outcomeTime'})\nreadmissions = readmissions.merge(admissions,on = ['pid','outcomeTime'],how='outer')\n# Handle missing admissions\nreadmissions.loc[((readmissions.outcome==1) | (readmissions.outcome==2) | (readmissions.outcome==3)) &amp; (readmissions.time1.isna()),'time1'] = readmissions.loc[((readmissions.outcome==1) | (readmissions.outcome==2) | (readmissions.outcome==3)) &amp; (readmissions.time1.isna()),'outcomeTime']\n# Read Relevant Codes\nicd9 = pd.read_csv(FLAGS.icd9,header=None,names=['code']).code.values\n# Add Adverse Events for each ICD9 code in icd9 dataframe. It also uses hierarchy defined in the dictionary, for example using \"487\" includes: 487.0, 487.1, 487.8, etc.\nlut = rep.dict.prep_sets_lookup_table(rep.dict.section_id('DIAGNOSIS_IP'),['ICD9_CODE:'+str(x) for x in icd9])\nip_diagnosis = rep.get_sig('DIAGNOSIS_IP',translate=False)\nip_diagnosis = ip_diagnosis[(lut[ip_diagnosis.val0]!=0)]\nop_diagnosis = rep.get_sig('DIAGNOSIS_OP',translate=False)\nop_diagnosis = op_diagnosis[(lut[op_diagnosis.val0]!=0)]\n</code></pre>"},{"location":"Python/Examples.html#load-medmodel-and-apply-predict-on-sample","title":"Load MedModel and apply (predict) on sample","text":"<pre><code>rep_path='' #Path of repositroy\nmodel_file ='' #Path of MedModel\nsamples_file = '' #path of samples or load samples from DataFrame using: samples.from_df(dataframe_object with the right columns)\nprint(\"Reading basic Repository structure for fitting model\")\nrep = med.PidRepository()\nrep.init(rep_path) #init model for first proccesing of \"model.fit_for_repository\"\n\u00a0\nprint(\"Reading Model\")\nmodel = med.Model()\nmodel.read_from_file(model_file)\nmodel.fit_for_repository(rep)\nsignalNamesSet = model.get_required_signal_names() #Get list of relevant signals the model needed to fetch from repository\n\u00a0\nprint(\"Reading Samples\")\nsamples = med.Samples()\nsamples.read_from_file(samples_file)\nids = samples.get_ids() #Fetch relevant ids from samples to read from repository\n\u00a0\nprint(\"Reading Repository\")\nrep.read_all(rep_path, ids, signalNamesSet) #read needed repository data\n\u00a0\n#Apply model:\nmodel.apply(rep, samples)\ndf = samples.to_df()\ndf.to_csv('output_file')\nsamples.write_to_file('write_to_samples_file')\n\u00a0\n#feature matrix exists in - model.features.to_df() . The \"samples\" object now has the scores\n</code></pre>"},{"location":"Python/Examples.html#learn-model-from-json-to-generate-matrix","title":"Learn model from json to generate matrix","text":"<pre><code>rep_path='' #Path of repositroy\njson_model ='' #Path of json\nsamples_file = '' #path of samples or load samples from DataFrame using: samples.from_df(dataframe_object with the right columns)\n\nprint(\"Reading basic Repository structure for fitting model\")\nrep = med.PidRepository()\nrep.init(rep_path) #init model for first proccesing of \"model.fit_for_repository\"\n\nprint(\"Reading Model\")\nmodel = med.Model()\nmodel.init_from_json_file(model_file)\nmodel.fit_for_repository(rep)\nsignalNamesSet = model.get_required_signal_names() #Get list of relevant signals the model needed to fetch from repository\n\nprint(\"Reading Samples\")\nsamples = med.Samples()\nsamples.read_from_file(samples_file)\nids = samples.get_ids() #Fetch relevant ids from samples to read from repository\n\nprint(\"Reading Repository\")\nrep.read_all(rep_path, ids, signalNamesSet) #read needed repository data\n\n#Learn model:\nmodel.learn(rep, samples)\nmodel.features.to_df().write_to_file('write_to_matrix_file')\n</code></pre>"},{"location":"Python/Examples.html#bootstrap-analysis","title":"Bootstrap analysis","text":"<pre><code>import pandas as pd\ndf=pd.read_feather('/nas1/Work/Users/Ilya/Mayo/Feathers/predictions_073.feather')\ndf=df[['true_V', 'prob_V']]\n#Example of Analyzing df with bootstrap\nbt=med.Bootstrap()\nres=bt.bootstrap(df['prob_V'], df['true_V'])\nall_measurment_names=res.keys()\nprint('AUC: %2.3f [%2.3f - %2.3f]'%(res['AUC_Mean'], res['AUC_CI.Lower.95'], res['AUC_CI.Upper.95']))\n#Can convert to dataframe with Measurement and Value columns:\nres_df=res.to_df()\nres_df[res_df['Measurement'].str.startswith('AUC')]\n</code></pre>"},{"location":"Python/Examples.html#bootstrap-analysis-on-samples","title":"Bootstrap analysis on samples","text":"<pre><code>samples=med.Samples()\nsamples.read_from_file('/nas1/Work/AlgoMarkers/Pre2D/pre2d_1_041219/Performance_no_drugs/OnTest/Partial_All_on_OnTest_2_no_drugs_filtered.preds')\nREP_PATH='/nas1/Work/CancerData/Repositories/THIN/thin_jun2017/thin.repository'\nJSON_TO_FILTER='/server/UsersData/alon/MR/Projects/Shared/Projects/configs/Diabetes/configs/bt_features.json'\nCOHORT_FILE='/server/UsersData/alon/MR/Projects/Shared/Projects/configs/Diabetes/configs/bt_params'\nbt=med.Bootstrap()\nres=bt.bootstrap_cohort(samples, REP_PATH,JSON_TO_FILTER, COHORT_FILE)\nres_df=res.to_df()\nres_df\n</code></pre>"},{"location":"Python/Examples.html#print-errors-in-medpython","title":"print errors in medPython","text":"<p><pre><code>print(med.cerr())\n</code></pre> </p>"},{"location":"Python/Examples.html#additional-examples","title":"Additional Examples","text":"<ul> <li>See <code>$MR_LIBS/Internal/MedPyExport/examples/MedProcUtils/</code> for more Python implementations. MR_LIBS is git repository</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html","title":"Extending and Developing the Medial C++ API for Python","text":"<p>This document outlines the low-level C++ details required to further develop the Python bindings for our API.</p>"},{"location":"Python/Extend%20and%20Develop.html#objectives","title":"Objectives","text":"<p>The goals are to:</p> <ol> <li>Use our API from Python.</li> <li>Employ Python as a \"Glue Language\".</li> <li>Enable prototyping, exploration, and discovery in Python.</li> <li>Ensure interoperability with other frameworks.</li> <li>Minimize maintenance overhead when updating code.</li> </ol>"},{"location":"Python/Extend%20and%20Develop.html#implementation-approach","title":"Implementation Approach","text":"<p>For maintainability, SWIG is chosen as the binding solution. Other options considered:</p> <ul> <li>Cython: Adds another language to the process.</li> <li>Boost::python: Limited support/resources; requires special definitions for each exported member.</li> <li>ctypes: Direct C calls, but parameter definitions are error-prone and may lack binary compatibility across platforms.</li> </ul> <p>Why SWIG?</p> <ul> <li>Mature project with extensive resources. TensorFlow uses SWIG for example.</li> <li>Handles memory and ownership complexities.</li> <li>Good NumPy support.</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#our-implementation-medpyexport","title":"Our Implementation: \"MedPyExport\"","text":"<ul> <li>Located in <code>$MR_ROOT/Libs/Internal/MedPyExport</code>.</li> <li>Wraps classes and exports only necessary functions for Python.</li> <li>Minimal learning curve for contributors familiar with C++.</li> <li>SWIG interface files (<code>.i</code>) are auto-generated during build.</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#notes-pitfalls","title":"Notes &amp; Pitfalls","text":"<ul> <li>Always use <code>std::vector</code> for vectors.</li> <li>Restart Jupyter Notebook after adding new functions/classes to reload wrappers.</li> <li>Use distinct names for NumPy parameters of different types.</li> <li>Method overloading is not supported (may be possible to implement).</li> <li>Avoid \"big-data\" loops; use NumPy/Pandas for vectorized operations.</li> <li>Build system is separate from the main CMake files.</li> <li>No direct Pandas DataFrame conversion yet.</li> <li>SWIG scans only simple C++ headers; keep implementation in <code>.cpp</code> files.</li> <li>Compilation works on both Windows and Linux (tested execution on Linux/Jupyter).</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#build-system","title":"Build System","text":"<ul> <li>Targeted for Linux machines</li> <li>Windows compilation works but does not produce a Python-loadable binary.</li> <li>Uses a specialized CMake file for SWIG integration.</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#directory-structure","title":"Directory Structure","text":"<p>Located in <code>$MR_ROOT/Libs/Internal/MedPyExport</code>:</p> <ul> <li><code>MedPyExport</code>: Source files.</li> <li><code>generate_binding</code>: CMake and binding generation files.</li> <li><code>MedPython</code>: SWIG interface files.<ul> <li><code>scripts</code>: Helper scripts for compilation.</li> </ul> </li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#key-files","title":"Key Files","text":"<ul> <li><code>make.sh</code>: Bash script to start CMake build.</li> <li><code>MedPython/MedPython.i</code>: Main SWIG interface.</li> <li><code>MedPython/medial-numpy.i</code>: NumPy SWIG interface.</li> <li><code>MedPython/MedPython.h</code>: Entry point for headers scanned by SWIG.</li> <li><code>MedPython/MedPython.c</code>: Entry point for C files scanned by SWIG.</li> <li><code>MedPython/pythoncode.i</code>: Python code for the binding.</li> <li><code>MedPython/scripts/make_apply.py</code>: Script to generate NumPy apply directives.</li> <li><code>MedPython/apply_directives.i</code>: Auto-generated SWIG directives.</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#extending-the-code","title":"Extending the Code","text":""},{"location":"Python/Extend%20and%20Develop.html#example-exporting-a-class","title":"Example: Exporting a Class","text":"<pre><code>// .h file for the class 'PidRepository'\n#include \"MedPyCommon.h\"\nclass MedPidRepository;\nclass MPPidRepository {\npublic:\n    MedPidRepository* o;\n    // ...\n};\n</code></pre> <ul> <li>Include <code>MedPyCommon.h</code> for utilities/macros.</li> <li>Exported class names start with <code>MP</code> to avoid conflicts.</li> <li>Use a pointer to the wrapped object.</li> <li>Keep headers simple for SWIG; implementation in <code>.cpp</code> files.</li> </ul> <p>Add all exported headers to <code>MedPyExport.h</code>:</p> <pre><code>#ifndef __MED_PY_EXPORT_H\n#define __MED_PY_EXPORT_H\n#include \"MedPyExportExample.h\"\n#include \"MPPidRepository.h\"\n#include \"MPDictionary.h\"\n// ...\n#endif\n</code></pre> <p>Python Usage:</p> <pre><code>import medpython as med\nrep = med.PidRepository()\n</code></pre>"},{"location":"Python/Extend%20and%20Develop.html#adding-a-new-class","title":"Adding a New Class","text":"<pre><code>class MPPidRepository {\npublic:\n    MedPidRepository* o;\n    MPPidRepository();\n    ~MPPidRepository();\n    int read_all(const string &amp;conf_fname);\n    string dict_name(int section_id, int id);\n    std::vector&lt;bool&gt; dict_prep_sets_lookup_table(int section_id, const std::vector&lt;std::string&gt; &amp;set_names);\n    // ...\n};\n</code></pre> <ul> <li>Basic types are mapped automatically.</li> <li>Use <code>std::vector</code> for vectors.</li> </ul> <p>Implementation Example:</p> <pre><code>#include \"MPPidRepository.h\"\n#include \"InfraMed/InfraMed/MedPidRepository.h\"\nMPPidRepository::MPPidRepository() : o(new MedPidRepository()) {}\nMPPidRepository::~MPPidRepository() { delete o; }\nint MPPidRepository::read_all(const std::string &amp;conf_fname) { return o-&gt;read_all(conf_fname); }\nstring MPPidRepository::dict_name(int section_id, int id) { return o-&gt;dict.name(section_id, id); }\n</code></pre>"},{"location":"Python/Extend%20and%20Develop.html#properties","title":"Properties","text":"<p>Implement getter/setter methods with <code>MEDPY_GET_</code> and <code>MEDPY_SET_</code> prefixes:</p> <pre><code>class MPSamples {\n    int MEDPY_GET_time_unit();\n    void MEDPY_SET_time_unit(int new_time_unit);\n};\n</code></pre> <p>Python Usage:</p> <pre><code>&gt;&gt;&gt; s = med.Samples()\n&gt;&gt;&gt; s.time_unit\n1\n</code></pre> <ul> <li>Omit the setter for read-only properties.</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#static-const-variables","title":"Static Const Variables","text":"<p>Mapped to class variables in Python:</p> <pre><code>class MPTime {\npublic:\n    static const int Undefined;\n    static const int Date;\n    static const int Years;\n};\n</code></pre> <p>Implementation:</p> <pre><code>const int MPTime::Undefined = MedTime::Undefined;\nconst int MPTime::Date = MedTime::Date;\n</code></pre> <p>Python Usage:</p> <pre><code>print(med.Time.Date)\n</code></pre>"},{"location":"Python/Extend%20and%20Develop.html#iterators","title":"Iterators","text":"<p>Array Iterator Example:</p> <pre><code>class MPSigVectorAdaptor {\npublic:\n    int __len__();\n    MPSig __getitem__(int i);\n};\n</code></pre> <ul> <li>Class name ends with <code>VectorAdaptor</code>.</li> <li>Implements <code>__len__</code> and <code>__getitem__</code>.</li> </ul> <p>Map Iterator Example:</p> <pre><code>class MPStringFeatureAttrMapAdaptor {\n    std::map&lt;std::string, FeatureAttr&gt;* o;\npublic:\n    int __len__();\n    MPFeatureAttr __getitem__(std::string key);\n    void __setitem__(std::string key, MPFeatureAttr&amp; val);\n    std::vector&lt;std::string&gt; keys();\n};\n</code></pre> <ul> <li>Class name ends with <code>MapAdaptor</code>.</li> <li>Implements <code>__len__</code>, <code>__getitem__</code>, and optionally <code>__setitem__</code>, etc.</li> </ul>"},{"location":"Python/Extend%20and%20Develop.html#numpy-arrays","title":"NumPy Arrays","text":"<p>Input Arrays:</p> <pre><code>class MPPidRepository {\n    int read_all(string conf_fname, MEDPY_NP_INPUT(int* pids_to_take, int num_pids_to_take));\n}\n</code></pre> <p>In-place Arrays:</p> <pre><code>void MedPyExportExample::numpy_vec_in_out(MEDPY_NP_INPLACE(double* vec, int m));\n</code></pre> <p>Output Arrays:</p> <pre><code>class MPFeatures {\n    void MEDPY_GET_weights(MEDPY_NP_OUTPUT(float** float_out_buf, int* float_out_buf_len));\n};\n</code></pre> <p>Variant Output Example:</p> <pre><code>void getitem(string key, MEDPY_NP_VARIANT_OUTPUT(void** var_arr, int* var_arr_sz, int* var_arr_type)) {\n    // Implementation sets var_arr, var_arr_sz, var_arr_type based on key\n    *var_arr_sz = 0;\n    if (key == \"i\")\n    {\n        *var_arr = (void*)malloc(sizeof(int) * 10);\n        *var_arr_sz = 10;\n        *var_arr_type = (int)MED_NPY_TYPES::NPY_INT;\n        for (int i = 0; i &lt; 10; i++)\n            ((*(int**)var_arr))[i] = i * 5;\n    }\n    else if (key == \"d\")\n    {\n        *var_arr = (void*)malloc(sizeof(double) * 20);\n        *var_arr_sz = 20;\n        *var_arr_type = (int)MED_NPY_TYPES::NPY_DOUBLE;\n        for (int i = 0; i &lt; 20; i++)\n            ((*(double**)var_arr))[i] = i * 2.5;\n    }\n    else if (key == \"f\")\n    {\n        *var_arr = (void*)malloc(sizeof(float) * 15);\n        *var_arr_sz = 15;\n        *var_arr_type = (int)MED_NPY_TYPES::NPY_FLOAT;\n        for (int i = 0; i &lt; 15; i++)\n            ((*(float**)var_arr))[i] = i * 0.33333f;\n    }\n    else if (key == \"n\")\n    {\n        *var_arr = nullptr;\n    }\n}\n</code></pre> <p>Supported NumPy Types:</p> <pre><code>NPY_BOOL, NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT,\nNPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG, NPY_FLOAT, NPY_DOUBLE,\nNPY_LONGDOUBLE, NPY_CFLOAT, NPY_CDOUBLE, NPY_CLONGDOUBLE, NPY_OBJECT,\nNPY_STRING, NPY_UNICODE, NPY_VOID, NPY_DATETIME, NPY_TIMEDELTA, NPY_HALF,\nNPY_NTYPES, NPY_NOTYPE, NPY_CHAR, NPY_USERDEF, NPY_NTYPES_ABI_COMPATIBLE\n</code></pre>"},{"location":"Python/Extend%20and%20Develop.html#helper-functions-macros","title":"Helper Functions &amp; Macros","text":"<ul> <li>Docstrings: <code>MEDPY_DOC(function_or_class_name, docstring)</code></li> <li>Ignore for SWIG: <code>MEDPY_IGNORE(...)</code> or <code>#ifdef SWIG ... #endif</code></li> <li>Buffer to Vector: <code>buf_to_vector(int_in_buf, int_in_buf_len, idx);</code></li> <li>Vector to Buffer: <code>vector_to_buf(o-&gt;weights, float_out_buf, float_out_buf_len);</code></li> </ul>"},{"location":"Python/Python%20AlgoMarker%20API%20Wrapper.html","title":"Python AlgoMarker API Wrapper","text":"<p>This project provides a Python wrapper for the AlgoMarker C library (<code>libdyn_AlgoMarker.so</code>). The wrapper allows you to interact with AlgoMarker using Python.</p>"},{"location":"Python/Python%20AlgoMarker%20API%20Wrapper.html#location","title":"Location","text":"<p>You can find the Python library and a tool for querying AlgoMarkers in the MR_Tools at this folder <code>$MR_Tools/AlgoMarker_python_API</code></p>"},{"location":"Python/Python%20AlgoMarker%20API%20Wrapper.html#files","title":"Files","text":"<ul> <li>AlgoMarker.py: Main Python wrapper for the C library. The wrapper will use the new JSON API if available, or fall back to the older API for compatibility.</li> <li>test_algomarker_lib.py: Utility tool that uses <code>AlgoMarker.py</code> to query AlgoMarker.</li> <li>AlgoMarker_minimal.py: Minimal version of the wrapper with fewer functions.</li> <li>simple_app_example.py: Example app demonstrating usage of the minimal wrapper.</li> </ul>"},{"location":"Python/Python%20AlgoMarker%20API%20Wrapper.html#example-usage","title":"Example Usage","text":"<p>Run the utility tool with:</p> <pre><code>python $MR_ROOT/Tools/AlgoMarker_python_API/test_algomarker_lib.py \\\n  --amconfig /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lungflag.amconfig \\\n  --output /tmp/results.txt \\\n  --add_data_json_path /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/data.single.json \\\n  --request_json_path /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/req.single.json \\\n  --amlib /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lib/libdyn_AlgoMarker.so\n</code></pre> <ul> <li><code>--amlib</code> (optional): Specify a custom library path.</li> <li>If your request JSON includes <code>\"data\"</code>, you can run:</li> </ul> <pre><code>python $MR_ROOT/Tools/AlgoMarker_python_API/test_algomarker_lib.py \\\n  --amconfig /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lungflag.amconfig \\\n  --output /tmp/results.txt \\\n  --request_json_path /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/req.full.json\n</code></pre>"},{"location":"Python/Python%20AlgoMarker%20API%20Wrapper.html#simple-usage-example","title":"Simple Usage Example","text":"<pre><code>from AlgoMarker_minimal import AlgoMarker\nimport json\n\nALGOMARKER_PATH = '/nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lungflag.amconfig'\nREQUEST_JSON = '/nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/req.full.json'\n\ndef read_text_file(path):\n    with open(path, 'r') as fr:\n        return fr.read()\n\nwith AlgoMarker(ALGOMARKER_PATH) as algomarker:\n    discovery_json = algomarker.discovery()\n    print('$&gt; discovery - first 100 characters:')\n    print(json.dumps(discovery_json, indent=True)[:100])\n    print('$&gt; calculate - first 200 characters:')\n    resp = algomarker.calculate(read_text_file(REQUEST_JSON))\n    print(json.dumps(resp, indent=True)[:200])\n\nprint('All Done')\n</code></pre>"},{"location":"Python/Python%20AlgoMarker%20API%20Wrapper.html#running-as-a-fastapi-server","title":"Running as a FastAPI Server","text":"<p>You can also run the wrapper as a FastAPI server. This will wrapper <code>AlgoMarker.py</code></p> <p>To configure, edit <code>run_server.sh</code>:</p> <ul> <li>Set <code>AM_CONFIG</code> to the path of your AlgoMarker <code>.amconfig</code> file.</li> <li>Set <code>AM_LIB</code> to the path of your compiled library (default is <code>/lib/libdyn_AlgoMarker.so</code> in the same directory as the amconfig file).</li> </ul>"},{"location":"Python/Python%20binding%20Troubleshooting.html","title":"Python Binding Troubleshooting","text":""},{"location":"Python/Python%20binding%20Troubleshooting.html#problem-1-import-med-does-not-work","title":"Problem 1: <code>\"import med\"</code> does not work","text":""},{"location":"Python/Python%20binding%20Troubleshooting.html#possible-cause-medpython-is-not-in-path","title":"Possible Cause: MedPython is not in path","text":"<p>Please make sure and print our PYTHONPATH environment variable: <pre><code>echo $PYTHONPATH\n</code></pre> and make sure it points out to a folder that contains <code>med.py</code> file. If not, please follow Setup</p>"},{"location":"Python/Python%20binding%20Troubleshooting.html#problem-2-medpython-fails-to-compile","title":"Problem 2: MedPython fails to compile","text":"<p>If you encounter errors like the following:</p> <pre><code>[ 27%] Swig compile medpython.i for python\n:1: Error: Unable to find 'swig.swg'\n:3: Error: Unable to find 'python.swg'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:8: Error: Unable to find 'exception.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:9: Error: Unable to find 'typemaps.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:10: Error: Unable to find 'std_string.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:11: Error: Unable to find 'std_vector.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medial-numpy.i:3295: Error: Unable to find 'std_complex.i'\n</code></pre> <p>This is likely due to an outdated <code>CMakeCache.txt</code> file containing old SWIG settings. To resolve this, delete the <code>CMakeCache.txt</code> file using the following command (Change MR_LIBS to the folder you clones MR_LIBS into):</p> <pre><code>rm $MR_LIBS/Internal/MedPyExport/generate_binding/CMakeBuild/Linux/Release/CMakeCache.txt\n</code></pre>"},{"location":"Python/Python%20binding%20Troubleshooting.html#problem-3-stderr-in-jupyter-notebooks","title":"Problem 3: stderr in Jupyter Notebooks","text":"<p>Some API messages are printed to stderr and may not appear in Jupyter notebooks. To display these messages, use the <code>cerr()</code> utility:</p> <pre><code>med.cerr()\n</code></pre>"},{"location":"Repositories/index.html","title":"Index","text":""},{"location":"Repositories/index.html#repositories","title":"Repositories","text":"<p>Link to Tutorial of using our ETL:\u00a0ETL Tutorial</p>"},{"location":"Repositories/index.html#load-file-format","title":"Load file format","text":"<p>Please refer to Load new repository</p>"},{"location":"Repositories/Load%20new%20repository.html","title":"Loading a New Repository","text":""},{"location":"Repositories/Load%20new%20repository.html#best-practices","title":"Best Practices","text":"<ul> <li>Keep it Simple: Load signals with minimal preprocessing. Handle outliers and clean data within the model itself, not during the ETL stage. This approach simplifies implementation, reduces ETL errors, and results in a more robust model. Future implementations will be easier with a straightforward ETL process.</li> <li>Ensure that all signals are in the correct units - each signal is expected to use the appropriate measurement unit.</li> <li>It is recommended to separate your code into two parts:<ul> <li>Code for fetching the data  </li> <li>Code for processing the data  </li> </ul> </li> </ul> <p>This makes the code easier to read and helps clarify what minimal processing was applied to each signal.</p> <p>You can use your own tools and methods to create \u201cloading files\u201d (described below), or use our ETL tool to both build the ETL process and test the results. More details here: ETL Tool You can start from this code example and change it for your own needs</p> <p>Steps to create loading files on your own:</p>"},{"location":"Repositories/Load%20new%20repository.html#step-1-prepare-load-files","title":"Step 1: Prepare Load Files:","text":"<p>In this step you will need to create ETL loading files for each signal.</p>"},{"location":"Repositories/Load%20new%20repository.html#data-file-format","title":"Data File Format","text":"<ul> <li>Files are TAB-delimited, no header.</li> <li>Columns:   1. Patient ID   2. Signal name   3. Time channel 0 value   4. ...   5. Time channel i value   6. Value channel 0 value   7. ...   8. Value channel i value   9. Any additional columns are ignored</li> </ul> <p>Notes:</p> <ul> <li>A single file can contain multiple signals (different values in the \"signal name\" column).</li> <li>Rows should be sorted by patient ID and the first time channel (if present).</li> <li>The number of time/value channels used depends on the signal definition.</li> <li>For categorical value columns, use strings without spaces (use underscores <code>_</code> instead). A suitable dictionary is required for conversion to numeric values.</li> </ul>"},{"location":"Repositories/Load%20new%20repository.html#example-blood-pressure-signal","title":"Example: Blood Pressure Signal","text":"<p>(1 time channel, 2 value channels; extra columns are ignored)</p> <pre><code>5000001 BP 20030324 60.0 100.0 246..00-1005010500 null value\n5000001 BP 20061218 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20071008 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20090309 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20100802 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20120125 67.0 90.0 246..00-1005010500\n</code></pre>"},{"location":"Repositories/Load%20new%20repository.html#step-2-prepare-configuration-file-load_config_file","title":"Step 2: Prepare Configuration file <code>load_config_file</code>","text":"<p>The configuration file uses a delimited format with the following settings:</p> <ul> <li>DESCRIPTION: Description of the repository (ignored by loader)</li> <li>RELATIVE: Use relative paths (set to 1)</li> <li>SAFE_MODE: If 1, stops on critical errors (set to 1)</li> <li>MODE: Use value 3</li> <li>PREFIX: Prefix for data file names stored in OUTDIR</li> <li>CONFIG: Output path for the repository config file (contains dictionaries, signals, etc.)</li> <li>SIGNAL: Path to signal definitions (Signal file format)</li> <li>FORCE_SIGNALS: Comma-separated list of required signals (e.g., BYEAR,GENDER); patients missing these are dropped</li> <li>DIR: Path for config files and data files (relative if RELATIVE=1)</li> <li>OUTDIR: Output directory for the repository</li> <li>DICTIONARY: Path(s) to dictionary files for categorical data</li> <li>DATA: Path(s) to data files</li> <li>LOAD_ONLY: (Optional) Comma-separated list of signals to load; others are ignored (useful for partial updates)</li> </ul>"},{"location":"Repositories/Load%20new%20repository.html#example-configuration-file","title":"Example Configuration File","text":"<pre><code># Example file - lines starting with \"#\" are comments\nDESCRIPTION     THIN18 data - full version\nRELATIVE        1\nSAFE_MODE       1\nMODE            3\nPREFIX          data/thin_rep\nCONFIG          thin.repository\nSIGNAL          thin.signals\nFORCE_SIGNALS   GENDER,BYEAR\nDIR             /nas1/Temp/Thin_2018_Loading/rep_configs\nOUTDIR          /nas1/Work/CancerData/Repositories/THIN/thin_2018_2\nDICTIONARY      dicts/dict.drugs_defs\nDICTIONARY      dicts/dict.bnf_defs\nDATA            ../demo2/GENDER\nDATA            ../demo2/BYEAR\nDATA            ../demo2/BDATE\n</code></pre> <p>After you have completed and prepered all loading files please execute:</p>"},{"location":"Repositories/Load%20new%20repository.html#step-3-load-the-repository","title":"Step 3: Load the Repository","text":"<pre><code>Flow --rep_create --convert_conf $PATH_TO_CONFIG_FILE --load_err_file $OPTIONAL_FILE_PATH_TO_STORE_ERRORS\n# Optional: Control error thresholds with --load_args, e.g.:\n# --load_args \"check_for_error_pid_cnt=0;allowed_missing_pids_from_forced_ratio=0.05;max_bad_line_ratio=0.05;allowed_unknown_catgory_cnt=50\"\n</code></pre>"},{"location":"Repositories/Load%20new%20repository.html#step-4-generate-reverse-index","title":"Step 4: Generate Reverse Index","text":"<p>After a successful load, generate the reverse index:</p> <pre><code>Flow --rep_create_pids --rep $REPOSITORY_PATH\n</code></pre>"},{"location":"Repositories/Medical%20vocabulary%20mappings.html","title":"Medical vocabulary mappings","text":"<p>There are several domains of medical knowledge/onthologies that is getting updated over time - for example diagnosis and medications. We have several tools for generating dictionaries for those vocabularies. \u00a0</p>"},{"location":"Repositories/Medical%20vocabulary%20mappings.html#updating-medical-vocabulary","title":"Updating Medical Vocabulary","text":"<p>We have several scripts under TOOLS git repository - $MR_ROOT/Tools/DictUtils: Steps to update medications:</p> <ol> <li>Download update vocabulary from:\u00a0https://athena.ohdsi.org/search-terms/start</li> <li>Select for example Drugs, Rx Norm + ATC</li> <li>Copy and extract the files into:\u00a0/nas1/Work/Data/Mapping/Medications - Or to equivalent folder for Diagnosis or Procedures, etc</li> <li>Run the script to extract those files into our dicts format with mapping from RX Norm to ATC -\u00a0$MR_ROOT/Tools/DictUtils/ontologies_scripts/RX_to_ATC.new.py </li> <li>generate_rx_codes() - generates definitions for RX Norm + ATC from \"CONCEPT.csv\" to $MR_ROOT/Tools/DictUtils/Ontologies/RX/dicts, $MR_ROOT/Tools/DictUtils/Ontologies/ATC/dicts:\u00a0</li> <li>generate_rx_maps() - generate mapping from RX Norm to ATC in\u00a0$MR_ROOT/Tools/DictUtils/Ontologies/RX/dicts</li> <li>add_atc_hir() - Creates the hierarchy for ATC codes</li> <li>create_atc_syn() - generate synonm dicts for ATC to include codes in old format of ATC_ABB_CDD instead of ATC:ABBCDD Jupyter notebook with some test on the raw files of OHDSI:\u00a0http://node-02:9000/notebooks/alon-internal/Medications_mapping.ipynb \u00a0 (get_rxnorm_dicts.py - is old script of different data source + RX_to_ATC.py)</li> </ol>"},{"location":"Repositories/Medical%20vocabulary%20mappings.html#todos","title":"TODOs:","text":"<ul> <li>Need to complete code for NDC - current code:\u00a0$MR_ROOT/Tools/DictUtils/ontologies_scripts/NDC_to_ATC.py</li> <li>SNOMED -\u00a0$MR_ROOT/Tools/DictUtils/ontologies_scripts/get_snomed.py - but it's based on different data, that requires registration and to send reports+regulations... Need to switch to OHDSI\u00a0</li> <li>ICD10, ICD9 -\u00a0$MR_ROOT/Tools/DictUtils/ontologies_scripts/icd*\u00a0 scripts \u00a0</li> </ul>"},{"location":"Repositories/Medical%20vocabulary%20mappings.html#loading-categorical-signals-with-those-dictionaries","title":"Loading categorical signals with those dictionaries:","text":"<p>After having base dictionaries with mappings, when loading new signal we can use those dictionaries and - search for missing codes, define them, use internal dictionary codes. The library is located in:\u00a0$MR_ROOT/Tools/RepoLoadUtils/common/dicts_utils.py \u00a0 You can use\u00a0create_dict_generic to \"merge\" your signal codes with existing known ontology (for example ICD10) - it will define the missing codes for you and print how many are they? They are also stored in a different dictionary, so they will be easily located, what codes are missing. If it's too much, you can consider updating the ontology dictionaries. Function inputs:</p> <ul> <li>cfg - Configuration object with \"work_dir\" that points to workdir path + \"dict_folder\" that points to path with the Generic medial vocabularies - for example \"$MR_ROOT/Tools/DictUtils/Ontologies/RX/dicts\"<ul> <li>work_dir points to path that includes those subfolders:<ul> <li>FinalSignals - With Prepared signals to load - input path for reading the signals</li> <li>rep_configs/dicts - where we are going to store dictionaries for loading process. The output path of this function</li> </ul> </li> <li>dict_folder - base path for searching dicts. You can pass it as empty string '' - But than you will need to specify full paths in\u00a0def_dicts, sets_dicts</li> </ul> </li> <li>def_dicts - list of dictionaries names that are located under\u00a0dict_folder . Contains \"DEF\" commands</li> <li>set_dicts - list of dictionaries names that are located under\u00a0dict_folder. Contains \"SET\" commands. We currently support Vocabulary dictionaries that has only \"DEF\" commands and \"SET\" command and you need to separate them, to use that tool. Till now the dicts are separated, so there is no concern.</li> <li>signal - name of the signal</li> <li>data_files_prefix - name of the data file in\u00a0FinalSignals that needs to be scanned for codes. Mainly we name the file as the name of the signal it contains, so it will be many times the same as signal</li> <li>header - the header of the datafile - since FinalSignals doesn't contains headers</li> <li> <p>to_use_list - list of columns from \"header\" that contains our codes for merging with the known dicts from \"def_dict + set_dicts\" \u00a0 Example usage of diagnosis loading in Optum - it contains additional 2 categorical columns: <pre><code>def_dicts=['dict.icd9dx', 'dict.icd10']\nset_dicts=['dict.set_icd9dx', 'dict.set_icd10', 'dict.set_icd9_2_icd10']\nheader=['pid','signal' ,'diag_date', 'diagnosis_code', 'diagnosis_status', 'diag_source']\ncategorical_cols=['diagnosis_code', 'diagnosis_status', 'diag_source']\ncreate_dict_generic(cfg, def_dicts, set_dicts, 'DIAGNOSIS', 'DIAGNOSIS', header, categorical_cols)\n</code></pre> \u00a0 Additional function for internal dicts is generate_dict_from_codes. Function inputs:</p> </li> <li> <p>map_df - path or dataframe with 2 columns, first column is the codes, the second column is description.\u00a0</p> </li> <li>outpath - where to store output dictionary</li> <li>min_code - from which code number to start the DEF This function construct dict with 2 rows for each code - the first is the \"code\" and the second row is the description. For example, we might pass \"ATC_XYZ \\t description\" - it will generate \"DEF 1 ATC_XYZ\" and \"DEF 1 description\".\u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Repositories/Repository%20Signals%20file%20format.html","title":"Repository Signals File Format","text":"<ul> <li>Lines beginning with <code>#</code> are comments.</li> <li>Each line in the file defines either:</li> <li>A Generic Signal Type     Use <code>GENERIC_SIGNAL_TYPE</code> to declare a new signal type. The format is: <code>GENERIC_SIGNAL_TYPE [type_alias] [GSV_string_spec]</code>     (Three tab-separated fields)<ul> <li><code>GENERIC_SIGNAL_TYPE</code>: Marks the line as a signal type definition.</li> <li><code>type_alias</code>: A short name for the signal type.</li> <li><code>GSV_string_spec</code>: The type specification. List time channels first (if any), then value channels.  </li> <li>Time channels use <code>T(X)</code>, where <code>X</code> is a comma-separated list of basic types (see Generic (Universal) Signal Vectors).  </li> <li>Example: <code>T(i)</code> means one integer time channel; <code>T(i,i)</code> means two integer time channels (can also be written as <code>T(i),T(i)</code>).  </li> <li>Value channels follow after time channels.</li> </ul> </li> <li>A Signal Definition     Use <code>SIGNAL</code> to define a signal. Each line should have at least six fields (extra fields are ignored). If fewer than six are provided, defaults are used for missing fields.<ul> <li><code>SIGNAL</code>: Marks the line as a signal definition.</li> <li><code>signal_name</code>: The name of the signal.</li> <li><code>signal_unique_code</code>: A unique numeric code for the signal within the repository.</li> <li><code>signal_type</code>: Specify the type directly as a <code>GSV_string_spec</code> (e.g., <code>T(i),V(f)</code> for one integer time channel and one float value channel), or refer to a previously defined type alias using <code>16:type_alias</code>.</li> <li><code>comment</code>: Free text, ignored by the system. Can be empty, but the next field must be present.</li> <li><code>categorical_bitmask</code>: A bitmask indicating which value channels are categorical (e.g., <code>10</code> means the first of two value channels is categorical, the second is numeric). Default is <code>0</code> for all value channels.</li> <li><code>units</code> (optional): Units for the signal, separated by <code>|</code> for multiple channels.</li> </ul> </li> </ul>"},{"location":"Repositories/Repository%20Signals%20file%20format.html#example","title":"Example","text":"<pre><code># Tab-delimited format\n\n# Legacy signals definition\nSIGNAL  GENDER      100     0       Male=1,Female=2    1  \nSIGNAL  BP          920     8    systolic,diastoloc    00    mmHg,mmHg\nSIGNAL  Hemoglobin  1000    1     cbc   0   mg/dL\nSIGNAL  RC          2309    1       Med All ReadCodes ^[0-9A-HJ-NP-U] .no I,O,V,W,X,Y    1\nSIGNAL  Drug        2400    8       Drugs: date, drug code, duration in days, use proper    10    days\n\n# Newer signals definition:\nGENERIC_SIGNAL_TYPE   VInt    V(i)\nGENERIC_SIGNAL_TYPE   LabT1V1    T(i),V(f)\nGENERIC_SIGNAL_TYPE   CategoricalT1V1    T(i),V(i)\nGENERIC_SIGNAL_TYPE   LabT1V2    T(i),V(f,f)\nGENERIC_SIGNAL_TYPE   Date2Val_i_f    T(i),V(i,f)\n\nSIGNAL  GENDER      100     16:VInt       Male=1,Female=2    1  \nSIGNAL  BP          920     16:LabT1V2    systolic,diastoloc    00    mmHg,mmHg\nSIGNAL  Hemoglobin  1000    16:LabT1V1     cbc   0   mg/dL\nSIGNAL  RC          2309    16:CategoricalT1V1       Med All ReadCodes ^[0-9A-HJ-NP-U] .no I,O,V,W,X,Y    1\nSIGNAL  Drug        2400    16:Date2Val_i_f       Drugs: date, drug code, duration in days, use proper    10    days\n</code></pre>"},{"location":"Repositories/Repository%20Viewers.html","title":"Repository Viewers","text":"<p>Repository viewers provide a simple graphical interface for viewing patient signals. The system uses a C++ backend server (built with Boost) and the plotly.js library for interactive charts, tables, heatmaps, and more.</p>"},{"location":"Repositories/Repository%20Viewers.html#how-it-works","title":"How It Works","text":"<ul> <li>The viewer sends a request to the server.</li> <li>The server responds with an HTML page via POST.</li> <li>The page is rendered with all graphics.</li> </ul>"},{"location":"Repositories/Repository%20Viewers.html#compiling","title":"Compiling","text":"<ul> <li>Compile AllTools with the <code>Tools/AllTools/full_build.sh</code> script.</li> <li>The main application is <code>SimpleHttpServer</code>, found under <code>Linux/Release</code> after building.</li> </ul>"},{"location":"Repositories/Repository%20Viewers.html#simplehttpserver-parameters","title":"SimpleHttpServer Parameters","text":"<ul> <li><code>rep</code>: Repository to use.</li> <li><code>plotly_config</code>: Configuration file (see below); usually, the default is sufficient.</li> <li><code>server_dir</code>: Directory for server files (default is usually fine).</li> <li><code>address</code>: Server IP (use <code>ip addr show</code> to find yours.</li> <li><code>port</code>: Server port (avoid 80, 8080, 7090, 8090, 7990; ensure your chosen port is free).</li> </ul>"},{"location":"Repositories/Repository%20Viewers.html#configuration-file","title":"Configuration File","text":"<ul> <li>Example and definitions: <code>$MR_ROOT/Libs/Internal/MedPlotly/MedPlotly/BasicConfig.txt</code></li> <li>Contains default parameters and panel definitions.</li> <li>Panels become plots and can contain multiple signals.</li> </ul>"},{"location":"Repositories/Repository%20Viewers.html#key-parameters","title":"Key Parameters","text":"<ul> <li><code>JSDIR</code>: Directory for JavaScript files.</li> <li><code>JSFILES</code>: Main plotly.js file.</li> <li><code>NULL_ZEROS</code>: If 1, skips zero values in plots (useful for outliers).</li> <li><code>LOG_SCALE</code>: If 1, uses logarithmic axis scaling.</li> <li><code>WIDTH</code>/<code>HEIGHT</code>: Default panel size.</li> <li><code>BLOCK_MODE</code>: If 1 (recommended), arranges graphs in a line if space allows.</li> <li><code>SIG &lt;sig_name&gt; &lt;parameters&gt;</code>: Override defaults for specific signals.</li> <li><code>DRUG_GROUP</code>: Defines drug groups for the drugs heatmap (see config examples).</li> <li><code>PANEL</code>: Defines a panel with name, title, signals, and optional size/params.</li> <li><code>VIEW</code>: Lists default panels to show.</li> <li><code>REP_PROCESSORS</code>: JSON file for MedModel to configure repository processors.</li> </ul>"},{"location":"Repositories/Repository%20Viewers.html#example-configurations","title":"Example Configurations","text":"<ul> <li><code>BasicConfig</code>: For THIN, AppleTree repositories.</li> <li><code>MHSConfig</code>: For Maccabi, KP.</li> <li><code>RambamConfig</code>: For Rambam repository.</li> <li><code>MimicConfig</code>: For Mimic repository.</li> </ul> <p>You can create or modify configs as needed. The MedPlotly library parses these files and generates plotly inputs based on panel definitions and user requests.</p>"},{"location":"Repositories/Repository%20Viewers.html#default-servers","title":"Default Servers","text":"<ul> <li>Default servers usually run on node-01. You can start your own in different node.</li> <li>If default servers are down, start your own or request a restart.</li> </ul>"},{"location":"Repositories/Repository%20Viewers.html#running-the-viewers","title":"Running the Viewers","text":"<ul> <li>Script location: <code>$MR_ROOT/Projects/Scripts/Bash-Scripts/run_viewer.sh</code> (included in your PATH).</li> <li>To edit server/port list: <code>$MR_ROOT/Projects/Scripts/Python-scripts/viewers_config.py</code></li> </ul> <pre><code>viewers start\n# To stop all viewers:\nviewers stop\n</code></pre> <ul> <li>Viewers run detached from your SSH session (they continue if your session ends).</li> <li>No output is printed to your screen.</li> <li>Error logs: <code>/nas1/Work/CancerData/Repositories/viewers_log</code></li> </ul> <p>The script to launch servers and display the index page is at <code>$MR_ROOT/Projects/Scripts/Python-scripts/viewers_runner.py</code>. This file shows the latest commands for starting servers.</p>"},{"location":"Repositories/Repository%20Viewers.html#viewer-features","title":"Viewer Features","text":"<ul> <li>Enter a PID number and press send.</li> <li>Mark a specific date (drawn as a vertical black line on graphs).</li> <li>Some viewers allow specifying a date range (useful for Rambam and Mimic3 viewers).</li> <li>Each graph is interactive (zoom, pan, hover, etc.).</li> <li>Select panels/signals from the list or add them in the signal charts box (space, semicolon, or newline separated).</li> </ul> <p>This flexibility lets you view signals not included in the default</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/index.html","title":"Index","text":""},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/index.html#etl-workflow","title":"ETL Workflow","text":"<p>Please follow ETL Tutorial</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL_process%20TODO.html","title":"ETL_process TODO","text":"<ul> <li>Extend tests<ul> <li>Add more numeric test, compare dists to MHS,THIN percentiles. test resolution</li> </ul> </li> <li>parallel the processing</li> <li>Allow \"silence\" run and to skip errors and present all in the end</li> <li>Aggregate stats for processing log</li> <li>Join with BDATE to extract age for stats/filters?</li> <li>Stop on error - raise error when prepare_final_signals fails</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/index.html","title":"Index","text":""},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/index.html#etl-workflow-the-recommended-tool","title":"\ud83d\udcc4 ETL Workflow: The Recommended Tool","text":"<p>Using our ETL tool streamlines data loading by providing a standardized, robust, and reproducible process.  While manual loading file creation is possible, our tool is highly recommended for its numerous benefits:</p> <ul> <li> <p>Validation and Error Reporting: The tool performs extensive checks to ensure correct data format, proper sorting, and valid lab values. It provides clear, informative error messages that help you quickly fix issues, saving significant time during the loading process.</p> </li> <li> <p>Automated Configuration: It eliminates the need for manual configuration by automatically generating all necessary files for the final loading process. This includes:</p> <ul> <li>Signals Config Defines signal types and units, allowing you to easily add new signals or override existing ones.</li> <li>Dictionary Files Automatically creates conversion dictionaries for all categorical signals (e.g., converting strings to numerical values). It can even pull hierarchical data from known ontologies, such as ICD-10 codes, when a specific prefix is used.</li> <li>Conversion Config A central configuration file that references all necessary dictionaries, signals, and data files for the load. Will be used by <code>Flow</code> app to convert those raw data files into InfraMed Binary format.</li> <li>Loading Script Generates a final script with a <code>Flow</code> to run the entire loading process in one go.</li> </ul> </li> <li> <p>Efficient Batch Processing: The tool supports batch processing, which is crucial for handling large datasets efficiently and managing memory usage. If a loading process fails, it can resume from the last successful step, preventing the need to restart from the beginning.</p> </li> <li> <p>Comprehensive Logging: The tool logs all processing steps, test results, and creates distribution graphs for all signals, providing a clear overview of the data and the loading process.</p> </li> <li> <p>Code Reusability: It reuses common data elements and testing procedures, drastically reducing the amount of code you need to write. Our scripts are typically 3-4 times shorter than older, manual ETL scripts.</p> </li> </ul> <p>ETL_infra.pptx (slide deck)</p> <p>The code for this infrastructure was written with less strict standards as it is not part of our main production environment. While using the infrastructure is designed to be comfortable, modifying it may be challenging due to this less rigorous policy. Bugs may also be present.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/index.html#best-practices","title":"Best Practices","text":"<p>Keep it Simple: Load signals with minimal preprocessing. Handle outliers and clean data within the model itself, not during the ETL stage.  This approach simplifies implementation, reduces ETL errors, and results in a more robust model.  Future implementations will be easier with a straightforward ETL process. The only thing to take care of in the ETL is formating and right units for labs - that's it.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/index.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>The first step is to setup the ETL infrastructure. Follow the detailed instructions in the Setup documentation to begin.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html","title":"ETL Process \u2013 Dynamic Testing of Signals","text":"<p>You can define both global tests (applied across all ETL processes) and local tests (specific to a given ETL process). Local tests can override global tests if they share the same name in the local path.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#test-locations","title":"Test Locations","text":"<ul> <li>Global tests: <code>$MR_ROOT/Tools/RepoLoadUtils/common/ETL_Infra/tests</code></li> <li>Local tests: <code>$CODE_DIR/tests</code></li> </ul> <p>The code is executed from <code>$MR_ROOT/Tools/RepoLoadUtils/common/ETL_Infra</code>. This means you can use relative paths to access config files, dictionaries, etc.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#test-organization","title":"Test Organization","text":"<ul> <li>Each test directory (global or local) contains subdirectories for groups of tests.</li> <li>Subdirectory names correspond to either:<ul> <li>A signal name, or  </li> <li>A group of signals (e.g., <code>\"labs\"</code>, <code>\"cbc\"</code>).  </li> </ul> </li> <li>Only signals matching the directory name will be tested.</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#test-function-format","title":"Test Function Format","text":"<p>Each test file must include a function called <code>Test</code> with the following signature:</p> <pre><code>def Test(df: pd.DataFrame, si, codedir: str, workdir: str) -&gt; bool:\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#arguments","title":"Arguments:","text":"<ul> <li>df: Input dataframe containing the signal to test</li> <li>si: Signal information object<ul> <li>si.t_ch: Array of time channel types (i = int, f = float, etc.)</li> <li>si.v_ch: Array of value channel types</li> </ul> </li> <li>codedir: Path to the ETL code (useful for accessing the config folder)</li> <li>workdir: Working directory for storing outputs</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#return-value","title":"Return value:","text":"<ul> <li>True if the test passes</li> <li>False if the test fails</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#example-test","title":"Example Test","text":"<p>Path: <code>$MR_ROOT/Tools/RepoLoadUtils/common/tests/labs/test_non_nulls.py</code></p> <p><pre><code>import pandas as pd\n\ndef Test(df: pd.DataFrame, si, codedir: str, workdir: str):\n    if len(df) == 0:\n        return True\n    cols = [x for x in df.columns if x == \"pid\" or \"value\" in x or \"time\" in x]\n    sig_name = df[\"signal\"].iloc[0]\n    # si.t_ch - contains array of each time channel type (for example \"i\" is integer, \"f\" float). v_ch is the same for value channels.\n    signal_columns = [\"time_%d\" % (i) for i in range(len(si.t_ch))] + [\n        \"value_%d\" % (i) for i in range(len(si.v_ch))\n    ]\n    signal_columns.append(\"pid\")\n    for col in cols:\n        if col not in signal_columns:\n            print(f\"Skip columns {col} which is not needed in signal {sig_name}\")\n            continue\n        null_date_cnt = len(df[df[col].isnull()])\n        if null_date_cnt / len(df) &gt; 0.001:\n            print(\n                \"Failed! There are %d(%2.3f%%) missing values for signal %s in col %s\"\n                % (null_date_cnt, 100 * null_date_cnt / len(df), sig_name, col)\n            )\n            return False\n        if null_date_cnt &gt; 0:\n            print(\n                \"There are %d(%2.3f%%) missing values for signal %s in col %s\"\n                % (null_date_cnt, 100 * null_date_cnt / len(df), sig_name, col)\n            )\n        df.drop(df.loc[df[col].isnull()].index, inplace=True)  # clean nulls\n        df.reset_index(drop=True, inplace=True)\n    print(\"Done testing nulls in signal %s\" % (sig_name))\n    return True\n</code></pre> This test verifies that no more than 1% null values exist in pid, time_0, value_0 for all labs signals. You can copy it into a local directory and adjust thresholds as needed.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#plotting-graphs","title":"Plotting Graphs","text":"<p>To generate HTML plots, use the plot_graph function:</p> <pre><code>import sys, os\nfrom ETL_Infra.plot_graph import plot_graph\n</code></pre> <ul> <li>Input:<ul> <li>A dataframe with two columns, or</li> <li>A dictionary {name: dataframe} (to plot multiple series)</li> </ul> </li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#running-tests-on-signals","title":"Running Tests on Signals","text":"<p>You can run or rerun tests with:</p> <p><pre><code>python $MR_ROOT/Tools/RepoLoadUtils/common/ETL_Infra/run_test_on_sig.py \\\n  --workdir $WORKDIR \\\n  --codedir $CODEDIR \\\n  --signal $SIGNAL\n</code></pre> * --signal can accept multiple signals (comma-separated).</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html","title":"Setup","text":"<p>To begin working with the ETL infrastructure, you need to clone the repository.  Note that this is not a PyPI package; it is designed to be used as part of the environment/codebase.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html#clone-the-repository","title":"Clone the Repository","text":"<p>Start by cloning the MR_Tools repository from GitHub to a folder on your computer.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html#configure-python-to-recognize-the-library","title":"Configure Python to Recognize the Library","text":""},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html#option-1-update-your-environment-variable","title":"Option 1: Update Your Environment Variable","text":"<p>The easiest way to use the infrastructure is by adding the \"RepoLoadUtils/common\" directory to your <code>PYTHONPATH</code> environment variable. This allows you to directly import modules from the ETL infrastructure in any Python file.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html#option-2-modify-individual-python-files","title":"Option 2: Modify Individual Python Files","text":"<p>If you prefer not to modify your environment settings, you can add the following code snippet at the beginning of each Python file that needs access to the ETL infrastructure. This temporarily adds the required directory to the system path, enabling module imports.</p> <pre><code>import sys\n# Replace \"ABSOLUTE PATH TO\" with the actual path on your computer\nsys.path.insert(0, \"ABSOLUTE PATH TO RepoLoadUtils/common\") \n# Now you can import the needed modules from ETL_Infra\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html#verify-the-setup","title":"Verify the Setup","text":"<p>To confirm everything is set up correctly, run the following command:</p> <pre><code>python -c 'import ETL_Infra'\n</code></pre> <p>Alternatively, if using Option 2, wrap the snippet in a script and add <code>import ETL_Infra</code> at the end to verify it works.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/00.Setup/index.html#next-steps","title":"Next Steps","text":"<p>The first step in the ETL process is to create a module or script that fetches data in batches. This method is highly efficient and preferred over returning a single <code>DataFrame</code> from a simple function.</p> <p>Follow the detailed instructions in the Data Fetcher documentation to begin.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html","title":"Data Fetching Step","text":"<p>The goal of this step is to create a function that returns a lazy iterator for data. This function, <code>fetch_function</code>, will fetch data in manageable batches, defined by a <code>batch_size</code> and a <code>start_batch</code> index. This approach is memory-efficient and ideal for large datasets.</p> <pre><code>def fetch_function(batch_size:int, start_batch:int) -&gt; Generator[pd.DataFrame, None, None]\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#step-by-step-guide-file-based-example","title":"Step-by-Step Guide: File-Based Example","text":"<p>This guide uses a file-based example to demonstrate the process, but the same principles apply to other data sources like databases (We also have helper functions for DBs).</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#1-setup-and-imports","title":"1. Setup and Imports","text":"<p>First, import the necessary helper libraries. The example uses pre-built functions for file handling.</p> <pre><code>from ETL_Infra.data_fetcher.files_fetcher import files_fetcher, list_directory_files, big_files_fetcher\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#2-list-files-helper-function","title":"2. List Files Helper Function","text":"<p>Create a helper function to list all relevant files from your raw data directory. This function should return a list of full file paths. You can use a regular expression (<code>file_regex</code>) to filter the files. It's a good practice to handle one file type or format at a time.</p> <p>We will use the helper function <code>list_files</code> that does that using regex. If there are multiple folders, you can write your own code to list full file paths to read/process.</p> <p><pre><code>def list_files(file_regex: str)-&gt; list[str]:\n    base_path = '/nas1/Data/client_data'\n    return list_directory_files(base_path, file_regex)\n</code></pre> </p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#3-file-path-parsing-function","title":"3. File Path Parsing Function","text":"<p>Next, write a function to read and parse a single file into a Pandas DataFrame. The only required constraint is that the resulting DataFrame must include a <code>pid</code> (patient identifier) column.</p> <pre><code>def read_single_file(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a single file into a DataFrame.\n\n    :param file_path: Path to the file to be read.\n    :return: DataFrame containing the data.\n    \"\"\"\n    df = pd.read_csv(file_path, sep=\"\\t\")\n    # Ensure the DataFrame contains a \"pid\" column\n    return df\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#4-the-data-fetcher-function","title":"4. The Data Fetcher Function","text":"<p>The final step is to create the main data fetcher, which acts as a \"lazy\" generator.  This function uses the <code>files_fetcher</code> helper to read files in batches, returning a concatenated DataFrame for each batch.  This lazy execution is a key feature; data is only loaded when you iterate over the result, which is useful for debugging and fast testing or reruning the full load script again and skiping actaul reading of the data if the processings was already completed.</p> <p>We will use <code>list_files</code> that we wrote before and <code>read_single_file</code> and will return a function with desired signature as described in <code>fetch_function</code>.</p> <p><pre><code>def generic_file_fetcher(\n    file_pattern: str,\n) -&gt; Callable[[int, int], Generator[pd.DataFrame, None, None]]:\n    \"\"\"\n    Creates a file fetcher function that reads files matching the given pattern.\n\n    :param file_pattern: Pattern to match files.\n    :return: A function that fetches files in batches based on requested batch size and starting index.\n    \"\"\"\n    file_fetcher_function = lambda batch_size, start_batch: files_fetcher(\n        list_files(file_pattern),\n        batch_size,\n        read_single_file,\n        start_batch,\n    )\n    return file_fetcher_function\n</code></pre> </p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#example-usage","title":"Example Usage","text":"<p>Here\u2019s how you can use the generic_file_fetcher to test and debug your data pipeline.  This example creates a dummy file and then uses the fetcher to read it in batches.</p> <pre><code># Example Usage\nimport pandas as pd\nimport os\n\ndata = pd.DataFrame(\n    {\"pid\": [1, 2, 3, 4, 5], \"value\": [1988, 1999, 2000, 2001, 2002]}\n)\ndata[\"signal\"] = \"BYEAR\"\n# Let's store the file somewhere and update `list_files`  to use same path\nBASE_PATH = \"/tmp\"\ndata.to_csv(f\"{BASE_PATH}/res.tsv\", sep=\"\\t\", index=False)\n\n# Create a data fetcher for the dummy file\nfunc_fetcher = generic_file_fetcher(\"res.tsv\")\nfile_fetcher = func_fetcher(1, 0)  # Read one file at a time, starting from index 0\n\nprint(\"Iterating on batches:\")\nfor i, df in enumerate(file_fetcher):\n    print(f\"Batch {i}:\\n{df}\")\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#dummy-example","title":"Dummy example","text":"<p>Another dummy example of writing a data fetcher directly, but without helper functions in step 2+3. Just demonstarting the usage of <code>yield</code> in python. <pre><code>def dummy_data_fethcer_of_fake_data(batch_size, start_batch):\n    #In this simple funtion, we ignore the \"batch_size, start_batch\" and returns lazy iterator with 2 DataFrames\n    yield pd.DataFrame({'pid':[1,2,3], 'data':['A','B', 'C']})\n    yield pd.DataFrame({'pid':[4,5,6], 'data':['D','E', 'F']})\n</code></pre> This will result in lazy iterator with 2 batches of dataframe.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#handling-big-files","title":"Handling big files","text":"<p>For very large files, you might need to process them by rows rather than by entire files. The <code>big_files_fetcher</code> helper function is designed for this. It requires a different type of <code>read_single_file</code> function that can handle row-based chunks.</p> <pre><code>def read_single_file_by_rows(\n    filepath: str, batch_size: int, start_from_row: int\n) -&gt; Generator[pd.DataFrame, None, None]:\n    \"\"\"\n    Parses a file in chunks, yielding DataFrames of specified batch size as number of rows.\n\n    :param filepath: Path to the file to be parsed.\n    :param batch_size: Number of rows per batch.\n    :param start_from_row: Row index to start reading from.\n    :return: Generator yielding DataFrames.\n    \"\"\"\n    # manipulate the file reading parameters as needed (keep chunksize and skiprows):\n    has_header = True  # Set to False if the file has no header\n    header = None\n    if has_header and start_from_row &gt; 0: # read header if skiping first row\n        header = pd.read_csv(filepath, sep=\"\\t\", nrows=0).columns.tolist()\n        start_from_row += 1\n\n    df_iterator = pd.read_csv(\n        filepath,\n        names=header,\n        sep=\"\\t\",\n        skiprows=start_from_row,\n        chunksize=batch_size,\n    )\n    for df in df_iterator:\n        # Do manipulations on df if needed or just return `df_iterator`\n        yield df\n</code></pre> <p>This modified function allows the <code>big_files_fetcher</code> to read a specified number of rows per batch, moving to the next file only when the current one is exhausted.</p> <p>Full usage/replacement for <code>generic_file_fetcher</code> can be seen here:</p> <pre><code>def generic_big_files_fetcher(\n    file_pattern: str,\n) -&gt; Callable[[int, int], Generator[pd.DataFrame, None, None]]:\n    \"\"\"\n    Creates a data fetcher function for large files, reading them in batches.\n    Iterates over files matching the given pattern and processes them in chunks by reading rows.\n\n    :param file_pattern: Pattern to match files.\n    :return: A function that fetches files in batches based on requested batch size, starting index.\n    batch_size controls the number of rows read from each file. When file ends, it will read the next file.\n    \"\"\"\n    # Use the labs_file_parser_lazy to read files in chunks\n    files = list_files(file_pattern)\n    data_fetcher_function = lambda batch_size, start_batch: big_files_fetcher(\n        files,\n        batch_size,\n        read_single_file_by_rows,\n        has_header=True,\n        start_batch=start_batch,\n    )\n    return data_fetcher_function\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/01.Data%20Fetching%20step/index.html#next-step-process-pipeline","title":"Next Step: Process Pipeline","text":"<p>Once you have your data fetcher function, you can pass it as an argument to the next step: creating a process pipeline for each data type. Follow this guide to continue</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html","title":"Index","text":""},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#building-the-etl-processing-pipeline","title":"Building the ETL Processing Pipeline","text":"<p>This section explains how to construct a processing pipeline in the ETL workflow using the <code>prepare_final_signals</code> function. This function applies your custom processing logic to data fetched in the previous step.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Import Necessary Functions:  Start by importing the <code>prepare_final_signals</code> function and your custom data_fetcher.</li> </ol> <p><pre><code>from ETL_Infra.etl_process import prepare_final_signals\nfrom parser import generic_file_fetcher\n</code></pre> 2. Define a Working Directory:  Specify the path where test results and output files will be stored.</p> <pre><code>WORK_DIR = \"...\"  # Specify your working directory path\n</code></pre> <ol> <li>Run the Pipeline:  Execute the pipeline with the following call:</li> </ol> <pre><code>prepare_final_signals(\n    generic_file_fetcher(\"^demo.*\"),  # Fetch files starting with \"demo\"\n    WORK_DIR,\n    \"demographic\",  # Name of this processing pipeline\n    batch_size=0,   # Process all files in a single batch\n    override=\"n\"    # Skip if already successfully completed\n)\n</code></pre> <p>\u27a1\ufe0f In this example:</p> <ul> <li>All files starting with <code>\"demo\"</code> are processed under the <code>\"demographic\"</code> pipeline (will be explained next).</li> <li>All files are read in a single batch (<code>batch_size=0</code>)`.</li> <li><code>override=\"n\"</code> prevents re-running an already completed process.</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#important-notes","title":"Important Notes:","text":"<ul> <li>You should repeat this process for each data type.</li> <li>Best Practice for PID Mapping: Start by processing demographic signals like BDATE and GENDER. This is because if patient IDs (pid) are strings, the ETL will create a numeric-to-string mapping based on these demographic signals. This mapping is then used for all other signals, ensuring consistent pid values across the entire dataset. More details on the pipeline can be seen next Higher-Level Pipeline Flow</li> <li>Use the <code>start_write_batch</code> parameter when multiple pipelines write to the same output signals. This ensures unique batch indices and prevents overwrites</li> <li>A single fetcher can feed multiple pipelines by specifying comma-separated names. For example: <pre><code>\"gender,byear\"\n</code></pre> This sends demographic data to both gender.py and byear.py.</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#defining-processing-logic","title":"Defining Processing Logic","text":"<p>Each pipeline automatically looks for a processing script named after the pipeline (e.g., <code>demographic.py</code>). This script must be located in the <code>signal_processings</code> folder alongside your main code.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#inside-the-script","title":"Inside the Script","text":"<ul> <li>Input: A <code>DataFrame df</code> from the fetcher.<ul> <li>You can also use <code>workdir</code> as WORKDIR folder path and <code>sig_types</code> and object with signal type definitions. A usefull function is to retrived other processed signal, in that example <code>BDATE</code> with <code>df_bdate = load_final_sig(workdir, sig_types, \"BDATE\")</code></li> </ul> </li> <li>Output: A transformed DataFrame in the required format, still called <code>df</code>.</li> <li>Any <code>print</code> statements will be captured in a log file.</li> </ul> <p>A Note on Signal Processing: </p> <ul> <li>If the DataFrame contains a <code>signal</code> column, the system generates a mapping file for easier renaming and mapping. Please refer to Signal Mapping.</li> <li>If <code>signal</code> is non-null, processing is grouped by signal names instead of pipeline names.</li> <li>Each signal belongs to hierarchical groups (tags and aliases). Example: <pre><code>SIGNAL  BDATE   102 16:my_SInt  demographic,singleton,date  0   date\nSIGNAL  Hemoglobin  1000    16:my_SDateVal  labs,cbc    0   g/dL\n</code></pre></li> <li>signal <code>\"BDATE\"</code> is linked to <code>demographic</code>, <code>singleton</code>, and <code>date</code>.</li> <li><code>\"Hemoglobin\"</code> belongs to <code>labs</code> and <code>cbc</code></li> </ul> <p>\u27a1\ufe0f Processing order: If a dedicated script exists (e.g., <code>Hemoglobin.py</code>), it will be used.  Otherwise, the system looks from the most specific to least specific tag (e.g., first <code>cbc.py</code>, then <code>labs.py</code>).</p> <p>Link to File Definition of All Signals</p> <p>Debugging To debug, add <code>breakpoint()</code> in your script. No changes to ETL infrastructure code are required.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#required-output-format","title":"Required Output Format","text":"<p>The final DataFrame must include:</p> <ul> <li><code>pid</code>: Patient identifier (already required in fetching step)</li> <li><code>signal</code>: The name of the signal (e.g., 'GENDER', 'Hemoglobin'). This must be a recognized signal. The system will provide an error with instructions if a signal is unknown (How to define a new signal if needed).</li> <li><code>time_X</code>: Time channels, where X is an index. These should be stored as integers in YYYYMMDD format.</li> <li><code>value_X</code>: Value channels, where X is an index. These can be float, integer, or string (for categorical data)</li> </ul> <p>\u274c Any other columns will be ignored during the final loading process.</p> <p>\u2705 Column order doesn\u2019t matter - the ETL system arranges them automatically.</p> <p>Special Cases</p> <ul> <li>For categorical signals, see Categorical Signals</li> <li>For signals that needs unit conversion, this guide can be helpful: Unit Conversion</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#example-processing-code","title":"Example Processing Code","text":"<p>Here's an example of the code you would write inside the <code>demographic.py</code> file to process a DataFrame and create a <code>GENDER</code> signal.</p> <pre><code># Input: DataFrame 'df' with patient data\n# Goal: Create 'GENDER' signal from 'Sex' column\n\n# The output DataFrame should have these columns:\n#   pid\n#   signal\n#   value_0 (string categorical, e.g., 'Male', 'Female')\n\n# 1. Select and rename columns\ndf = df[['pid', 'Sex']].rename(columns={'Sex': 'value_0'})\n\n# 2. Add the 'signal' column with the signal name\ndf['signal'] = 'GENDER'\n\n# 3. Standardize the values\ndf.loc[df['value_0'] == 'male', 'value_0'] = 'Male'\ndf.loc[df['value_0'] == 'female', 'value_0'] = 'Female'\n\n# 4. Finalize the DataFrame by keeping only required columns and removing duplicates\ndf = df[['pid', 'signal', 'value_0']].drop_duplicates().reset_index(drop=True)\n\n# The 'df' is now ready for the next stage of the pipeline\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#higher-level-pipeline-flow","title":"Higher-Level Pipeline Flow","text":"<p>Every pipeline includes the following stages:</p> <ol> <li> <p>PID Mapping: </p> <ul> <li>Convert non-numeric IDs to numeric.</li> <li>Mapping stored in <code>[WORK_DIR]/FinalSignals/ID2NR</code>.</li> <li>Demographic signals (signals with \"demographic\" tag) create the mapping; all others join on it.</li> </ul> </li> <li> <p>Signal Mapping: </p> <ul> <li>If <code>signal</code> column exists and not-null, optional name mappings can be done using <code>configs/map.tsv</code>. </li> <li>File lists source \u2192 destination mappings, ordered by frequency. Empty lines without destination value will be ignored in the mapping.</li> </ul> </li> <li> <p>Data Transformation: </p> <ul> <li>Transform the DataFrame to the final format. This is the process unit that we wrote here in the ETL.</li> </ul> </li> <li> <p>Testing: </p> <ul> <li>Run automatic format tests, followed by deeper tests (e.g., lab distributions).</li> <li>See Extending and adding tests</li> </ul> </li> <li> <p>Storing: </p> <ul> <li>Sort and save the processed data in <code>[WORK_DIR]/FinalSignals</code>.</li> </ul> </li> </ol> <p></p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/index.html#next-step-prepare-dicts-if-needed","title":"Next Step: Prepare Dicts if needed","text":"<p>If your project requires client-specific dictionaries, follow this guide: Prepare Special Dicitonaries</p> <p>Once you have you finished, follow this guide to finalize the loading</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html","title":"Categorical Signals &amp; Custom Dictionaries","text":"<p>This page explains how categorical signals are handled in the ETL process, with examples of when to use known ontologies, how to deal with client-provided values, and how to integrate custom mapping dictionaries.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#use-case-1-known-signals-with-standard-ontologies","title":"Use Case 1 \u2013 Known Signals with Standard Ontologies","text":"<p>When using a known categorical signal from ETL_INFRA_DIR/rep_signals/general.signals (e.g., DIAGNOSIS, Drug, PROCEDURE), the ETL automatically applies existing ontologies and mappings between codes.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#example","title":"Example:","text":"<p>For the Drug signal, if you create values with the <code>RX_CODE</code> prefix, the ETL will detect this and automatically pull:</p> <ul> <li>The <code>RX_CODE</code> dictionary,</li> <li>The <code>ATC</code> dictionary, and</li> <li>The mapping between <code>RX_CODE</code> and <code>ATC</code>.</li> </ul> <p>You only need to set the correct prefixes in <code>prepare_final_signals</code> processings. The call to <code>finish_prepare_load</code> takes care of the rest. No need to do anything special.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#known-ontologies-and-prefixes","title":"Known Ontologies and Prefixes","text":"Coding system prefix description ICD10_CODE: Diagnosis or procedure with ICD10 codes. For PROCEDURE signal, uses procedure ontology ICD9_CODE Diagnosis or procedure with ICD9 codes. For PROCEDURE signal, uses procedure ontology ATC_CODE: Medication prescriptions in ATC codes RX_CODE: Medications prescriptions in RX norm NDC_CODE: Medications in NDC codes"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#use-case-2-new-signals-from-client-list-of-values","title":"Use Case 2 \u2013 New Signals from Client (List of Values)","text":"<p>Sometimes we receive a signal that is not part of a known ontology and comes only as a list of values from the client.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#example_1","title":"Example:","text":"<p>A signal like Cancer_Type with values such as:</p> <ul> <li><code>Adenocarcinoma</code></li> <li><code>Small_Cells</code></li> <li>etc. (extracted from cancer patients)</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#what-to-do","title":"What to do:","text":"<ul> <li>Define the new categorical signal in CODE_DIR/configs/rep.signals \u27a1\ufe0f No manual mapping is needed. It will processed in the end as part of <code>finish_prepare_load</code> call later</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#use-case-3-new-or-known-signals-with-additional-client-dictionaries","title":"Use Case 3 \u2013 New or Known Signals with Additional Client Dictionaries","text":"<p>Sometimes the signal is known (e.g., DIAGNOSIS) or new, but the client provides extra mapping dictionaries.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#example_2","title":"Example:","text":"<p>The client uses an internal coding system (<code>EDG_CODE</code>) and provides:</p> <ol> <li>Translation dictionary - maps internal codes to descriptions.<ul> <li>Example: <code>`EDG_CODE:1234</code> \u2192 Diabetes type II</li> </ul> </li> <li>Mapping dictionary - maps internal codes to another known ontology.<ul> <li>Example: <code>EDG_CODE:1234</code> (Diabetes type II) \u2192 <code>ICD10_CODE:E11</code></li> </ul> </li> </ol>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#notes","title":"Notes:","text":"<ul> <li>Sometimes only #1 (translation) is available \u2192 still valid.</li> <li>Sometimes only #2 (mapping) is available \u2192 also valid.</li> <li>If the ontology is common and reusable, we may store the mapping dictionary in ETL for future use. We will need to change the code in <code>create_dicts.py</code>, currently it is not very easily extended.</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#how-to-use","title":"How to use","text":"<p>Use the function <code>prepare_dicts</code> with up to two optional dataframes:</p> <ul> <li>Translation dictionary:</li> </ul> Column Meaning <code>code</code> Internal code <code>description</code> Human-readable description <ul> <li>Mapping dictionary:</li> </ul> Column Meaning <code>client_value</code> Value from client <code>ontology_code</code> Code from our known ontology"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#how-to-reading-the-output-of-prepare_dicts-finish_prepare_load","title":"How-To: Reading the Output of prepare_dicts / finish_prepare_load","text":"<p>During processing, the ETL produces log messages with statistics about how the dictionaries were handled.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#what-to-expect","title":"What to Expect","text":"<ul> <li>Known codes detected - how many values already exist in our mappings.</li> <li>New codes detected - how many values were introduced for the first time (e.g., new ICD10 codes for new diseases).</li> <li>Automatic mapping attempts - in some cases, new codes are mapped by truncating strings to a higher-level category (e.g., grouping a specific disease into a broader disease family). \u00a0</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#why-it-matters","title":"Why It Matters","text":"<ul> <li>Helps identify if client data aligns well with existing ontologies.</li> <li>Flags new codes that may need review or long-term integration.</li> <li>Provides confidence that signal values were normalized as expected. \u00a0</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html","title":"unit_conversion","text":"<p>The <code>unit_conversion.py</code> module in ETL_Infra provides utilities for managing and applying unit conversions in lab data. It includes two primary functions:</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#functions","title":"Functions","text":"<p><code>process_unit_conversion</code></p> <p>Generates a statistics file with:</p> <ul> <li>Signals and units ranked by frequency</li> <li>A conversion table for review</li> </ul> <p>Parameters:  * <code>fullDF</code>: DataFrame with:     - <code>signal</code> column (signal name)     - <code>unit</code> column (unit name)     - <code>value_0</code> column (numeric value) * <code>outFile</code>: Path to save the conversion table and statistics * <code>samples_per_signal</code>: Number of sample rows per group</p> <p>Output: * None (writes results to <code>outFile</code>)</p> <p><code>fix_units</code></p> <p>Applies unit conversions using a prepared configuration file.</p> <p>Parameters:  * <code>fullDF</code>: DataFrame with <code>signal</code>, <code>unit</code>, and <code>value_0</code> columns * <code>inFile</code>: Path to the unit conversion configuration file</p> <p>Output: * Returns a DataFrame with units converted</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#recommended-workflow-for-lab-data","title":"Recommended Workflow for Lab Data","text":"<ol> <li>Generate conversion config - Run generate_labs_mapping_and_units_config from etl_process with your DataFrame (requires signal and unit columns).</li> <li>Edit the conversion table - Review and adjust the generated table as needed.</li> <li>Apply conversions - Use <code>map_and_fix_units</code> from etl_process with the same inputs to apply conversions. This returns the updated DataFrame.</li> </ol>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#example-usage-in-labspy","title":"Example usage in <code>labs.py</code>:","text":"<pre><code>generate_labs_mapping_and_units_config(df)\n# After editing the conversion file, you can comment out the line above to avoid regenerating it.\ndf=map_and_fix_units(df)\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#references","title":"References","text":"<ul> <li>Example configuration file</li> <li>Full code example</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/Full%20code%20example%20-%20LungFlag%20Taiwan%20labs.html","title":"Full code example","text":"<p>The scripts does:</p> <ol> <li>Renaming of columns</li> <li>date columns conversion</li> <li>mapping of LABLOINC codes to signals using $CODE_DIR/configs/map.tsv file</li> <li>Removing rows without \"numeric\" results </li> <li>Filtering null columns</li> <li>removing \"ignored\" signal list that was mapped, but we don't need/want to load </li> <li>Cleaning prefix for \"value\" columns suffix like \"(Manual checked)\"</li> <li>numeric conversion of values - removal of non numeric values</li> <li>Transforming \"unit\" column and taking lowercase for unit</li> <li>Calling unit conversion functions: <code>generate_labs_mapping_and_units_config</code>, map_and_fix_units</li> </ol> <p>$CODE_DIR/signal_processings/labs.py <pre><code>\u00a0#You have dataframe called \"df\". Please process it to generare signal/s of class \"labs\"\n#Example signal from this class might be BP\n#The target dataframe should have those columns:\n#    pid\n#    signal\n#    time_0 - type i\n#    value_0 - string categorical (rep channel type s)\n#    value_1 - string categorical (rep channel type s)\n#Source Dataframe - df.head() output:\n#     pid ACCOUNTIDSE2    ORDERTASKIDSE HOSPITALCODE EXEHOSPITALCODE DEPTCODE  ...    SPECIMENNO     SAMPLINGDATETIME SPECIMENTYPECODE RESULTUNIT                      REGULAREXPRESSIONDEFINEDRANGE  signal\n#0   4728    A00015780  201301020010156           T0              T0      NaN  ...  130102903195  2013-01-02 10:31:04              BLO       g/dL  [0-14d]12.0-20.0 [15-30d]10.0-15.3 [31d-0.5y]8...    None\n#1  39977    A00028394  201301310013839           T0              T0      NaN  ...  130131904446  2013-01-31 10:42:04              BLO         pg  [0-14d]31.1-35.9 [15-30d]29.9-35.3 [31d-0.5y]2...    None\n#2  67616    A00036755  201302110003945           T0              T0      NaN  ...  130211062196  2013-02-11 14:49:22              BLO       g/dL  [0-14d]12.0-20.0 [15-30d]10.0-15.3 [31d-0.5y]8...    None\n#3  23150    A00041791  201302220023413           T0              T0      NaN  ...  130222001672  2013-02-22 15:56:49              BLO          %  [0-14d]36.0-60.0 [15-30d]30.5-45.0 [31d-0.5y]2...    None\n#4  11117    A00045620  201303040026380           T0              T0      NaN  ...  130304908805  2013-03-04 15:35:12              BLO          %                                                NaN    None\n#\n#[5 rows x 22 columns]\ndef clean_suffix(df, suffix):\n    df['value_0']=df['value_0'].apply(lambda x: x.strip()[:-len(suffix)] if x.strip().endswith(suffix) else x)\n    return df\ndf=df.rename(columns={'SAMPLINGDATETIME': 'time_0', 'CONFIRMRESULT': 'value_0'})\ndf['time_0']=df['time_0'].map(lambda x: x.split()[0].replace('-','')).astype(int)\ndf=df[['pid', 'time_0', 'value_0', 'RESULTUNIT', 'LABLOINC', 'ORIGINALLABORDERFULLNAME']]\n#df[['LABLOINC', 'ORIGINALLABORDERFULLNAME']].groupby('LABLOINC').agg(['count','min']).reset_index().sort_values(('ORIGINALLABORDERFULLNAME', 'count'), ascending=False).to_csv('configs/maps.tsv', index=False, sep='\\t')\nmap_labs=pd.read_csv(os.path.join('configs', 'map.tsv'), sep='\\t', usecols=['LABLOINC', 'target']).set_index('LABLOINC')\ndf=df.set_index('LABLOINC').join(map_labs, how='left').reset_index().rename(columns={'target':'signal'}) #.drop(columns=['ORIGINALLABORDERFULLNAME'])\nmissing_map=len(df[df['signal'].isnull()])\nif missing_map &gt;0:\n    print(f'missing codes {missing_map}')\ndf=df[['pid', 'signal', 'time_0', 'value_0', 'RESULTUNIT', 'LABLOINC']]\nbefore_nana=len(df)\ndf=df[df['value_0'].notnull()].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed empty values. size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\nbefore_nana=len(df)\ndf=df[df.signal.notnull()].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed unmapped signals. size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\nbefore_nana=len(df)\nignore_labs=set(['CK-MB', 'Normobl', 'INR', 'IGNORE', 'PT', 'PTT', 'CK-isoenzyme'])\ndf=df[(~df.signal.isin(ignore_labs))].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed IGNORE signal. size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\n#Covert number\ndf=clean_suffix(df, '(Manual checked)')\ndf=clean_suffix(df, '(NRBC excluded)')\n#extract number:\ndf['value_00']=pd.to_numeric(df.value_0.apply(lambda x: re.compile('^([0-9]+(\\.[0-9]+)?)(\\s.*)').sub(r'\\1', x).strip()) ,errors='coerce')\n\ndf['has_num']=df['value_0'].apply(lambda x: len(re.compile('[0-9]').findall(x))&gt;0)\nprint('Excluded values:')\nprint(df[~df['has_num']].value_0.value_counts())\nbefore_nana=len(df)\ndf=df[df['has_num']].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed non numeric size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\ndf=df.drop(columns=['has_num'])\n\nprint('Excluded more non numeric values:')\nprint(df[df['value_00'].isnull()].value_0.value_counts())\nbefore_nana=len(df)\ndf=df[df['value_00'].notnull()].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed bad values size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\ndf['value_0']=df['value_00']\ndf=df.drop(columns=['value_00'])\n#Now handle units = &gt; All looks good and in the same unit\ndf['RESULTUNIT']=df['RESULTUNIT'].astype(str).apply(lambda x: x.lower())\ndf=df.rename(columns={'RESULTUNIT': 'unit'})\n#generate_labs_mapping_and_units_config(df, 5)\n#Please edit the file \"/mnt/earlysign/workspace/LungFlag/ETL/configs/map_units_stats.cfg\" and then comment out previous line for speedup in next run\ndf=map_and_fix_units(df)\ndf=df.drop(columns=['signal.original', 'mapped'])\n</code></pre> </p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/config%20file%20of%20unit%20conversion.html","title":"Example config/output file of unit conversion","text":"<p>Based on Taiwan Conversions (almost all signals has single unit). transformed all units to lowercase to merge same units before calling this function. The file was created Automatically and only the row \"Ca\" was edited =&gt; mmol/l. Rows without edits will be kept \"as is\", no unit conversion and in most cases that's OK.</p> <ul> <li>signal is the \"source\" signal in Taiwan - already did the mapping before with a different file. when \"to_signal\" is empty will keep the same name as \"signal\"</li> <li>count - how common this signal</li> <li>unit - the unit with this signal</li> <li>unitcount - How common is the signal+unit combination. If single unit, it will be equal to count. Will sum up to count when taking all rows for the same signal</li> <li>to_signal - \u00a0when \"to_signal\" is empty will keep the same name as \"signal\"</li> <li>to_unit - was taken from global+local signals file definitions configuration (just for convenient, not in use). Each signal definition has units</li> <li>multiple_by, additive_b0 - the linear transformation. Default is 1 for multiple_by and 0 for additive_b0 if not given (dioing nothing be default to multiple by 1 and add 0)</li> <li>value_0 - example values</li> </ul> <p>Example of top 20 rows: <pre><code>signal  count   unit    unitCount   to_signal   to_unit multiple_by additive_b0 value_0\nGlucose 201128  mg/dl   201128      mg/dL           [107.0, 319.0, 127.0, 140.0, 91.0]\nHemoglobin  145250  g/dl    145216      g/dL            [10.2, 13.2, 11.5, 14.5, 13.2]\nHemoglobin  145250  %   34      g/dL            [13.8, 15.1, 19.9, 12.4, 17.8]\nHematocrit  144843  %   144840      %           [27.6, 30.0, 26.3, 48.2, 27.4]\nHematocrit  144843  %pcv    3       %           [27.0, 27.0, 26.0, 26.0, 21.0]\nCreatinine  139001  mg/dl   139001      mg/dL           [1.3, 1.0, 0.9, 0.7, 1.0]\nMCV 135235  fl  135235      fL          [88.6, 89.8, 92.6, 86.1, 99.2]\nPlatelets   135159  k/?gl   135091      10*9/L          [189.0, 330.0, 409.0, 39.0, 297.0]\nPlatelets   135159  10^3/ul 68      10*9/L          [100.0, 36.0, 134.0, 31.0, 25.0]\nWBC 134959  k/?gl   134893      10*9/L          [5.13, 12.84, 4.39, 4.8, 1.83]\nWBC 134959  10^3/ul 66      10*9/L          [49.22, 0.14, 1.3, 37.49, 0.24]\nRBC 134620  m/?gl   134558      10*12/L         [4.41, 4.2, 5.32, 4.32, 3.16]\nRBC 134620  10^6/ul 62      10*12/L         [4.03, 4.03, 3.04, 2.79, 2.46]\nMCHC-M  134404  g/dl    134404      g/dL            [32.5, 32.5, 32.6, 33.6, 34.7]\nMCH 134065  pg  134065      pg          [29.0, 29.2, 31.2, 28.4, 23.3]\nRDW 129021  %   129021      %           [15.9, 15.5, 12.2, 17.2, 15.0]\nALT 122626  u/l 122626      U/L         [92.0, 20.0, 58.0, 21.0, 21.0]\neGFR    90973   ml/min/1.73 m^2 90973                   [108.8, 57.5, 130.8, 91.5, 106.7]\n....\nCa  35375   mmol/l  35349       mg/dL   4.01        [2.2, 2.13, 2.3, 1.84, 2.25]\nCa  35375   mg/dl   26      mg/dL           [9.6, 10.9, 9.8, 10.3, 9.9]\n</code></pre></p> <p>In this example, the <code>Ca</code> signal with the <code>mmol/l</code> unit was given a conversion factor of <code>4.01</code>. This factor is used to transform the values from <code>mmol/l</code> to the target unit of <code>mg/dL</code>. You can see this in the example values: the original values are around 2, while data from <code>mg/dL</code> unit has values around 10. For all other signals in the example, the units were consistent and did not require any conversion, so the conversion fields were left blank. \u00a0 Full file: \u00a0 map_units_stats.cfg</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/03.Finalize%20Load/index.html","title":"Finalizing the Load Process","text":"<p>his step completes the ETL pipeline and prepares the repository for use.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/03.Finalize%20Load/index.html#recap-of-earlier-steps","title":"Recap of earlier steps","text":"<ul> <li>Prepare signals: Run <code>prepare_final_signals</code> for each data type (see previous step)</li> <li>Handle client dictionaries (if needed): Use <code>prepare_dicts</code> for categorical signals</li> </ul> <p>Now we will finalize the preparation and generate all configuration files needed for loading by using a third function: <code>finish_prepare_load</code>.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/03.Finalize%20Load/index.html#finish_prepare_load","title":"<code>finish_prepare_load</code>","text":"<p>Finalizes preparation and loads your data into the repository.</p> <pre><code>finish_prepare_load(WORK_DIR, '/nas1/Work/CancerData/THIN/thin_20XX', 'thin')\n</code></pre> <p>Parameters:</p> <ul> <li><code>WORK_DIR</code> - path to the working directory (string)</li> <li><code>REPOSITORY_OUTPUT_DIR</code> - destination folder for the repository (string)</li> <li><code>REPO_NAME</code> - name of the repository (string)</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/03.Finalize%20Load/index.html#full-workflow-example","title":"Full Workflow Example","text":"<p>Here\u2019s a complete example combining all steps:</p> <pre><code>import pandas as pd\nfrom ETL_Infra.etl_process import *\nfrom parser import generic_file_fetcher, generic_big_files_fetcher\n\nWORK_DIR = '/nas1/Work/demo_ETL'\n\n# Step 1: Prepare signals\nprepare_final_signals(\n    generic_file_fetcher(\"^demo.*\"),  # Fetch files starting with \"demo\"\n    WORK_DIR,\n    \"demographic\",  # Name of this processing pipeline\n    batch_size=0,   # Process all files in a single batch\n    override=\"n\"    # Skip if already successfully completed\n)\nprepare_final_signals(\n    generic_big_files_fetcher(\"^labs.*\"),  # Fetch files starting with \"labs\"\n    WORK_DIR,\n    \"labs\",  \n    batch_size=1e6,   # Process each 1M lines in a single batch\n    override=\"n\"    \n)\n\n# Step 2 (optional): Handle custom dictionaries\n# Provide client dicts as DataFrames: def_dict, set_dict (or None)\nprepare_dicts(WORK_DIR, 'DIAGNOSIS', def_dict, set_dict)\n\n# Step 3: Finalize and load\nfinish_prepare_load(WORK_DIR, '/nas1/Work/CancerData/THIN/thin_20XX', 'thin')\n</code></pre>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/03.Finalize%20Load/index.html#function-reference","title":"Function Reference","text":"<p>1. <code>prepare_final_signals</code> Processes and tests each data type. Handles batching if needed.</p> <ul> <li>Arguments:</li> <li><code>data_fetcher</code> or <code>DataFrame</code>: Source of your data</li> <li><code>workdir</code>: Working directory for outputs</li> <li><code>signal_type</code>: Name/type of the signal (used for classification)</li> <li><code>batch_size</code>: Batch size (0 = no batching)</li> <li><code>override</code>: <code>'y'</code> to overwrite, <code>'n'</code> to skip completed signals</li> </ul> <p>2. <code>prepare_dicts</code> Creates mapping dictionaries for categorical signals.</p> <ul> <li>Arguments:</li> <li><code>workdir</code>: Working directory</li> <li><code>signal</code>: Signal name</li> <li><code>def_dict</code>: DataFrame with internal codes and descriptions (optional)</li> <li><code>set_dict</code>: DataFrame mapping client codes to known ontology</li> </ul> <p>3. <code>finish_prepare_load</code> Finalizes preparation, generates signals, and loads the repository.</p> <ul> <li>Arguments:</li> <li><code>workdir</code>: Working directory</li> <li><code>dest_folder</code>: Destination for the repository</li> <li><code>dest_rep</code>: Repository name (prefix)</li> <li><code>to_remove</code> (optional): List of signals to skip</li> <li><code>load_only</code> (optional): List of signals to load only</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/ETL%20Tutorial/03.Finalize%20Load/index.html#extending-and-testing","title":"Extending and Testing","text":"<p>For guidance on extending the process and adding automated tests, see Test Extention</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html","title":"High-Level Overview of the ETL Process","text":"<p>This document outlines the high-level structure and flow of the ETL (Extract, Transform, Load) process. It covers the core file structure and the sequential steps involved in loading data.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html#etl-file-structure","title":"\ud83d\udcc1 ETL File Structure","text":"<p>The ETL process uses three main directories:</p> <ul> <li>ETL_INFRA_DIR: This directory, located at MR_TOOLS git repo <code>RepoLoadUtils\\common\\ETL_Infra</code>, contains the standalone ETL infrastructure. It's portable and doesn't require other files to function.</li> <li>CODE_DIR: This is where you write the code specific to your ETL task.</li> <li>WORK_DIR: This directory serves as the output location for the ETL process.</li> </ul> <p>For more details on the contents of each directory, click the links above.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html#the-etl-process-flow","title":"\u2699\ufe0f The ETL Process Flow","text":"<p>The ETL process is managed by a <code>load.py</code> script located in the CODE_DIR. This script orchestrates the loading process by calling the <code>prepare_final_signals</code> function, which in turn initiates a \"parser\" for each signal. <code>load.py</code> is a convention name, it is not necessarily needs to be like that.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html#step-1-handling-signals","title":"Step 1: Handling Signals","text":"<ol> <li>Status Check: The <code>prepare_final_signals</code> function first checks the status of the required signal.<ul> <li>If <code>override</code> is specified, the signal is loaded from the start.</li> <li>If the signal is already loaded, it's skipped.</li> <li>Otherwise, it resumes loading from the last interrupted batch or starts from the beginning.</li> </ul> </li> <li>PID Validation: The <code>pid</code> column is mandatory in the parser's output.<ul> <li>If <code>pid</code> column is missing, the process will ERROR and STOP.</li> <li>If <code>pid</code> is a string, it's mapped to a numeric value. This mapping is only performed for demographic signals (e.g., BDATE, GENDER). A <code>pid</code> in another signal (e.g., DIAGNOSIS) that hasn't been seen in a demographic signal will be excluded. Therefore, demographic signals must be processed first with <code>prepare_final_signals</code>.</li> </ul> </li> <li>Preliminary Check: Before processing, the system checks if the signal data is already complete and valid.<ul> <li>If the dataframe contains all necessary columns and attributes as defined in <code>rep_signals/general.signals</code> and passes all preliminary tests, no further processing is needed. The data is sorted, and the signal processing is marked as complete.</li> <li>Otherwise, the process continues to the next step.</li> </ul> </li> </ol>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html#step-2-determining-the-processing-unit","title":"Step 2: Determining the Processing Unit","text":"<p>The system identifies the appropriate \"processing unit\" based on the data.</p> <ol> <li>When <code>signal</code> column exists:<ul> <li>The most specific code is executed based on the signal name and its classifications. For example, for the \"BDATE\" signal (classified as \"demographic\" and \"singleton\"), the system would search for <code>BDATE.py</code>, then <code>demographic.py</code>.</li> <li>The signals configuration file and their classifications is located in the <code>general.signals</code> file in ETL_INFRA_DIR or in \"configs/rep.signals\" added to CODE_DIR.</li> </ul> </li> <li>Without a <code>signal</code> column:<ul> <li>The <code>prepare_final_signals</code> function's parameter is used to determine the signal. This also allows for multiple signals to be passed (e.g., \"BDATE,GENDER\"), and the most specific code is found for each.</li> </ul> </li> </ol> <p>If no relevant processing unit is found, a new one is created with instructions. In interactive mode, a Python environment opens for real-time processing and inspection. In non-interactive mode, the process fails, waiting for the unit's code to be filled in.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html#step-3-testing-and-finalizing-the-signal","title":"Step 3: Testing and Finalizing the Signal","text":"<ol> <li>Post-Processing Tests: After the processing unit returns the signal file, two types of tests are performed:<ul> <li>General Tests: These check for valid time/value channels, numeric values, valid dates, and illegal characters.</li> <li>Specific Tests: These are associated with the signal name and its classifications, such as comparing signal distribution or counting outliers. These tests can be added globally in ETL_INFRA_DIR or locally in CODE_DIR.</li> </ul> </li> <li>Post-Test Actions: Once the signal passes all tests:<ul> <li>Columns are sorted and organized.</li> <li>Statistics for categorical signals are collected to aid in dictionary creation.</li> <li>The batch or signal state is updated to reflect successful completion.</li> </ul> </li> </ol>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/index.html#step-4-dictionaries-and-final-load","title":"Step 4: Dictionaries and Final Load","text":"<ol> <li>Dictionary Creation: If your data requires specific dictionaries, you can call the <code>prepare_dicts</code> function.<ul> <li>For categorical features like \"Drug\", \"PROCEDURES\", and \"DIAGNOSIS\" that have specific prefixes (e.g., <code>ICD10_CODE:*</code>), the system can automatically recognize and use the correct ontology.</li> <li>Statistics from these steps are collected for the final report.</li> </ul> </li> <li>Finalization: The final step is to call <code>finish_prepare_load</code>. This function:<ul> <li>Processes all categorical signal dictionaries.</li> <li>Generates a merged \"signals\" file.</li> <li>Creates a <code>convert_config</code> file.</li> <li>Prepares the <code>Flow</code> command to execute the loading process.</li> </ul> </li> </ol>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/CODE_DIR.html","title":"\ud83d\udcc1 CODE_DIR: Your ETL Workspace","text":"<p>The <code>CODE_DIR</code> is the primary location for all code specific to your ETL process. It typically contains two main Python files: a helper library for parsing and <code>load.py</code>, which is the entry point for the entire loading process. </p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/CODE_DIR.html#key-directories-and-files","title":"Key Directories and Files","text":"<ul> <li><code>configs</code>: This folder stores configurations specific to your ETL. While it might be empty, it commonly contains settings for unit and signal mapping.<ul> <li><code>rep.signals</code>: This file is created for you as a template. You can override or add new signal definitions that are specific to your repository. New signals should use an ID of <code>3000</code> or greater. The file format is consistent with the global signal format.</li> <li><code>map_units_stats.cfg</code>: Used if you are performing signal mapping and unit conversions. This file contains the configuration for those conversions.</li> <li>Additional configuration files can be added as needed. For more details: unit_conversion</li> </ul> </li> <li> <p><code>signal_processings</code>: This folder contains the custom logic for processing different signals and data types.</p> <ul> <li><code>XXXX.py</code>: Each file here is a Python script with specific processing logic. For example, <code>labs.py</code> would process all signals tagged as \"labs\" (unless a more specific file exists). The system uses a hierarchical search: for a signal tagged \"labs,cbc,Hemoglobin,\" it first looks for <code>Hemoglobin.py</code>, then <code>cbc.py</code>, then <code>labs.py</code>. This structure promotes code reuse.</li> <li>If no suitable logic was found, a template python file with signal name and instructions will be created automatically. The template shows you the expected input dataframe (<code>df</code>) and the required output format, guiding you to write the appropriate processing code. For example: <pre><code>#You have dataframe called \"df\". Please process it to generare signal/s of class \"smoking\"\n#Example signal from this class might be Smoking_Status\n#The target dataframe should have those columns:\n#    pid\n#    signal\n#    time_0 - type i\n#    value_0 - string categorical (rep channel type i)\n#Source Dataframe - df.head() output:\n#     time_0     ahdcode ahdflag      data1   data2      data3 data4 data5 data6  medcode signal  pid\n#0  19880705  1008050000       Y             INP001                N           Y  4K22.00   None    1\n#1  19921209  1005010200       Y  81.000000          28.300000                    22A..00   None    1\n#2  19921209  1003050000       Y          Y       2                               136..00   None    1\n#3  19921209  1003040000       Y          N       0                               137L.00   None    1\n#4  19930000  1001400086       Y                                                  537..00   None    1\n</code></pre></li> </ul> </li> <li> <p><code>tests</code>: An optional and rarely used folder for adding extra tests specific to your ETL process. It follows the same format as the <code>tests</code> folder in <code>ETL_INFRA_DIR</code>, and its contents are merged with the global tests during execution.</p> </li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/ETL_INFRA_DIR.html","title":"\ud83d\udcc2 ETL_INFRA_DIR: A Closer Look","text":"<p>The <code>ETL_INFRA_DIR</code> contains the standalone ETL infrastructure, available in the MR_Tools repository under <code>RepoLoadUtils/common/ETL_Infra</code>. It's designed to be portable and can be used on a remote machine without external dependencies.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/ETL_INFRA_DIR.html#core-files-and-folders","title":"Core Files and Folders","text":"<ul> <li><code>etl_process.py</code>: The main Python module to import into your specific ETL code.</li> <li><code>dicts</code>: Stores dictionaries for known medical ontologies and their mappings. This is a resource for the infrastructure; only edit this folder to update global dictionaries for all future work.</li> <li><code>examples</code>: Provides sample loading processes for various repositories. The <code>THIN</code> example is fully implemented.</li> <li><code>rep_signals</code>: Contains global definitions for standard signals. These definitions can be overridden in your local ETL code but should generally remain as is. Only edit this folder to update signal definitions for all future work.<ul> <li><code>general.signals</code>: A file with definitions for all known signals, including type, unit, categorical status, and \"tags\". Tags are crucial as they determine the processing logic and tests to be executed. The File Format: Repository Signals file format<ul> <li>For example, a signal tagged as \"labs\" and \"cbc\" will first look for <code>Hemoglobin.py</code>, then <code>labs.py</code>, then <code>cbc.py</code> for processing logic.</li> <li>All tests associated with <code>hemoglobin</code>, <code>labs</code>, and <code>cbc</code> will be executed.</li> </ul> </li> <li><code>signals_prctile.cfg</code>: Contains quantile information for each signal listed in <code>general.signals</code>. This data is used to test lab signals.</li> <li><code>lab_zero_value_allowed.txt</code>: A list of signals where a value of <code>0</code> is permitted. An error will be raised if a zero value is found for a signal not on this list and the rate is above a defined threshold.</li> </ul> </li> <li><code>tests</code>: Holds additional, specific tests for signals. The ETL process always performs basic structural checks (e.g., correct channels, data types).<ul> <li><code>TAG_NAME</code> (e.g., \"labs\"): Each file within this folder represents a different test that will be executed for every signal with that specific tag. To add additional tests see\u00a0ETL_process dynamic testing of signals</li> </ul> </li> <li><code>data_fetcher</code>: A library with helper functions for the parsing phase, which fetches data from databases or files. For example, it includes functions for batching large files.</li> </ul>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/WORK_DIR.html","title":"\ud83d\udcc1 WORK_DIR: The Output Directory","text":"<p>The <code>WORK_DIR</code> is where all output files from the ETL process are automatically generated. You should not manually edit any files in this folder. It serves as the single source of truth for the state of the ETL and holds all final outputs and logs.</p>"},{"location":"Repositories/Solution%20details%20-%20ETL_process%20tool/High%20level%20-%20important%20paths/WORK_DIR.html#core-files-and-folders","title":"Core Files and Folders","text":"<ul> <li><code>loading_status.state</code>: A state file that tracks the loading status of each signal.<ul> <li>It has three columns: <code>signal</code>, <code>date+time</code> (last creation date), and <code>status</code>.</li> <li>Signals with a \"Completed\" status are skipped unless the <code>override</code> flag is used. Overriding a signal is functionally equivalent to manually deleting its row from this file.</li> <li>Signals with an \"In Process\" status will continue from the first unprocessed batch.</li> </ul> </li> <li><code>loading_batch_status.state</code>: Tracks the loading status at the batch level. If the process crashes, it will resume from the last unprocessed batch. Unless  <code>override</code> was set to true</li> <li><code>FinalSignals</code>: This directory contains the final, processed signal files ready for loading with Flow. It may also include an <code>ID2NR</code> file for converting patient IDs to a numeric format.</li> <li><code>rep_configs</code>: A directory that holds the necessary configuration files for the repository loading process, including the signals file, <code>convert_config</code>, and a script to run Flow. This is created automatically.</li> <li><code>outputs</code>: Contains reports and detailed test analysis for each signal.<ul> <li><code>$SIGNAL_NAME</code>: A dedicated folder for each signal (e.g., <code>Hemoglobin</code>). It holds test results, comparisons to reference distributions, and analysis for each batch and whole data combined in the end. HTML plots with distribution of values will be presented</li> <li><code>test.$SIGNAL_NAME.log</code>: A log file containing the results of all tests run on a signal. If a test fails, the process stops, and you must fix the issue before proceeding. This file is overwritten with each new run (override = true) or appended to previous logs and stored by batch and final test for all batches in the end.</li> </ul> </li> <li><code>signal_processings_log</code>: A directory for logging the signal processing steps.<ul> <li><code>process_XXXX.log</code>: Captures all print statements from your processing scripts (e.g., <code>process_labs.log</code>). It shows the output for each batch sequentially.</li> <li><code>XXXX.log</code>: If you use interactive mode, this file logs the commands and outputs from your data inspection, serving as a record of your ETL development process.</li> </ul> </li> </ul>"},{"location":"Research/index.html","title":"Research","text":"<ul> <li>What-If<ul> <li>An envelope script for Causal Inference on Synthetic Data</li> <li>Checking Causal Inference on Synthetic Data</li> <li>Generating Syntethic Data for Causal-Inference</li> </ul> </li> <li>But Why - Explainers<ul> <li>Sanity test experiment for debuging</li> <li>ButWhy experiments results<ul> <li>TestGibbs</li> </ul> </li> <li>Experiments - Stage B</li> <li>Experiments - Stage C (Freeze Version 1)</li> </ul> </li> <li>Conferences<ul> <li>AIME 2019</li> <li>Boston MLHC (ML in HealthCare) 2017</li> <li>KDD 2017</li> </ul> </li> <li>Journal Club Page</li> <li>Factorization Machines</li> <li>Raindrop</li> <li>Missing Values Based Uncertainty Analysis</li> <li>Right Censoring</li> </ul>"},{"location":"Research/Factorization%20Machines.html","title":"Factorization Machines","text":""},{"location":"Research/Factorization%20Machines.html#rationale","title":"Rationale","text":"<p>Classifiers currently used in Medial make use of different types of features, in particular features based on e.g. RC (read codes) or ATC (drug codes). Due to a large number of RC and ATC codes, the total number of RC/ATC features can be very high (200K in CKD Fast Progressors project). In addition, these features are extremely sparse. It would be computationally prohibitive to handle all this features in XGBoost. In order to solve this problem, there exists a \"category_depend\" mechanism, which reduces a number of RC/ATC features by keeping only features which are highly correlated to the outcome. Therefore, information about some read codes or drugs is lost.\u00a0 \u00a0 This project was undertaken in order to check whether it is possible to retain this information by means of\u00a0 \"Factorization Machines\" method, which is suited to work with a large number\u00a0 of extremely sparse features, and is\u00a0widely used in Recommender System.\u00a0 \u00a0</p>"},{"location":"Research/Factorization%20Machines.html#factorization-machines_1","title":"Factorization machines","text":"<p>Factorization Machines (FM) are\u00a0generic supervised learning models that map arbitrary real-valued features into a low-dimensional latent factor space\u00a0and can be applied naturally to a wide variety of prediction tasks including regression, classification, and ranking.\u00a0FMs can estimate model parameters accurately under very sparse data and train with linear complexity, allowing them to scale to very large data sets. FMs are widely used for real-world recommendation problems. FM model is described in details in the article by Steffen Rendle.</p>"},{"location":"Research/Factorization%20Machines.html#libfm","title":"libFM","text":"<p>libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least squares (ALS) optimization as well as Bayesian inference using Markov Chain Monte Carlo (MCMC). Links to the source code and Windows executable can be found at libfm.org LibFM manual is available here.</p>"},{"location":"Research/Factorization%20Machines.html#workflow","title":"Workflow","text":"<p>We tested performance on libFM on a number of datasets, all based on the source dataset from the CKD Fast Progressors project. We decided to omit all the lab test-based features, since these are dense, while libFM is more suited to work on sparse binary and categorical features. Each subsequent set includes more features than the previous one, up to 200K+ features. Where possible we compared results returned by libFM to those of XGBoost, trained on respective data (using the same configuration that was used in CKD Fast Progressors project)</p>"},{"location":"Research/Factorization%20Machines.html#datasets","title":"Datasets","text":"<p>1. Age, Gender columns only 2. Age,Gender and all RC-related and ATC-related columns of type \"category set\" used in\u00a0CKD Fast Progressors project\u00a0(not including features generated via \"category_depend\" mechanism) 3. \u00a0Age,Gender,RC-related and ATC-related columns of type \"category set\" and \"category_depend\" used in\u00a0CKD Fast Progressors project\u00a0 4. Age,Gender and a sparse matrix of RC and ATC-related features generated using Embeddings project\u00a0(with some columns dropped using \"shrinkage\" mechanism, descibed in the Embeddings project documentation)\u00a0\u00a0 5. Age,Gender and a sparse matrix of RC and ATC-related features generated using\u00a0Embeddings\u00a0project\u00a0(this time without using \"shrinkage'). Resulting matrix has 200K+ columns.</p>"},{"location":"Research/Factorization%20Machines.html#results","title":"Results","text":"<ol> <li>Age, Gender columns only</li> <li>Age,Gender and all RC-related and ATC-related columns of type \"category set\" used in\u00a0CKD Fast Progressors project\u00a0 (not including features generated via \"category_depend\" mechanism) Results produced by libFM are marginally better in this scenario</li> </ol> Classifier AUC Linear 0.541 XGBoost 0.573 libFM 0.581 <ol> <li>Age,Gender,RC-related and ATC-related columns of type \"category set\" and \"category_depend\" used in\u00a0CKD Fast Progressors project\u00a0</li> <li>Age,Gender and a sparse matrix of RC and ATC-related features generated using\u00a0Embeddings\u00a0project\u00a0(with some columns dropped using \"shrinkage\" mechanism, descibed in the Embeddings project documentation)\u00a0\u00a0</li> <li>Age,Gender and a sparse matrix of RC and ATC-related features generated using\u00a0Embeddings\u00a0project\u00a0(this time without using \"shrinkage'). Resulting matrix has 200K+ columns. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</li> </ol>"},{"location":"Research/Journal%20Club%20Page.html","title":"Journal Club Page","text":"<p>Following are papers/presentations/links presented in the Journal Club over time, for your pure enjoyment \u00a0</p>"},{"location":"Research/Journal%20Club%20Page.html#xgboost","title":"XGBoost","text":"<p>xgboost_article.pdf xgboost_weight_quantile_sketch.pdf </p>"},{"location":"Research/Journal%20Club%20Page.html#deep-learning","title":"Deep Learning","text":"<p>DeepLearningIntroduction.pptx </p>"},{"location":"Research/Journal%20Club%20Page.html#openmp","title":"OpenMP","text":"<p>OpenMPIntro.pptx</p>"},{"location":"Research/Missing%20Values%20Based%20Uncertainty%20Analysis.html","title":"Missing Values Based Uncertainty Analysis","text":"<p>Use case: KP, Lung Cancer, monthly samples (monthly, without suspected, at least 3 months before outcome for cases) Question / goal / idea: When we have missing values we can:</p> <ul> <li>a) ignore the patient (if he has 'too much missing info')</li> <li>b) impute the missing values, or</li> <li>c) using GIBBS estimation of score std to decide to\u00a0on which patients it is more important to fill the missing values What is better? \u00a0 Setting: MR/Projects/Shared/AlgoMarkers/LungCancer/scripts_uncertainty/adjust_and_calculate.sh MR/Projects/Shared/AlgoMarkers/LungCancer/configs_uncertainty/GIBBS.json</li> </ul> <p>Evaluation: Easier to start with a notebook, but we intend to add a function to bootstrap_app, to report result by Top N Options:</p> <ul> <li>a) One sample per patient (if you using bootstrap_app - we should chose the sample before the app)</li> <li>b) all samples per patient, or\u00a0</li> <li> <p>c) simulation (relatively complicated in this use case - hopefully we won't need it) \u00a0 Baseline Basic evaluation - in the graph, number of cases in the Top N, one sample for patient, age 50-80</p> </li> <li> <p>if we take all patients (3215 on average)</p> </li> <li>if we take just patients with smoking intensity information (1964 patients eligible on average)</li> <li>if we take just patients with WBC in the last 6 months (1438 eligible on average) A few notes:</li> <li>Results are for Ex or Current smokers - if we included never smokers results would have been a bit better ...</li> <li> <p>I got similar results when we take all samples per patient\u00a0 Thus, it is better to take all patients, using the standard imputer, but we will stick to\u00a0Ex or Current smokers Next, we will explore ways to be do even better using the variability in potential score, using GIBBS imputation statistics \u00a0 GIBBS -\u00a0na\u00efve Preliminaries:</p> </li> <li> <p>The script adjust_and_calculate.sh train the adjusted model on all samples - DONE (leakage! should be re-visited later)\u00a0</p> </li> <li>However, apply (calculate) on all samples is far too slow</li> <li>So the Notebook: KP_uncertainty_generate_samples take one sample per patient, and apply GIBBS just for the sample</li> <li> <p>We now have 15 examples so we can test results variance Naive approach would be to replace pred_0 with pred_0 + score std based on GIBBS, to prioritize high uncertainty (or by\u00a0\u00a0pred_0 - score std for the opposite) Both options do not help. Furthermore, if we take (for instance)\u00a0pred_0 +/- 3 X score std result deteriorates. \u00a0 Missing data completion Assume we have M tickets for data completion. We will choose the highest M patients with missing information, with the highest\u00a0pred_0 + score std based on GIBBS.</p> </li> <li> <p>For now we assume 100% compliance</p> </li> <li>Note that it night not be optimal to take the highest M, as (when we not the working point and expected cutoff) those in the top with small std do not need data completion. However, they are not many, and as the results are not sensitive to M, we keep it this way for code simplicity, and to get results for several working points in one run. To simulate data completion:</li> <li>Optional, use data from the future:<ul> <li>Relevant only for smoking features</li> <li>Data exists in ~1/3 of cases and ~1/3 of controls\u00a0</li> </ul> </li> <li>2nd option, find the m (for instance m=20) closest patients<ul> <li>Closets means same gender, same smoking status, distance is measured over all other (normalized) features ,with higher weight for age, and smoking features</li> <li>Choose 1 patient randomly from the 20, and take his values</li> </ul> </li> <li>For patients with missing data but not in the top M, use imputation by our standard imputer (i.e., use original score)</li> <li>Implemented in Notebook:\u00a0KP_uncertainty_missing_values_completion After that we re-calculate the score for those M patients (prediction is made based on split with the right model from the cross validation), take Top N and check performance. MAJOR DRAWBACK In smoking features calculation, we impute\u00a0Smok_Days_Since_Quitting by code (and not by the imputer), so:</li> <li>GIBBS cannot take it into account, and\u00a0</li> <li>for our test\u00a0Smok_Days_Since_Quitting is never missing Furthermore, as you can see in the research summary of the LEAN model,\u00a0Smok_Days_Since_Quitting alone cuts significantly the gap between with and without smoking features.</li> </ul> <p>MAIN RESULT - no significant change in performance .... Example from 1 run: </p> <ul> <li>axis x - N</li> <li>axis y- cancer detected</li> <li>parameters (most important - results are not sensitive to these parameters ...):<ul> <li>sample #09\u00a0</li> <li>M = 2000 (how many data completions)</li> <li>m = 20 (how many closest friends to randomly chose from for imputation)</li> <li>W - 5 for age, 3 for smoking features and BMI, 1 for labs</li> <li>Impute from future - True</li> <li>Priority for impute - pred_0 + std by GIBBS \u00a0 Statistics over 10 datasets, each with 3 run (imputation has random):  Side notes</li> </ul> </li> </ul> <ol> <li>The 'hope for success' was in the data completion procedure, as we complete missing values conditioned on the actual status (case/control). Indeed, for patients with data completion, the average score for cases increased more than for controls, but apparently not more enough. See example in the following table - change of score for M=5000</li> <li>The data completion increases the cutoff score for Top N. It makes sense, as the standard imputation impute to the mean, and we sample from the distribution, hence have more extreme results.</li> <li>Is the data completion by closest friends OK? Other then exploring some examples, we compare the std in score calculated in GIBBS, and the actual std we get (among patients with expected similar std). See the result in the graph - axis x is GIBBS and axis y is what we get. Trend is similar but we higher variability, could be because GIBBS calculate the std on an over fitted model (trained over all the data)</li> </ol>"},{"location":"Research/Raindrop.html","title":"Raindrop","text":"<p>/home/Repositories/MHS/build_Feb2016_Mode_3/maccabi.repository </p> <pre><code>/home/Repositories/MHS/RepProcessed/static/ BYEAR.tsv GENDER.tsv /home/Repositories/MHS/RepProcessed/dynamic/ Hemoglobin.tsv ... (rest of the signals) \n</code></pre> <pre><code>/home/Repositories/MHS/RepProcessed/static/ /home/Repositories/MHS/RepProcessed/dynamic/\n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/static\n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/dynamic\n</code></pre> <pre><code> /home/Repositories/MHS/RepProcessed/dynamic.feather /home/Repositories/MHS/RepProcessed/static.feather \n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/dynamic.feather\n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/static.feather\n</code></pre>"},{"location":"Research/Raindrop.html#training-data-preparation","title":"Training data preparation","text":""},{"location":"Research/Raindrop.html#convert-into-the-raindrop-input-format","title":"Convert into the Raindrop input format","text":"<p>M12data_Large/process_scripts/LoadMaccabi.py</p> <pre><code>Repository data \n    /home/Repositories/MHS/RepProcessed/dynamic.feather\n    /home/Repositories/MHS/RepProcessed/static.feather\nSamples \n    /server/Work/Users/Ilya/LGI/outputs.MHS/Samples.no_exclusions/train.730.1_per_control.samples\nNOTE:   \n    These are the train samples used by the CRC model\n</code></pre> <pre><code>/home/Ilya/Raindrop/M12data_Large/processed_data/arr_outcomes.npy\n/home/Ilya/Raindrop/M12data_Large/processed_data/PTdict_list.npy\n/home/Ilya/Raindrop/M12data_Large/processed_data/subsampled_df.pickle\nNOTE:   \n    Here \"subsampled_df.pickle\" contains data from the .samples file\n</code></pre>"},{"location":"Research/Raindrop.html#convert-absolute-time-into-days-clean-zero-length-entries","title":"Convert absolute time into days, clean zero-length entries","text":"<p>Convert absolute time into days since 20010101 and remove pids which do not have associated dynamic data TRAINING dataset</p> <pre><code>20221103_Fix_Train_FromScratch.ipynb\n</code></pre> <pre><code>/home/Ilya/Raindrop/M12data_Large/processed_data/\n    PTdict_list.npy\n    arr_outcomes.npy\n    subsampled_df.pickle\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTrain/processed_data/\n    PTdict_list.npy\n    arr_outcomes.npy\n    subsampled_df.pickle\n</code></pre>"},{"location":"Research/Raindrop.html#switch-from-absolute-to-relative-time-wrt-sample-date","title":"Switch from absolute to relative time (wr.t. sample date)","text":"<p>http://node-05:9000/notebooks/ilya-internal/20221213_RelativeTimePlusOne.ipynb</p> <pre><code>/home/Ilya/Raindrop/LargeTrain/processed_data/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTrainRelative/\n</code></pre>"},{"location":"Research/Raindrop.html#add-offset-1-to-the-relative-time-to-avoid-0-being-interpreted-as-missing-data","title":"Add offset 1 to the relative time (to avoid 0 being interpreted as missing data)","text":"<pre><code>/home/Ilya/Raindrop/LargeTrainRelative/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTrainRelativePlusOne/\n</code></pre>"},{"location":"Research/Raindrop.html#testing-data-preparation","title":"Testing data preparation","text":""},{"location":"Research/Raindrop.html#reducing-to-a-single-cohort","title":"Reducing to a single cohort","text":"<p>We started from applying the Raindrop model to   the WHOLE Test set, see</p> <p><pre><code>/nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest/bootstrap_raindbow_model.sh\n</code></pre> However, applying the model to the whole test set   resulted in high memory consumption (about 100GB had to be allocated)  Since we are mainly interested in the performance on one specific cohort:   \"MULTI Time-Window:0,365 Age:40,89\", we decided to restrict the test set to this cohort only.  The Test365 samples file was prepared as follows:</p> <pre><code>1)     Create file \"M12data_LargeTest/bootstrap/rainbow.crc.Raw\"\n    using following command\n    [see M12data_LargeTest/bootstrap_rainbow_model.sh]\n    bootstrap_app \\\n    --use_censor 0 \\\n    --rep /home/Repositories/MHS/build_Feb2016_Mode_3/maccabi.repository \\\n    --input ${RUN_PATH_FULL_LEAN}/bootstrap/rainbow.crc.preds \\\n    --output ${RUN_PATH_FULL_LEAN}/bootstrap/rainbow.crc \\\n    --cohort \"${COHORT}\" \\\n    --working_points_pr 1,3,5,7,10,15,20,25,30,35,40,45,50,55,60,65,70,80,90,95,99 \\\n    --working_points_fpr 1,3,5,7,10,15,20,25,30,35,40,45,50,55,60,65,70,80,90,95,99 \\\n    --working_points_sens 75,80,85,90,95,97,99 \\\n    --output_raw\n    Input:  /nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest/bootstrap/rainbow.crc.preds\n    Output:  /nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest/bootstrap/rainbow.crc.Raw\n2)    Take \"rainbow.crc.Raw\" as an input and run notebook 20221023_ExtractTestCohort.ipynb to\n    create cohort in \"samples\" file format.\n    Code:   http://node-05:9000/notebooks/ilya-internal/20221023_ExtractTestCohort.ipynb\n    Input:  M12data_LargeTest/bootstrap/rainbow.crc.Raw \n    Output: M12data_LargeTest/test_cohorts/test_365.samples\n</code></pre>"},{"location":"Research/Raindrop.html#convert-test365-samples-into-the-raindrop-input-format","title":"Convert Test365 samples into the Raindrop input format","text":"<p>M12data_LargeTest365/LoadMaccabiTest.py</p> <pre><code>M12data_LargeTest/test_cohorts/test_365.samples\nRepProcessed/dynamic.feather \nRepProcessed/static.feather\n</code></pre> <pre><code>M12data_LargeTest365/processed_data/subsampled_df.pickle [contain (probably subsampled) input data]\nM12data_LargeTest365/processed_data/PTdict_list.npy\nM12data_LargeTest365/processed_data/arr_outcomes.npy\n</code></pre>"},{"location":"Research/Raindrop.html#convert-absolute-time-into-days-clean-zero-length-entries_1","title":"Convert absolute time into days, clean zero-length entries","text":"<p>Convert absolute time into days since 20010101 and remove pids which do not have associated dynamic data TRAINING dataset</p> <pre><code>20221103_Fix_Test365_FromScratch.ipynb\n</code></pre> <p><pre><code> /nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest365/processed_data/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTest365/processed_data/\n    PTdict_list.npy\n    arr_outcomes.npy\n    subsampled_df.pickle\n</code></pre></p>"},{"location":"Research/Raindrop.html#switch-from-absolute-to-relative-time-wrt-sample-date_1","title":"Switch from absolute to relative time (wr.t. sample date)","text":"<p>http://node-05:9000/notebooks/ilya-internal/20221213_RelativeTimePlusOne.ipynb</p> <pre><code>/home/Ilya/Raindrop/LargeTest365/processed_data/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTest365Relative/\n</code></pre>"},{"location":"Research/Raindrop.html#modifications-applied-to-the-raindrop-code","title":"Modifications applied to the Raindrop code","text":"<p>Original Raindrop code is not well-suited for experimentation   for following reasons:</p> <ul> <li>Raindrop.py contains code specific to handling different datasets.</li> <li>Training and testing are done in the same loop</li> <li>In order to get a confidence interval for AUC authors train models for five different splits, rather than using Bootstrap</li> <li>Many configuration parameters are hardcoded We made following modifications to the code:</li> <li>Strip Raindrop.py of the code not relevant to MHS</li> <li>Implement MHS processing based on the code handling Physionet 2012 (denoted P12 in the code). We will denote the code specific to MHS dataset as M12.</li> <li>Split the code into RaindropTrain.py and RaindropTest.py</li> <li>Add command-line parameters that allow setting configuration parameters from the command line</li> <li>Implement model evaluation using Bootstrap, in order to be consistent with the SOTA model evaluation approach. New files:</li> <li>RaindropTrain.py</li> <li>RaindropTest.py Obsoleted files:</li> <li>Raindrop.py</li> </ul>"},{"location":"Research/Raindrop.html#model-evaluation","title":"Model evaluation","text":"<p>After the model is applied to the test data during the call to RaindropTest.py we convert predictions into a format expected by the Medial's bootstrap_app.  The script that implements this conversion is called BootstrapPrepare.py  Code:</p> <pre><code>code/BootstrapPrepare.py\n</code></pre>"},{"location":"Research/Raindrop.html#script-infrastructure-for-fast-experimentation","title":"Script infrastructure for fast experimentation","text":"<p>In order to be able to run experiments fast while logging all the necessary information for retrospective analysis,  we implemented a folder template which contains all or some of following files:  train_test_bootstrap.sh this file trains a model, applies it to the test set and then computes AUC with confidence interval using bootstrap_app  log.txt this file contains logs of the ./train_test_bootstrap.sh invokation</p>"},{"location":"Research/Raindrop.html#experiments","title":"Experiments","text":""},{"location":"Research/Raindrop.html#largerelativeaftertrainfix","title":"LargeRelativeAfterTrainFix/","text":"<pre><code>Train model on RELATIVE TIME.\nThe \"fix\" mentioned in the folder name is that we make sure relative time is never zero by adding 1. This is necessary since 0 is interpreted as a missing value by the Raindrop code.\nAUC     0.829[0.814 - 0.843]\nNOTE:\n    We also applied model to the test data after EACH epoch\n    to see the dynamics of the AUC as a function of epoch.\n</code></pre>"},{"location":"Research/Raindrop.html#largerelative_d_ob_16","title":"LargeRelative_D_OB_16/","text":"<pre><code>Based on LargeRelativeAfterTrainFix/\nbut this time we increase dimension of the observation embedding to\nStatus:\n    There's no performance improvement as compared to D_OB=4\n    AUC     0.828[0.811 - 0.842]\n</code></pre>"},{"location":"Research/Raindrop.html#largerelative_nlayers_4","title":"LargeRelative_nlayers_4/","text":"<pre><code>Increase the number of TransformerEncoder layers from 2 to 4.\nStatus:\n    There's no performance improvement as compared to 2 layers\n    AUC     0.825[0.812 - 0.839]\n</code></pre>"},{"location":"Research/Raindrop.html#largematchedfull","title":"LargeMatchedFull/","text":"<pre><code>Train classifier based on FULL matching data\nWe first converted FULL matched data into Relative format\nusing 20221218_PrepareFullMatched_Data_RelativeTime.ipynb,\nthe resulting data written to\n/home/Ilya/Raindrop/LargeTrainMatchedRelative/\nStatus:\n    AUC     0.814[0.799 - 0.828]\n    Performance of the model trained on the matched data \n    is degraded, as compared to the model trained on the unmatched\n    data (AUC     0.829[0.814 - 0.843])\n</code></pre>"},{"location":"Research/Raindrop.html#todo","title":"TODO","text":""},{"location":"Research/Raindrop.html#increase-dimensionality-of-the-positional-embedding","title":"Increase dimensionality of the positional embedding","text":"<p>This will increase the time resolution of Raindrop</p>"},{"location":"Research/Raindrop.html#use-inverted-time-series-collect-output-of-the-first-position-in-traonsformerencoder-same-way-it-is-done-when-using-bert-for-classification","title":"Use inverted time series + collect output of the first position in TraonsformerEncoder (same way it is done when using BERT for classification)","text":"<p>This may help the network focus on the latest observation</p>"},{"location":"Research/Raindrop.html#train-network-on-data-where-missing-values-were-fixed-using-ad-hoc-algoritm","title":"Train network on data where missing values were fixed using ad-hoc algoritm","text":"<p>Since \"observation propagation\" stage of the Raindrop algorithm turn out to be degenerate, this may improve the performance</p>"},{"location":"Research/Raindrop.html#disable-observation-propagation-stage-at-all","title":"Disable observation-propagation stage at all","text":"<p>Since \"observation propagation\" stage of the Raindrop algorithm turn out to be degenerate, we expect almost no performance degradation here</p>"},{"location":"Research/Raindrop.html#summary","title":"Summary","text":"<p><pre><code>Performance of the Raindrop model (AUC 0.829) is significantly below the performance of SOTA CRC model.\n</code></pre> </p>"},{"location":"Research/Right%20Censoring.html","title":"Right Censoring","text":"<p>Definition: Right censoring is the phenomenon of finite length of follow up. When we try to predict an event within a time window using a retrospective database, not all patients that stand the criterion of prediction at a given time have a follow up that covers the whole time window.</p> <p>If the event does happen within a shorter time the longer follow up is not needed. If the event does not happen within the follow up we do not know if it could still happen within the time window but after end of folow up.</p> <p>This phenomena is important for long prediction window or in databases of high turn around and short follow up. Right censoring is expected to give higher weight to short range events as the long range events have a higher probability of being censored.</p> <p>Right censoring can happen when patient is lost to follow up, when the experiment period ends or when patient dies (if the predicted event is not death). The following paper discusses the IPCW method to compensate for this censoring: https://www.sciencedirect.com/science/article/pii/S1532046416000496?via%3Dihub</p> <p>They assume that the loss to follow up distribution is independent from outcome. For each included prediction that had enough follow up till event (case) or till end of prediction window (control) they give weight that represents all other predictions that were censored before an event or end of window happened.</p> <p>We implemented their method for a model that predicts severe aortic stenosis within 5 years and also all cause death for same time window. We calculated weights on prediction file (using all test samples, including the ones that were censored), and used the weighted bootstrap to get the performance.</p> <p>As expected AUC deteriorated by up to 2% when implementing this on the test only. Implementation on training set is complicated if you also want to match for years.</p>"},{"location":"Research/But%20Why%20-%20Explainers/index.html","title":"But Why - Explainers","text":"<p>The But Why project.</p> <p>Goal: Explain prediction of model for specific sample - \"local feature importance\" Challenge: There are several definitions\\options of how to explain a specific prediction.\u00a0 Method: Using Shapley Values definition. You can read more about it in Wikipedia. \u00a0 Background: Shapley values have several properties which makes them more\u00a0suitable\u00a0for explaining model predictions. It gives better result in\u00a0experiments that are favorable\u00a0among testers compared to other methods (science jornals claim that).</p> <p>Shapley Values - simplified: Given a specific prediction sample and requirement to explain the prediction by its contributing variables, Shapley Values divides the contribution fairly (comes from game theory) and is unique solution\u00a0 For this division of contributions which answer some important properties. Each variable is given an importance score based on how the score is effected by the knowledge of this specific variable value. For example - if a variable is a copy of another variable (or\u00a0deterministic\u00a0function of the other variable) - it will\u00a0receive\u00a0the same importance as the other variable\u00a0regardless\u00a0of the model (the variables should contribute the same). If variable doesn't\u00a0affect\u00a0the score - it will get 0 importance. The sum of all variable's importance is the score. The problem is that Shapley value is hard to calculate and can only be estimated. The minimal properties for unique solution:</p> <ul> <li>Null player - If variable does't\u00a0affect\u00a0the score in any scenario (knowledge of the variable) - it will get 0 importance</li> <li>Symetric/Consitent/Fairness - If i,j are unknown features and in all cases when 1 of them is known the score changes the same as the other one is known - they will get the same contribution score</li> <li>Linearity - If we have 2 functions v,w and define a new function v+w to explain which sums the 2 functions - so the contribution value for each varaible\u00a0 in each function v,w can also be summed.\u00a0 \u00a0 Framework for experiments: The git repository is\u00a0MR_Projects - mapped into\u00a0$MR_ROOT/Projects/Shared/Projects</li> </ul> <p>The experiment will be done on multiple model to test for\u00a0robustness. All the models are on THIN, except CRC which is on MHS. The models are:</p> <ul> <li>Diabetes model<ul> <li>Repository: THIN 2017</li> <li>Json to train model:\u00a0$MR_ROOT/Projects/Shared/Projects/configs/But_Why/test_models/pre2d_model.json</li> <li>Train\\Test samples:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers_samples/diabetes</li> <li>GAN model to generate synthetic samples for shapley calculation if needed:\u00a0/server/Work/Users/Alon/But_Why/outputs/GAN/pre2d_gan_model.txt</li> </ul> </li> <li>CRC model<ul> <li>Repository: MHS</li> <li>Json to train model:\u00a0$MR_ROOT/Projects/Shared/Projects/configs/But_Why/test_models/crc.json</li> <li>Train\\Test samples:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers_samples/crc</li> <li>GAN model to generate synthetic samples for shapley calculation if needed - Not exist, need to create one</li> </ul> </li> <li>Flu+Complications simpler model (All relevatn drugs, all relevant diagnosis, Age, Gender and 5 top lab tests features)<ul> <li>Repository: NWP</li> <li>Json to train model:\u00a0in flu project fo nwp. The final binary of trained model is in here:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/base_model.bin</li> <li>Train\\Test samples:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers_samples/flu_nwp</li> <li>GAN model to generate synthetic samples for shapley calculation if needed -\u00a0Not exist, need to create one</li> </ul> </li> </ul> <p>The Experiment: Experiment will be performed on\u00a0each model. We will try to explain the test samples (special samples were collected for each problem - for example low hemoglobin and high score in CRC and the opposite... several examples for each model) with different explainers. Explainers are\u00a0PostProcessor of\u00a0ModelExplainer. We will try to explain the prediction with different configurations\\method for each sample in the model. After collecting all the results we will go over the alternatives and Coby Metzger(someone else?) will score them by:</p> <ul> <li>Good</li> <li>Fair</li> <li> <p>Bad General comments:</p> </li> <li> <p>Options to test - in blined (method changes each row randomly)</p> </li> <li>Test 10 examples for each\u00a0Explainer\u00a0setting for sanity before the full\u00a0experiment</li> <li>Final step to validate with someone else besides Coby (Karni?)\u00a0</li> </ul> <ul> <li>Grouping</li> <li>Output - return top 10 of most contributing with abs value. - will return contribution and normalized contribution in the output (both magnitudes).\u00a0 will also print value of representative feature in the group (the representative will be chosen by the most important feature in the group) Selected problems:</li> </ul> <ol> <li>CRC - about 1400 features and 40 test samples</li> <li>Diabetes - about 100 features and 143 test samples -\u00a0need to reduce number of samples (not need that much)</li> <li>Flu simpler - Drug, Diagnosis, Age, Gender + 7 lab features and 20 test samples</li> </ol> <p>Explainer settings:</p> <ol> <li>KNN - with threshold on 5%\u00a0without covariance fix</li> <li>tree without covariance fix</li> <li>tree with covariance fix</li> <li>LIME with GAN - do some test to choose the required number of masks.\u00a0without covariance fix</li> <li>Shapley with GAN\u00a0\u00a0- do some test to choose the required number of masks.\u00a0without covariance fix</li> <li>missing_shapley - retrain model with random missing values samples and use the retrained model with masks (much faster shapley, less accurate).\u00a0\u00a0\u00a0without covariance fix total test: 6 * (40+20+143) = 1218\u00a0 testIf diabetes will have less samples (for example 30) - 540 tests \u00a0</li> </ol> <p>The Explainers: All the different explainers are configured unders: <pre><code>ll $MR_ROOT/Projects/Shared/Projects/configs/But_Why/explainers_cfgs\n</code></pre> Each post_processor is json for adjust model with ModelExplainer to test - it can be different method or different parameters. the name of the explainer is the name between explainer. to .json</p> <ul> <li>KNN\u00a0- coby method for KNN</li> <li>LIME_GAN - LIME Shapley with GAN</li> <li>LIME_gibbs_LightGBM - Lime Shapley with Gibb sampling of LightGBM model to estimate each variable value probability based on other variables in the Gibb sampling</li> <li>LIME_gibbs_QRF - Lime Shapley with Gibb sampling of QRF model to estimate each variable value probability based on other variables in the Gibb sampling. Faster than LightGBM but less accurate</li> <li>shapley_GAN - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) thatthe sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses GAN to generate the samples in each sampled mask experiment.</li> <li>shapley_GAN_sample_by_size - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) and uses sampling that is not random that the sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses GAN to generate the samples in each sampled mask experiment.</li> <li>shapley_gibbs_lightGBM - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) thatthe sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses Gibb sampling of LightGBM model to estimate each variable value probabilitybased on other variables in the Gibb sampling</li> <li>shapley_gibbs_QRF - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) thatthe sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses Gibb sampling of QRF model to estimate each variable value probabilitybased on other variables in the Gibb sampling. Faster than LightGBM but less accurate</li> <li>tree - Uses tree algorithm for trees</li> <li>missing_shap - learn a respond model to mimic model result on missing values Can also add more configurations - Play with processings like grouping, covariance sum, to zero missing values and more... Need to define and close all options to test! \u00a0 How to generate explain results for model: Use the scripts in: \u00a0 <pre><code>ll $MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.*.sh\n#or just use to run all experiments for crc,diabetes and flu:\n$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/run_all.sh\n</code></pre> script args are: 1. the explainer cfg name: look for the available names above under\u00a0The Explainers section. For example \"tree\" or \"KNN\" or \"LIME_gibbs_QRF\" 2. Run_mode - a number with 2 bits:</li> </ul> <ol> <li> <p>First bit if on - will run explainer to explain predictions on the test samples - will override existing results</p> </li> <li> <p>Second bit if on - Will run train\\adjust model - Will force readjust modelFor Examples: 0 - do nothing just convert outputs. 1 - only apply and run explainer to explain test samples. 2 - will only run adjust_model. 3- will do both</p> </li> <li>Summary run mode flag - if zero (default argument value) will finish run after Train\\Apply explainer (depend on Run_Mode). if got 1 - will create a blinded random_test with all the explainers. If got 2 - will summarize all scores for all the explainers and will create a report* Full example for diabetes: <pre><code>#Force to run Train and Apply on KNN_TH. 3 - means in binary 11 - run train &amp; Apply. Default is 0 to run only whats needed. If model noe exists will run train, if no results exists will run apply.\n$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.diabetes.sh KNN_TH 3\n#Or from (has symbolic link to the git scripts)\n/server/Work/Users/Alon/But_Why/scripts/test_explainers.diabetes.sh tree 3\n</code></pre> \u00a0 \u00a0 I have script that creates a blinded report (the first argument\u00a0KNN_TH is not being used, but you must pass it. it can be either any of the possible explainers in the experiment) <pre><code>$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.diabetes.sh KNN_TH 0 1\n</code></pre> Now look at /server/Work/Users/Alon/But_Why/outputs/explainers/diabetes/compare_blinded.tsv Example from diabetes blinded report:  As you may see after each sample of patient+time there is a line with \u201cSCORES\u201d to score each explainer - wrote for example \"Good\" for the first explainer. Each explainer ranks the top 10 groups of features to explain the prediction score pred_0. Example for line 1 in explainer_1: Glucose:=-1.30226(36.29%) {Glucose.avg.win_0_3650:=100.31133} \u2013 which means the contribution of Glucose group of signals is -1.30 (pay attention to the minus sign) which means Glucose levels reduce the score. In the brackets you may see the normalized contribution of the signal in percentage \u2013 Glucose is responsible for 36.29% of the score. Lastly, you may see the value of the most important feature in the group \u201cGlucose.avg.win_0_3650\u201d and the value is 100.31.. \u00a0 *** You need to be consistent with the scores (case insensitive), either \u201cgood\u201d, \u201cfair\u201d \u201cbad\u201d or number, just keep the same scores (don\u2019t use free text, if need to comment use the line below)\u00a0 *** \u00a0 the explainer's names for each line (patient+ prediction time) is recorded in the file\u00a0map.ids.tsv\u00a0 Example of map.ids.tsv for each record:  In each line we can see the order of explainers *Macro Analysis of explainers: Analyze each explainer results: how many Good, Fair, Bad for each model and all together. To create summary resport run: <pre><code>#after collecting scores from the expirment form Coby or other physician, to summarize and map to explainer names report run (the diffrent form previous command is the last argument which is 2 instead of 1)\n$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.diabetes.sh KNN_TH 0 2\n</code></pre> The summary can be found in files:</li> </ol> <ul> <li>summary.tsv - For each record (patient + prediction time) and explainer (now with the name, not blinded) will reveal Coby scores. A table for each sample records X explainer's names - the values in the cells are Coby's scores</li> <li>summary.sum.tsv - A summary for each explainer name and each score how mnay times it happened. Conclude into conclusion. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html","title":"Experiments - Stage B","text":"<p>We will preform an additional experiment (not blinded) and more focused to achieve those goals:</p> <ul> <li>Understand/decide about rules to clean the output.<ul> <li>For example: weird results like giving high contribution to missing value(we already have a parameter to zero it)</li> <li>Check score for threshold and validate we have enough positive contributions or negative, if the patient wasn't flagged?</li> <li>Use threshold for absolute and relative contribution scores - for example: patient with low score maybe should not see normal lab results with small contribution</li> </ul> </li> <li>Understand/decide about the outputs - currently we have groups of features and we reveal top contributing feature with it. Also presenting the feature value and percentage from all contribution. Do we need to change it?<ul> <li>Presents top 10 groups by absolute values- maybe we should sort otherwise, depend on the score?currently we have decided to do it that way</li> <li>Groups: will split each lab signal into 2 groups of : values, trends. categorical values are aggregated by time windows</li> <li>Preventative feature in each group - present the most contributing feature in the group. what to do if it has opposite contribution sign?</li> </ul> </li> <li>Complete the documentation of how much time it takes for LIME, trees to run.<ul> <li>CRC - 23.5 seconds for 120 samples. means 3-4 s sample in second (using parallel)...\u00a0 \u00a0A\u00a0 little slow out of scope: - how to present it in gui, separate into reason we can affect and not... \u00a0 Will be done on CRC, Flu and with trees_with_covarinace_fix, LIME_GAN \u00a0 Update!!! Added new method: tree_iterative.</li> </ul> </li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html#pre2d-results","title":"Pre2D results:","text":"<p>The results on pre2d were already pretty good (with maybe some improvement over tree explainer without covariance fix). Just wanted to make sure the tree_iterative doesn't make things worse. tree_iterative is similar (with\u00a0maybe\u00a0slightly improvement comapre to with_cov)</p> Explainer_name 2 3 4 5 &lt;EMPTY&gt; Total exp Average score Average L^0.5 tree_iterative 21 46 46 25 2 138 3.543478261 1.864308126 tree_with_cov 16 54 51 17 2 138 3.5 1.856313886 <p>*in pre2d the covariance fix made the results of but why a little worse in the first stage of experiments.\u00a0</p>"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html#crc-results","title":"CRC results:","text":"run Explainer_name 2 3 4 5 &lt;EMPTY&gt; Mean_Score Mean_Score_0.5 first run tree_iterative_cov 1 4 15 7 94 4.037037037 1.999810838 first run tree_iterative 1 6 16 5 93 3.892857143 1.963816368 first run tree_with_cov 5 10 10 2 94 3.333333333 1.809767105"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html#comments","title":"Comments:","text":"<ul> <li>coby noticed that some missing value features are getting high contribution - we should use the flag to zero it?\u00a0Coby Metzger - are you sure? maybe because of the groups some features in the group are not missing? - Coby thinks it's OK, the imputed value for example when the glucose is high and no HbA1C, the imputation for HbA1C is also high. He likes it. He gave more reasons...</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20C%20%28Freeze%20Version%201%29.html","title":"Experiments - Stage C (Freeze Version 1)","text":"<p>After some discussions and ideas to improve the covariance fix in order to better handle with similar and dependent features. As you probably remember, Shapley Values should split the contribution equally to features that are the same, so if important features has many similar features, it may result in a wrong ButWhy report. This method is used togheter with the \"iterative\" method. \u00a0 The current method is to calculate the covariance matrix of the features and multiply it by the contributions. The problem arises when you have groups. When using groups those are the new options (the first option is what we had in the last experiment): In all options these are the definitions: Let's mark the features covariance matrix as F(i,j) is size NxN (N is the number of features). Build \"covariance matrix\" for groups (if we have G groups, the matrix size is GxG), lets mark it C matrix. C(i,i)=1. C(i,j)=C(j,i) the matrix is symmetric</p> <ol> <li>C(i,j) := max{ F(k,l) | k is feature in group i, l is feature in group j }</li> <li>Let's mark feature contribution for the prediction as vector T in size N as the number of features (taking the contribution of the features without iterations) .C(i,j) := Sigma( k is feature in group i, l is feature in group j) { F(k,l)T(k)T(l) } / Sigma( k is feature in group i, l is feature in group j) { 1T(k)T(l) } The advantage in equation #2 is that it should maybe better since it's not taking \"max\" and using specific feature contributions. \u00a0 Added another options to the calculation of the \"covariance\" matrix of features differently by using mutual information between features instead of correlation. The idea is to catch better non-linear behaviors like BMI and some other and more complicate feature dependencies that linear model can miss. Used normalization factor to control the values to be between 0-1. 0 - the features are independent, 1 - you can determine the second feature from the first feature. The equation for mutual information is KLD between the joint features distribution and calculation of the joint probability assuming they are independent (you measure the information gain between the assumption of in-dependency to what you observe in the data). Normalization is done by dividing with the entropy of the joint distribution. The normalization causes all number to be between 0-1 as we want and duplicate features will get 1. \u00a0 The following file has 4 methods to compare (covariance or mutual information and equation 1 or equation 2) W:\\Users\\Alon\\But_Why\\outputs\\Stage_B\\explainers\\crc\\reports\\compare_new_cov_fix\\compare_all.xlsx. I did it for CRC which is the most challenging problems since we have many features and similar once.</li> </ol>"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20C%20%28Freeze%20Version%201%29.html#results","title":"Results","text":"<p>Alon Results\u00a0compare_all.Alon.xlsx</p> Method/score 1 2 3 4 5 Average Average_0.5 Tree_iterative_covariance(New equation) 0 0 0 2 20 4.909091 2.214607252 Tree_iterative_mutual_information(New equation) 0 0 0 3 19 4.863636 2.20387689 Tree_iterative_mutual_information(MAX) 0 0 0 10 12 4.545455 2.128764351 Tree_iterative_covariance(MAX) 0 0 7 9 6 3.954545 1.979125614 <p>Coby results compare_all - Coby.xlsx:</p> Method/score 1 2 3 4 5 Average Average_0.5 Tree_iterative_covariance(New equation) 0 0 3 11 7 4.19047619 2.04041087 Tree_iterative_mutual_information(New equation) 0 0 8 10 3 3.761904762 1.931648114 Tree_iterative_mutual_information(MAX) 0 1 5 5 3 3.714285714 1.913047967 Tree_iterative_covariance(MAX) 0 6 10 4 0 2.9 1.690289472"},{"location":"Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20C%20%28Freeze%20Version%201%29.html#conclusions","title":"Conclusions","text":"<ul> <li>The new equation seem to improve the results. Use it instead of max, that's the default in ExplainProcessings (use_max_cov=0)</li> <li>The mutual information might improve the results. I can't see it when using the new equations since the average score is in saturation (almost all recieved 5 out of 5). Coby and I noticed a huge improvement for the mutual information compared to the covariance when using MAX instead of new equation. Yet, we both got that the best results for CRC are new equations with covaraince.Waiting for\u00a0Avi Shoshan and\u00a0Yaron to review the file and grade the results themselves if they want (maybe they will see different things). Currently my recommandation is to use the new equation with covaraince (also faster learning).\u00a0 \u00a0 To create a nice ButWhy report, you might use:</li> </ul> <ol> <li>adjust model app to add post_processor with explainer to the model. Later you can change some parameters if needed using change_model (without relearn)</li> <li>CreateExplainnReport app to generate a nice report \u00a0</li> </ol>"},{"location":"Research/But%20Why%20-%20Explainers/Sanity%20test%20experiment%20for%20debuging.html","title":"Sanity test experiment for debuging","text":"<p>Date:\u00a018.11.2019It looks like all our methods are tuned (without a major bug).\u00a0The results of all the methods look reasonable.Coby passed over 8 samples (out of 140 available) for diabetes predictor (grouped by signals into 11 groups from 69 features) and scored them from 1-4 (the higher the better).We can see the distribution of the scores for each method and the average score: \u00a0</p> Explainer_name 1 2 3 4 &lt;EMPTY&gt; Average Score Tree 0 1 4 3 132 3.25 missing_shap 0 2 2 4 132 3.25 LIME_GAN 0 2 3 3 132 3.125 SHAP_GAN 0 2 3 3 132 3.125 Tree_with_cov 0 3 1 4 132 3.125 knn_with_th 2 2 1 3 132 2.625 knn 0 5 2 1 132 2.5 <p>* Tree = regular tree shapley implementation * missing_shap - shapley algorithm with 500 random masks. but instead of using GAN or gibbs to generate samples, we use additional predictor (xgboost regression model) to predict the diabetes model scores when seeing random masks of missing values.\u00a0\u00a0the grouping is calculated internally. * LIME_GAN - LIME algorithm with GAN - sampling random masks and fitting a model -\u00a0the grouping is calculated internally. * SHAP_GAN - Shapley algorithm with GAN. sampling random masks and not an exact calculation - the grouping is calculated internally.* Tree_with_cov - the tree implementation with covariance fix * knn_with_th - the KNN algorithm with threshold of 5% to explainknn - the KNN algorithm without threshold</p>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html","title":"ButWhy experiments results","text":"<ul> <li>Expirement models<ul> <li>NWP_Flu\u00a0</li> <li>CRC</li> <li>Pre2D</li> </ul> </li> <li>Conclusions</li> <li>Appendix - Gibbs in Flu NWP\u00a0</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#expirement-models","title":"Expirement models","text":""},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#nwp_flu","title":"NWP_Flu","text":"<p>The model has 22 features, most of them are binary (Drugs, Diagnosis category_set). The non categorical are: Age, Smoking, SpO2, Resp_Rate, Flu.nsamples, Complications.nsamples, Memebership. In this run, Added Shapley gibbs explainer (22 features is OK for shapley gibbs -see apendix for more details).\u00a0 scores are the higher the better from 1-5. this is the histogram of 18 examples of flu. Also added average on sqrt of the 1-5 scores to increase the importance for improving on low scores compare to higher scores - no big change here.</p> Explainer_name 1 2 3 4 5 Mean_Score Mean_of_Sqrt_Score Tree_with_cov 0 2 3 8 5 3.888889 1.955828857 Tree 0 2 3 10 3 3.777778 1.929599082 SHAP_Gibbs_LightGBM 0 1 7 8 2 3.611111 1.889483621 missing_shap 0 1 9 4 4 3.611111 1.885941263 LIME_GAN 1 4 7 4 2 3.111111 1.736296992 SHAP_GAN 2 4 6 6 0 2.888889 1.669397727 knn 0 7 6 5 0 2.888889 1.682877766 knn_with_th 8 2 5 2 1 2.222222 1.42915273 <p>Summary - in the simple case of 22 features Tree_with_Covariance preforms the best and than the regular tree. Not far behind\u00a0SHAP_Gibbs_LightGBM and\u00a0missing_shap which preforms similarly. \u00a0 Reference to expirement results:</p> <ul> <li>compare_blinded.tsv - the blinded experiment - for each sample random shuffle of explainers outputs. and in xlsx format:\u00a0compare_blinded.xlsx</li> <li>map.ids.tsv - the order of each explainer</li> <li>summary.tsv - results for each sample - with explainers aligned (not blinded) - after joining map.ids.tsv with compare_blinded.tvs</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#crc","title":"CRC","text":"Explainer_name 1 2 3 4 5 &lt;EMPTY&gt; Score Score_Sqrt Tree_with_cov 0 1 13 21 3 1 3.684211 1.911555 Tree 0 3 16 17 2 1 3.473684 1.853358 LIME_GAN 0 15 11 12 0 1 2.921053 1.691204 SHAP_GAN 0 14 16 6 2 1 2.894737 1.683788 missing_shap 4 18 7 8 1 1 2.578947 1.574112 knn 6 17 9 6 0 1 2.394737 1.516581 knn_with_th 6 18 10 4 0 1 2.315789 1.494115 <p>Reference to expirement results:</p> <ul> <li>compare_blinded.tsv\u00a0-\u00a0the blinded experiment - for each sample random shuffle of explainers outputs. and in xlsx format: compare_blinded_CRC.xlsx</li> <li>map.ids.tsv -\u00a0\u00a0the order of each explainer</li> <li>summary.sum.tsv\u00a0- results for each sample - with explainers aligned (not blinded) - after joining map.ids.tsv with compare_blinded.tvs</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#pre2d","title":"Pre2D","text":"Explainer_name 1 2 3 4 5 &lt;EMPTY&gt; Score Score_of_Sqrt SHAP_GAN 0 4 43 86 5 2 3.666667 1.908082456 LIME_GAN 0 3 49 78 7 3 3.649635 1.903398585 Tree 0 5 48 82 3 2 3.601449 1.890708047 Tree_with_cov 0 10 63 63 2 2 3.413043 1.838648351 missing_shap 1 26 76 34 0 3 3.043796 1.732886234 knn 3 43 61 29 2 2 2.884058 1.680713177 knn_with_th 22 36 56 22 2 2 2.608696 1.582454126 <p>Reference to expirement results:</p> <ul> <li>compare_blinded.tsv\u00a0- the blinded experiment - for each sample random shuffle of explainers outputs. and in xlsx format:\u00a0</li> <li>map.ids.tsv\u00a0- the order of each explainer</li> <li>summary.tsv\u00a0- results for each sample - with explainers aligned (not blinded) - after joining map.ids.tsv with compare_blinded.tvs</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#conclusions","title":"Conclusions","text":"<p>Summary Table all expirements:</p> Method Flu 1 Flu 0.5 CRC 1 CRC 0.5 Diabetes 1 Diabetes 0.5 L1 L0.5 Tree_with_cov 3.888889 1.955828857 3.684211 1.912 3.413043 1.8386484 3.662048 1.902011 Tree 3.777778 1.929599082 3.473684 1.853 3.601449 1.890708 3.617637 1.891222 SHAP_Gibbs_LightGBM 3.611111 1.889483621 3.611111 1.889484 LIME_GAN 3.111111 1.736296992 2.921053 1.691 3.649635 1.9033986 3.227266 1.776967 SHAP_GAN 2.888889 1.669397727 2.894737 1.684 3.666667 1.9080825 3.150098 1.753756 missing_shap 3.611111 1.885941263 2.578947 1.574 3.043796 1.7328862 3.077951 1.73098 knn 2.888889 1.682877766 2.394737 1.517 2.884058 1.6807132 2.722561 1.626724 knn_with_th 2.222222 1.42915273 2.315789 1.494 2.608696 1.5824541 2.382236 1.501907 <ul> <li>The tree algorithm works the best in gerneal when the predictor is tree based. the covariance fix also improves it slightly.</li> <li>The LIME\\SHAP are pretty similar. The LIME is slightly better and faster so it's preferable over SHAP. They are also model agnostic, but hareder to train. Gibbs might imporve the results (but be much slower) and might be usefull if we use it on not too many features/groups of features.</li> <li>The missing_shap - very simple and fast model (also model agnostic). It preforms good in some problems, but has some train parameters the are important to tune.\u00a0In previous experiments in Pre2D it was much better (used different train parameters that made it worse comapre to the previous run). After runing with better params, I see it's even better than Shapley,LIME methods..BUG found in paramters in missing_shap that had disabled the grouping and cause problem - need to run again (Can't run with Grouping and \"group_by_sum=1\", should use the grouping mechanisim in missing_shap)...Bug found when training with wrong weights in missing_shap when using groups!</li> <li>KNN - should be used without threshold. For now, it haven't prove itself enougth to be used.</li> <li>If we use Trees predictors without groups - the shapley values should do the job without covariance fix. It's a unique solution that preserve fairness.\u00a0</li> </ul>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#appendix-gibbs-in-flu-nwp","title":"Appendix - Gibbs in Flu NWP","text":"<p>The Gibbs shows seperation of 0.6 between the generated samples and the real ones (when using random mask with probability 0.5 for each feature) and seperation of 0.719 when generating all features. This is high quality generation of matrix. For example GAN show seperation of 0.99 when generating all features and 0.74 when choosing random masks. Test gibbs script</p> <p><pre><code>$MR_ROOT/Projects/Shared/But_Why/Linux/Release/TestGibbs --rep /home/Repositories/KPNW/kpnw_jun19/kpnw.repository --train_samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/flu_nwp/train.samples --test_samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/flu_nwp/validation_full.samples --model_path /server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/base_model.bin --run_feat_processors 1 --save_gibbs /server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/gibbs_tests/test_gibbs.bin --save_graphs_dir /server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/gibbs_tests/gibbs_graphs --gibbs_params \"kmeans=0;select_with_repeats=0;max_iters=0;predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=0;num_trees=80;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=30;feature_fraction=0.8;bagging_fraction=0.25;bagging_freq=4;is_unbalance=true;num_leaves=80};num_class_setup=num_class;calibration_string={calibration_type=isotonic_regression;verbose=0};calibration_save_ratio=0.2;bin_settings={split_method=iterative_merge;min_bin_count=200;binCnt=150};selection_ratio=1.0\" --predictor_type xgb --predictor_args \"tree_method=auto;booster=gbtree;objective=binary:logistic;eta=0.1;alpha=0;lambda=0.1;gamma=0.1;max_depth=4;colsample_bytree=1;colsample_bylevel=0.8;min_child_weight=10;num_round=100;subsample=0.7\" --gibbs_random_range 1 --gibbs_sampling_params \"burn_in_count=500;jump_between_samples=20;samples_count=50000;find_real_value_bin=1\"   --test_random_masks 0 \n</code></pre> \u00a0 Some Feature graphs examples - gibbs generated VS real: Binary features: Gender - Males rate - 47.88% in real VS 47.68% in generated. Diagnosis.Asthma rate in real 17.17% VS 17.08%. Admission hostpital_observation rate in real 2.98% VS 3.15% In Age, there seems like there is binning issue in gibbs for odd age values. In smoking, there are a lot of unique values and the gibbs only returns 1 out of 150 bins (should bin the feature before trying to seperate. not done yet) - might cause some of the power to seperate between real and the generated... All the others looks very good. Some graphs (real date is in blue, the generated gibbs is in orange): Click here to expand...  </p>"},{"location":"Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/TestGibbs.html","title":"TestGibbs","text":"<p>There is utility to test the gibbs imputations method - it has several options for controlling gibbs parameters for training, testing it. The default test is to impute all values (harder) - generate a full example and compare it to the original population. You can also contol to test only \"earsing\" randomly masks of values and imputing them (easier for gibbs). Please git pull\u00a0https://github.com/Medial-EarlySign/MR_Projects/tree/main/But_Why\u00a0and compile TestGibbs, then run with TestGibbs --base_config $CONFIG_FILE\u00a0 or just specify all parameters in the command. \u00a0 Gibbs config examples with all params: <pre><code>save_gibbs=/nas1/Temp/Test_gibbs/gibbs_model #Binary model of gibbs object. Will start from if exists\nsave_gibbs_cmp_log=/nas1/Temp/Test_gibbs/compare_gibbs_log\nsave_graphs_dir=/nas1/Temp/Test_gibbs/gibbs_graphs\nmissing_value=-65336\ntest_random_masks=0\ndown_sample=0\n#When learn_to_impute_missing_value is 1, the imputer will also take missing value imputation as legal value to impute. Useful for simple compare to test matrix that has missing values VS generated data using gibbs\nlearn_to_impute_missing_value=1\n#optional file path with name of features to filter from matrix and keep only those\n#Flow --print_model_info --f_model /nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl 2&gt;&amp;1 | grep FEAT | awk -F\" \" '{print $3}' | sort &gt; /nas1/Temp/Test_gibbs/feats_list\n#Removed Smoking_Status features - they are strongly connected and it causes problems in gibbs\nsel_features=/nas1/Temp/Test_gibbs/feats_list \n#Input option 1 - directly provide train/test matrices\n#load_matrix_path= #Provide specific matrix for train\n#test_gibbs_mat= #Provide specific matrix for test\n#Input option 2 - Provide rep and samples and model to generate matrix\nrun_feat_processors=0\nrep=/home/Repositories/KP/kp.repository\nmodel_path=/nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl\ntrain_samples=/nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl.train_samples\ntest_samples=/nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl.test_samples\nmax_loops=1 #Rerrun multiple times to take only \"good\" samples each time. If bigger than 1\ngibbs_random_range=1 #Only used if test_random_masks is on - erase mask and put those values in random range\nstop_at_sens=0.95 #Rerrun multiple times to take only \"good\" samples each time. threshold for good samples\ngibbs_params=predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=0;num_trees=100;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=20;feature_fraction=0.8;bagging_fraction=1;bagging_freq=4;is_unbalance=true;num_leaves=80};calibration_save_ratio=0.2;calibration_string={calibration_type=isotonic_regression;verbose=0};num_class_setup=num_class;bin_settings={split_method=iterative_merge;min_bin_count=500;binCnt=100};selection_count=500000\ngibbs_sampling_params=burn_in_count=1000;jump_between_samples=50;samples_count=10000;find_real_value_bin=1\n#For testing the generated samples and compare to test samples\npredictor_type=xgb\npredictor_args=tree_method=auto;booster=gbtree;objective=binary:logistic;eta=0.1;alpha=0;lambda=0.1;gamma=0.1;max_depth=4;colsample_bytree=1;colsample_bylevel=0.8;min_child_weight=10;num_round=100;subsample=0.7\n</code></pre> <pre><code>Linux/Release/TestGibbs --base_conf /server/UsersData/alon/MR/Projects/Shared/Projects/configs/UnitTesting/examples/MultipleImputations/TestGibbs_cfg.cfg\n</code></pre> \u00a0 The result will appear in\u00a0 /nas1/Temp/Test_gibbs</p>"},{"location":"Research/Conferences/index.html","title":"Conferences","text":"<p>This page will contain\u00a0Conferences details</p>"},{"location":"Research/Conferences/AIME%202019.html","title":"AIME 2019","text":"<p>Papers Internal: W:\\Conferences\\AIME_2019\\pdfs Papers External: P:\\Conferences\\AIME_2019 \u00a0 PPT from Sharepoint :\u00a0AIME 2019.pptx </p>"},{"location":"Research/Conferences/Boston%20MLHC%20%28ML%20in%20HealthCare%29%202017.html","title":"Boston MLHC (ML in HealthCare) 2017","text":"<p>Boston MLHC</p> <p>https://mucmd.org/</p> <p>Search for the articles in arxive:</p>"},{"location":"Research/Conferences/Boston%20MLHC%20%28ML%20in%20HealthCare%29%202017.html#accepted-papers","title":"Accepted Papers","text":"<ul> <li>Piecewise-constant parametric approximations for survival learning Jeremy Weiss*, Carnegie Mellon University</li> <li>Spatially-Continuous Plantar Pressure Reconstruction Using Compressive SensingAmirreza Farnoosh, Northeastern University; Mehrdad Nourani, University of Texas at Dallas; Sarah Ostadabbas*, Northeastern University</li> <li>Classifying Lung Cancer Severity with Ensemble Machine Learning in Health Care Claims DataSavannah Bergquist*, Harvard University; Gabriel Brooks, Dartmouth-Hitchcock Medical Center; Nancy Keating, Harvard Medical School, Brigham and Women's Hospital; Mary Beth Landrum, Harvard Medical School; Sherri Rose, Harvard Medical School</li> <li>Predicting long-term mortality with first week post-operative data after Coronary Artery Bypass Grafting using Machine Learning modelsJos\u017d Forte*, University of Groningen; Marco Wiering, University of Groningen; Hjalmar Bouma, University Medical Center Groningen; Fred de Geus, University Medical Center Groningen; Anne Epema, University Medical Center Groningen</li> <li>ShortFuse: Biomedical Time Series Representations in the Presence of Structured InformationMadalina Fiterau*, Stanford University; Suvrat Bhooshan, Stanford University; Jason Fries, Stanford University; Charles Bournhonesque, Stanford University; Jennifer Hicks, Stanford University; Eni Halilaj, Stanford University; Christopher Re, Stanford University; Scott Delp, Stanford University</li> <li>Towards Vision-based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene ComplianceAlbert Haque*, Stanford University; Michelle Guo, Stanford University; Alexandre Alahi, Stanford University; Amit Singh, Lucile Packard Children's Hospital; Serena Yeung, Stanford University; N. Lance Downing, Stanford; Terry Platchek, Lucile Packard Children's Hospital; Li Fei-Fei, Stanford University</li> <li>Surgeon Technical Skill Assessment using Computer Vision based AnalysisHei Law*, University of Michigan; Jia Deng, University of Michigan, Ann Arbor; Khurshid Ghani, University of Michigan</li> <li>Predicting Surgery Duration with Neural Heteroscedastic RegressionZachary Lipton*, UCSD; Nathan Ng, UCSD; Rodney Gabriel , UCSD; Charles Elkan, UCSD; Julian McAuley, UC San Diego</li> <li>Temporal prediction of multiple sclerosis evolution from patient-centered outcomesESamuele Fiorini, University of Genoa; Andrea Tacchino, Italian Multiple Sclerosis Foundation - Scientific Research Area; Giampaolo Brichetto, Italian Multiple Sclerosis Foundation - Scientific Research Area; Alessandro Verri, University of Genova, Italy; Annalisa Barla*, Universit\u02c6 degli Studi di Genova</li> <li>Clustering Patients with Tensor DecompositionMatteo Ruffini*, UPC; Ricard Gavald\u02c6, UPC; Esther Lim\u2014n, Institut Catal\u02c6 de la Salut</li> <li>Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning ApproachAniruddh Raghu*, MIT; Marzyeh Ghassemi, MIT; Matthieu Komorowski, Imperial College London; Leo Celi, MIT; Pete Szolovits, MIT</li> <li>Modeling Progression Free Survival in Breast Cancer with Tensorized Recurrent Neural Networks and Accelerated Failure Time ModelYinchong Yang*, Siemens AG, LMU M\u0178nchen; Volker Tresp, Siemens AG and Ludwig Maximilian University of Munich ; Peter Fasching, Department of Gynecology and Obstetrics, University Hospital Erlangen</li> <li>Hawkes Process Modeling of Adverse Drug Reactions with Longitudinal Observational DataYujia Bao*, University of Wisconsin-Madison; Zhaobin Kuang, University of Wisconsin, Madison; Peggy Peissig, Marshfield Clinic Research Foundation; David Page, University of Wisconsin, Madison; Rebecca Willett, University of Wisconsin, Madison</li> <li>Patient Similarity Using Population Statistics and Multiple Kernel LearningBryan Conroy*, Philips Research North America; Minnan Xu-Wilson, Philips Research North America; Asif Rahman, Philips Reserach</li> <li>A Video-Based Method for Automatically Rating AtaxiaRonnachai Jaroensri*, MIT CSAIL; Amy Zhao, MIT; Fredo Durand, MIT; John Guttag, MIT; Jeremy Schmahmann, Massachusetts General Hospital; Guha Balakrishnan, MIT; Derek Lo, Yale University</li> <li>Visualizing Clinical Significance with Prediction and Tolerance RegionsMaria Jahja*, North Carolina State University; Daniel Lizotte, UWO</li> <li>Predictive Hierarchical Clustering: Learning clusters of CPT codes for improving surgical outcomesElizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, and Katherine Heller</li> <li>An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis DetectionJoseph Futoma, Sanjay Hariharan, Katherine Heller, Mark Sendak, Nathan Brajer, Meredith Clement, Armando Bedoya, and Cara O'BrienMarked Point Process for Severity of Illness AssessmentKazi Islam*, UC Riverside; Christian Shelton, UC Riverside</li> <li>Diagnostic Inferencing via Improving Clinical Concept Extraction with Deep Reinforcement Learning: A Preliminary StudyYuan Ling, Philips Research North America; Sadid A. Hasan*, Philips Research North America; Vivek Datla, Philips Research North America; Ashequl Qadir, Philips Research North America; Kathy Lee, Philips Research North America; Joey Liu, Philips Research North America; Oladimeji Farri, Philips Research North America</li> <li>Generating Multi-label Discrete Patient Records using Generative Adversarial NetworksEdward Choi*, Georgia Institute of Technology; Siddharth Biswal, Georgia Institute of Technology; Bradley Malin, Vanderbilt University; Jon Duke, Georgia Institute of Technology; Walter Stewart, Sutter Health; Jimeng Sun, CS</li> <li>Quantifying Mental Health from Social Media using Learned User EmbeddingsSilvio Moreira*, INESC-ID; Glen Copperfield, qntfy.io; Paula Carvalho, INESC-ID; M\u2021rio Silva, INESC-ID; Byron Wallace, Northeastern</li> <li>Clinical Intervention Prediction and Understanding using Deep NetworksNathan Hunt*, MIT; Marzyeh Ghassemi, MIT; Harini Suresh, MIT; Pete Szolovits, MIT; Leo Celi, MIT; Alistair Johnson, MIT</li> <li>Understanding Coagulopathy using Multi-view Data in the Presence of Sub-Cohorts: A Hierarchical Subspace ApproachArya Pourzanjani*, UCSB; Tie Bo Wu, UCSB; Richard M. Jiang, UCSB; Mitchell J. Cohen, Denver Health Medical Center; Linda R. Petzold, UCSB</li> <li>Towards a directory of rare disease specialists: Identifying experts from publication historyZihan Wang*, University of Toronto; Michael Brudno, U Turonto; Orion Buske, Centre for Computational Medicine, SickKids Hospital</li> <li>Reproducibility in critical care: a mortality prediction case studyAlistair Johnson*, MIT; Tom Pollard, MIT; Roger Mark, MIT</li> </ul>"},{"location":"Research/Conferences/Boston%20MLHC%20%28ML%20in%20HealthCare%29%202017.html#accepted-clinical-abstracts","title":"Accepted Clinical Abstracts","text":"<ul> <li>Extracting Information from Electronic Health Records Using Natural Language Processing \u2013 Knowledge Discovery from Unstructured InformationVasua Chandrasekaran, Jinghua He, Monica Reed Chase, Aman Bhandari, Christopher Frederick, and Paul Dexter</li> <li>Using Machine Learning to Recommend Oncology Clinical TrialsAnasuya Das, Leifur Thorbergsson, Aleksandr Grigorenko, David Sontag, Iker Huerga</li> <li>Accounting for diagnostic uncertainty when training a Machine Learning algorithm to detect patients with the Acute Respiratory Distress SyndromeNarathip Reamaroon, Michael W. Sjoding, Kayvan Najarian</li> <li>Visual Supervision of Unsupervised Clustering of Patients with ClustervisionAdam Perer*, IBM Research; Bum Chul Kwon, IBM Research; Janu Verma, IBM Research; Kenney Ng, IBM Research; Ben Eysenbach, MIT; Christopher deFilippi, INOVA; Walter Stewart, Sutter Health</li> <li>MS Mosaic: First Steps (and Stumbles) Toward a Patient-Centered Mobile Platform for Multiple Sclerosis Research and CareLee Hartsell</li> <li>Light Field Otoscope 3D Imaging of Diseased Ears in an Alaska Native PopulationManuel Martinello, Harshavardhan Binnamangalam, Philip Hofstetter, John Kokesh, Samantha Kleindienst, Tiffany Romain, Noah Bedard, and Ivana Tosic</li> </ul>"},{"location":"Research/Conferences/KDD%202017.html","title":"KDD 2017","text":"<p>Review of relevant papers in kdd 2017: KDD%202017 \u00a0 Refernce to all other paper in: \u00a0\u00a0 in Linux: /nas1/Work/Conferences/KDD_2017/pdfs</p>"},{"location":"Research/What-If/index.html","title":"What-If","text":"<ul> <li>Synthetic Data Generation</li> <li>Checking Causal Inference on Synthetic Data</li> <li>An envelope script for Causal Inference on Synthetic Data</li> </ul>"},{"location":"Research/What-If/An%20envelope%20script%20for%20Causal%20Inference%20on%20Synthetic%20Data.html","title":"An envelope script for Causal Inference on Synthetic Data","text":"<p>The script H:/MR/Projects/Shared/CausalEffects/CausalEffectScripts/run_process.py is an envelope for generating synthetic data for causal-inference, applying several methods, and analyzing. At the moment, the script should be executed on node-05. The scripts parmeters are: <pre><code>run_process.py --help\nusage: run_process.py [-h] --config CONFIG [--start START] [--end END]\n                      [--show]\nCheck Causal Effects Approaches on Toy Model\noptional arguments:\n  -h, --help       show this help message and exit\n  --config CONFIG  configuration file\n  --start START    start stage\n  --end END        end stage\n  --show           show stages\n  --fast           skip slow stages\n</code></pre></p> <ul> <li> <p>config is the only required parameter. An example of a configuration file is -\u00a0 <pre><code>nSamples=15000\nnums=--numA 10 --numB 10 --numC 10\npTreatment=0.25\nseed1=123\nseed2=22\nseed3=11\nparams=--pOutcome 0.1 --outSigMin 0.08 --outSigMax 0.95 --treatSigWidth 5.0 --outSigWidth 5.0 --outcomePolyDegree 2 --treatmentPolyDegree 2 --treatmentFactor 0.4\ndir=/nas1/Work/Users/yaron/CausalEffect/ToyModel_ITE/Test1\n</code></pre> Note that all keys are required, and no sapces allowed around the assignment sign. All keys are used for generating the synthetic data, and ** is also used for all further steps</p> </li> <li> <p>** tells the script to list all stages and stop: <pre><code>run_process.py --config dummy --show\n['Generate', 'Stats', 'Naive.LGBM', 'IPW.LGBM', 'Naive.LGBM.RCT', 'Naive.NN', 'IPW.NN', 'Naive.NN.RCT', 'Quasi.NN', 'Quasi.Full_NN', 'Quasi.LGBM', 'CFR', 'SHAP', 'IPW.SHAP', 'Performance']\n</code></pre></p> </li> <li> <p>** and ** allow running only a subset of the script's stages</p> </li> <li>slow stages are - IPW.NN and Quasi.Full_NN The stages of the script perform the following tasks-</li> </ul> Generate Generate the synthetic data Stats Some analyses on synthetic data, including generation of true validation ITEs file Naive.LGBM \u2192 IPW.SHAP check_toy_model using various methods Performance Check performance of various methods using correlation to true ITE <p>A part of the Stats stage is running the script H:/MR/Projects/Shared/CausalEffects/CausalEffectScripts/analyze_risk_matrix.py *\u00a0 *that generates a PDF file () with various graphs. However, due to Python issues we currently do not run it within the envelope script and it should be executed seperately The output of the envelope script is a file **/Summary which include both config file information, as well as the performance evaluation information: <pre><code>nSamples=15000\nnums=--numA 10 --numB 10 --numC 10\npTreatment=0.25\nseed1=123\nseed2=22\nseed3=11\nparams=--pOutcome 0.1 --outSigMin 0.08 --outSigMax 0.95 --treatSigWidth 5.0 --outSigWidth 5.0 --outcomePolyDegree 2 --treatmentPolyDegree 2 --treatmentFactor 0.4\ndir=/nas1/Work/Users/yaron/CausalEffect/ToyModel_ITE/Test1\nITEs.true : 1.000000\nITEs.Model.NN.RCT : 0.536027\nITEs.Model.LGBM.RCT : 0.531990\nITEs.Quasi.Full_NN : 0.315594\nITEs.Quasi.NN : 0.313484\nITEs.Quasi.LGBM : 0.284961\nITEs.CFR : 0.249583\nITEs.Model.LGBM : 0.222051\nITEs.IPW.LGBM : 0.216663\nITEs.shap : 0.201008\nITEs.Model.NN : 0.175503\nITEs.IPW.NN : 0.119160\nITEs.ipw_shap : 0.110271\n</code></pre> </p>"},{"location":"Research/What-If/Checking%20Causal%20Inference%20on%20Synthetic%20Data.html","title":"Checking Causal Inference on Synthetic Data","text":"<p>The program for testing causal-inference methods on synthetic data is located in -\u00a0H:\\MR\\Projects\\Shared\\CausalEffects\\CausalEffectsUtils\\check_toy_model The program parameters are :\u00a0 <pre><code>check_toy_model --help\nProgram options:\n  --help                                produce help message\n  --trainMatrix arg                     train data file (bin)\n  --testMatrix arg                      test data file (bin)\n  --validationMatrix arg                validation data file (bin)\n  --params arg                          serialized true model file\n  --validationITE arg                   File of validation ITE\n  --out arg                             output file for ITE graph\n  --read_models                         read required models from file\n  --write_models                        write generated models to file\n  --models_prefix arg                   prefix of models for read/write\n  --gen_model_params arg (=lightgbm;num_threads=15;num_trees=200;learning_rate=0.05;lambda_l2=0;metric_freq=250;bagging_fraction=0.5;bagging_freq=1;feature_fraction=0.8;max_bin=50;min_data_in_leaf=250;num_leaves=120)\n                                        parameters and definition of\n                                        classifiers which are not specifically\n                                        given\n  --gen_reg_params arg (=xgb;alpha=0.1;colsample_bytree=0.5;eta=0.01;gamma=0.5;booster=gbtree;objective=reg:linear;lambda=0.5;max_depth=3;min_child_weight=100;num_round=250;subsample=0.5)\n                                        parameters and definition of regressors\n                                        which are not specifically given\n  --nbootstrap arg (=100)               # of bootstrap rounds\n  --nfolds arg (=8)                     # of folds for cross validation\n  --gen_nn_params arg (=batch_size=1000;function=relu;max_num_batches=30000;checkpoint_num_batches=250;data=features;keep_prob=0.8;learning_rate=1e-3;nhidden=200;nlayers=2)\n                                        parameters for nn regressionscript\n  --do_true                             get ITE from true model\n  --do_direct                           directly model true ite\n  --do_model                            get ITE from single model\n  --model_params arg                    parameters and definition of outcome\n                                        predictor\n  --add_propensity arg (=0)             If True will add propensity score to\n                                        direct model for outcome\n  --bonus arg (=0)                      bonums for splitting by treatment in\n                                        outcome prediction using xgboost\n  --do_nn_model                         get ITE from single NN model\n  --do_two_models                       get ITE from single model\n  --model0_params arg                   parameters and definition of outcome\n                                        predictor for untreated\n  --mode1_params arg                    parameters and definition of outcome\n                                        predictor for treated\n  --do_weighted                         get ITE from propensity weighted model\n  --prop_params arg                     parameters and defition of propensity\n                                        score\n  --weighed_model_params arg            parameters and definition of outcome\n                                        predictor\n  --do_g_comp                           get ITE using g-computation\n  --g_comp_params arg                   parameters and definition of outcome\n                                        predictor\n  --g_comp_prop_params arg              parameters and definition of propensity\n                                        predictor\n  --g_comp_reg_params arg               parameters and definition of\n                                        counter-factuals regressor\n  --g_comp_reg_script arg (=/nas1/UsersData/yaron/MR/Tools/quasi_oracle/PythonScripts/ite_predictor.py)\n                                        Python script for t-prediction (ITE)\n  --g_comp_reg_script_output arg        output for Python script for\n                                        counter-factuals regressor\n  --g_comp_reg_script_input arg (=g_comp_matrix)\n                                        input for Python script for\n                                        counter-factuals regressor\n  --g_comp_reg_script_params arg        parameters for python script for\n                                        counter-factuals regressor\n  --g_comp_cf_params arg                parameters and definition of\n                                        counter-factuals classifier\n  --gNumCopy arg (=10)                  number of copies per sample in\n                                        counterfactual matrix\n  --gAddTestMatrix                      add test matrix to counter-factual\n                                        regression matrix\n  --do_two_models_g_comp                get ITE using g-computation\n  --g_comp_params0 arg                  parameters and definition of outcome\n                                        predictor for treatment=0\n  --g_comp_params1 arg                  parameters and definition of outcome\n                                        predictor for treatment=1\n  --do_nn_quasi_oracle                  get ITE using quasi-oracle\n  --do_quasi_oracle                     get ITE using quasi-oracle\n  --e_params arg                        Quasi-Oracle e-prediction params\n                                        (propensity)\n  --m_params arg                        Quasi-Oracle m-prediction params\n                                        (outcome without explicit treatment)\n  --t_params arg                        Quasi-Oracle t-prediction params (ITE)\n  --t_script arg (=/nas1/UsersData/yaron/MR/Tools/quasi_oracle/PythonScripts/ite_predictor.py)\n                                        Python script for t-prediction (ITE)\n  --t_script_output arg                 output for Python script for\n                                        t-predictions (ITE)\n  --t_script_input arg (=ite_matrix)    input for Python script for\n                                        t-predictions (ITE)\n  --t_script_params arg                 parameters for python script for\n                                        t-predictions (ITE)\n  --do_oracle                           get ITE using an oracle\n  --treatment_params arg                serialized treatment model file\n  --extend_matrix                       add quadratic features to t-modeling\n                                        matrix\n  --preds_files_suffix arg              Siffix for predictions files\n  --optimization_file arg               File for optimization of t-predictor\n                                        parameters\n  --summary_file arg                    File for summary of results\n  --do_external                         read predictions from csv file\n  --preds_file arg                      predictions file (untreated/treated\n                                        pairs)\n  --do_external_predictor               generate predictions from a\n                                        MedPredictor\n  --predictor_file arg                  predictor files (ITE from features\n                                        without Treatment)\n  --do_external_script                  generate predictions using a script\n  --script arg                          external script to run\n  --script_input arg                    script data input (--data ...)\n  --script_params arg                   script parameters\n  --script_output arg                   script prediction output (--preds ...)\n  --do_cfr                              run counterfactual regression\n  --script_dir arg (=.)                 script data directory\n  --cfr_train_script arg (=/nas1/UsersData/yaron/MR/Projects/Shared/CausalEffects/CausalEffectScripts/cfrnet-master/cfr_net_train.py)\n  --cfr_trans_script arg (=/nas1/UsersData/yaron/MR/Projects/Shared/CausalEffects/CausalEffectScripts/csv2cfr.py)\n  --do_shap                             use Shapley values for ITE\n  --do_ipw_shap                         use Shapley values on IPW-corrected\n                                        model for ITE\n  --do_weighted_nn_model                get ITE from propensity weighted NN\n                                        model\n</code></pre> The program uses ** and * to learn model(s) for evaluation individual treatment effecsts (ITE) and then applies the model(s) on *. The programs outputs include descrptive information in , as weill as tabular output in **. Each line in ** countains three tab-delimited columns -\u00a0\u00a0</p> Method-Name True-ITE Estimated-ITE <p>The true ITE is either generated from the generative model (if given, in .bin and .treatment.bin) or read from file (**) Methods currently implemented are:</p> Method Description Comment <p>do_true</p> true ITE from generative models do_direct Learn a regression model to directly evaluate ITE on trainMatrix and apply on validationMatrix This is a debugging method as it assumes true ITE is known for the trainMatrix do_model Learn a naive model \u0192(x,T)\u2192y , and evaluate ITE = \u0192(x,1) - \u0192(x,0) do_nn_model Same as do_model but using external learning/predictions scripts for model Allows interfacing with TensorFlow do_two_models Learn two models\u0192<sub>T=1</sub>(x)\u2192y and \u0192<sub>T=0</sub>(x)\u2192y and evaluate ITE = \u0192<sub>T=1</sub>(x) - \u0192<sub>T=0</sub>(x) do_weighted Use Inverse Propensity Weighting (IPW) to learn \u0192(x,T)\u2192y , and evaluate ITE = \u0192(x,1) - \u0192(x,0) do_weighted_nn_model Same as do_weighted but using external learning/predictions scripts for model Allows interfacing with TensorFlow do_g_comp <p>Use \"G-Computation\" - create counter-factuals using a model, and then use them for learning a</p><p>second model.</p> IPW optional for first model do_two_models_g_comp A combination of do_g_comp &amp; do_two_models <p>do_quasi_oracle</p> Evalute ITE using Quasi-Oracle - e* and m* evaluated internally and ITE using an external script ITE is evaluated using an external script to allow using TensorFlow NN <p>do_nn_quasi_oracle</p> Evalute ITE using Quasi-Oracle - e* and m* also evaluated using external scripts Allows interfacing with TensorFlow on all stages do_oracle Similar to Quasi-Oracle, only using true e and m instead of estimated e* and m* his is a debugging method as it assumes true e and m are known do_external Import ITE from file and generate out file do_external_predictor Read a MedPredictor object and apply on validationMatrix to generate ITE do_external_script Apply an external script to generate ITE on validationMatrix do_cfr Apply Uri Shalit's CounterFactual Regression Methods Use downloaded scripts do_shap Use Shapley of naive model \u0192(x,T)\u2192y values as estimators for ITE do_ipw_shap Use Shapley of IPW-learned model \u0192(x,T)\u2192y values as estimators for ITE"},{"location":"Research/What-If/Generating%20Syntethic%20Data%20for%20Causal-Inference.html","title":"Generating Syntethic Data for Causal-Inference","text":"<p>The program for generating synthetic data is located in -\u00a0H:\\MR\\Projects\\Shared\\CausalEffects\\CausalEffectsUtils\\generate_realistic_data the program implements the following model -\u00a0 \u00a0 Where -\u00a0</p> <ul> <li>Poly-Tree = Tree with polynomials at the\u00a0 nodes</li> <li>Transformed-input = apply the following transformation before calculating the polynomial (currently n=3 is hard-coded):</li> <li>Noisy-Indexing = apply logistic function on value, with minimal/maximal values moved from\u00a0[0.0,1.0] to\u00a0[\u03b5,1-\u03b5'], and then use that as probability for deciding on the dichotomic Treatment/Outcome</li> <li>+/- = requiring that the first-order contributions of a parameter to the polynomial is set to be positive (negative)</li> <li> <p>Parameters for running the generation are: <pre><code>generate_realistic_data --help\nProgram options:\n  --help                               produce help message\n  --seed arg (=12345)                  randomization seed\n  --scale arg (=1)                     scale of random features\n  --nSamples arg                       number of samples\n  --numA arg                           # of variables affecting only treatment\n  --numB arg                           # of variables affecting treatment and\n                                       outcome (confounders)\n  --numC arg                           # of variables affecting only outcome\n  --polyDegree arg (=3)                polynom model degree\n  --treatmentPolyDegree arg (=1)       polynom model degree for treatment\n  --outcomePolyDegree arg (=1)         polynom model degree for outcome\n  --pTreatment arg                     probablity of applying treatment\n  --treatSigMax arg (=0.899999976)     maximal value of treatment probability\n  --treatSigMin arg (=0.0500000007)    minimal value of treatment probability\n  --treatSigWidth arg (=0.100000001)   width of treatment sigmoid function\n  --pOutcome arg                       probablity of positive outcome\n  --outSigMax arg (=0.975000024)       maximal value of output probability\n  --outSigMin arg (=0.0250000004)      minimal value of output probability\n  --outSigWidth arg (=0.5)             width of output sigmoid function\n  --treatmentFactor arg (=0.150000006) Scaling factor of Treatment before\n                                       outcome calcluation\n  --matrix arg                         output matrix file (bin)\n  --params arg                         output params file (bin)\n  --output arg                         output log file\n  --risk arg                           output risk scores file\n</code></pre></p> </li> <li> <p>Note that -\u00a0</p> <ul> <li>Currently, the depth of the trees is hard-coded as depth=2</li> <li>SigWidth determine the scaling of the logistic function, normalized by the distribution of the input variable. The lower it is, the lower the slope at the step is (i.e., very large widths correspond to step function)</li> <li>Features\u00a0 are generated as Uniform[0,**]</li> <li>Treatment is scaled by treatmentFactor before application of final polynomial</li> <li>risk is a debugging output matrix giving various intermediate values (e.g. Outcome/Treatment/Risk scores, various probabilities, etc.)</li> <li>The models are written into *.bin and *.treatment.bin</li> </ul> </li> <li>Additional projects in the same solution include -\u00a0<ul> <li>Generate matrices given the model: <pre><code>generate_realistic_data_from_model --help\nProgram options:\n  --help                 produce help message\n  --seed arg (=12345)    randomization seed\n  --scale arg (=1)       scale of random features\n  --nSamples arg         number of samples\n  --numA arg             # of variables affecting only treatment\n  --numB arg             # of variables affecting treatment and outcome\n                         (confounders)\n  --numC arg             # of variables affecting only outcome\n  --matrix arg           output matrix file (bin)\n  --params arg           input model params file (bin)\n  --treatment_params arg treatment model params file (bin)\n  --output arg           output log file\n  --rct_p arg            RCT data. Randomize Treatment with given probability\n</code></pre> The program generates a random feaures matrix (Uniform[0,]), either uses\u00a0.treatment.bin to set the treatment or randomly selects it (if ** is given) for a Randomized Controlled Trial scenario, and then uses\u00a0**.bin to set the output \u00a0</li> <li>Various utilities for handling the synthetic data/model - <pre><code>utils --help\nProgram options:\n  --help                produce help message\n  --mode arg            what should I do ?\n  --matrix arg          matrix to use for evaluation\n  --dir arg             directory for outcome and treatment models\n  --preds arg           predictions to evaluate\n  --epreds arg          e predictions for evaluation\n  --mpreds arg          m predictions for evaluation\n  --model arg           model to print\n  --out arg             output file\n  --csv arg             input csv file\n  --bin arg             output bin file\n</code></pre></li> </ul> </li> </ul> <p>Possible modes include:</p> <ol> <li>print - get a human-readable version of a generative model (from params.bin****</li> <li>getProbs - generate a vector of the output probabilities given a matrix and a model</li> <li>getAUC - get the maximal possible AUC for outcome prediction (known true probabilities)</li> <li>csv2bin - translate a csv matrix to binary format (serialized MedFeatures) \u00a0</li> </ol>"}]}