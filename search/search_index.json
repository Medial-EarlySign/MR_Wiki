{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":"<p>A note on our journey: Medial EarlySign was a company that developed a proprietary platform for machine learning on electronic medical records. Following the company's liquidation, the decision was made to release the core software as an open-source project to allow the community to benefit from and build upon this technology.</p> <p>Our platform is designed to transform complex, semi-structured Electronic Medical Records (EMR) into machine-learning-ready data and reproducible model pipelines. The framework is optimized for the unique challenges of sparse, time-series EMR data, delivering low memory usage and high-speed processing at scale.</p> <p>It was conceived as a \"TensorFlow\" for machine learning on medical data.</p> <p>All software is now open-sourced under the MIT license. Some of the models developed by Medial EarlySign that are currently in production are available exclusively through our partners.</p> <p>The framework was battle-tested in production across multiple healthcare sites and was a key component of an award-winning submission to the CMS AI Health Outcomes Challenge.</p>"},{"location":"index.html#why-use-this-platform","title":"Why Use This Platform?","text":"<ul> <li>High-Performance Processing: Engineered for large-scale, sparse EMR time-series data where general-purpose libraries like pandas fall short.</li> <li>Reusable Pipelines: Save valuable engineering time by providing shareable, tested pipelines and methods.</li> <li>Built-in Safeguards: Mitigate common pitfalls like data leakage and time-series-specific overfitting.</li> <li>Production-Ready: Designed for easy deployment using Docker or minimal distroless Linux images.</li> </ul>"},{"location":"index.html#core-components","title":"Core Components","text":"<p>The platform is built on three key pillars:</p> <ul> <li>MedRepository: A compact, efficient data repository and API for storing and accessing EMR signals. Querying categorical signals like perscriptions and diagnosis in an easy and efficient API. </li> <li>MedModel: An end-to-end machine learning pipeline that takes data from MedRepository or JSON EMR inputs to produce predictions and explainability outputs. It supports both training and inference.</li> <li>Medial Tools: A suite of utilities for training, evaluation, and workflow management, including bootstrap analysis, fairness checks, and explainability.</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<ul> <li>Build a new model: Follow the step-by-step Tutorials to build a model from scratch.</li> <li>Use an existing model: Browse the collection of Models or learn how to deploy a model with AlgoMarker Deployment.</li> </ul>"},{"location":"index.html#resources","title":"Resources","text":"<ul> <li>MR_LIBS: The core infrastructure libraries.</li> <li>MR_Tools: Tools and pipelines built on top of MR_LIBS.</li> <li>MR_Scripts: A collection of helper scripts and utilities.</li> </ul> <p>Explore the documentation to understand the architecture and tools.</p>"},{"location":"CONTRIBUTING.html","title":"CONTRIBUTING","text":""},{"location":"CONTRIBUTING.html#contributing","title":"Contributing","text":"<ul> <li>Please use discussion pages to suggest new feature</li> <li>Help improve documentation in MR_WIKI</li> <li>Create a pull request</li> </ul>"},{"location":"GEMINI.html","title":"Project: Documentation Wiki of MES Framework","text":""},{"location":"GEMINI.html#documentation-agent-instructions","title":"Documentation Agent Instructions","text":"<p>Your primary task is to generate or modify MkDocs-compatible Markdown documentation. Adhere strictly to the following output, content, and review guidelines.</p>"},{"location":"GEMINI.html#output-format-strict-technical-requirements","title":"Output Format (Strict Technical Requirements)","text":"<ol> <li>MkDocs &amp; Markdown Compliance:<ul> <li>All generated content must be in valid Markdown format.</li> <li>Indentation Rule: Use four spaces for indentation ONLY within nested lists (lists within lists) to ensure MkDocs compatibility. Do not use this indentation for standard, top-level lists.</li> <li>Spacing Rule: Maintain a single empty line between regular prose/text and any subsequent list (bulleted or numbered) for optimal readability and parsing.</li> </ul> </li> <li>Output Directory and File Placement:<ul> <li>Place all generated or modified files under the main documentation path: <code>MR_WIKI</code>.</li> <li>Ensure files are placed logically into the correct subdirectories within <code>MR_WIKI</code>.</li> <li>ABSOLUTELY DO NOT place Markdown documentation files inside application code directories. All documentation belongs under the <code>MR_WIKI</code> path.</li> </ul> </li> <li>Internal Linking Policy:<ul> <li>Preserve Links: You must not remove any existing internal or external links in the content unless specifically instructed.</li> <li>Suggested Removal Tag: If you identify a link that appears unnecessary or irrelevant, you must mark it clearly using the tag: <code>@@@[SUGGEST_REMOVE_LINK]</code>.</li> </ul> </li> </ol>"},{"location":"GEMINI.html#documentation-structure-and-content","title":"Documentation Structure and Content","text":"<ol> <li>Structural Authority: You are fully empowered to edit existing files or propose/create multiple new Markdown files if, and only if, this results in a more logical, comprehensive, and superior user experience and documentation structure.</li> <li>Clarity and Tone: Rephrase all content to maximize clarity, conciseness, and practicality for the end-user. Prioritize actionable steps, simple explanations, and a professional, helpful tone.</li> <li>Gap Identification Tag: If any necessary information, examples, code snippets, or crucial sections are missing from the content you are generating or editing, insert the clear placeholder tag: <code>@@@[PLEASE_COMPLETE_MISSING_INFO]</code>.</li> </ol>"},{"location":"GEMINI.html#review-and-summary-mandatory-report","title":"Review and Summary (Mandatory Report)","text":"<p>Upon completion of the documentation generation/modification task, you must provide a final summary in the chat based on the following criteria:</p> <ol> <li>Structure Justification: Provide a clear explanation of why the proposed/modified file structure is superior (or why the new structure is the most logical choice) for the end-user.</li> <li>Required Human Actions: Provide a list of all content gaps that require human author intervention, based on the <code>@@@[PLEASE_COMPLETE_MISSING_INFO]</code> tags you inserted.</li> <li>Content Improvement Summary: Summarize how this documentation is now more clear, concise, and practical for the end-user, referencing the content changes made.</li> <li>Further Suggestions: List any additional suggestions for future enhancement, tooling improvements, or expansion of the documentation effort.</li> </ol>"},{"location":"LICENCE.html","title":"LICENCE","text":"<p>MIT License</p> <p>Copyright (c) Medial EarlySign</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"Archive/AlgoMarker%20Testing.html","title":"AlgoMarker Testing","text":""},{"location":"Archive/AlgoMarker%20Testing.html#the-new-way-to-generate-json","title":"The new way to generate JSON:","text":"<p>The tool AMApiTester can now generate request/response JSON files which can be used by the AlgoAnalyzer. It is done by providing --json-reqfile and --json-resfile in the commandline. These options currently work only with the --single testing mode.  To use it you should execute the command normally and provide req/req file names to output the JSON data to, for example:</p> <p><pre><code>./AMApiTester \\\n  --amfile /nas1/Products/LGI-ColonFlag-3.0/QA_Versions/LGI_3.1.0.0/libdyn_AlgoMarker.25102018_1.so \\\n  --single --print_msgs \\\n  --rep $REP \\\n  --samples ./samples/LastHg.samples \\\n  --model /nas1/Products/LGI-ColonFlag-3.0/QA_Versions/LGI_3.1.0.0/LGI-Flag-3.1.model \\\n  --amconfig /nas1/Products/LGI-ColonFlag-3.0/QA_Versions/LGI_3.1.0.0/LGI-Flag-3.1.amconfig \\\n  --am_res_file ./res/LGI_test_am.preds \\\n  --med_res_file ./res/LGI_test_med.preds \\\n  --json-reqfile ./res/LGI_test_req.json \\\n  --json-resfile ./res/LGI_test_res.json\n</code></pre> The JSON generating is done in a library project called AlgoMarker/CommonTestingTools.</p>"},{"location":"Archive/AlgoMarker%20Testing.html#applytool","title":"ApplyTool","text":"<p>ApplyTool is another tool with JSON capabilities. This toold is the result from the work with ShareCare. it can:</p> <p><pre><code># 1. Convert JSON reqfile to a tab-separated repository data file:\n/nas1/UsersData/shlomi/MR/Libs/Internal/AlgoMarker/Linux/Release/ApplyTool --convert_reqfile_to_data --convert_reqfile_to_data_infile ./long_req.json --convert_reqfile_to_data_outfile ./long_req.data\n\u00a0\n# 2. Drectly feed JSON request data directly into a given AlgoMarker and get the results (.preds)\n/nas1/UsersData/shlomi/MR/Libs/Internal/AlgoMarker/Linux/Release/ApplyTool \\\n--rep /nas1/Work/CancerData/Repositories/KP/kp.repository \\\n--model /nas1/Products/LungCancer/QA_Versions/lc_2_300719/lungcancer.model \\\n--apply \\\n--apply_repdata_jsonreq ./long_req.json \\\n--apply_dates_to_score ./long_req.samples.tsv \\\n--apply_outfile ./long_req_from_json.pred\n</code></pre> </p>"},{"location":"Archive/AlgoMarker%20Testing.html#the-old-way-msgs_file-python-scripts","title":"The old way (\u2013msgs_file + python scripts)","text":"<p>The old way to create and test JSON req files involved using AMApiTester with additional commands --print_msgs --msgs_file TSV_Codes_file.The AMApiTester is being ran as usual but the error codes that the Algomarker emits will be saved into the file specified by --msgs_file . for example:</p> <pre><code>./Linux/Release/AMAPITester \n  --rep /home/Repositories/THIN/thin_jun2017/thin.repository \n  --samples /nas1/Work/Shared/notebooks/shlomi-internal/AATester/pre2d_validate_OnTest_2-10k.samples \n  --model /server/Products/Pre2D/QA_Versions/1.0.0.10/pre2d.model \n  --amconfig /server/Products/Pre2D/QA_Versions/1.0.0.10/pre2d.amconfig \n  --print_msgs \n  --msgs_file /nas1/Work/Shared/notebooks/shlomi-internal/AATester/pre2d_validate_OnTest_2-10k.codes.tsv \n  --ignore_sig Drug \n  --single \n  --am_res_file /nas1/Work/Shared/notebooks/shlomi-internal/AATester/pre2d_validate_OnTest_2-10k.preds.tsv\n</code></pre> <p>Then we will use a python script as follows to convert the codes+preds files into json: (also available as notebook in here: http://node-04:9000/user/shlomi-internal/notebooks/shlomi-internal/AATester/Phase1.ipynb) Note: the conf object contains script configuration you may change for your specific settings.</p> <p><pre><code>import json\nimport med\nimport pandas as pd\nimport numpy as np\ntest_tag = '800k'\nconf= {\n    'x_api_key': 'NzlldZ#QwZGVmNTY4ZmUwZjczZT1MTl2',\n    'customer_id':'Earlysign',\n    'aa_version': '1.1.2.2',\n    'calculator_name': 'Pre2D',\n    'calculator_result_unit': None,\n    'repository' : '/home/Repositories/THIN/thin_jun2017/thin.repository',\n#    'preds_infile' : 'pre2d-oldmodel-{}.preds.tsv'.format(test_tag),\n#    'codes_infile' : 'pre2d-oldmodel-{}.codes.tsv'.format(test_tag),\n#    'preds_infile' : 'pre2d_validate_OnTest_2-{}.preds.tsv'.format(test_tag),\n#    'codes_infile' : 'pre2d_validate_OnTest_2-{}.codes.tsv'.format(test_tag),\n    'preds_infile' : './data_files2/pre2d_validate-{}.preds.tsv'.format(test_tag),\n    'codes_infile' : './data_files2/pre2d_validate-{}.codes.tsv'.format(test_tag),\n    'skipped_outfile' : './data_files2/pre2d_validate-{}.skipped.tsv'.format(test_tag),\n#    'requests_outfile' : 'pre2d_validate_OnTest_2-requests-{}.json'.format(test_tag),\n#    'responses_outfile' : 'pre2d_validate_OnTest_2-responses-{}.json'.format(test_tag),\n    'requests_outfile' : './data_files2/pre2d_validate-requests-{}.json'.format(test_tag),\n    'responses_outfile' : './data_files2/pre2d_validate-responses-{}.json'.format(test_tag),\n    'signals' : 'GENDER HDL BYEAR ALT Triglycerides WBC HbA1C BMI Glucose'.split(' ')  # no Drug sig for now\n    # signals for lgi : 'Monocytes# Basophils% Eosinophils% RDW Platelets Eosinophils# MCHC-M MCH MCV MPV RBC Hematocrit Basophils# GENDER Neutrophils# Hemoglobin Neutrophils% BYEAR Lymphocytes% Lymphocytes# WBC Monocytes%'.split(' ')\n}\ncode_to_status_tbl = {\n    300:2,\n    301:2,\n    310:2,\n    311:2,\n    320:1,\n    321:2,\n    390:0,\n    391:1,\n    392:2\n}\nunits_tbl = {\n    'BMI' : 'kg/m^2',\n    'Glucose' : 'mg/dL',\n    'HbA1C' : '%',\n    'HDL' : 'mg/dL',\n    'Triglycerides' : 'mg/dL',\n    'ALT' : 'U/L',\n    'RBC' : '10^6/uL',\n    'Na' : 'mmol/L',\n    'Weight' : 'Kg',\n    'WBC' : '10^3/uL',\n    'Basophils#' : '#',\n    'Basophils%' : '%',\n    'Eosinophils#' : '#',\n    'Eosinophils%' : '%',\n    'Hematocrit' : '%',\n    'Hemoglobin' : 'g/dL',\n    'Lymphocytes#' : '#',\n    'Lymphocytes%' : '%',\n    'MCH' : 'pg/cell',\n    'MCHC-M' : 'g/dL',\n    'MCV' : 'fL',\n    'Monocytes#' : '#',\n    'Monocytes%' : '%',\n    'MPV' : 'mic*3',\n    'Neutrophils#' : '#',\n    'Neutrophils%' : '%',\n    'Platelets' : '10^3/uL',\n    'RDW' : '%',\n    'MSG' : '#'\n}\ndef encode_pid_signal_data(signame, unit='', datapoints=[]):\n    data = {}\n    data['code'] = signame\n    if unit=='' and signame in units_tbl:\n        unit = units_tbl[signame]\n    data['unit'] = unit\n    data['data'] = datapoints;\n    return data\ndef encode_signal_datapoint(timestamps, values):\n    return {'timestamp' : timestamps, 'value' : values}\ndef req_excapsulate(orig_req, x_api_key):\n    return {\n        'body': orig_req,\n        'header': {\n            \"Accept\": \"application/json\",\n#            \"x-api-key\": x_api_key,\n            \"Content-Type\": \"application/json\"\n        }\n    }\ndef encode_request(request_id='',customer_id='Earlysign',pid=None,calculator='',signals=[], x_api_key=''):\n    req = {}\n    req['requestId'] = request_id\n    req['customerId'] = customer_id\n    if pid!=None: req['patientId'] = pid\n    req['calculator'] = calculator\n    req['signals'] = signals\n    return req_excapsulate(req, x_api_key)\ndef now_timestamp():\n    from datetime import datetime\n    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n#generate results\ndef encode_response(request_id='', version='1.1.0', calculator='', customer_id='Earlysign',\n                    status=0, result=None, calculation_time_stamp=\"\", messages=[], accountId=\"TestAccount\"):\n    return {\n        \"requestId\": request_id,\n        \"version\": version,\n        \"calculator\": calculator,\n        \"customerId\": customer_id,\n        \"accountId\": accountId,\n        \"status\": status,\n        \"result\": result,\n        \"calculationTimeStamp\": str(calculation_time_stamp),\n        \"messages\": messages\n    }\ndef result_value_format(fval):\n    if '.' in str(fval) and str(fval).replace('.','',1).isdigit(): \n        return (\"%.4f\" % float(fval)).rstrip('0').rstrip('.')\n    else: return fval\ndef encode_result(date, value, result_type='Numeric', unit=None, status=0):\n    if status == 2: return None\n    return { \n        \"resultType\": result_type,\n        \"unit\": unit,\n        \"value\": str(result_value_format(value)),\n        \"validTime\": date*1000000\n    }\ndef encode_messages(code, text, status):\n    return {\n        \"code\" : code,\n        \"text\": text,\n        \"status\" : status\n    }\ndef value_format(fval):\n    if '.' in str(fval) and str(fval).replace('.','',1).isdigit(): \n        return (\"%.3f\" % float(fval)).rstrip('0').rstrip('.')\n    else: return fval\n\u00a0\n# Load Repository\nrep = med.PidRepository()\nrep.read_all(conf['repository'],[],['GENDER'])\nprint(med.cerr())\n\u00a0\n#load preds\npreds_df = pd.read_table(conf['preds_infile'])\npreds_df = preds_df.drop(['EVENT_FIELDS','outcome','outcomeTime','split'],axis=1) #.set_index(['id'])\npreds_df = preds_df.rename(columns={'pred_0':'result'})  \n#load codes\ncodes_df = pd.read_table(conf['codes_infile'])\ncodes_df.columns = ['msg_type','pid','date','i','j','k','code','msg_text']\ncodes_df = codes_df[codes_df['msg_type'] != 'SharedMessages']\ncodes_df = codes_df.drop(['msg_type','i','j','k'], axis=1)\n\u00a0\npids = preds_df['id'].unique().astype(np.int32)\n\u00a0\n#load signals\nsignals={}\nfor signal_name in conf['signals']:\n    signals[signal_name] = rep.get_sig(signal_name, pids=pids).set_index('pid')\n\u00a0\nreq_data = []\nresp_data = []\nreq_id_count=0\nnow_ts = now_timestamp()\nresp_file = open(conf['responses_outfile'], 'w')\nreq_file = open(conf['requests_outfile'], 'w')\nresp_file.write('[\\n')\nreq_file.write('[\\n')\nskipped_samples=[]\nfor pred_index,pred_row in preds_df.iterrows():\n    pid, pred_time, pred_result = int(pred_row['id']), int(pred_row['time']), pred_row['result']\n    if pred_time % 100 == 0: \n        skipped_samples.append([pid,pred_time])\n        continue;\n    skip_cur_pid = False\n    pid_signals_data = []\n    #pid_data = []\n    request_id='req_{}_{}_{}'.format(req_id_count, pid, pred_time)\n    #generate request\n    for sig_name,sig_df in signals.items():\n        if pid in sig_df.index:\n            datapoints = []\n            for index, row in sig_df.loc[pid:pid].iterrows():\n                values = []\n                timestamps = []\n                row_is_future = False\n                for col_name,col_val in  row.iteritems():\n                    if 'val' in col_name: values.append(value_format(col_val))\n                    elif 'date' in col_name or 'time' in col_name:\n                        if int(col_val) &gt; pred_time: \n                            row_is_future = True\n                            break\n                        if int(col_val) % 100 == 0:\n                            skip_cur_pid = True\n                            skipped_samples.append([pid,pred_time])\n                            break\n                        timestamps.append(int(col_val))\n                    else: raise Exception(\"unknown column name '{}'\".format(col_name))\n                if not row_is_future: datapoints.append(encode_signal_datapoint(timestamps=timestamps, values=values))\n                if skip_cur_pid: break\n            pid_signals_data.append(encode_pid_signal_data(signame=sig_name, datapoints=datapoints))\n        if skip_cur_pid: break\n    if skip_cur_pid: continue;\n    #pid_signals_data.append(pid_data)\n    pid_req = encode_request(request_id=request_id,\n                             customer_id=conf['customer_id'],calculator=conf['calculator_name'],\n                             signals=pid_signals_data,x_api_key=conf['x_api_key'])\n    #generate resoponse\n    messages = []\n    status = 0\n    for row in codes_df[(codes_df['pid']==pid) &amp; (codes_df['date']==pred_time)].iterrows():\n        row = dict(row[1])  \n        msg_status = code_to_status_tbl[row['code']]\n        status = max(status, msg_status)\n        messages.append(encode_messages(code=row['code'], text=row['msg_text'], status=msg_status))\n    resp_data = encode_response(request_id=request_id, calculator=conf['calculator_name'],\n                    result=encode_result(date=pred_time, value=pred_result, result_type='Numeric', unit=conf['calculator_result_unit'], status=status),\n                    messages=messages, status=status, calculation_time_stamp=now_ts,version=conf['aa_version']\n                   )\n    if req_id_count != 0:\n        resp_file.write('\\n,\\n')\n        req_file.write('\\n,\\n')\n    #req_data.append(pid_req)\n    json.dump(pid_req, req_file, sort_keys=False, indent=1)\n    json.dump(resp_data, resp_file, sort_keys=False, indent=1)\n    req_id_count += 1\n    if req_id_count % 1000 == 0:\n        print(\"request # {}\".format(req_id_count))\nresp_file.write('\\n]')\nreq_file.write('\\n]')\nresp_file.close()\nreq_file.close()\nprint(\"skipped {} samples\".format(len(skipped_samples)))\n\u00a0\n</code></pre> </p>"},{"location":"Archive/AlgoMarker%20Testing.html#insert-json-req-files-into-the-db-to-be-used-by-aa","title":"Insert JSON req files into the DB to be used by AA","text":"<p>Once you have JSON req/res files you may want to insert them into a SQL DB so it can be used by AA. The following python script takes the JSON files and stores then in the DB: (also available here:\u00a0http://node-04:9000/user/shlomi-internal/notebooks/shlomi-internal/AATester/Phase1-DB-insert.ipynb) Note: the conf object contains script configuration you may change for your specific settings.</p> <p><pre><code>import json\nimport med\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy as sa\ntest_tag = '800k'\nconf= {\n#    'requests_infile' : 'pre2d_validate_OnTest_2-requests-{}.json'.format(test_tag),\n#    'responses_infile' : 'pre2d_validate_OnTest_2-responses-{}.json'.format(test_tag),\n    'requests_infile' : './data_files2/pre2d_validate-requests-{}.json'.format(test_tag),\n    'responses_infile' : './data_files2/pre2d_validate-responses-{}.json'.format(test_tag),\n    'db_name':'ObjectStore',\n    'db_user':'postgres',\n    'db_table':'Test',\n    'epic_flag': 'avi800k'\n}\nresp_file = open(conf['responses_infile'], 'r')\nreq_file = open(conf['requests_infile'], 'r')\ndummy = req_file.readline()\ndummy = resp_file.readline()\n\u00a0\ndef get_sql_engine(SQL_SERVER, DBNAME,  username='', password=''):\n    if SQL_SERVER == 'MSSQL':\n        return create_mssql_engine(DBNAME, username, password)\n    elif SQL_SERVER == 'POSTGRESSQL':\n        return create_postgres_engine(DBNAME, username, password)\n    elif SQL_SERVER == 'D6_POSTGRESSQL':\n        return create_postgres_url(DBNAME, username, password)\n    print(SQL_SERVER + ' Unkowen source')\n\ndef load_json_obj(jsonfile):\n    jstr=''\n    line=jsonfile.readline().rstrip()\n    if not line or line == ']': return None\n    while line and line != ',' and line != ']':\n        jstr += line + '\\n'\n        line=jsonfile.readline().rstrip()\n    try:\n        return json.loads(jstr)\n    except:\n        print(jstr)\n        raise\ndef create_postgres_engine(dbname, username, password):\n    engine = sa.create_engine('postgresql://'+username+':'+password+'@node-03:5432/'+dbname)\n    return engine\n\u00a0\nengine = get_sql_engine('POSTGRESSQL',DBNAME=conf['db_name'],username=conf['db_user'])\n# create table\nmeta = sa.MetaData(engine)\ntable = sa.Table(conf['db_table'], meta,\n    sa.Column('Id', sa.String),\n    sa.Column('TestId', sa.String),\n    sa.Column('ExpectedHTTPStatus', sa.Integer),\n    sa.Column('Request', sa.String),   #sa.dialects.postgresql.JSON),\n    sa.Column('ExpectedResponseBody', sa.String)) #, sa.dialects.postgresql.JSON))\nconn = engine.connect()\n\u00a0\ndef req_excapsulate(orig_req, x_api_key='NzlldZ#QwZGVmNTY4ZmUwZjczZT1MTl2'):\n    return orig_req\n    return {\n        'body': orig_req,\n        'header': {\n            \"Accept\": \"application/json\",\n#            \"x-api-key\": x_api_key,\n            \"Content-Type\": \"application/json\"\n        }\n    }\nreq, resp = req_excapsulate(load_json_obj(req_file)) , load_json_obj(resp_file)\ni=0\ndata=[]\nwhile req!=None and resp != None:\n    #print(i,req['requestId'])\n    i += 1\n    if req['body']['requestId'] != resp['requestId']:\n        raise Exception('Error - NON MATCHING REQ IDs {} != {}'.format(req['body']['requestId'],resp['requestId']))\n    requestId = req['body']['requestId']\n\n    data.append({\n        'Id':requestId,\n        'TestId':requestId,\n        'ExpectedHTTPStatus':200,\n        \"Request\" : json.dumps(req, sort_keys=False, indent=0).replace('\\n',''),\n        \"ExpectedResponseBody\" : json.dumps(resp, sort_keys=False, indent=0).replace('\\n',''),\n        \"Epic\" : conf['epic_flag']\n    })\n\n    if len(data)&gt;=1000:\n        res = conn.execute(table.insert(),data)\n        print('{} rows inserted (+{})'.format(i, res.rowcount))\n        data=[]\n\n    req, resp = req_excapsulate(load_json_obj(req_file)) , load_json_obj(resp_file)\nif len(data)&gt;0:\n    res = conn.execute(table.insert(),data)\n    print('{} rows inserted (+{})'.format(i, res.rowcount))\n    data=[]\n</code></pre> </p>"},{"location":"Archive/Data%20json%20file%20format.html","title":"Data json file format","text":"<p><pre><code>{\n    \"patient_id\": 1,\n    \"signals\": [\n        { SIGNAL_CODE_BLOCK },\n        { SIGNAL_CODE_BLOCK },\n        ...\n    ]\n}\n</code></pre> \u00a0 SIGNAL_CODE_BLOCK structure:</p> <p><pre><code>{\n    \"code\": \"SINGAL_NAME\",\n    \"data\": [\n     {\n      \"timestamp\": [ 20120902, ADDITIONAL_TIME_CHANNELES if has ],\n      \"value\": [ \"51\", ADDITIONAL_VALUE_CHANNELES if has ]\n     },\n    ... ADDIOTNAL data points if has\n    ]\n}\n</code></pre> \u00a0 Can also be encapsulated my \"multiple\" attribute for array of patients like <pre><code>{ \n\"multiple\": [\n    {PATIENT BLOCK}\n]\n}\n</code></pre></p>"},{"location":"Archive/Feature%20Importance%20by%20Matching.html","title":"Feature Importance by Matching","text":"<p>The purpose of the tool is to provide a way to rank the features of a model by their importance. The tool can be found in\u00a0$MR_ROOT/Tools/feature_importance_by_matching (The script is written in python 3, so please run source /opt/medial/python36/enable before) \u00a0 Explanation of the algorithm: For a given model, and a validation set, we go over all the features, and perform matching per feature. Then we measure out performance on the resulting validation set (after the matching). The lower the AUC after the matching, the more important the feature is. There are several approaches for handling missing data (per feature):</p> <ol> <li>ignore - match only on non-missing data, and apply the model on non-missing data.</li> <li>keep - match only on non-missing data, and apply on all data (including missing data).</li> <li>match - match on data including the \"missing value\". (This will measure whether existence influences performance)</li> </ol> <p>Main Advantages:</p> <ol> <li>The method works for all predictor types.</li> <li>Very correlated features, will receive similar score (when using the built-in feature importance of boosted trees it is not guaranteed)</li> <li>It can be measure on different subgroups.\u00a0 \u00a0 App help: help <pre><code>python FeatureImportanceByMatching.py -h\nusage: FeatureImportanceByMatching.py [-h] [--f_model] [--f_samples]\n                                      [--f_split] [--f_out] [--handle_missing]\n                                      [--split] [--rep_dir] [--missing_value]\n                                      [--temp_ext] [--train_values]\noptional arguments:\n  -h, --help         show this help message and exit\n  --f_model          a list of .model file(s) seperated by commas\n  --f_samples        Med Samples file\n  --f_split          Splits file\n  --f_out            results file\n  --handle_missing   ignore/keep/match. Ignore - remove all missing data per\n                     feature. Keep - matches on non-missing data, but uses\n                     them in the evaluation (with or without imputing). Match\n                     - matches also on missing-values\n  --split            Chooses splits to run over. Can be -1, for all splits. if\n                     more than one model is received, the splits are deduced\n                     from the list. The first model runs on split 0, the\n                     second on split 1, etc. if only one model is received, a\n                     specific split can be chosen\n  --rep_dir          The repository location\n  --missing_value\n  --temp_ext\n  --train_values     A list of Train values to run on. Seperated by commas\n</code></pre> \u00a0 example run (single model, checked on TRAIN = 2, on all splits): Single model <pre><code>python /nas1/UsersData/ron/MR/Tools/feature_importance_by_matching/feature_importance_by_matching.py --f_model model_14_matched_new_serialization_S4.model --train_values 2 --f_split /server/Work/Users/Ron/Projects/LungCancer/results/kp4.split --f_samples /nas1/Work/Users/Ron/Projects/LungCancer/results/data_NSCLC_AllStages_rand_controls_test.samples --rep_dir /home/Repositories/KP/kp.repository --f_out /server/Work/Users/Ron/tmp/final_keep_validation.csv --temp_ext 2 --handle_missing keep --split -1\n</code></pre></li> </ol> <p>Output:</p> AUC_-1 num_cases_-1 num_controls_-1 FTR_000016.Smoke_Pack_Years_Last 0.727166 10130 51679 FTR_000016.Smoke_Pack_Years_Max 0.727209 10262 51068 Age 0.770081 7384 64064 FTR_000016.Smoke_Days_Since_Quitting 0.784554 10316 80226 FTR_000016.Never_Smoker 0.785706 6698 75159 FTR_000018.ICD9_Diagnosis.category_set_CHRONIC_OBSTRUCTIVE_PULMONARY_DISEASE_AND_ALLIED_CONDITIONS.win_0_1825 0.807776 6698 48157 FTR_000016.Ex_Smoker 0.815043 6698 72780 FTR_000016.Current_Smoker 0.821688 6698 38247 FTR_000196.WBC.max.win_0_1000 0.827246 7520 65185 FTR_000206.WBC.max.win_0_180 0.82975 6860 55505 FTR_000166.WBC.avg.win_0_180 0.829889 6902 47572 FTR_000076.WBC.last.win_0_1000 0.830072 6956 51712 FTR_000086.WBC.last.win_0_180 0.830709 6956 51720 FTR_000156.WBC.avg.win_0_1000 0.832037 6263 85626 FTR_000023.ICD9_Diagnosis.category_set_Atherosclerosis_of_aorta.win_0_1825 0.832246 6698 41601 FTR_000375.MPV.slope.win_0_180 0.833498 349088 6401233 FTR_000003.BMI.last.win_0_1000 0.834903 6445 87130 FTR_000246.WBC.min.win_0_180 0.835032 7922 55937 FTR_000237.BMI.min.win_0_1000 0.835203 6496 85827 FTR_000077.BMI.last.win_0_1000 0.835241 6445 87130 FTR_000024.ICD9_Diagnosis.category_set_Pneumonia_organism_unspecified.win_0_1825 0.835519 6698 54443 FTR_000247.BMI.min.win_0_180 0.836186 6372 99915 FTR_000020.ICD9_Diagnosis.category_set_Unspecified_cataract.win_0_1825 0.836318 6698 73134 FTR_000385.MPV.slope.win_180_365 0.836843 191315 3575267 FTR_000087.BMI.last.win_0_180 0.837052 6490 88311 FTR_000006.Race.category_set_Hispanic 0.837911 6698 111430 FTR_000157.BMI.avg.win_0_1000 0.837999 6502 90668 FTR_000167.BMI.avg.win_0_180 0.838049 6473 91703 FTR_000017.ICD9_Diagnosis.category_set_NEOPLASMS.win_0_1825 0.838428 6698 92619 FTR_000241.ALT.min.win_0_1000 0.838959 8876 108161 FTR_000249.RDW.min.win_0_180 0.839067 7318 109643 <p>Cross validation models <pre><code>python /nas1/UsersData/ron/MR/Tools/feature_importance_by_matching/feature_importance_by_matching.py --f_model /server/Work/Users/Ron/Projects/LungCancer/results/model_14_matched_new_serialization/model_14_matched_new_serialization_S0.model,/server/Work/Users/Ron/Projects/LungCancer/results/model_14_matched_new_serialization/model_14_matched_new_serialization_S1.model,/server/Work/Users/Ron/Projects/LungCancer/results/model_14_matched_new_serialization/model_14_matched_new_serialization_S2.model,/server/Work/Users/Ron/Projects/LungCancer/results/model_14_matched_new_serialization/model_14_matched_new_serialization_S3.model --f_split /server/Work/Users/Ron/Projects/LungCancer/results/kp4.split --f_samples /nas1/Work/Users/Ron/Projects/LungCancer/results/data_NSCLC_AllStages_rand_controls_test.samples --rep_dir /home/Repositories/KP/kp.repository --f_out /server/Work/Users/Ron/tmp/final_ignore.csv --handle_missing ignore\n</code></pre></p> <p>Output:</p> AUC_0 num_cases_0 num_controls_0 AUC_1 num_cases_1 num_controls_1 AUC_2 num_cases_2 num_controls_2 AUC_3 num_cases_3 num_controls_3 FTR_000016.Smoke_Pack_Years_Max 0.733953628 6314 9833 0.717666446 6344 20844 0.724255891 6089 9696 0.720991539 6220 15948 FTR_000016.Smoke_Pack_Years_Last 0.744411799 6077 8444 0.720141033 6325 21646 0.721083434 6385 10206 0.728145602 5629 14545 Age 0.783317771 5295 39359 0.754944255 5448 36092 0.766191168 5555 40680 0.771505086 5298 39905 FTR_000016.Smoke_Days_Since_Quitting 0.785923485 6216 35062 0.766391787 5961 40651 0.781382274 6251 40937 0.771579571 5833 38015 FTR_000016.Never_Smoker 0.791571303 5082 55046 0.77305685 5071 57675 0.779874573 5151 57199 0.774991289 4858 55102 FTR_000018.ICD9_Diagnosis.category_set_CHRONIC_OBSTRUCTIVE_PULMONARY_DISEASE_AND_ALLIED_CONDITIONS.win_0_1825 0.815796736 5082 35751 0.796582793 5071 38441 0.804225316 5151 38264 0.807473626 4858 38091 FTR_000016.Ex_Smoker 0.824501508 5082 54478 0.800734288 5071 53847 0.814249134 5151 57586 0.809579254 4858 56054 FTR_000016.Current_Smoker 0.825532654 5082 26218 0.826870739 4460 99620 0.808831065 5151 26827 0.812956649 4858 28142 FTR_000072.Fev1/Fvc.last.win_0_1000 0.83209193 5096 4479 0.818574876 5083 7697 0.822039338 5166 6320 0.822220991 4877 2586 FTR_000166.WBC.avg.win_0_180 0.833838416 5109 36562 0.815876931 5147 44685 0.826947734 5235 44602 0.823005009 4876 37969 FTR_000076.WBC.last.win_0_1000 0.834624262 5224 37443 0.817439667 5202 43072 0.82916079 5204 46254 0.823459916 4922 39064 FTR_000023.ICD9_Diagnosis.category_set_Atherosclerosis_of_aorta.win_0_1825 0.836941879 5082 34024 0.830272386 4442 99620 0.826985956 5151 34015 0.835493215 4336 99541 FTR_000086.WBC.last.win_0_180 0.837322007 5224 37443 0.818235671 5202 43072 0.829018689 5204 46254 0.823977992 4922 39064 FTR_000156.WBC.avg.win_0_1000 0.837389164 5082 39501 0.819275137 5062 49334 0.829247078 5161 47586 0.825347893 4862 37463 FTR_000206.WBC.max.win_0_180 0.837659938 5621 46116 0.816170507 5152 46064 0.827233071 5216 52009 0.823816339 5103 48289 FTR_000196.WBC.max.win_0_1000 0.838796405 5129 47076 0.818557109 5160 44960 0.829243251 5181 56152 0.822024224 5119 47294 FTR_000073.Fev1.last.win_0_1000 0.840316741 5085 9408 0.823280688 5086 11385 0.831241031 5170 10011 0.835535451 4874 3737 FTR_000024.ICD9_Diagnosis.category_set_Pneumonia_organism_unspecified.win_0_1825 0.841383332 5082 38869 0.820221877 5071 37927 0.830885134 5151 41396 0.829260049 4858 38792 FTR_000034.Neutrophils#.last.win_0_1000 0.8414434 5084 43231 0.822897521 5062 44875 0.831818755 5151 45538 0.82980608 4867 46110 FTR_000246.WBC.min.win_0_180 0.842240851 5249 37396 0.819987728 5118 49069 0.832915091 5247 47823 0.83007257 4986 36409 FTR_000031.Monocytes#.last.win_0_1000 0.843646388 5407 43466 0.821802796 5502 51362 0.831679933 5561 57089 0.829247336 5920 49320 FTR_000020.ICD9_Diagnosis.category_set_Unspecified_cataract.win_0_1825 0.843975091 5082 54524 0.824506856 5071 56527 0.83141977 5151 58683 0.833809387 4858 59486 FTR_000371.ALT.slope.win_0_180 0.844015073 19668 372739 0.827174705 19811 372874 0.832105611 20595 373320 0.839818564 17004 378299 FTR_000241.ALT.min.win_0_1000 0.844766747 5560 70206 0.827439257 5537 66793 0.836373541 5476 50788 0.839883852 5353 64840 FTR_000237.BMI.min.win_0_1000 0.845845202 5092 33838 0.824890975 5102 37080 0.833291796 5172 60717 0.833668567 4918 49283 FTR_000006.Race.category_set_Hispanic 0.84598234 5082 83462 0.823307454 5071 80921 0.832274366 5151 83665 0.832633109 4858 83278 FTR_000007.Race.category_set_White 0.846040192 5082 72290 0.826862386 5071 75924 0.833886423 5151 75482 0.836231476 4858 79942 FTR_000438.Platelets.max_diff.win_0_1000 0.846050492 5116 48635 0.827134105 5032 56610 0.836288611 5164 60156 0.834964983 4821 59436 FTR_000381.ALT.slope.win_180_365 0.846331196 15521 289138 0.826137448 16115 278830 0.837091626 16074 284632 0.837829487 13404 289433 FTR_000398.Platelets.win_delta.win_0_1000_360_360000 0.846663968 5094 42315 0.825911643 5318 35494 0.833804808 5209 37532 0.832352591 5083 35865 FTR_000077.BMI.last.win_0_1000 0.84713334 5092 33914 0.826946916 5080 58664 0.835387062 5154 69546 0.834389195 4862 58298 FTR_000408.Platelets.win_delta.win_0_180_360_360000 0.847178304 5094 42316 0.824804654 5318 35494 0.834945526 5164 37049 0.833575495 5083 35865 <p>For reference, attached the results of the XGboost built in\u00a0feature importance: \u00a0</p> feature score_0 score_1 score_2 score_3 FTR_000016.Smoke_Pack_Years_Max 259.702 212.071 183.935 234.62 FTR_000018.ICD9_Diagnosis.category_set_CHRONIC_OBSTRUCTIVE_PULMONARY_DISEASE_AND_ALLIED_CONDITIONS.win_0_1825 71.57 80.449 82.948 76.758 FTR_000016.Smoke_Days_Since_Quitting 71.186 81.102 84.451 72.819 Age 56.403 51.533 53.785 62.204 FTR_000016.Current_Smoker 29.897 34.597 34.698 23.889 FTR_000078.Platelets.last.win_0_1000 24.108 13.13 17.429 15.055 FTR_000087.BMI.last.win_0_180 23.869 34.161 29.702 22.672 FTR_000006.Race.category_set_Hispanic 23.261 21.348 26.93 22.792 FTR_000024.ICD9_Diagnosis.category_set_Pneumonia_organism_unspecified.win_0_1825 21.935 19.672 28.496 18.31 FTR_000410.MCV.win_delta.win_0_180_360_360000 21.348 13.731 12.699 0 FTR_000073.Fev1.last.win_0_1000 20.228 23.157 17.586 20.243 FTR_000072.Fev1/Fvc.last.win_0_1000 18.526 15.036 16.872 14.859 FTR_000003.BMI.last.win_0_1000 18.323 15.689 18.129 18.462 FTR_000051.Protein_Total.last.win_0_1000 18.295 15.438 15.202 22.549 FTR_000140.MCV.last_delta.win_180_365 18.243 13.012 0 13.768 FTR_000456.WBC.max_diff.win_180_365 18.012 14.08 14.206 12.618 FTR_000080.MCV.last.win_0_1000 17.933 7.729 7.705 10.382 FTR_000168.Platelets.avg.win_0_180 17.584 12.695 11.632 10.626 FTR_000135.MPV.last_delta.win_0_180 17.521 13.491 12.411 12.544 FTR_000398.Platelets.win_delta.win_0_1000_360_360000 17.39 20.414 17.091 17.776 FTR_000434.RBC.win_delta.win_365_730_360_360000 17.144 0 0 0 FTR_000167.BMI.avg.win_0_180 16.796 10.941 10.017 14.597 FTR_000118.Platelets.last_delta.win_0_1000 16.78 15.599 15.253 11.584 FTR_000046.GGT.last.win_0_1000 16.471 12.297 15.633 13.474 FTR_000166.WBC.avg.win_0_180 16.422 14.816 11.957 15.878 FTR_000237.BMI.min.win_0_1000 16.237 13.591 18.65 17.107 FTR_000157.BMI.avg.win_0_1000 16.101 14.105 14.636 16.19 FTR_000298.Platelets.std.win_180_365 15.931 7.649 15.135 0 FTR_000446.WBC.max_diff.win_0_180 15.929 7.143 11.97 10.551 FTR_000460.MCV.max_diff.win_180_365 15.911 0 12.824 17.53 FTR_000088.Platelets.last.win_0_180 15.774 15.49 11.129 14.004 FTR_000133.Triglycerides.last_delta.win_0_180 15.768 9.572 0 11.45"},{"location":"Archive/How%20to%20upload%20new%20pypi%20packages.html","title":"How to upload new pypi packages","text":"<ol> <li>On the external computer, go to:\u00a0P:\\tools\\python_env\u00a0(P: being the \"Public\" drive)</li> <li>For convenience, delete files in \"packages_extra\"\u00a0(the packages from previous runs)</li> <li>Run \"get_package_by_name.sh\" and input names of all packages you wish to download.\u00a0It will automatically download dependencies.</li> <li>Copy contents of \"packages_extra\" to:internal at /nas1/Work/python-packages/3.10.</li> <li>Run pip install normally. \u00a0 \u00a0 \u00a0</li> </ol>"},{"location":"Archive/Libraries%20and%20Tools%20Blog%20Page.html","title":"Libraries and Tools Blog Page","text":"<p>A page to chronically announce information regarding the MR Libs and Tools code repositories. Please include name and date when you add a record and try to stick to the format. \u00a0</p>"},{"location":"Archive/Libraries%20and%20Tools%20Blog%20Page.html#-alon-build-script-to-compile-all-our-apps-with-documentition-of-git-version","title":"- Alon - Build script to compile all our apps with documentition of git version","text":"<p>When including MedUtils we can use medial::get_git_version() which returns a string with git information about Libs and Tools Repositories used to complie the application. Running our apps with --debug will print the app version and --version will just output the application version and exit.</p> <ul> <li>To compile all our tools with version info use build scripts in Scripts Repo: $MR_ROOT/Projects/Scripts/Bash-Scripts/full_build.sh</li> <li>example of version output running: bootstrap --version Version Info: Build on 14-01-2019_17:42:20 =&gt;Libs Git Head: 8fdbf55f32fc550420f300d391c88295bcb3a46a by avi at 2019-01-14 Last Commit Note: pushing the correct val in the diabetes registry =&gt;Tools Git Head: 97e7e06988a606781dda39625507d9702e146ac9 by avi at 2019-01-14 Last Commit Note: a diabetes read code list capabale of working with a registry rep processor</li> </ul>"},{"location":"Archive/Libraries%20and%20Tools%20Blog%20Page.html#06012018-shlomi-minor-changes-in-our-python-environment-and-binding","title":"06/01/2018 - Shlomi -\u00a0Minor changes in our Python Environment and binding","text":"<ul> <li>The old Jupyter on node-02 (based on Anaconda2 ) was shut down. The old notebooks are available at: (node-02) /home/python/Now http://node-02:8888 will direct you to the New Py2/Py3/R enabled Jupyter installation. Keep in mind that it is preferable to use JupytherHub which is available at http://node-0X:9000/ since it is much more suitable for multiple users. The kernel you run is owned by your unix user and it directs you to your own directory at startup. Use port 8888 to share notebooks and present work.Use /server/Work/Shared/notebooks/ to copy and manipulate the actual files.</li> <li>The \"Generic Python Binding\" installation was deleted. From now on, if you want to use it, you will have to compile your own:cd $MR_ROOT/Libs/Internal/ ; git pullcd $MR_ROOT/Libs/Internal/MedPyExport/generate_binding ; ./make-all.shJupyterhub kernels (port 9000) will find your compiled Python Binding automatically, regular Jupyter kernels (port 8888) , OTOH, will require the usual sys.path.insert(\u2026)</li> <li>The nltk sample data was eventually moved from /opt/medial/dist/usr/share/nltk_data on all nodes to /server/Work/Shared/nltk_data to free space on local nodes HDs.</li> <li>The latest commit to the Python Binding will provide a readable online help in the python console using 'help(med)'. Try it. Other documentation will be added to the Wiki.</li> <li>It is now possible to run 'import med' instead of the usual 'import medpython as \u2026'It will provide a \"cleaner\" namespace. \u00a0</li> </ul>"},{"location":"Archive/Libraries%20and%20Tools%20Blog%20Page.html#16122018-avi-medio-and-serializableobject-libraries","title":"16/12/2018 - Avi - MedIO and SerializableObject libraries","text":"<ul> <li>MedIO was separated from MedUtils to be a standalone library</li> <li>SerializableObject was separated from MedProcessTools to be a standalone library</li> <li>Libraries compile, Flow compiles, you may need to do some changes to your projects for them to compile</li> <li>You need to update MR_LIBS_NAME (see the landing page - it was updated)</li> <li>You need to pull the resources scripts for the MR_LIBS_NAME in Linux</li> <li>From now on if you need any of the MedIO routines use:\u00a0 #include  <li>From now on if you need the SerializableObject class use:\u00a0 #include"},{"location":"Archive/Libraries%20and%20Tools%20Blog%20Page.html#18122018-avi-clutter-cleaning-stage-1","title":"18/12/2018 - Avi Clutter cleaning stage 1","text":"<p>Several changes in libraries, in order to get rid of old unused code, and break out messy libraries into meaningful libraries, to increase code quality and decrease dependencies.</p> <ul> <li>MedFeat is no more : you served us well, may you rest in peace. Contained mainly old unused code, needed remains were left in other places.</li> <li>MedSplit : a very small library, left over from MedFeat.</li> <li>Retired : a new library with old unused code (in case we will in some weird scenario need to compile some very old code)</li> <li>global time variables were moved to MedTime , their natural place.</li> <li>rand_1, rand_N were moved to MedGlobalRNG.h in MedUtils , since they are now implemented there, using them requires only the include of this h file and no need for the MedUtils library itself.</li> <li>MedMat : was split into a new library , if you need a MedMat make sure to : #include  <li>med_stoi, med_stof : created annying dependencies and were moved to SerializableObject : a more natural\u00a0position.</li> <li>Again MR_LIBS_NAME was updated, see the landing page for windows, and pull the startup.sh script for linux (don't forget to close/open visual studio and linux terminals after the update)</li>"},{"location":"Archive/Outcomes%20files.html","title":"Outcomes files","text":"<p>Outcome files are meant to be a formal definition for outcome groups of patients used in learning and testing. An outcome has the following types</p> <ul> <li>binary - the outcome is 0 or 1 (we always use 0 for controls and 1 for the outcome itself)</li> <li>categorical - the outcome is in the range 0...(ncateg-1).</li> <li> <p>regression - a continous outcome. \u00a0 Each outcome is a list of rows, in each row we have:</p> </li> <li> <p>patient id</p> </li> <li>date/time: the sampling date for the outcome (typically before the raw date)</li> <li>outcome value</li> <li>length: length of the outcome in time (from the time point and on, or a certain number of days after it, 0 means from this point and on)</li> <li>raw date: the actual date of the outcome \u00a0</li> </ul>"},{"location":"Archive/Outcomes%20files.html#formal-definition-of-an-outcome-file","title":"Formal definition of an outcome file","text":"<p>An outcome file is a tab delimited text file with the following lines: NAME  DESC  TYPE  : binary - is a 0/1 outcome, multi - is a multicategory outcome in the range 0...(ncateg-1) , regression is a continous outcome NCATEG  : needed when TYPE multi is used. Defining the number of categories in the outcome. EVENT   IGNORE  : pids to always ignore when learning and testing this outcome: this is needed more in pre outcome stages in which we still need to filter and select matched controls for our outcome."},{"location":"Archive/Python%20Environments.html","title":"Python Environments","text":""},{"location":"Archive/Python%20Environments.html#global-paths","title":"Global paths","text":"<p>Python environments can be found in here: /nas1/Work/python-env Python packages can be found in here: /nas1/Work/python-packages \u00a0 current python environment is python36. in order to activate: <pre><code>source\u00a0\u00a0/nas1/Work/python-env/python36/bin/activate\n#Or if your scripts git repository is updated:\npython_env python36\n</code></pre></p>"},{"location":"Archive/Python%20Environments.html#create-new-python-environment","title":"Create new python environment:","text":"<p><pre><code>$MR_ROOT/Projects/Resources/python_env/create_py_env.sh\u00a0\n#Can accept optional argument for location to setup the new environment (includes the name) - for example /nas1/Work/python-env/python36\n#second argument is 0/1 flag to override existing if exists\n</code></pre> The setup creates pip default config file in the environment to install new packages from\u00a0/nas1/Work/python-packages. it can be found under /nas1/Work/python-env/$YOUR_ENV_NAME/pip.conf Now you can just pip install when you want to install new package. The only thing you have to update is\u00a0/nas1/Work/python-packages, using the next step.</p>"},{"location":"Archive/Python%20Environments.html#medial-library","title":"Medial library","text":"<p>Add code to activate script \u00a0 <pre><code>PYTHONPATH=$MR_ROOT/Libs/Internal/MedPyExport/generate_binding/Release/medial-python36\nexport\u00a0PYTHONPATH\n</code></pre> Add jupyter varaible: <pre><code>export JUPYTER_RUNTIME_DIR=/var/opt/medial/dist/jupyter_kernels\n</code></pre> </p>"},{"location":"Archive/Python%20Environments.html#gpu-support-update-the-activate-script","title":"GPU support - update the activate script:","text":"<p>LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64\"</p>"},{"location":"Archive/Python%20Environments.html#downloadupdate-packages-to-local-repository-of-packages-nas1workpython-packages","title":"Download/Update packages to local repository of packages /nas1/Work/python-packages:","text":"<p>You need to update the folder\u00a0/nas1/Work/python-packages. There is script in the External Terminal 192.168.200.209 in\u00a0 <pre><code>P:\\tools\\python_env\\get_packages.bat\n###COMMENT### OR RUN THE PYTHON SCRIPT WITH ARGUMENTS FOR SPECIFIC PACKAGE - FOR EXAMPLE \"pandas\", CAN ALSO SPECIFY TEXT FILE WITH LINE FOR NEW PACKAGE WITH --package_list ARGUMENT:\nC:\\PYTHON37\\python-3.7.2.amd64\\python.exe --python_version \"3.6\"  --save_path \"$PATH_TO_STORE_PACKAGES\" --error_log \"errors.log\" --succ_log \"succ.log\" --log_level 2 --package_name \"pandas\"\n</code></pre> The script downloads all packages from P:\\tools\\python_env\\packages.list file and store them at P:\\tools\\python_env\\packages. It creates error.list in the same folder with unfound packages. &lt;=Not anymore, it just adds new packages there (please use get_packages.bat and not update.OLD.bat) After the process completes, copy\u00a0P:\\tools\\python_env\\packages into the Internal environment\u00a0/nas1/Work/python-packages.</p>"},{"location":"Archive/Python%20Environments.html#helpfull-script-to-install-multiple-packages-in-the-python-environment-from-list","title":"Helpfull script to install multiple packages in the python environment from list:","text":"<p>Can be done with pip install -r $LIST_OF_PACKAGES after activating the correct python environment or by script:\u00a0 $MR_ROOT/Projects/Resources/python_env/install_env.sh that installs the packges one by one from pacakges.list file and creates log directory in the virtual environment of failed/success installations. \u00a0</p>"},{"location":"Archive/Python%20Environments.html#remarks","title":"Remarks","text":"<p><pre><code>&gt;&gt;&gt; import tensorflow as tf\n2020-10-29 13:34:21.939428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n&gt;&gt;&gt; tf.config.experimental.list_physical_devices('GPU')\n020-10-29 13:54:01.090284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n2020-10-29 13:54:02.042019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n2020-10-29 13:54:02.042844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:\npciBusID: 0000:81:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n2020-10-29 13:54:02.042894: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n2020-10-29 13:54:02.048298: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n2020-10-29 13:54:02.052290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n2020-10-29 13:54:02.054009: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n2020-10-29 13:54:02.058161: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n2020-10-29 13:54:02.060941: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n2020-10-29 13:54:02.068200: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n2020-10-29 13:54:02.071301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n&gt;&gt;&gt; tf.config.experimental.list_physical_devices('CPU')\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n</code></pre> \u00a0 In other nodes you will see: <pre><code>&gt;&gt;&gt; import tensorflow as tf\n2020-10-29 13:37:11.865139: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /server/Work/Libs/Boost/latest/stage/lib:/usr/local/cuda-10.0/lib64\n2020-10-29 13:37:11.865459: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n&gt;&gt;&gt; tf.config.experimental.list_physical_devices('GPU')\n[]\n&gt;&gt;&gt; tf.config.experimental.list_physical_devices('CPU')\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n</code></pre> \u00a0   - med libary is added to the python path - the deafult search path is \"$MR_ROOT/Libs/Internal/MedPyExport/generate_binding/Release/medial-python36\". if not found, you will need to compile a sutiable version for this python. You can change library build to other path by controling this environment variable - PYTHONPATH - to point the new med library</p>"},{"location":"Archive/Python%20Environments.html#jupyter-notebook","title":"Jupyter Notebook","text":"<p>The jupyter is set as a service in each of the nodes 1-5. The scripts can be found in here: /etc/init.d/jupyter To startthe service: <pre><code>sudo service jupyter start\n</code></pre> You can also stop it or check status by passing \"stop\" or \"status\". logs can be found in here:\u00a0/var/log/jupyter.log,\u00a0/var/log/jupyter.err You can open the notebook using the address -\u00a0http://node-01:9000/\u00a0- and change node number as needed. Make sure to enter personal workspace. \u00a0</p>"},{"location":"Archive/SHAP%20Feature%20Importance.html","title":"SHAP Feature Importance","text":"<p>SHAP Feature Importance tool caluclates the SHAP values for XGboost or LightGBM models, and generate a pdf document with feature importance information. the tool can be found in\u00a0\u00a0$MR_ROOT/Tools/ShapFeatureImportance/. The tool receives a model file and a samples file (an additional model for filtering the samples is optional).\u00a0 Important:</p> <ul> <li>Should run in python 3 (use the command: source /opt/medial/python36/enable)</li> <li>it is not recommended to work on more than 20,000 samples. Usually 10,000 can give a clear enough graphs (use num_samples parameter) \u00a0 The final document consists 4 plots:</li> </ul> <ol> <li>Importance of single features:****</li> <li>Mean of absolute SHAP values for most important ****</li> <li>****</li> <li>Importance of signals - grouped by type*In this figure, signals are aggregated together (SHAP values are summed) with their feature type - for numerical features, feaures are aggregated according to the following feature groups:'value' ('last','avg',',min','max'),\u00a0'trend' ('slope', 'win_delta','last_delta','max_diff'), 'time' : 'last_time', 'std': 'std'} .\u00a0\u00a0* Parameters:**</li> </ol> <ul> <li>samples_file::\u00a0 Input samples file</li> <li>rep_file:: Repository file</li> <li>model_file_json_filter::\u00a0 Model to use for generating filters. (optional)</li> <li>model_file:: Model:: main model</li> <li>output_file:: Ouput file name</li> <li>filter_params:: Parameters for filtering (optional - \"bootstrap fomatting\". Example: \"Time-Window:30,180;Age:65,75;Gender:2,2\")</li> <li>sig_rename_dict:: Renaming dictionary - renaming the signal name for the last graph (optional. Example: \"LDL_over_HDL:LDL over HDL,category_set_ATC_C10A____:Drug-LIPID MODIFYING AGENTS\")</li> <li>num_samples :: default= inf\u00a0 :: Bound the number of samples - for faster running Workflow:</li> </ul> <ol> <li>If\u00a0filter_params exists, a model is applied (if available, according to the \"model_file_json_filter\", otherwise only a model with age and gender is applied).</li> <li>The samples are than filtered according to the\u00a0filter_params</li> <li>After filtering, a unique random sample is chosen per id.</li> <li>The main model (model_file) is applied for features matrix generation</li> <li>SHAP values are calculated</li> <li>features are aggregated to groups.</li> </ol> <p>Example: Running Example<pre><code>python shap_feature_importance_tool.py --num_samples 10000 --sig_rename_dict \"LDL_over_HDL:LDL over HDL,category_set_ATC_C10A____:Drug-LIPIDMODIFYING AGENTS\" --samples_file /server/Work/AlgoMarkers/AAA/aaa_1.0.0.2/RegistryAndSamples/aaa_train_age_matched_matched.samples --rep_file /home/Repositories/THIN/thin_2018/thin.repository --model_file_json_filter /server/UsersData/ron-internal/MR/Projects/Shared/AlgoMarkers/aaa/configs/analysis/ever_smokers_json.json --model_file /server/Work/AlgoMarkers/AAA/aaa_1.0.0.2/Performance/model_6_S4.model --output_file /server/Work/Users/Ron/tmp/shap_5.pdf --filter_params \"Time-Window:30,180;Age:65,75;Gender:2,2;Ex_or_Current_Smoker:0.5,1.5\"\n</code></pre></p>"},{"location":"Archive/text_file_processor.html","title":"text_file_processor","text":"<p>A fast tool to process columnized text file - it's very similar to paste.pl but it's written in C++ (which makes it faster) and has more options. \u00a0 text_file_processor --help <pre><code>##     ## ######## ########  ####    ###    ##\n###   ### ##       ##     ##  ##    ## ##   ##\n#### #### ##       ##     ##  ##   ##   ##  ##\n## ### ## ######   ##     ##  ##  ##     ## ##\n##     ## ##       ##     ##  ##  ######### ##\n##     ## ##       ##     ##  ##  ##     ## ##\n##     ## ######## ########  #### ##     ## ########\nProgram General Options:\n  --help                help &amp; exit\n  --h                   help &amp; exit\n  --base_config arg     config file with all arguments - in CMD we override\n                        those settings\n  --debug               set debuging verbose\nProgram Options:\n  --input arg           the location to read input (\"-\" for stdin)\n  --output arg          the location to write output file  (\"-\" for stdout)\n  --delimeter arg (=    )  the input file delimeter\n  --has_header arg (=1) True if the file has header\n  --config_parse arg    the config command to parse line\n</code></pre> Example run <pre><code>./text_file_processor --input /server/Work/Users/Alon/Models/outputs/debug.txt --output - --config_parse=\"0;3;4;7;1;He;file:/server/Work/Users/Alon/Models/outputs/byears.thin2#0#0#1\" &gt; /tmp/debug\n\u00a0\nRead 10 keys for dictionary /server/Work/Users/Alon/Models/outputs/byears.thin2 in 0 seconds\nWarning: has 4987 not found keys within 17792 rows.\nExample:\"19099934\"\nDone Processing 17794 lines in 0 seconds\n</code></pre> in this example I have used truncated byears file with only 10 patients and thier byear. the program outputs that we have only 10 keys in the byears.thin2 and that for 4987 patients we haven't found a match with example for pid that we couldn't find a match to... If everything is OK we won't see warnings \u00a0 - the input and output may be \"-\" to use stdin/stdout. - you have delimiter option to control input file delimeter (default is TAB) the config_parse is consists of columns creation based on \";\" between each column. you may use one of the tree options to create column: - specify just a number - and it will copy the input column number - specify a constant string - in the example \"He\" that will be filled as a column - specify columns pasted from file. you need to start with \"file:\" specify the path to the file and specidy 3 list of numbers seperated by \"#\" (you may selected multiple columns for example \"1,3\" will use column 1 + 3).\u00a0   - the first list of number is the columns in the specified file to join with.   - the second list is the columns in the input to join with   - the third list is the columns to select from the specified file to paste \u00a0</p>"},{"location":"Archive/Deprecated/Find%20Required%20Signals.html","title":"Find Required Signals","text":"<p>Find Required Signals is a tool for finding minimal requirement signals. The idea is to find combinations of signals that for a given model achieve a certain performance requirement. The tool can be found in\u00a0MR_Tools/MedProcessUtils/findRequiredSignals and compiled as part as AllTools Algorithm Overview:\u00a0 The algorithm runs over combinations of signals of growing sizes, starting from combinations of size one. At each combination size, the model runs over different combination sets. For each combination, rep-processors that delete the signals that are not part of the current combination are added, and performance is evaluated. The number of possible combinations at combination size\u00a0k\u00a0is generally\u00a0number of signals\u00a0over\u00a0k.\u00a0Since this number grows very fast, we limit ourselves with a parameter provided by the user\u00a0max_num_tests_per_iter,\u00a0and we search only over\u00a0n\u00a0signals, where\u00a0n\u00a0is the largest integer so that\u00a0n\u00a0over\u00a0k\u00a0is smaller than\u00a0max_num_tests_per_iter.\u00a0The\u00a0n chosen signals\u00a0are the best\u00a0n\u00a0signals from the previous stage. \u00a0</p>"},{"location":"Archive/Deprecated/Find%20Required%20Signals.html#parameters","title":"Parameters:","text":"<ul> <li>Input parameters<ul> <li>model_file::\u00a0 *.model file.</li> <li>samples_file:: samples file.</li> <li>rep_file::\u00a0repository file.</li> <li>bootstrap_json: bootstrap json file</li> </ul> </li> <li>Output parameters<ul> <li>out_file:: output file</li> <li>evaluations_to_print:0: Number of evaluations to print in each stage (0 - Only combinations that pass the required performance. 1 - The best result, 2 - The two best results. etc. -1 means print all/</li> </ul> </li> <li>Algorithm parameters<ul> <li>req:\"BDATE,BYEAR,GENDER\": comma-separated list of required signals (signals that are always used).</li> <li>max_num_tests_per_iter:: maximal allowed tests to be done in a iteration. Determines how quick the algorithm finishes. Large number of tests - means that more signals will be checked, but running time will grow.\u00a0Recommendation: evaluate it\u00a0 with a short dummy run</li> <li>required_ratio: : required performance compared to the full model (either that or required_abs should be entered).</li> <li>required_abs:: absolute required performance\u00a0(either that or required_ratio should be entered).</li> <li>num_iterations_to_continue:0 :Number of iterations to perform after we get to the required performance.</li> <li>maximal_num_of_signals:666:Maximal number of signals allowed in a required signal set</li> <li>bootstrap_params:sample_per_pid:1: Parameters for bootstrap. e.g. sample_per_id=1 ('/' separated) file\")</li> <li>cohort_params:Age:45,120/Time-Window:0,365: Parameters for defining the bootstrap cohort. e.g.\u00a0Age:50,75/Time-Window:0,365</li> <li>msr_params:AUC:\u00a0Define the performance measurement. e.g. AUC or SENS,FPR,0.2 for Sensitivity at FPR=20%</li> <li>skip_supersets: true : Whether to skip supersets of acceptable combinations (for example: if signals X,Y are good enough, whether to run on X,Y,Z)</li> <li>delete_signals: : Signals which exists, but never checked (meaning that they are forced to be deleted always).</li> </ul> </li> </ul>"},{"location":"Archive/Deprecated/Find%20Required%20Signals.html#example-running-example","title":"Example: Running Example","text":"<p>Running Example<pre><code>findRequiredSignals --model_file example_model.model --samples_file example_samples.samples --rep_file /server/Work/CancerData/Repositories/KP/kp.repository --out_file required_sigs_out.txt --msr_params AUC --max_num_tests_per_iter 1000 --required_ratio 0.95 --evaluations_to_print 0 --num_iterations_to_continue 2 --cohort_params Age:45,80/Time-Window:120,365\n</code></pre> Output example:</p> <p>Output example<pre><code>Required Performance0.817788\nSignals Performance\nSmoking_Status 0.82436\nICD9_Diagnosis,Pack_Years 0.823099\nWBC,Pack_Years 0.818223\nSmoking_Quit_Date,Pack_Years 0.817865\nSmoking_Quit_Date,WBC,ICD9_Diagnosis 0.821482\n</code></pre> </p>"},{"location":"Archive/Deprecated/SignalsDependencies.html","title":"SignalsDependencies","text":"<p>The Code exists in: MR_Tools/SignalsDependencies and is basically using library functions in MedRegistry object - method called \"calc_signal_stats\" after loading the registry. This tool will allow you to discover relevant categorical signals (for example: readcodes or drugs) that has statstical connection to you outcome within a defined time-window. The tool will create MedSamples based on the signal time points and than label those samples based on the registry and \"labeling_params\" parameter which defines the rules for the labeling - either case, control or excluded (if can't determine for example). It will create contingency\u00a0table from samples within time-window for each gender and age group:</p> <ul> <li>Signal value doesn't exists (the patient didn't have certain readcode value in the time window) &amp; the registry outcome is false - will be calcluated based on the incidence rate of the outcome in this age bin</li> <li>Signal value doesn't exists (the patient didn't have certain readcode value in the time window) &amp; the registry outcome is true\u00a0- will be calcluated based on the incidence rate of the outcome in this age bin</li> <li>Signal value exists (the patient certain readcode value in the time window) &amp; the registry outcome is false</li> <li> <p>Signal value exists (the patient certain readcode value in the time window) &amp; the registry outcome is true It will allow you the sort and filter the results using fdr (false detection rate, minimal count for signal existence, minimal coutn for positive registry in siganl existence.. and more) It will also allow you to create and look at specific tables of Male,Female and all age-group for certain readcode value to see the connection between the specific signal and the registry \u00a0 The registry format is tab-delimited: [PID, Start_Date, End_Date, RegistryValue]\u00a0 Start_date - is the outcome registry start time for the outcome to be labeled (in cancer it's the first time the patient got cancer) End_date - is the outcome registry finish time (where after it the outcome value isn't valid anymore) - for example it may be censoring date. for control it's the last time we know it's still control for more details reffer to\u00a0MedRegistry\u00a0 \u00a0 Explain on labeling_params and inc_labeling_params can be given in\u00a0TimeWindowInteraction. Those arguments are LabelParams objet that defines how to label sampels. \u00a0 Important parameters for the tool (that most be supplied, don't use default ones unless you know what you are doing):</p> </li> <li> <p>global_rep - Repository path</p> </li> <li>registry_path - the path to the MedRegistry file</li> <li>labeling_params - the parameters to control how to label the samples created by the signal time points</li> <li>test_from_window, test_to_window - to control the time window</li> <li>test_main_signal - the signal to test If you are using default parameters, you are at high risk of a problem.</li> </ul>"},{"location":"Archive/Deprecated/SignalsDependencies.html#hirarchy-filtering-parameters","title":"Hirarchy Filtering parameters:","text":"<p>The filtering happens in this method: medial::contingency_tables::filterHirarchy The filtering happens in this order:</p> <ol> <li>float filter_child_count_ratio (default value is 0.05)  If child ratio count is similar to the parent, keep only parent code. For example child has 10,000 samples and parent has 10,100 samples. The additional 100 samples out of 10,100 are little ~1% which is less than default value of 5%, so the child is eliminated.</li> <li>Those are used together  float filter_child_pval_diff (default value is 1e-10 )  float filter_child_lift_ratio (default value is 0.05) When both p_value difference between parent and child is below filter_child_pval_diff AND diff in average lift is below filter_child_lift_ratio  , will remove parent. The parent \"behaves\" differently from at least 2 children, so aggregation of those children into the parent category might be unreasonable.  3 .float filter_child_removed_ratio (default value is 1) Only when node has child that pass the above filters and at least 1 child eliminated.  If the aggregated sum of removed samples due to filtered children is high, consider removal of parent code. For example: if parent has 10,000 samples, and removed children with 8,000 codes, than remove the parent, since aggregation of the children below the parent is unreasonable.</li> </ol>"},{"location":"Archive/Deprecated/SignalsDependencies.html#examples","title":"Examples","text":""},{"location":"Archive/Deprecated/SignalsDependencies.html#labeling_params-parameter-examples","title":"labeling_params parameter examples:","text":"<p>since this parameter is tricky, here are some examples:</p> <ul> <li> <p>Outcome which can happen several times for a specific period (For example Flu).\u00a0labeling_params=\"label_interaction_mode=0:after_start|1:before_start,after_start;conflict_method=all\"Explaination - cases has the settings of \"before_start,after_start\",\u00a0 which means the from_time_window from the signal time should happen before registry start time records (tha patient starts as control) AND the to_time_window from the signal time should happen after the start time of the same registry record (the patient turned into case). Controls has the settings of \"before_end,after_start\" - which means you should have some overlap with control period of non pregnancy - start time window of signal is before end of control period and end time window of signal is after the start. conflict_method=all - means if we have sample which is also control and also case be those settings - treat it (for counting prupose) also as control and as case.</p> </li> <li> <p>outcome which occours once (for example cancer)-\u00a0\u00a0labeling_params=\"label_interaction_mode=0:after_start,before_end|1:before_start,after_start;conflict_method=max\"Explanation - controls are the ones who the from_time_window of the sample occours after_start of registry control period AND before_end of the same registry control period - means the whole time window is inside the control period. Cases are those where the from_time_window is before_start of specific case time period and the to_time_window is after_start of that same case period. the end_period for cases in this registry is not use since there is no \"due\" date for cancer.</p> </li> </ul>"},{"location":"Archive/Deprecated/SignalsDependencies.html#run-examples","title":"Run Examples:","text":"<p>Program Help Program Help<pre><code>SignalsDependencies --help\n##     ## ######## ########  ####    ###    ##\n###   ### ##       ##     ##  ##    ## ##   ##\n#### #### ##       ##     ##  ##   ##   ##  ##\n## ### ## ######   ##     ##  ##  ##     ## ##\n##     ## ##       ##     ##  ##  ######### ##\n##     ## ##       ##     ##  ##  ##     ## ##\n##     ## ######## ########  #### ##     ## ########\n\u00a0\nProgram General Options:\n  -h [ --help ]                         help &amp; exit\n  --help_module arg                     help on specific module\n  --base_config arg                     config file with all arguments - in CMD we override those settings\n  --debug                               set debuging verbose\n --version                              prints version information of the program\nGlobal Options:\n  --global_rep arg                      repository path to fetch signal\\registry\n  --global_stats_path arg               location to save or load stats dictionary for fast load in the second time for more filtering\n  --global_override arg (=0)            whather or not override stats file if already exists\n  --global_age_bin arg (=5)             age bin size (default is 5 years)\nRegistry Options:\n  --registry_path arg                   location to load registry txt file\n  --registry_init_cmd arg               An init command for registry creation on the fly\n  --registry_save arg                   location to export registry txt file created by this tool on the fly\n  --registry_filter_train arg (=1)      if True will filter TRAIN==1\n  --labeling_params arg (=conflict_method=all;label_interaction_mode=0:all,before_end|1:before_start,after_start;censor_interaction_mode=all:within,all)\n                                         the labeling params\n  --sub_sample_pids arg (=0)             down-sample pids till this number to speedup calculation. If 0 no sub sampling\nCensoring Registry Options:\n  --censoring_registry_path arg         location to load registry txt file for censoring\n  --censoring_registry_type arg (=keep_alive)\n                                        censoring registry default path\n  --censoring_registry_args arg         An init command for registry creation on the fly\nTest Signal Options:\n  --test_main_signal arg                main signal to look for it's values when creating test signal\n  --test_from_window arg (=0)           min time before the registry to look for signal (if negative means search forward)\n  --test_to_window arg (=365)           max time before the registry to look for signal (if negative means search forward)\n  --test_hierarchy arg                  Hierarchy type for test signal. free string to filter regex on parents to propage up. to cancel pass \"None\"\n  --inc_labeling_params arg (=conflict_method=all;label_interaction_mode=all:before_end,after_start)\n                                        params for outcome registry interaction with age bin\nFiltering Options:\n  --filter_min_age arg (=20)            minimal age for filtering population in stats table nodes\n  --filter_max_age arg (=90)            maximal age for filtering population in stats table nodes\n  --filter_gender arg (=3)              filter by gender - 1 for male, 2 - for female, 3 - take both\n  --filter_positive_cnt arg (=0)        minimal positive count in registry for signal value to keep signal value from filering out\n  --filter_total_cnt arg (=100)         minimal count for signal value to keep signal value from filering out to remove small redandent signal values\n  --filter_pval_fdr arg (=0.0500000007) filter p value in FDR to filter signal values\n  --filter_min_ppv arg (=0)             filter minimal PPV for signal value and registry values\n  --filter_positive_lift arg (=1)       should be &gt;= 1. filtering of lift &gt; 1. from which value? for example at least lift of 1.5\n  --filter_negative_lift arg (=1)       should be &lt;= 1. filtering of lift &lt; 1. from which value? for example at tops lift of 0.8\n  --filter_child_pval_diff arg (=1.00000001e-10)\n                                        p value diff to consider similar in hirarchy filters\n  --filter_child_lift_ratio arg (=0.0500000007)\n                                        lift change ratio in child comapred to parent to consider similar\n  --filter_child_count_ratio arg (=0.0500000007)\n                                        count similarty to consider similar in hirarchy filters\n  --filter_child_removed_ratio arg (=1) If child removed ratio is beyond this and has other child taken - remove parent\n  --filter_stats_test arg (=mcnemar)    which statistical test to use: (mcnemar,chi-square,fisher)\n  --filter_chi_smooth arg (=0)          number of balls to add and smooth each window\n  --filter_chi_at_least arg (=0)        diff in ratio for bin to cancel - will get change of at least greater than\n  --filter_chi_minimal arg (=0)         the minimal number of observations in bin to keep row\n  --filter_fisher_smooth arg (=0)       fisher bandwith for division to happend. in ratio change from total_count of no_sig to with_sig [0-1]\nOutput Options:\n  --output_value arg (=-1)              if exists will print stats table for only selected signal value else will print all sorted list\n  --output_full_path arg                will only treat if output_value &lt;&gt; NULL. if exist will use to run all and print out all pids data with corersponding\n                                        signal in output_value\n  --output_debug arg (=0)               If true &amp; output_value!=-1 will output intersetions into file for debuging\n</code></pre> \u00a0 Example Usage can be found here: <pre><code>SignalsDependencies --base_config /nas1/Work/Users/Alon/UnitTesting/examples/signalDependency/pregnancy.cfg\n</code></pre> \u00a0 The File config file content (where all parameters may be override with command arguments) is: Config File Example \u00a0Expand source Config File Example<pre><code>#The Repository path:\nglobal_rep = /home/Repositories/THIN/thin_jun2017/thin.repository\n#A param which controls if to override \"global_stats_path\" file or use it if exists and save running time:\nglobal_override = 0\n#the bining of age\nglobal_age_bin = 5\n#the output binary dictionary of the stats to save for faster load in next time (when we want to play with the filtering params)\n#global_stats_path = /server/Work/Users/Alon/Models/outputs/dicts/preg_rc_after.dict\nglobal_stats_path = /server/Work/Users/Alon/Models/outputs/dicts/preg_rc_before.new.dict\n#registry args:\n#each line in registry format looks like: [PID, Start_Date, End_Date, RegistryValue]\n#registry can be create on the fly by specifying \"registry_init_cmd\" or with command:\n#created by create_registry --rep $rep_thin --registry_type binary --registry_censor_init \"max_repo_date=20170101;start_buffer_duration=365;end_buffer_duration=365;duration=1095;signal_list=RC\" --conflicts_method none --registry_init \"max_repo_date=20170101;start_buffer_duration=365;end_buffer_duration=365;config_signals_rules=/server/Work/Users/Alon/Models/configs/registry.pregnancy.cfg\"  --registry_save /server/Work/Users/Alon/Models/registry/pregnancy.reg\nregistry_path = /server/Work/Users/Alon/Models/registry/pregnancy.new.reg\n#,Hemoglobin,Glucose,LDL,ALT,Creatinine\ncensoring_registry_args = max_repo_date=20170101;start_buffer_duration=365;end_buffer_duration=365;duration=1095;signal_list=RC\n#the signal to fetch and test compared to the registry\\samples file\nregistry_filter_train = 1\nlabeling_params = label_interaction_mode=0:before_end,after_start|1:before_start,after_start;censor_interaction_mode=all:within,all;conflict_method=all\ninc_labeling_params = label_interaction_mode=all:before_end,after_start;censor_interaction_mode=all:within,all;conflict_method=all\n#test signal\ntest_main_signal = RC\n#the time window parameters in days:\ntest_from_window = 30\ntest_to_window = 720\n#some filtering parameters of the output - age- ranges, minimal count of positives, minimal total count for signal...\nfilter_min_age = 15\nfilter_max_age = 50\n#To filter by gender - 1 for only males, 2 -only females, 3-males and females(default)\nfilter_gender = 2\nfilter_positive_cnt = 0\nfilter_total_cnt = 1000\nfilter_pval_fdr = 0.05\nfilter_min_ppv = 0\nfilter_chi_smooth = 10\nfilter_chi_minimal = 10\nfilter_chi_at_least = 0\n#whater to bring general stats on all the values, or print specific tables of male,gender with all age-groups for specific readcode\\drug value\noutput_value = -1\n</code></pre> The output for all values (the example for the pregnancy in time window before the pregnancy read code of at least 1 month till 2 years): Output example <pre><code>read [267598] records on both male and female stats.\nFilter male stats - using only females\nusing mcnemar statistical test\nAfter Filter Age [15 - 50] have 0 keys in males and 137519 keys in females\ncreating scores vector with size 137519\nAfter total filter has 18095 signal values\nAfter positive filter has 18095 signal values\nAfter positive ratio filter has 18095 signal values\nAfter Hirarchy filter has 11964 signal values\nAfter lifts has 11964 signal values\nAfterFDR has 6908 signal values\nSignal Chi-Square scores (6908 results):\nIndex   Signal_Value    Chi-Square_Score        pValue  dof     Count_allBins   Positives_allBins       LiftProb_allBins        Signal_Name\nStatRow: 1      287376  196.492 0       7       10324   1434    4.4591  GROUP: IVF      G_8C84.\nStatRow: 2      122633  224.836 0       7       3896    847     3.5023  Fertility_counselling_#1        6778.11\nStatRow: 3      122614  3430.6  0       7       34254   9975    3.3106  Pre-pregnancy_counselling       676..00\nStatRow: 4      106282  1099.49 0       7       30692   5717    3.2144  Fertility_problem       1AZ2.00\nStatRow: 5      287372  485.476 0       7       24642   3745    3.1663  GROUP: Treatment_for_infertility        G_8C8..\nStatRow: 6      142172  139.051 0       7       7288    1022    3.1596  IVF     8C84.11\nStatRow: 7      270286  771.086 0       7       11988   2828    3.1565  GROUP: Fertility_counselling_#1 G_6778.\nStatRow: 8      122884  747.336 0       7       11399   2762    3.1528  Pre-conception_advice   67IJ.00\nStatRow: 9      270272  3432.34 0       7       39358   10753   3.1124  GROUP: Pre-pregnancy_counselling        G_676..\nStatRow: 10     292418  478.115 0       7       11690   2244    3.1101  GROUP: Seen_in_fertility_clinic_#1      G_9N07.\nStatRow: 11     262820  147.48  0       7       19118   1796    3.0742  GROUP: Hepatitis_C_antibody_test        G_43X2.\nStatRow: 12     268007  1049.69 0       7       13455   3485    3.0688  GROUP: Planning_to_start_family G_6125.\nStatRow: 13     289114  459.583 0       7       10729   2171    3.0278  GROUP: Referral_to_fertility_clinic     G_8HTB.\nStatRow: 14     122632  547.773 0       7       8122    1988    2.9820  Procreat/fertility_counselling  6778.00\nStatRow: 15     342228  111.969 0       7       2130    460     2.7558  GROUP: Advice_relating_to_pregnancy_and_fertility       G_ZG9..\nStatRow: 16     120165  124.06  0       6       1852    462     2.6691  Planning_to_start_family        6125.11\nStatRow: 17     311447  771.139 0       7       19060   3776    2.6687  GROUP: Subfertility     G_K5Byz\nStatRow: 18     255323  2462.54 0       7       86948   14995   2.6330  GROUP: Genitourinary_symptoms_NOS       G_1AZ..\nStatRow: 19     142164  286.2   0       7       8664    1608    2.6292  Treatment_for_infertility       8C8..00\nStatRow: 20     262814  180.03  0       7       27468   2508    2.5821  GROUP: Hepatitis_antibody_test  G_43X..\nStatRow: 21     264801  138.169 0       7       6613    892     2.5780  GROUP: Urine_sex_hormone_titre  G_46J..\nStatRow: 22     292409  68976.8 0       7       4769351 359266  2.4232  GROUP: Encounter_administration G_9N...\nStatRow: 23     254586  5232.19 0       7       119552  22218   2.4079  GROUP: Last_menstrual_period_-1st_day   G_1513.\nStatRow: 24     106283  1515.82 0       7       57061   9796    2.3924  Infertility_problem     1AZ2.11\nStatRow: 25     170163  225.844 0       7       8639    1390    2.2657  Missed_miscarriage      L02..11\nStatRow: 26     270530  571.406 0       7       18561   3127    2.2606  GROUP: Pre-conception_advice    G_67IJ.\nStatRow: 27     254603  487.864 0       7       19694   2774    2.2502  GROUP: Missed_period    G_151I.\nStatRow: 28     311873  194.39  0       7       11567   1667    2.2349  GROUP: Bleeding_in_early_pregnancy      G_L10y.\nStatRow: 29     122760  280.372 0       7       15156   2331    2.2160  Pregnancy_advice        67A..00\nStatRow: 30     311416  1845.72 0       7       78378   13105   2.1946  GROUP: Infertility_-_female     G_K5B..\nStatRow: 31     254592  1212.24 0       7       43897   6499    2.1937  GROUP: Menstrual_period_late    G_1517.\nStatRow: 32     254233  98.3707 0       7       5035    111     2.1780  GROUP: H/O:_pneumonia   G_14B2.\nStatRow: 33     170257  99.7106 0       7       4515    673     2.1447  Inevitable_miscarriage_complete L045.11\nStatRow: 34     292563  311.94  0       7       15052   2266    2.1305  GROUP: Seen_by_health_visitor   G_9N23.\nStatRow: 35     311867  954.495 0       7       60058   8672    2.0825  GROUP: Haemorrhage_in_early_pregnancy   G_L10..\nStatRow: 36     40006   137.015 0       7       30239   192     2.0744  Heart_Disease   Heart_Disease\nStatRow: 37     306897  223.302 0       7       19487   196     2.0243  GROUP: Cerebrovascular_disease  G_G6...\nStatRow: 38     261609  137.577 0       7       24348   2370    2.0054  GROUP: HIV_antibody/antigen_(Duo)       G_43d5.\nStatRow: 39     267999  38651.2 0       7       2959607 274901  2.0043  GROUP: Family_planning  G_61...\nStatRow: 40     264551  8138.74 0       7       397242  52412   1.9975  GROUP: Urine_pregnancy_test     G_465..\n</code></pre> We can see the results are reasonable for pre pregnancy - we see some pre pregnancy tests\\vaccinations consults, fertility and IVF treatments... the resutls for the after pregnancy are also reasonables.. you may try it out yourself \u00a0 Example run for printing out specific value 106283 (which is \"Infertility_problem\"): <pre><code>SignalsDependencies --base_config /nas1/Work/Users/Alon/UnitTesting/examples/signalDependency/pregnancy.cfg --output_value 106283\n</code></pre> And the output (there are some gender problems with the pregancny readcode which needs to be eliminated. Some patients are seen as Males with pregnancy readcode - but small amount. for example there are 39 males in the age 20-25 which will be pregnant in THIN out of almost 3M males who are in THIN in those years): <pre><code>read [267598] records on both male and female stats.\nAfter Filter Age [15 - 50] have 130079 keys in males and 137519 keys in females\nusing mcnemar statistical test\nTest Signal:106283 - Infertility_problem\nStats for Males\n              [NO_SIG&amp;REG_FALSE, NO_SIG&amp;REG_TRUE, SIG&amp;REG_FALSE, SIG&amp;REG_TRUE]\nAge_Bin: 15: [1031209,          9,         80,          0] score=    0.00070 [ 0.001%,  0.000%] indep_ratio=0.001%\nAge_Bin: 20: [895861,         10,        875,          0] score=    0.00977 [ 0.001%,  0.000%] indep_ratio=0.001%\nAge_Bin: 25: [884222,         17,       2739,          0] score=    0.05266 [ 0.002%,  0.000%] indep_ratio=0.002%\nAge_Bin: 30: [889268,         28,       4275,          0] score=    0.13460 [ 0.003%,  0.000%] indep_ratio=0.003%\nAge_Bin: 35: [867361,         25,       3201,          0] score=    0.09226 [ 0.003%,  0.000%] indep_ratio=0.003%\nAge_Bin: 40: [823505,         16,       1631,          0] score=    0.03169 [ 0.002%,  0.000%] indep_ratio=0.002%\nAge_Bin: 45: [768280,         13,        597,          1] score=    0.97005 [ 0.002%,  0.167%] indep_ratio=0.002%\nAge_Bin: 50: [701062,          9,        230,          0] score=    0.00295 [ 0.001%,  0.000%] indep_ratio=0.001%\nStats for Females\n              [NO_SIG&amp;REG_FALSE, NO_SIG&amp;REG_TRUE, SIG&amp;REG_FALSE, SIG&amp;REG_TRUE]\nAge_Bin: 15: [1068203,      42935,        764,        162] score=   80.54961 [ 3.864%, 17.495%] indep_ratio=3.875%\nAge_Bin: 20: [1200618,     103881,       5911,       1052] score=  154.07719 [ 7.963%, 15.108%] indep_ratio=8.001%\nAge_Bin: 25: [1304870,     158141,      12256,       2489] score=  196.26870 [10.809%, 16.880%] indep_ratio=10.870%\nAge_Bin: 30: [1251426,     165653,      14471,       3471] score=  338.85030 [11.690%, 19.346%] indep_ratio=11.785%\nAge_Bin: 35: [1120124,      92800,       9884,       2144] score=  488.71722 [ 7.651%, 17.825%] indep_ratio=7.751%\nAge_Bin: 40: [1028137,      25450,       3439,        450] score=  233.07304 [ 2.416%, 11.571%] indep_ratio=2.449%\nAge_Bin: 45: [944238,       2424,        486,         26] score=   22.31869 [ 0.256%,  5.078%] indep_ratio=0.259%\nAge_Bin: 50: [857528,        179,         54,          2] score=    1.96521 [ 0.021%,  3.571%] indep_ratio=0.021%\n</code></pre> We may see that in Females young females 15-20 the\u00a0Infertility_problem readcode has percentage of 17.495% in the time before pregnancy compared to 3.864% in the females 15-20 that don't have Infertility_problem. We see that when you have Infertility_problem you are more probable to get pregnant in the future. this population wants to get pregnant in the first place and eventually sucessed\u00a0\u00a0</p>"},{"location":"Archive/Deprecated/action_outcome_effect.html","title":"action_outcome_effect","text":"<p>The tool can be found\u00a0at MR_Tools/action_outcome_effect The tool check treatment/action effect on outcome. the main input is MedSamples+json to create matrix or MedFeatures(the matrix itself) with list of confounders.</p>"},{"location":"Archive/Deprecated/action_outcome_effect.html#disclaimers","title":"Disclaimers:","text":"<ol> <li>We need to list all covariates that effect action\\treatment. because it's human based decision it should be doable to list all. If we miss covariate or confounder we may have missleading results</li> <li>the matching process may be prone to weighting errors that effect matching or predictor with poor results. we may look at the second_weighted_auc and would like it to be as close as it can to 0.5 (0.5 is perfect match, no more information in the covariates left)</li> <li>We need strong ignorabilty (so except to 1st point for listing all covariates) we need all the population to have probability for action\\treatment that is not pure 0 or 1. we have defined cutoff probability to drop patients who have no dilemas. so the final population in which we show results is different from the requested original (probablity no patients who are very healthy). we may look at the covariated distribution in the new population after drop</li> <li>the action\\treatment may effect indirectly on outcome through other covariates. For example taking statins will lower your LDL value and that's what lower your risk for stroke\\MI. It's important to understand that if you have 2 patient with dilema for treatment the treatment effect may occour indirectly by the treatment and we also measure that. We are not testing for direct treatment effect only \u00a0</li> </ol>"},{"location":"Archive/Deprecated/action_outcome_effect.html#what-the-tool-does","title":"What the tool does?","text":"<ol> <li>Selects a model thats when using it's prediction score on cross validation sqeeze all the information in the covariates:If we do inverse probabilty reweighting (similar method like matching to match populations) and try to learn validation model with cross validation we reach low AUC.It selects the model that the secondry validaiton model after the matching achieves the worst AUC.</li> <li>Trains a model for predicting the action\\treatment and calibrates the scores to probabilty\u00a0</li> <li>Matches or Reweight with the model score to cancel the confounders - it drops patients who are only treated\\only untreated because we can't measure treatment effect on them.It also show comperasion of the populations before and after the matching for each of the covariates</li> <li>writes the stats (number of cases,contols with the original outcome) for each of the given groups to compare.</li> <li>Does all the process in bootstrap manner to ahve mean, std, CI for each measured number on each group \u00a0</li> </ol>"},{"location":"Archive/Deprecated/action_outcome_effect.html#app-help","title":"App Help","text":"<p>get help from the app</p> <p><pre><code>$&gt; action_outcome_effect --h\n##     ## ######## ########  ####    ###    ##\n###   ### ##       ##     ##  ##    ## ##   ##\n#### #### ##       ##     ##  ##   ##   ##  ##\n## ### ## ######   ##     ##  ##  ##     ## ##\n##     ## ##       ##     ##  ##  ######### ##\n##     ## ##       ##     ##  ##  ##     ## ##\n##     ## ######## ########  #### ##     ## ########Program General Options:\n  -h [ --help ]         help &amp; exit\n  --base_config arg     config file with all arguments - in CMD we override those settings\n  --debug               set debuging verbose\nProgram options:\n  --rep arg                                                                     the repository if needed for age\\gender cohorts\n  --input arg                                                                   the location to read input\n  --input_type arg (=samples)                                                   the input type (samples,samples_bin,features,medmat_csv,features_csv)\n  --down_sample_max_cnt arg (=0)                                                the maximal size of input to downsample to. if 0 won't do\n  --change_action_prior arg (=-1)                                               If &gt; 0 will change prior of action in the sample to be the given number\n  --output arg                                                                  the location to write output\n  --json_model arg                                                              json model for creating features for the filtering of cohorts in bootstrap\n  --confounders_file arg                                                        the file with the list of confounders [TAB] binSettings init line\n  --patient_groups_file arg                                                     each sample and belongness - same order as input\n  --patient_action arg                                                          each sample and belongness - same order as input\n  --probabilty_bin_init arg                                                     the init line for probabilty binning (only in bin squezzing method)\n  --features_bin_init arg                                                       the init line for features. if not empty and probabilty_bin_init not empty will\n                                                                                so bin splitting(only in bin squezzing method)\n  --model_type arg (=xgb)                                                       the model type to learn\n  --models_selection_file arg                                                   the model selection file with all params\n  --method_price_ratio arg (=-1)                                                if -1 will do reweight. if 0 - no matching. otherwise matching with this price\n                                                                                ratio\n  --pairwise_matching_caliper arg (=-1)                                         If &gt; 0 will do pairwise matching using caliper for std on prctiles\n  --nFolds_train arg (=5)                                                       0 means use all. no train\\test. other number splits the matrix\n  --nFolds_validation arg (=5)                                                  0 means use all. no train\\test. other number splits the matrix\n  --min_probablity_cutoff arg (=0.00499999989)                                  the minimal probabilty cutoff in reweighting\n  --model_validation_type arg (=qrf)                                            the predictor type of the validation\n  --model_validation_init arg (=ntrees=100;maxq=500;spread=0.0001;min_node=50;ntry=4;get_only_this_categ=1;n_categ=2;type=categorical_entropy;learn_nthreads=1;predict_nthreads=1)\n                                                                                the predictor init line of the validation predictor\n  --model_prob_clibrator arg (=caliberation_type=binning;min_preds_in_bin=200;min_prob_res=0.005)\n                                                                                the init line for model probability calibrator\n  --bootstrap_subsample arg (=1)                                                bootstrap subsample param in each loop if nbotstrap&gt;1\n  --nbootstrap arg (=1)                                                         bootstrap loop count\n</code></pre> example Run with config file with all arguments in /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/base_config_example.cfg:</p> example run<pre><code>Linux/Release/action_outcome_effect --base_config /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/base_config_example.cfg\nWARNING: header line contains unused fields [EVENT_FIELDS,]\n[date]=2, [id]=1, [outcome]=3, [outcome_date]=4, [split]=5,\nread [635795] samples for [635795] patient IDs. Skipped [0] records\nSamples has 635795 records. for uniq_pids = [ 0=621655 1=14140 ] all = [ 0=621655 1=14140 ]\nPrinting by prediction time...\nYear    Count_0 Count_1 ratio\n2007    131494  2398    0.017910\n2008    137281  2661    0.019015\n2009    137154  2791    0.019944\n2010    121066  3011    0.024267\n2011    94660   3279    0.033480\nAdding new features using /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json\ninit model from json file [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json], stripping comments and displaying first 5 lines:\n{\n       \"processes\":{\n        \"process\":{\n            \"process_set\":\"0\",\nmodel_json_version [1]\nUSING DEPRECATED MODEL JSON VERSION 1, PLEASE UPGRADE TO model_json_version: 2\ninit model from json file [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json], stripping comments and displaying first 5 lines:\n{\n       \"processes\":{\n        \"process\":{\n            \"process_set\":\"0\",\nAdding new rep_processor set [0]\nAdding new feature_processor set [0]\nAdding new feature_processor set [1]\nfp_type [imputer] acting on [7] features\nAdding new feature_processor set [2]\nNOTE: no [predictor] node found in file\nMedRepository: read config file /home/Repositories/THIN/thin_jun2017/thin.repository\nMedRepository: reading signals: BP,BYEAR,Cholesterol,DM_Registry,Drug,GENDER,HDL,LDL,Smoking_quantity,Triglycerides,\nRead 10 signals, 635795 pids :: data  1.552GB :: idx  0.051GB :: tot  1.603GB\nRead data time 5.216681 seconds\nadding virtual signals from rep type 0\nMedModel::get_required_signal_names 10 signalNames 0 virtual_signals\nMedModel::get_required_signal_names 10 signalNames 0 virtual_signals after erasing\nMedModel::learn() : learn rep processors time 777.803 ms\nMedModel::learn() : learn feature generators 4.993 ms\nMedModel::learn() : generating learn matrix time 1310.35 ms\nMedModel::learn() : feature processing learn and apply time 156.1 ms\nprinting stats for outcome:\nSamples has 635795 records. for uniq_pids = [ 0=621655 1=14140 ] all = [ 0=621655 1=14140 ]\nDone reading 635795 lines from [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_groups.list]\nHas 120 groups\nDone reading 635795 lines from [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_action.list]\nno_outcome_no_action: 534437, no_outcome_yes_action: 87218, yes_outcome_no_action:11515, yes_outcome_yes_action:2625\noutcome_prior=2.22%(14140 / 635795). action_prior=14.13%(89843 / 635795).\nno_action_prob_outcome=2.109%, yes_action_prob_outcome=2.922%\nDone reading 11 Confounders in [/server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/confounders.list]\nI have 1 models in model selection.\nStarting bootstrap loop 1\nprinting stats for learning action:\nSamples has 635788 records. for uniq_pids = [ 0=344975 1=56611 ] all = [ 0=546381 1=89407 ]\nscores hist: HISTOGRAM[0.0%:0.000, 10.0%:0.008, 20.0%:0.020, 30.0%:0.036, 40.0%:0.057, 50.0%:0.083, 60.0%:0.116, 70.0%:0.162, 80.0%:0.235, 90.0%:0.366, 100.0%:0.997]\nProb hist: HISTOGRAM[0.0%:0.000, 10.0%:0.005, 20.0%:0.020, 30.0%:0.037, 40.0%:0.060, 50.0%:0.085, 60.0%:0.119, 70.0%:0.169, 80.0%:0.244, 90.0%:0.361, 100.0%:0.886]\nTRAIN_CV_AUC = 0.817, TRAIN_AUC_BINNED=0.824\nWarning: matching group has very small counts - skipping group=0.019608 [199, 4]\nWarning: matching group has very small counts - skipping group=0.009926 [398, 4]\nWarning: matching group has very small counts - skipping group=0.009877 [401, 4]\nWarning: matching group has very small counts - skipping group=0.009756 [399, 4]\nWarning: matching group has very small counts - skipping group=0.006633 [597, 4]\nWarning: matching group has very small counts - skipping group=0.004963 [401, 2]\nWarning: matching group has very small counts - skipping group=0.004926 [399, 2]\nWarning: matching group has very small counts - skipping group=0.003731 [802, 3]\nWarning: matching group has very small counts - skipping group=0.003317 [1206, 3]\nWarning: matching group has very small counts - skipping group=0.003306 [603, 2]\nWarning: matching group has very small counts - skipping group=0.003300 [603, 2]\nWarning: matching group has very small counts - skipping group=0.002982 [1003, 3]\nWarning: matching group has very small counts - skipping group=0.002481 [1608, 4]\nWarning: matching group has very small counts - skipping group=0.002475 [806, 2]\nWarning: matching group has very small counts - skipping group=0.001990 [1002, 2]\nWarning: matching group has very small counts - skipping group=0.001658 [604, 1]\nWarning: matching group has very small counts - skipping group=0.001657 [1207, 2]\nWarning: matching group has very small counts - skipping group=0.001656 [602, 1]\nWarning: matching group has very small counts - skipping group=0.001653 [1203, 2]\nWarning: matching group has very small counts - skipping group=0.001650 [1812, 3]\nWarning: matching group has very small counts - skipping group=0.001325 [3015, 4]\nWarning: matching group has very small counts - skipping group=0.001244 [2410, 3]\nWarning: matching group has very small counts - skipping group=0.001243 [2412, 3]\nWarning: matching group has very small counts - skipping group=0.001242 [2409, 3]\nWarning: matching group has very small counts - skipping group=0.001241 [805, 1]\nWarning: matching group has very small counts - skipping group=0.000902 [2217, 2]\nAfter Matching has 88530.0 controls and 82542.0 cases with 48.250% percentage\nSECOND_WEIGHTED_AUC = 0.499\nComparing populations - original population has 635795 sampels, matched has 171072 samples. Features distributaions:\nBAD feature Age :: original mean=53.255[30.000 - 78.000],std=14.132. matched mean=59.597[40.000 - 78.000],std=11.555. mean_diff_ratio=11.908%\nBAD feature FTR_000003.DM_Registry.ever_DM_Registry_Diabetic.win_0_100000 :: original mean=0.050[0.000 - 1.000],std=0.219. matched mean=0.110[0.000 - 1.000],std=0.313. mean_diff_ratio=119.001%\nBAD feature FTR_000004.Current_Smoker :: original mean=0.244[0.000 - 1.000],std=0.429. matched mean=0.282[0.000 - 1.000],std=0.449. mean_diff_ratio=15.259%\nGOOD feature FTR_000005.BP.last.win_0_1095 :: original mean=80.974[64.000 - 100.000],std=10.374. matched mean=82.565[67.000 - 100.000],std=10.551. mean_diff_ratio=1.964%\nGOOD feature FTR_000006.BP.last.win_0_1096.t0v1 :: original mean=134.039[108.000 - 166.000],std=17.669. matched mean=139.302[112.000 - 172.000],std=17.973. mean_diff_ratio=3.926%\nBAD feature FTR_000007.LDL.last.win_0_1095 :: original mean=128.172[73.359 - 189.189],std=36.009. matched mean=145.561[84.942 - 204.633],std=37.765. mean_diff_ratio=13.567%\nBAD feature FTR_000008.Cholesterol.last.win_0_1095 :: original mean=209.340[146.718 - 277.992],std=39.961. matched mean=231.132[166.023 - 297.297],std=41.690. mean_diff_ratio=10.410%\nGOOD feature FTR_000009.HDL.last.win_0_1095 :: original mean=55.937[34.749 - 84.942],std=16.492. matched mean=53.698[30.888 - 84.942],std=16.333. mean_diff_ratio=4.003%\nBAD feature FTR_000010.Triglycerides.last.win_0_1095 :: original mean=130.621[53.100 - 274.350],std=96.524. matched mean=164.286[61.950 - 336.300],std=116.033. mean_diff_ratio=25.773%\nBAD feature FTR_000013.Drug.category_set_BloodPressureDrugs.win_0_1095 :: original mean=0.108[0.000 - 1.000],std=0.310. matched mean=0.166[0.000 - 1.000],std=0.372. mean_diff_ratio=54.031%\nGOOD feature Gender :: original mean=1.550[1.000 - 2.000],std=0.497. matched mean=1.484[1.000 - 2.000],std=0.500. mean_diff_ratio=4.258%\npredictor AUC with CV to diffrentiate between populations is 0.751\nProcessed 1 out of 5(20.00%) time elapsed: 3.7 Minutes, estimate time to finish 14.7 Minutes\nStarting bootstrap loop 2\n...\n\u00a0\n</code></pre> <p>you may see that after the matching the secondry model doesn't achieves good results \"SECOND_WEIGHTED_AUC = 0.499\". but you can see that the population is very different from the original requested one. a predictor can seperated them with AUC=0.751. you can see that the \"DM_Registry\" has more than doubled it's value from 0.05 to 0.11!! the GOOD/BAD keywords only shows you where the populations are differs. you need to remember that those diffrences are unavoidable - to induce causality you have to look on population with strong ignorabilty! If you have for example very healty patients with LDL = 70, low BMI and without statins (and you never see treated patients with those covariates in the data), you just can't induce treatment effect on them so you have to drop them. the same thing happend on \"very sick people\". inducing treatment effect only works on grey zones when we have dilemas \u00a0 It also output the results to /tmp/LDL.txt (if nbootstrap==1 all the numbers are without STD, and CI):</p> <p>head of output<pre><code>head  /tmp/LDL.txt\nmean_incidence_precantage=2.13% chi-square=4649.937     DOF=119 prob=0.000\nrisk_factor     controls_count  cases_count     case percentage lift    chi-square      chi-prob\nLDL_Group=0.00, LDL_Delta=-1, Treated_Group=0   54.0    5.0     8.47%   3.97    11.37   0.00\nLDL_Group=0.00, LDL_Delta=-1, Treated_Group=1   26.6    2.4     8.41%   3.94    3.37    0.07\nLDL_Group=0.00, LDL_Delta=-1, Treated_Group=2   47.9    0.0     0.00%   0.00    1.02    0.31\nLDL_Group=0.00, LDL_Delta=-2, Treated_Group=0   6.0     3.0     33.33%  15.63   41.97   0.00\n</code></pre> We may see each risk_factor and it's stats - number of controls,cases</p>"},{"location":"Archive/Deprecated/action_outcome_effect.html#input-arguments","title":"Input Arguments:","text":"<p><code>cat base_config_example.cfg</code></p> <pre><code>rep = /home/Repositories/THIN/thin_jun2017/thin.repository\n#input = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_2/final_matrix.bin\n#input_type = features\ninput = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/random_ldl_test_no_statins.samples\ninput_type = samples\njson_model = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/input_option_1/ldl_test.json\noutput = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/output/results.txt\npatient_action =  /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_action.list\nconfounders_file = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/confounders.list\npatient_groups_file = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/pid_groups.list\n#features_bin_init = split_method=partition_mover;binCnt=100;min_bin_count=100;min_res_value=0\n#probabilty_bin_init = split_method=iterative_merge;min_bin_count=100;binCnt=500\nmodel_type = xgb\nmodels_selection_file = /server/Work/Users/Alon/UnitTesting/examples/action_outcome_effect/xgb_model_selection.cfg\nnFolds_train = 5\nnFolds_validation = 4\nmin_probablity_cutoff = 0.005\nmodel_validation_type = qrf\nmodel_validation_init = ntrees=100;maxq=500;spread=0.0001;min_node=50;ntry=4;get_only_this_categ=1;n_categ=2;type=categorical_entropy;learn_nthreads=20;predict_nthreads=20\nmodel_prob_clibrator = caliberation_type=binning;min_preds_in_bin=200;min_prob_res=0.005\n#method_price_ratio = -1\nmethod_price_ratio = 6.2\nnbootstrap = 5\n</code></pre> <p>the input argument is the main data file and it can be MedSamples with json to create matrix or the MedFeatures itself.</p> <ul> <li>patient_action - a file with same number of lines as the samples in the input, each line is correspond to the same sample in the input. it may be 0/1 for [no treatment, treatment] mark for each sample</li> <li>confounders_file - a file list with all the confounders search name (searching contains in the column names) in the matrix of input. each line consist of the the confounder search name</li> <li>example: $&gt; head confounder.list</li> </ul> <pre><code>Age\nGender\nLDL.last\n</code></pre> <p>here we have 3 confounders: Age,Gender and LDL.last</p> <ul> <li>patient_groups_file -\u00a0a file with same number of lines as the samples in the input, each line is correspond to the same sample in the input. it will be the risk group name for the sample for later split.\u00a0 you may write for example in line: \"Age=20-40;Gender=Male\" to mark the sample as belong to that group in the output results</li> <li>models_selection_file - a file with initialization of the parameters of the model. you may provide more than one option for parameter with \",\" and than the tool will select the best option over all available options.\u00a0 you may also provide only one option and than the tool will just use this. for example: $&gt; head xgb_model_selection.cfg</li> </ul> <p><pre><code>max_depth=5,6,7\nnum_round=200\neta=0.1,0.3\n</code></pre> here we fixed num_round to be 200 and checks all the options of max_depth = 5 or 6 or 7 with eta = 0.1 or 0.3 - we have 6 model options \u00a0</p>"},{"location":"Archive/Deprecated/actions_index.html","title":"Actions Index","text":""},{"location":"Archive/Deprecated/actions_index.html#overview","title":"Overview","text":"<p>This guide summarizes some tasks and workflows for working with MedSamples, models, and repositories. Each section provides a brief description and links to detailed instructions or related tools.</p>"},{"location":"Archive/Deprecated/actions_index.html#1-match-medsamples-by-year-or-other-criteria","title":"1. Match MedSamples by Year or Other Criteria","text":"<p>Subsample your data by matching medical samples based on year or other criteria. This helps remove temporal bias, ensuring your model does not learn from the sample collection time, but instead relies on independent features.</p> <p>This approach is also useful for evaluating model performance when removing the information gain of a specific signal. For example, matching by age allows you to test model performance when age cannot be directly exploited as a predictor. The model still sees age, but conditioning on its value equalizes the probability of being a case, so age cannot be used for performance gain.</p> <p>See: Using Flow To Prepare Samples and Get Incidences</p>"},{"location":"Archive/Deprecated/actions_index.html#2-train-a-model-from-json","title":"2. Train a Model from JSON","text":"<p>See: Flow</p>"},{"location":"Archive/Deprecated/actions_index.html#3-calculate-model-score-on-samples","title":"3. Calculate Model Score on Samples","text":"<p>See: Flow</p>"},{"location":"Archive/Deprecated/actions_index.html#4-create-feature-matrix-for-samples","title":"4. Create Feature Matrix for Samples","text":"<p>See: Flow</p>"},{"location":"Archive/Deprecated/actions_index.html#5-adjust-model","title":"5. Adjust Model","text":"<p>Add or retrain <code>rep_processor</code> or <code>post_processor</code> components for calibration, explainability, or to modify an existing model. See: adjust_model</p>"},{"location":"Archive/Deprecated/actions_index.html#6-change-model","title":"6. Change Model","text":"<p>Remove or modify model components (e.g., enable debug logs, limit memory usage by setting smaller batch sizes). See: change_model</p>"},{"location":"Archive/Deprecated/actions_index.html#7-simplify-model-remove-signals","title":"7. Simplify Model / Remove Signals","text":"<p>Iteratively add or remove signals to simplify the model. See: Iterative Feature Selector</p>"},{"location":"Archive/Deprecated/actions_index.html#8-analyze-feature-importance-model-behavior","title":"8. Analyze Feature Importance &amp; Model Behavior","text":"<p>Analyze global feature importance, feature interactions, and the effect of each important feature or signal on model output. See: Feature Importance</p> <p>Automated tests for feature importance are available: Feature Importance Test</p> <p>You can also use model_signal_importance. This tool keeps the model fixed (no retraining or signal changes), but evaluates the effect of providing or removing specific signals from the input. This is useful for frozen models to assess the impact of signal availability (e.g., if a client can or cannot provide certain inputs).</p>"},{"location":"Archive/Deprecated/actions_index.html#9-bootstrap-performance-analysis","title":"9. Bootstrap Performance Analysis","text":"<p>See: bootstrap_app</p>"},{"location":"Archive/Deprecated/actions_index.html#10-compare-or-estimate-model-performance-on-a-different-repository-comparre-samples","title":"10. Compare or Estimate Model Performance on a Different Repository, Comparre samples","text":"<p>TestModelExternal is a tool designed to compare differences between repositories or sample sets when applying a model. It builds a propensity model to distinguish between repositories or samples, revealing differences and enabling straightforward comparison of feature matrices. The main goal is to identify complex patterns when comparing data. See: TestModelExternal</p>"},{"location":"Archive/Deprecated/actions_index.html#11-create-and-load-repository-from-files","title":"11. Create and Load Repository from Files","text":"<p>See: Load new repository</p>"},{"location":"Archive/Deprecated/actions_index.html#12-create-random-splits-for-traintestall-patients","title":"12. Create Random Splits for Train/Test/All Patients","text":"<p>See: Using Splits</p>"},{"location":"Archive/Deprecated/actions_index.html#13-filter-traintest-by-train-signal","title":"13. Filter Train/Test by TRAIN Signal","text":"<pre><code>FilterSamples --rep $REPOSITORY_PATH --samples $INPUT_SAMPLES_PATH --output $OUTPUT_SAMPLES_PATH --filter_train $FILTER_TRAIN_VAL\n</code></pre> <ul> <li><code>TRAIN == 1</code>: Training set (70%)</li> <li><code>TRAIN == 2</code>: Test set (20%)</li> <li><code>TRAIN == 3</code>: Validation set (10%)</li> </ul>"},{"location":"Archive/Deprecated/actions_index.html#14-print-model-info","title":"14. Print Model Info","text":"<p>See: Flow Model Info</p>"},{"location":"Archive/Deprecated/actions_index.html#15-filter-samples-by-bt-cohort","title":"15. Filter Samples by BT Cohort","text":"<p>Include <code>json_mat</code> even if not required by definition. <pre><code>FilterSamples --filter_train 0 --rep ${REP_PATH} --filter_by_bt_cohort \"Time-Window:90,730;Age:50,80;Suspected:0,0;Ex_or_Current:1,1\" --samples ${INPUT} --output ${OUTPUT} --json_mat ${JSON}\n</code></pre> See: bootstrap_app for details on the <code>--filter_by_bt_cohort</code> syntax.</p>"},{"location":"Archive/Deprecated/actions_index.html#16-check-model-compatibility-with-repository-suggest-adjustments","title":"16. Check Model Compatibility with Repository / Suggest Adjustments","text":"<p>When applying a model to a different repository, some adjustments may be needed.</p> <p>For example, MedModel strictly checks for required signals. If a signal is missing but not critical, you can mark it as acceptable by adding a rep processor for an \"empty\" signal. For more information, see Flow fit_model_to_rep.</p>"},{"location":"Archive/Deprecated/actions_index.html#fixing-missing-dictionary-definitions","title":"Fixing Missing Dictionary Definitions","text":"<p>When training on one repository and testing on another, you may encounter missing diagnoses.</p> <p>To find missing codes: <pre><code>Flow --print_model_signals --f_model $MODEL_NAME --rep $REP --transform_rep 1 --output_dict_path $PATH\n</code></pre></p> <p>To resolve:</p> <ul> <li>Add missing codes to the target dictionary, matching SECTION codes as needed.</li> <li>For example:</li> <li>Add <code>ICD9_CODE:786.09</code> if missing.</li> <li>Add <code>ICD9_CODE:420-429.99</code> for range codes.</li> <li>For named codes (e.g., <code>MALIGNANT_NEOPLASM_OF_LIP_ORAL_CAVITY_AND_PHARYNX</code>), find the equivalent numeric code (e.g., <code>140-149</code>) and add it.</li> </ul> <p>Can be compiled in AllTools.</p>"},{"location":"Archive/Deprecated/GANs%20for%20imputing%20matrices/index.html","title":"GANs for imputing matrices","text":""},{"location":"Archive/Deprecated/GANs%20for%20imputing%20matrices/index.html#general-idea","title":"General Idea","text":"<p>Given a feature matrix with missing values, we can use GAN bases ideas to impute the missing values. Formally we have a matrix\u00a0(Eqaution) , generated by our model, with missing values. Assume we also have a binary masks matrix\u00a0(Eqaution) in which\u00a0(Eqaution) \u00a0iff (Eqaution) is not missing. The patterns of the missing value matrices are far from being random, as some features always exist, some always exist when others exist, and some have very correlated mask bits. Our goal is to train a GAN with a Generator G, and a discriminator D. The generator G is a function (which we will train as a deep learning network) that gets an input vector (Eqaution) , and a mask vector (Eqaution) stating which channels are known, and generates (Eqaution)\u00a0, a complete imputed vector, in which the masked variables are exactly as in the input, and the rest are imputed. So:</p> <ul> <li>(Eqaution)</li> <li>(Eqaution)\u00a0is an imputed value if\u00a0(Eqaution) The discriminator D\u00a0is another (deep network) function that gets a vector (Eqaution) and a mask (Eqaution) and tries to estimate if the vector confined to the lit up channels is a real sample or one that went through imputing. The reason we do this with a mask vector is that we may not have enough examples in which\u00a0(Eqaution)\u00a0is given to us on all channels... we may only have a large\u00a0(Eqaution)\u00a0matrix that has many missing values in it. Hence we don't have full real examples to feed into the discriminator learning, and we need to confine D to answer the question of real vs. fake confined to only on some of the channels. This creates a problem on its own - the problem of making sure that the population of masks given for real samples is the same as the population of masks given for fake samples. While training, we do competition rounds in which G is trained to generate values for channels that are hidden from it, and D is trying to tell real from fake. The loss functions are arranged in away that will push G to generate examples that D finds hard to discriminate. \u00a0 \u00a0</li> </ul>"},{"location":"Archive/Deprecated/GANs%20for%20imputing%20matrices/TrainingMaskedGAN.html","title":"TrainingMaskedGAN","text":"<p>Please clone this:\u00a0https://github.com/Medial-EarlySign/MR_Projects/tree/main/ButWhy \u00a0 There is a python script called \"channels_GAN\\train_channels_GAN.py\" Example running command from directory with \"train_channels_GAN.py\": \u00a0 Train scripts, generate 3 files <pre><code>train_channels_GAN.py --rep ${TRAIN_REP} --get_mat --gpu \"0\" --gen_nhiddens 400,400,400 --disc_nhiddens 100,100,100 \\\n--gen_noise 0.01 --disc_noise 0.2 --gen_keep_prob 0.5 --disc_keep_prob 0.5 --gen_learning_rate 0.001 \\\n--disc_learning_rate 0.001 --max_auc_batch 1000 --disc_global_p 0.5 --gen_global_p 0.5 --cross_entropy_weight 0.5 \\\n--batch_size 1000 --csvs_freq 5000 --batch_num 100000 --n_dsteps 5 --n_gsteps 3 --round 1 --nout 50000 \\\n--work_output_dir ${output_directory} --samples ${SAMPLES_PATH} --model ${WORK_PATH}/base_model.bin --sub_sample 0\n\u00a0\n#disc_nhiddens  - discriminator hidden layers\n#gen_nhiddens  - generator hidden layers\n#model - to generate matrix with missing values to learn the GAN. this is trained model path, Or when no \"get_mat\", you can specify matrix directly in \"data\", \"validation_data\" argument.\n</code></pre> \u00a0 Example output files: W:\\Users\\Alon\\But_Why\\outputs\\GAN\\crc_gan_model.txt there are 2 more files with additional suffix, when used, please specify the shortest file path without suffix \u00a0</p>"},{"location":"Archive/Deprecated/create_registry/index.html","title":"create_registry","text":"<p>A tool to create MedRegistry and if provided sampling startegy parameters to create MedSamples. The program steps:</p> <ul> <li>Creates or load from text file MedRegistry. Can provide file path or config file to generate MedRegistry</li> <li>Creates or load from text file\u00a0MedRegistry\u00a0for censoring (a time periods where that marks the patients \"Membership\" period, it's also in format of MedRegistry) - OPTIONAL, if not given, assume the patient has full membership</li> <li>Creates\u00a0MedLabel from registry and censor_registry with problem definition arguments - time window argument + labeling policy arguments. This object knows how to \"label\" the outcome for a sample in a given time or decide to exclude it based on MedRegistry and the LabelParams.</li> <li>Creates MedSamples from MedSamplingStrategyarguments of how to sample + additional filtering arguments you may provide to restrict sampling (Age, Years...)</li> </ul>"},{"location":"Archive/Deprecated/create_registry/index.html#create-samples-from-medregistry","title":"Create samples from MedRegistry","text":"<pre><code>create_registry --rep $REP_PATH --registry_load $PATH_TO_MED_REGISTRY_FILE --registry_active_periods_complete_controls_sig MEMBERSHIP \\\n --labeling_params $LABELING_PARAMS --sampler_type $SAMPER_TYPE --sampler_args $SAMPLER_ARGS --samples_save $OUTPUT_PATH_FOR_MEDSAMPLES \\\n--filtering_params $OPTIONAL_FILTERING_PARAMS_LIKE_AGE\n#Can also provide additional MedRegistry for Membership if it is not a signal, by passing \"--censor_load\".\n</code></pre> <ul> <li>LABELING_PARAMS - defines how to label the sample - is it case/control or other outcome value?- The initialization text for MedLabels</li> <li>SAMPER_TYPE - type of sampler. The options are in here (code documentation make_sampler) or can look for informaiton here:\u00a0MedSamplingStrategy</li> <li>SAMPLER_ARGS - for the specific Sampler, the arguments for it.\u00a0MedSamplingStrategy\u00a0or browse the arguments of the specifc sampler</li> <li>OPTIONAL_FILTERING_PARAMS_LIKE_AGE - parameters to filter the samples like age. FilterParams. For example \"min_age=0;max_age=90;min_time=20120101;max_time=20180101\" The SAMPLER_TYPE, SAMPLER_ARGS are independent of the labeing - they define just when to give score or try to give score. \u00a0 Example: We have Influenza MedRegistry where the patient is marked and control when he has Membership and not influenza. In influenza events we have a 1 day time window of influenza. We want to see which patient will have flu within 1 year. The censor registry is the membership signal - we require no gaps in the whole year. The labeling params: \"time_from=0;time_to=365;censor_time_from=0;censor_time_to=365;conflict_method=max;censor_interaction_mode=all:within:within;label_interaction_mode=0:within,within|1:before_start,after_start\"\u00a0 The sampling: <code>--sampler_type yearly --sampler_args \"start_year=2016;end_year=2018;prediction_month_day=901;day_jump=365\"</code> Which predicts in 1st of September in each year from 2016 to 2017, is can give score within the specific time for the patient (has a match for a certain registry record)</li> </ul>"},{"location":"Archive/Deprecated/create_registry/index.html#create-medregistry-from-command","title":"Create MedRegistry from command","text":"<p><pre><code>create_registry --rep $REP_PATH --registry_type $REGISTRY_TYPE --registry_init  $REGISTRY_ARGS --registry_save $OUTPUT_PATH_FOR_MEDREGISTRY \n# --use_active_for_reg_outcome can pass this argument with censor registry, whether with \"--registry_active_periods_complete_controls_sig\" or \"--registry_active_periods_complete_controls\"\n</code></pre> Needs to explain and give some examples on how to create MedRegistry</p> <ul> <li>REGISTRY_TYPE : either \"binary\" for binary problems , \"categories\" (for outcome with more than case/controls states). and \"keep_alive\" that is mostly used to generate membership period based on patients activity</li> <li>REGISTRY_ARGS : the arguments for the registry type. For example \"binary\" has those options:<ul> <li>max_repo_date - the maximal repoistory date to cut registry date. in format YYYYMMDD</li> <li>start_buffer_duration - buffer duration from first \"rule\" start. Mostly, set to 0</li> <li>end_buffer_duration - buffer duration from last \"rule\" end duration. In case we want to \"trim\" the last period. Mostly, set to 0</li> <li>allow_prediciton_in_case - if true will continue to process rules even if in \"case\" time period</li> <li>seperate_cases - will allow more than one time period of case. Might be useful fot influenza events that may occur several times.</li> <li>config_signals_rules - file path to registry rules to define the MedRegistry records<ul> <li>Tab delimted file with 2 columns: RegistrySignalTypeand it's arguments Example\u00a0config_signals_rules\u00a0 file for CKD from 2 to 3 and up: <pre><code>#Definition of controls- if CKD_State is less than 2, mark it as 0 for at least 1 year if not contradicted or contiuned:\nrange   signalName=CKD_State;duration_flag=365;take_only_first=0;outcome_value=0;min_value=0;max_value=2\n#Definition of cases - if CKD_State is 3 and more, mark it as 1 for at least 1 year if not contradicted or contiuned:\nrange   signalName=CKD_State;duration_flag=365;take_only_first=0;outcome_value=1;min_value=3;max_value=9\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"Archive/Deprecated/create_registry/Create%20Membership%20registry%20example%20command.html","title":"Create Membership registry example command","text":"<p><pre><code>create_registry --rep ${REP_PATH} --registry_type keep_alive --registry_init \"duration=${CONNECT_BUFFER};max_repo_date=${MAX_REP_DATE};secondry_start_buffer_duration=0;start_buffer_duration=0;end_buffer_duration=0;signal_list=${SIGNAL_LIST}\" --registry_save $OUTPUT_PATH\n</code></pre> Parameters explained:</p> <ul> <li>REP_PATH - repository path</li> <li>SIGNAL_LIST - list of signals with comma. Those list will be used to calculate membership, if there is event of either one of them, the membership signal will continue. Example: Hemoglobin,DIAGNOSIS,Drug,Glucose</li> <li>CONNECT_BUFFER - how many days to take from each \"event\" of signal appearance to consider the patient as a member - going forward. Example values: 365 or 730</li> <li>MAX_REP_DATE - maximal time of repository.</li> </ul>"},{"location":"Archive/FeatureGenerator/index.html","title":"FeatureGenerator","text":""},{"location":"Archive/FeatureGenerator/ModelFeatGenerator.html","title":"ModelFeatGenerator","text":"<p>The input model should have been trained on a set of samples different from the samples it is ran on at generation time, it is then used to generate predictions on the current samples at the given time points. The input model can also be used to impute a feature\u00a0 - this functionality is suppose to be moved to a\u00a0separate feature processor. fg_type -\u00a0 \"model\". (required) name - the name to be used in the feature name. Will default to the model file name and then to\u00a0\"ModelPred\" if not given. modelFile - model file, will be used as the feature name if name is not given. impute_existing_feature - use the model to impute a feature in the matrix. Will not work if more than 1 time is given since it is\u00a0 a separate functionality. defaults to 0. n_preds - the number of prediction per sample per time - 1 for binary\u00a0classification which is the default. time_unit_sig - the signal time unit - defaults to the\u00a0global_default_windows_time_unit. time_unit_win\u00a0- the times vector time unit - defaults to the\u00a0global_default_windows_time_unit. times - a vector of times before the sample for which the prediction should be given. Defaults to 0 if no values are given. <pre><code>{\n    \"feat_generator\": \"model\",\n    \"name\": \"$NAME_OF_FEATURE\",\n    //Optional for manipulating prediction time:\n    \"time_unit_win\": \"Hours\", \"times\": \"30, 100, 200\",\n    //Option 1 - using trained model. path to binary MedModel\n    \"file\": \"/nas1/Work/Users/ReutF/model_feature/pre2d_light_S0.model\"\n    //Option 2 - json for learning the model, path to samples if they are different from outer model training. There is a flag to filter and ensure we are using the same patient ids (no leakage from other splits)\n    \"model_json\": \"$PATH_TO_JSON\",\n    \"model_train_samples\": \"$PATH_TO_TRAIN_SAMPLES_IF_DIFFERENT_FROM_CURRENT\"\n}\n</code></pre>  \u00a0 \u00a0 override_predictions cane be used to copy predictions into our feature matrix without running the model (if we have the predictions from a previous run). There is no way to do this using the json file The feature id nember in the feature matrix is constant for all features resulting from running this generator once (FTR_000001.regression_pred_t_100.1,\u00a0FTR_000001.regression_pred_t_200.1,\u00a0\u00a0FTR_000001.regression_pred_t_30.1) \u00a0</p>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html","title":"Unified Smoking Feature Generator","text":""},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#background","title":"Background","text":"<p>The purpose of the Unified Smoking Feature Generator is to generate smoking related features based on different types of available smoking information. It was based on THIN and KPSC databases.</p>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#input-signals","title":"Input Signals","text":"<p>The generator currently require the following signals (it doesn't depend anymore on the THIN smoking_quantity signal):</p> Signal THIN KPSC KPNW Smoking_Status v v v Smoking_Quit_Date v x v Pack_Years x v x Smoking_Intensity [Cigs/Day] v x v <p>Smoking_Duration  [Years]</p> x v v <p>The\u00a0Smoking_Status signal is a categorical signal, with the following values:\u00a0Never, Passive, Former, Current, Never_or_Former.\u00a0 \u00a0 Extraction of the status in THIN is described in the Appendix</p>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#output-features","title":"Output Features","text":"<p>Boolean features:</p> <ol> <li>Current_Smoker</li> <li>Ex_Smoker</li> <li>Never_Smoker</li> <li>Passive_Smoker</li> <li>Unknown_Smoker</li> <li>NLST_Criterion - 1 if age between 55 to 74, pack years &gt; 30, time since quitting &lt; 15 years.</li> </ol> <p>features:</p> <ol> <li>Smok_Days_Since_Quitting - For current smokers - 0, For Former smokers, time since quitting, for Never Smokers - time since birth</li> <li>Smok_Years_Since_Quitting - same as previous, but in years</li> <li>Smok_Pack_Years_Max - Maximal report of pack years (pack years if available) if not, it is estimated (and can be corrected with intensity</li> <li>Smok_Pack_Years - the same as\u00a0Smok_Pack_Years_Max</li> <li>Smok_Pack_Years_Last - Last pack years report (without estimation)</li> <li>Smoking_Intensity - Number of pack per day</li> <li>Smoking_Years - Smoking duration.</li> </ol>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#config-example","title":"Config Example","text":"<pre><code>\"model_actions\": [\n    {\n      \"action_type\": \"feat_generator\",\n      \"fg_type\": \"unified_smoking\",\n      \"smoking_features\": \"Current_Smoker,Ex_Smoker,Never_Smoker, Unknown_Smoker,Smoking_Years,Smok_Years_Since_Quitting,Smok_Pack_Years,Smoking_Intensity\"\n    }\n  ]\n</code></pre>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#logic-explanation","title":"Logic Explanation","text":"<p>The most basic information we need to extract is smoking status on different time points The logic is based on the paper:\u00a0Development of an algorithm for determining smoking status and behaviour over the life course from UK electronic primary care records \u00a0https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5217540/pdf/12911_2016_Article_400.pdf The workflow is built\u00a0 from the following methods:</p> <ol> <li>genFirstLastSmokingDates - For each status find the first time and last time it appears. This is the input for setting the status at each smoking status report.</li> <li>genSmokingStatus - generate for each point in smoking status vector a corrected smoking status. See Figure 1</li> <li>genSmokingRanges - Build Smoking status ranges\u00a0</li> <li>genLastStatus - Set the\u00a0Boolean\u00a0 smoking status features (take the last status according the the previous method output)</li> <li>calcQuitTime - generates Smok_Days_Since_Quitting/Smok_Years_Since_Quitting.\u00a0Check that last status in the ranges vector - If former smoker, take the delta between sample time to beginning of the \"former smoking\" period, if Current smoker, take 0. if never smoker return time since birth date.</li> <li>calcSmokingIntensity - returns smoking intensity (averages the smoking intensity vector).</li> <li>calcPackYears - Set pack years according to the pack years vector.</li> <li>calcSmokingDuration - Return duration. runs over the ranges vector and integrates the period in which the status is \"Current smoker\"</li> <li>fixPackYearsSmokingIntensity - Fix pack years using smoking intensity\u00a0 and duration. If Intensity is unknown and pack years is known calculate intensity. \u00a0 Example: Taken from THIN, birth date : July 1959, sample date 05/08/2011. Marked in Grey - Input (Raw) Data</li> </ol> Smoking Status 19900315 Current 19970227 Never 19970227 Never_or_Former 20060824 Never Smoking Intensity <p> </p> <p>19900315 15.000000</p> 19970227 0.000000 20060824 0.000000 20060824 0.000000 Quit time  Pack years Smoking Status Processed 19590700 UNKNOWN_SMOKER 19900315 CURRENT_SMOKER 19970227 EX_SMOKER 19970227 EX_SMOKER 20060824 EX_SMOKER Smoking Status Ranges 19590700-19781231 UNKNOWN_SMOKER 19790101-19930904 CURRENT_SMOKER 19930905-20110805 EX_SMOKER Intensity Out: 15 Duration Out: 14.684932 Quit time: 17.926027 Pack years: 11.013699"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/index.html#appendix-extracting-smoking-status-in-thin","title":"Appendix - Extracting Smoking Status in THIN","text":"<p>In THIN database, smoking status is extracted from Read codes.\u00a0 The mapping from codes to status is taken from\u00a0 \"Development of an algorithm for determining smoking status and behaviour over the life course from UK electronic primary care records: \u00a0https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5217540/pdf/12911_2016_Article_400.pdf I have noticed that there are a lot of \"collisions\" in the smoking status vector when using\u00a0 this mapping (meaning two different status in the same date) - ~10%.\u00a0 After removing non-conclusive\u00a0 Read codes - this was reduced to ~0.5%. When the old THIN smoking feature generator was used in a simple LR model for lung cancer AUC was improved in 1 point. See original and modified mapping in the table below.\u00a0 smoking_readcodes_combined.csv Figure 1 - Logic for setting the smoking status. The code that generates the smoking vectors in THIN: http://bitbucket:7990/projects/MED/repos/gensmoking/browse \u00a0 \u00a0</p>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/Code%20Documnetation.html","title":"Code Documnetation","text":""},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/Code%20Documnetation.html#learn-flow","title":"Learn flow","text":"<p>General comment: Passive smoker is treated as Never smoker status. NEVER_OR_EX_SMOKER =&gt; it is being resolved by all the smoking information if the patient has no indication of smoking, ex smoking or quit date, it is being used as never, otherwise EX.</p> <ol> <li>After reading signals it calls\u00a0getQuitAge to store\u00a0ageAtEx,deltaTime vectors for each patient. It ignores samples and being calculated per patient (uses last sample to ensure we are not using information after last time that will be later filtered to speedup. It has dates for each \"change\" so we can later filter it and use the information that is only available at prediction time).</li> <li>getQuitAge</li> <li>Calls\u00a0genFirstLastSmokingDates =&gt;\u00a0vector dates:\u00a0vector whenever something changes in the smoking info (either new smoking status information date or quit smoking date value, that refers the past).\u00a0and map&lt;&gt; smokingStatusDates - for each smoking status first and last date of this indication. IT is being calculated by iterating over smoking_status signal + using smoking_quit date signal. When there is quit_date, it results in \"current_smoker\" status in the day before and \"ex_smoker\" status in quit date and it process this information to update\u00a0smokingStatusDates. Also handles Never_or_Ex smoking status. If no indication of current smoking or ex smoking in\u00a0smokingStatusDates after all this logic (means, even no quit date) it will use this information to update never_smoker status, otherwise it will use to update ex smoker. <li>Calls\u00a0genSmokingStatus =&gt; vector\u00a0smokingStatusVec: iterate \"dates\" vector that indicates any change in smoking info and in each date updates the current status by decision tree. return if stopped smoking the stopped age and delta time between last current indication and stopped smoking date. If didn't stop - missing values. It's in order to learn relation between age that stopped smoking and delta time from quit date to last current</li> <li>Learns a linear model from age at stopped smoking to time delta from quit date to last current status =&gt; slope + bias parameter</li>"},{"location":"Archive/FeatureGenerator/Unified%20Smoking%20Feature%20Generator/Code%20Documnetation.html#generate-flow","title":"Generate flow","text":"<ol> <li>calls\u00a0genFirstLastSmokingDates as in _learn, but for each sample, limited till prediction date</li> <li>Calls\u00a0genSmokingStatus as in _learn, but for each sample, limited till prediction date</li> <li>Calls\u00a0genSmokingRanges. generates time ranges for smoking status and returns it in sorted vector by date of\u00a0smokeRanges. Iterates through status and update in each smoking status change:</li> <li>If past state was never/unkown =&gt;and changed to current smoking. set the start smoking time to current date status or to Age of 20 if no previous smoking status (was unknown and no indication of never after age 20). If there is indication of never smoker after age 20, this is the minimal start point for being never smoker till the smoking indication.</li> <li>If status is changed between\u00a0 current_smoker and EX_Smoker - switch the time range in the middle\u00a0</li> <li>If past state was never/unkown =&gt;and changed to EX smoking. calculates age that stopped smoknig indication</li> <li>if past was unknown and current indication is never. end unknown time range and start new time period for never smoker from this indication date. If had Quit_Smoking date, the difference in time between current and EX will be 1 day - no problem and no holes. If the change is due to smoking status - there is a hole in time between the statuses and it will use the linear slope from the age to determine the age the patient really stopped smoking.\u00a0</li> <li>Calls\u00a0genLastStatus - fetches last smoking status from\u00a0smokeRanges for smoking_statuts features</li> <li>Calls\u00a0calcQuitTime - Measure the number of days/years that stopped smoking. If current it's 0, if Never - it's like quit on his birth date, if ex smoker - using the ex date</li> <li>Calls\u00a0calcSmokingIntensity - Smoking intensity valid values are 0-200 otherwise ignored. above 140 value is trimmed to 140 cigars/day.\u00a0 calculates average number in all time window, 0 value for never smoker</li> <li>Calls\u00a0calcPackYears - takes values &gt; 0. calculates max and last pack years in the time till the sample. If never smokers - it's 0. If has pack_years &gt;0, changes never_smoker status to 0. if not current_smoker, turns on the ex_smoker (MEANS you need to load PACK_YEARS only for smokers and non zero values)</li> <li>Calls\u00a0calcSmokingDuration - 0 for never smoker. if not unknown smoker. Calculate smoking_duration and\u00a0smokingDurationBeforeLastPackYears.\u00a0</li> <li>Calls\u00a0getLastSmokingDuration -that return the last time and value of intensity before sample time. Calculates this again for last time before last pack_years date indication.\u00a0</li> <li>Sums the current_smoker statuts time ranges - instead of using full time period, uses the\u00a0lastDurationDateBeforTestDate as starting time point for calculation, since till this time you have the smoking intensity provided by the user. It does this also for last_pack_years - till the last pack years information</li> <li>Calls\u00a0fixPackYearsSmokingIntensity: if no last pack years and has smoking duration + intensity =&gt; calculate pack years. If last pack years exists, validate it when smoking duration exists and not zero. When smoking intensity exists - add the delta from last, max pack years date multiply by smoking intensity(average). If smoking intensity is missing =&gt; complete the intensity from pack years + duration You can set \"debug_file\" argument to debug smoking generation</li> </ol>"},{"location":"Infrastructure%20Library/index.html","title":"Infrastructure Home Page","text":""},{"location":"Infrastructure%20Library/index.html#overview-of-medial-infrastructure","title":"Overview of Medial Infrastructure","text":"<p>Medial EarlySign provides an infrastructure to convert Electronic Medical Records (EMR) - a complex, semi-structured time-series dataset into machine-learning-ready data and reproducible model pipelines. The library is optimized for sparse time-series EMR data and is designed for low memory usage and fast processing at scale. Unlike images or free text, EMR data can be stored in complex format. The infrastructure standardize both the storage and the processing of time-series signals. We can think about this infrastructure as \"TensorFlow\" of medical data machine learning.</p> <p>Key benefits at a glance:</p> <ul> <li>Fast and memory-efficient processing for large-scale EMR sparse time series where general-purpose libraries (e.g., pandas) are often impractical.</li> <li>Shareable, tested pipelines and methods that save engineering time and reduce duplicated effort.</li> <li>Built-in safeguards to reduce data leakage and time-series-specific overfitting.</li> <li>Production-ready: easily deployable in Docker or minimal Linux images.</li> </ul> <p>This framework is deployed in production across multiple healthcare sites and played a key role in our award-winning submission to the CMS AI Health Outcomes Challenge.</p>"},{"location":"Infrastructure%20Library/index.html#howto-use-this","title":"Howto Use this","text":"<ul> <li>Use en existing model: Deploy a model</li> <li>Build your own model - Tutorials</li> </ul> <p>This page only covers: * Explaination about the infrastructure, the components * How to extend the infrastructure and write application that uses it</p> <p>For usage, please refer to Tutorials</p>"},{"location":"Infrastructure%20Library/index.html#main-contributers-from-recent-years","title":"Main contributers from recent years:","text":"<ul> <li>Avi Shoshan</li> <li>Yaron Kinar</li> <li>Alon Lanyado</li> </ul>"},{"location":"Infrastructure%20Library/index.html#challenges","title":"Challenges","text":"<ul> <li>Variety of Questions: Risk prediction (e.g., cancer, CKD), compliance, diagnostics, treatment recommendations</li> <li>Medical Data Complexity: Temporal irregularity, high dimensionality (&gt;100k categories), sparse signals, multiple data types</li> <li>Retrospective Data Issues: Noise, bias, spurious patterns, policy sensitivity</li> </ul>"},{"location":"Infrastructure%20Library/index.html#goals","title":"Goals","text":"<ul> <li>Avoid reinventing common methodologies each project. Sometimes complicated code/logic with debugging</li> <li>Maintain shareable, versioned, regulatory\u2011compliant pipelines</li> <li>Facilitate reproducible transfer from research to product</li> <li>Provide end-to-end support: data import \u2192 analysis \u2192 productization</li> </ul>"},{"location":"Infrastructure%20Library/index.html#platform-requirements","title":"Platform Requirements","text":"<ul> <li>Performance: Ultra-efficient in memory &amp; time (&gt;100x compare to native python pandas in some cases, mainly in preprocessing)</li> <li>Extensibility: Rich APIs, configurable pipelines, support new data types</li> <li>Minimal Rewriting &amp; Ease Of Usage: JSON\u2011driven configs, unified codebase, python API to the C library</li> <li>Comprehensive: From \"raw\" data to model deployment</li> <li>Reproducible &amp; Versioned: Track data, code, models, and parameters</li> </ul>"},{"location":"Infrastructure%20Library/index.html#infrastructure-components","title":"Infrastructure Components","text":"<ol> <li>MedRepository: a high-performance EMR time-series store<ul> <li>Fast retrieval of any patient\u2019s full record or a specific signal across all patients.</li> <li>Unified representation: each signal consists of zero or more time channels plus zero or more value channels, all tied to a patient ID.<ul> <li>Static example: \"Birth year\" \u2192 no time channels, one value channel.</li> <li>Single-time example: \"Hemoglobin\" \u2192 one time channel (test date), one value channel (numeric result).</li> <li>Interval example: \"Hospitalization\" \u2192 two time channels (admission and discharge dates).</li> </ul> </li> <li>Hierarchical support for categorical medical ontologies <ul> <li>Enables seamless integration and translation between different systems when working with a frozen model or algorithm. </li> <li>Example: A query for ICD-10 codes starting with \"J\" (respiratory diseases) will also automatically map to corresponding categories in systems like Epic. When dictionary of mapping between ICD and Epic is added, no need to change the model. </li> <li>Ontology mappings are managed by MedDictionary, which supports many-to-many hierarchical relationships across coding systems.</li> </ul> </li> </ul> </li> <li>Modular processing pipeline (sklearn-style)<ul> <li>Rep Processors: Clean or derive \"raw\" virtual signals, while preventing leakage of future data<ul> <li>Example: Outlier cleaner that omits values only when abnormality is detected by future readings (e.g., a hemoglobin value on 2023-Feb-04 flagged only by a 2023-May-21 test remains until after May 21).</li> <li>Example: Virtual BMI signal computed from weight/height, or imputed when only two of three inputs exist</li> </ul> </li> <li>Feature Generators: Convert cleaned signals into predictive features.<ul> <li>Examples:<ul> <li>\"Last hemoglobin in past 365 days\"</li> <li>\"Hemoglobin slope over three years\"</li> <li>\"COPD diagnosis code during any emergency admission in last three years\"</li> </ul> </li> </ul> </li> <li>Feature Processors: Operate on the feature matrix-imputation, selection, PCA, etc. </li> <li>Predictors/Classifiers: LightGBM, XGBoost, or custom algorithms.</li> <li>Post-processing: Score calibration, explainability layers, fairness adjustments, etc.</li> </ul> </li> <li>JSON-driven pipeline configuration - Define every processor, feature generator, and model step in a single JSON file. Json Format     Example json for training a model:</li> </ol> Click to expend example json<pre><code>   {\n    \"$schema\": \"https://raw.githubusercontent.com/Medial-EarlySign/MR_Tools/refs/heads/main/medmodel_schema.json\",\n    \"model_json_version\": \"2\",\n    \"serialize_learning_set\": \"0\",\n    \"model_actions\": [\n        \"json:full_rep_processors.json\", // Import a json from current folder with other componenets - in this case, outlier cleaners, signal panel completers, etc.\n    // Features\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"age\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"gender\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"unified_smoking\",\n            \"tags\": \"smoking\",\n            \"smoking_features\": \"Current_Smoker, Ex_Smoker, Unknown_Smoker, Never_Smoker, Passive_Smoker, Smok_Days_Since_Quitting , Smok_Pack_Years_Max, Smok_Pack_Years_Last,Smoking_Years,Smoking_Intensity\"\n        },\n        // Cancers in Dx\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=10950\"\n            ],\n            \"time_unit\": \"Days\",\n            \"sets\": [\n                \"ICD9_CODE:140-149,ICD9_CODE:150-159,ICD9_CODE:160-165,ICD9_CODE:170,ICD9_CODE:171,ICD9_CODE:172,ICD9_CODE:174,ICD9_CODE:175,ICD9_CODE:176,ICD9_CODE:179-189,ICD9_CODE:200-208,ICD9_CODE:209.0,ICD9_CODE:209.1,ICD9_CODE:209.2,ICD9_CODE:290.3,ICD9_CODE:230-234\"\n            ],\n            \"signal\": \"ICD9_Diagnosis\",\n            \"in_set_name\": \"Cancers\"\n        },\n    // Statistical features - will take: last, average, min, max, etc. for each time window: 0-180, 0-365. 365-730, 0-1095 prior prediction day in days and for each signal: Hemoglobin, WBC...\n    // In total will create: 8*4*4 = 128 features\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": [\n                \"last\",\n                \"last_delta\",\n                \"avg\",\n                \"max\",\n                \"min\",\n                \"std\",\n                \"slope\",\n                \"range_width\"\n            ],\n            \"window\": [\n                \"win_from=0;win_to=180\",\n                \"win_from=0;win_to=365\",\n                \"win_from=365;win_to=730\",\n                \"win_from=0;win_to=1095\"\n            ],\n            \"time_unit\": \"Days\",\n            \"tags\": \"labs_and_measurements,need_imputer,need_norm\",\n            \"signal\": [\n                \"Hemoglobin\",\n                \"WBC\",\n                \"Platelets\",\n                \"Albumin\"\n            ]\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": [\n                \"last_time\"\n            ],\n            \"window\": [\n                \"win_from=0;win_to=180\",\n                \"win_from=0;win_to=365\",\n                \"win_from=365;win_to=730\",\n                \"win_from=0;win_to=1095\"\n            ],\n            \"time_unit\": \"Days\",\n            \"tags\": \"labs_and_measurements,need_imputer,need_norm\",\n            //Take only panels - to remove repititions:\n            \"signal\": [\n                \"BMI\",\n                \"Creatinine\",\n                \"WBC\",\n                \"Cholesterol\",\n                \"Glucose\",\n                \"Hemoglobin\",\n                \"Albumin\"\n            ]\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"category_depend\",\n            \"signal\": \"DIAGNOSIS\",\n            \"window\": [\n                \"win_from=0;win_to=10950;tags=numeric.win_0_10950\",\n                \"win_from=0;win_to=365;tags=numeric.win_0_365\"\n            ],\n            \"time_unit_win\": \"Days\",\n            \"regex_filter\": \"ICD10_CODE:.*\",\n            \"min_age\": \"40\",\n            \"max_age\": \"90\",\n            \"age_bin\": \"5\",\n            \"min_code_cnt\": \"200\",\n            \"fdr\": \"0.01\",\n            \"lift_below\": \"0.7\",\n            \"lift_above\": \"1.3\",\n            \"stat_metric\": \"mcnemar\",\n            \"max_depth\": \"50\",\n            \"max_parents\": \"100\",\n            \"use_fixed_lift\": \"1\",\n            \"sort_by_chi\": \"1\",\n            \"verbose\": \"1\",\n            \"take_top\": \"50\"\n        },\n        // Feature selector to remove features with 99.9% same value, there are other options, like lasso, by model importance, etc.\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                {\n                    \"fp_type\": \"remove_deg\",\n                    \"percentage\": \"0.999\"\n                }\n            ]\n        },\n        // Imputer - simple choise of choosing median value by stratifying to age, gender and smoking status - will commit for all features with \"need_imputer\" tag\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                {\n                    \"fp_type\": \"imputer\",\n                    \"strata\": \"Age,40,100,5:Gender,1,2,1:Current_Smoker,0,1,1:Ex_Smoker,0,1,1\",\n                    \"moment_type\": \"median\",\n                    \"tag\": \"need_imputer\",\n                    \"duplicate\": \"1\"\n                }\n            ]\n        },\n        // Normalizer - will commit for all features with \"need_imputer\" tag\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                {\n                    \"fp_type\": \"normalizer\",\n                    \"resolution_only\": \"0\",\n                    \"resolution\": \"5\",\n                    \"tag\": \"need_norm\",\n                    \"duplicate\": \"1\"\n                }\n            ]\n        }\n    ],\n    \"predictor\": \"xgb\",\n    \"predictor_params\": \"tree_method=auto;booster=gbtree;objective=binary:logistic;eta=0.050;alpha=0.000;lambda=0.010;gamma=0.010;max_depth=6;colsample_bytree=0.800;colsample_bylevel=1.000;min_child_weight=10;num_round=200;subsample=0.800\" }\n</code></pre> <ol> <li>Comprehensive evaluation toolkit<ul> <li>Bootstrap-based cohort analysis allows batch testing across thousands of user-defined subgroups (e.g., age 50\u201380, males only, prediction window of 365 days, COPD patients).</li> <li>Automatically extracts AUC, ROC points at each 1% FPR increment, odds ratios, PPV/NPV, and applies incidence-rate adjustments or KPI weights</li> <li>Includes explainability and fairness audits</li> </ul> </li> <li>Unified API wrapper for production deployment<ul> <li>Ready for productization out of the box, no need to reinvent integration or design a new interface each time. See AlgoMarker</li> <li>Packages the entire end-to-end pipeline (raw time-series ingestion through inference) into a single, stable SDK.</li> <li>Core infrastructure implemented in C++ for performance and portability, with a lightweight Python wrapper for seamless integration.</li> <li>Although powered by C++, the team mainly uses and maintains workflows via the Python SDK, ensuring rapid development and minimal friction. Experienced user might use the C++ API more often, since the python interface is more limited. </li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/index.html#basic-pages","title":"Basic Pages","text":"<ul> <li>MedModel learn and apply\u00a0</li> <li>RepProcessors:<ul> <li>RepProcessors Practical Page</li> </ul> </li> <li>FeatureGenerators:<ul> <li>Feature Generator Practical Guide</li> </ul> </li> <li>FeatureProcessors:<ul> <li>FeatureProcessor practical guide</li> </ul> </li> <li>MedPredictors<ul> <li>MedPredictors practical guide</li> </ul> </li> <li>PostProcessors:<ul> <li>PostProcessors Practical Guide</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/index.html#other-links","title":"Other links","text":"<p>Home page for in depth pages explaining several different aspects in the infrastructure Some interesting pages:</p> <ul> <li>Setup Environment</li> <li>How to Serialize : learn the SerializableObject libarary secrets.</li> <li>PidDynamicRecs and versions</li> <li>Virtual Signals</li> </ul>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html","title":"MedModel JSON Format","text":"<p>This guide explains the structure and usage of the MedModel JSON format for defining machine learning pipelines within the Medial infrastructure. The JSON file orchestrates all model steps, from raw data processing to prediction and post-processing, making your workflow modular, reproducible, and easy to configure.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#what-is-a-medmodel-json","title":"What is a MedModel JSON?","text":"<p>A MedModel JSON file describes:</p> <ul> <li>The pipeline of data processing and machine learning components (like data cleaning, feature generation, modeling, etc.)</li> <li>The order and configuration of each processing step</li> <li>The parameters for each component (as key-value pairs). This will call the component \"init\" function with the key value pairs to initialize the component. This allows a simpler way to add components and pass arguments to them from the json.</li> <li>How to reference additional configuration files or value lists</li> </ul> <p>This enables flexible, versioned, and shareable model definitions-ideal for both research and production.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#how-to-write-a-medmodel-json-step-by-step","title":"How to Write a MedModel JSON: Step-by-Step","text":"<p>Let\u2019s walk through building a JSON model file, explaining each section.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#1-general-fields","title":"1. General Fields","text":"<p>These fields configure the overall behavior of the pipeline. Most are optional and have sensible defaults.</p> <pre><code>{\n  \"$schema\": \"https://raw.githubusercontent.com/Medial-EarlySign/MR_Tools/refs/heads/main/medmodel_schema.json\",\n  \"model_json_version\": \"2\",                // Required: version of the format (always use \"2\")\n  \"serialize_learning_set\": \"0\",            // Optional: whether to save training samples in the model (default \"0\")\n  \"generate_masks_for_features\": \"0\",       // Optional: track which features were imputed (default \"0\")\n  \"max_data_in_mem\": 100000,                // Optional: controls batch size for large data (default is unlimited)\n  \"take_mean_pred\": \"1\"                     // Optional: use mean for prediction (default \"1\")\n}\n</code></pre> <p>Tip: Only <code>model_json_version</code> is required for most users. The rest can typically be left out. Tip2: Use The $schema for autocomplete and validation, even though it is incomplete.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#2-the-pipeline-model_actions","title":"2. The Pipeline: <code>model_actions</code>","text":"<p>This is the heart of the model definition-a list of components executed in order. Each component is an object specifying:</p> <ul> <li><code>action_type</code>: What kind of step this is (data cleaning, feature generation, etc.)</li> <li>Other keys: Parameters specific to the step</li> </ul> <p>Component types:</p> <ul> <li><code>rep_processor</code> or <code>rp_set</code>: Process raw signals<ul> <li>List of available rep_processor: Rep Processors Practical Guide. You should select and sepecify a the type name in <code>rp_type</code> field. For example: <pre><code>{\n  \"rp_type\": \"$SELECT_TYPE\",\n  ... Specific Component arguments that will be passed as dictionary key and value to \"init\" function of the component\n}\n</code></pre></li> </ul> </li> <li><code>feat_generator</code>: Creates features from cleaned signals<ul> <li>List of available feat_generator: Feature Generator Practical Guide. You should select and sepecify a the type name in <code>fg_type</code> field. For example: <pre><code>{\n  \"fg_type\": \"$SELECT_TYPE\",\n  ... Specific Component arguments that will be passed as dictionary key and value to \"init\" function of the component\n}\n</code></pre></li> </ul> </li> <li><code>fp_set</code>: Post-processes the feature matrix (imputation, selection, normalization)<ul> <li>List of available feature processors: FeatureProcessor practical guide. You should select and sepecify a the type name in <code>fp_type</code> field. For example: <pre><code>{\n  \"fp_type\": \"$SELECT_TYPE\",\n  ... Specific Component arguments that will be passed as dictionary key and value to \"init\" function of the component\n}\n</code></pre></li> </ul> </li> <li><code>predictor</code>: The machine learning algorithm. List of available predictors: MedPredictor practical guide. you should specify the selected predictor as <code>predictor</code> and it parameters as <code>predictor_params</code></li> <li><code>post_processor</code>: Final calibration or adjustment.<ul> <li>List of available post processors: PostProcessors Practical Guide and specify the type in <code>post_processor</code>. Example: <pre><code>{\n  \"fp_post_processortype\": \"$SELECT_TYPE\",\n  ... Specific Component arguments that will be passed as dictionary key and value to \"init\" function of the component\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#example-walkthrough","title":"Example Walkthrough","text":"<p>Let\u2019s walk through an example and explain each major step:</p> <pre><code>{\n  \"$schema\": \"https://raw.githubusercontent.com/Medial-EarlySign/MR_Tools/refs/heads/main/medmodel_schema.json\",\n  \"model_json_version\": \"2\",\n  \"serialize_learning_set\": \"0\",\n  \"model_actions\": [\n    // Step 1: Load additional rep_processors from another file for modularity\n    \"json:full_rep_processors.json\",\n\n    // Step 2: Generate simple features for age, gender, and smoking status\n    { \"action_type\": \"feat_generator\", \"fg_type\": \"age\" },\n    { \"action_type\": \"feat_generator\", \"fg_type\": \"gender\" },\n    { \"action_type\": \"feat_generator\", \"fg_type\": \"unified_smoking\", \"tags\": \"smoking\", \"smoking_features\": \"Current_Smoker,Ex_Smoker,...\" },\n\n    // Step 3: Generate categorical features (e.g., cancer diagnosis in the last 30 years)\n    {\n      \"action_type\": \"feat_generator\",\n      \"fg_type\": \"basic\",\n      \"type\": \"category_set\",\n      \"window\": [\"win_from=0;win_to=10950\"], // Days\n      \"sets\": [\"ICD9_CODE:140-149,ICD9_CODE:150-159,...\"],\n      \"signal\": \"ICD9_Diagnosis\",\n      \"in_set_name\": \"Cancers\"\n    },\n\n    // Step 4: Generate statistical features (last, avg, min, max, etc.) for signals and time windows\n    {\n      \"action_type\": \"feat_generator\",\n      \"fg_type\": \"basic\",\n      \"type\": [\"last\", \"avg\", \"max\", \"min\"],\n      \"window\": [\n        \"win_from=0;win_to=180\",\n        \"win_from=0;win_to=365\"\n      ],\n      \"signal\": [\"Hemoglobin\", \"WBC\", \"Platelets\"]\n    },\n\n    // Step 5: Feature selection (remove features with near-constant values)\n    {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        { \"fp_type\": \"remove_deg\", \"percentage\": \"0.999\" }\n      ]\n    },\n\n    // Step 6: Imputation (fill missing values using the median stratified by age, gender, smoking)\n    {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        { \"fp_type\": \"imputer\", \"strata\": \"Age,40,100,5:Gender,1,2,1\", \"moment_type\": \"median\", \"tag\": \"need_imputer\", \"duplicate\": \"1\" }\n      ]\n    },\n\n    // Step 7: Normalization (for features needing normalization)\n    {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        { \"fp_type\": \"normalizer\", \"resolution_only\": \"0\", \"resolution\": \"5\", \"tag\": \"need_norm\", \"duplicate\": \"1\" }\n      ]\n    }\n  ],\n\n  // Step 8: Specify the predictor and its parameters\n  \"predictor\": \"xgb\", // e.g., XGBoost\n  \"predictor_params\": \"tree_method=auto;booster=gbtree;objective=binary:logistic;...\"\n}\n</code></pre> <p>How it works:</p> <ol> <li>The pipeline loads additional processors from a separate JSON file (for modularity).</li> <li>It generates demographic and behavioral features.</li> <li>It creates diagnosis-based categorical features.</li> <li>It computes statistical features over defined time windows.</li> <li>It removes features with little variation.</li> <li>It imputes missing values using well-defined rules.</li> <li>It normalizes certain features.</li> <li>It trains the model using XGBoost, with custom parameters.</li> </ol> <p>This example uses an additional file \"full_rep_processors.json\" next to it. Here is the content inside</p> full_rep_processors.json full_rep_processors.json<pre><code>{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"conf_cln\",\n      \"conf_file\":\"path_rel:cleanDictionary.csv\",\n      \"time_channel\":\"0\",\n      \"clean_method\":\"confirmed\",\n      \"signal\":\"file_rel:all_rules_sigs.list\",\n      \"print_summary\" : \"1\",\n      \"nrem_suff\":\"nRem\"\n      //,\"verbose_file\":\"/tmp/cleaning.log\"\n    },\n    {\n      \"rp_type\":\"conf_cln\",\n      \"conf_file\":\"path_rel:cleanDictionary.csv\",\n      \"val_channel\":[\"0\", \"1\"],\n      \"clean_method\":\"confirmed\",\n      \"signal\": [\"BP\"],\n      \"print_summary\" : \"1\",\n      \"nrem_suff\":\"nRem\"\n      //,\"verbose_file\":\"/tmp/cleaning.log\"\n    }\n  ]\n},\n{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"sim_val\",\n      \"signal\":\"file_rel:all_rules_sigs.list\",\n      \"type\":\"remove_diff\",\n      \"debug\":\"0\"\n    }\n\n  ]\n},\n{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"rule_cln\",\n      \"addRequiredSignals\":\"1\",\n      \"time_window\":\"0\",\n      \"tolerance\":\"0.1\",\n      \"calc_res\":\"0.1\",\n      \"rules2Signals\":\"path_rel:ruls2Signals.tsv\",\n      \"consideredRules\":[ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\" ] \n\n      //,\"verbose_file\":\"/tmp/panel_cleaning.log\"\n    }\n  ]\n},\n{\n  \"action_type\": \"rp_set\",\n  \"members\": [\n    {\n      \"rp_type\":\"complete\",\n      \"sim_val\":\"remove_diff\",\n      \"config\":\"path_rel:completion_metadata\",\n      \"panels\":[\"red_line\", \"white_line\", \"gcs\", \"bmi\"]\n    },\n    {\n      \"rp_type\":\"calc_signals\",\n      \"calculator\":\"eGFR\",\n      \"missing_value\":\"-65336\",\n      \"work_channel\":\"0\",\n      \"names\":\"eGFR_CKD_EPI\",\n      \"signals_time_unit\":\"Date\",\n      \"max_time_search_range\":\"0\",\n      \"signals\":\"Creatinine,GENDER,BDATE\",\n      \"calculator_init_params\":\"{mdrd=0;ethnicity=0}\"\n    },\n    {\n      \"rp_type\":\"calc_signals\",\n      \"calculator\":\"eGFR\",\n      \"missing_value\":\"-65336\",\n      \"work_channel\":\"0\",\n      \"names\":\"eGFR_MDRD\",\n      \"signals_time_unit\":\"Date\",\n      \"max_time_search_range\":\"0\",\n      \"signals\":\"Creatinine,GENDER,BDATE\",\n      \"calculator_init_params\":\"{mdrd=1;ethnicity=0}\"\n    }\n  ]\n}  \n</code></pre> <ul> <li>It uses <code>conf_cln</code> for configuring simple and fixed outliers by valid range and configuration file <code>cleanDictionary.csv</code> with those ranges.</li> <li>It uses <code>all_rules_sigs.list</code> to list down all the avaible signals are create cleaner for each of those signals. See List Expension fo mkore details</li> <li>It uses <code>sim_val</code> to remove inputs on the same date with contradicting values and remove duplicate rows if the values are the sames</li> <li>It uses <code>rule_cln</code> to clear outliers based on equations and relations between signals. Gor example: BMI=Weight/Height^2, if a difference of more than tolerance (default is 10%) observed the values will be dropped. We can configure using <code>ruls2Signals.tsv</code> what happens if contradiction observed, whather to drop all signals in the relation, or just one specific - for example drop only the BMI. </li> <li>It uses <code>complete</code> - to complete missing values in panels from other relational signals. For example BMI is missing and we have Weight, Height. It uses <code>completion_metadata</code> to control the resulted signals resolution. </li> <li>It uses <code>calc_signals</code> to generate virtual signals for eGFR.</li> </ul>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#3-referencing-other-files","title":"3. Referencing Other Files","text":"<p>To keep your pipeline modular and maintainable, you can reference external files directly in your JSON configuration. Here are the supported reference types:</p> <ul> <li><code>\"json:somefile.json\"</code>: Imports another JSON file containing additional pipeline components.</li> <li><code>\"file_rel:signals.list\"</code>: Loads a list of values from a file and expands them as a JSON array. Useful for features or signals lists. For details on how lists are expanded, see List Expansion.</li> <li><code>\"path_rel:config.csv\"</code>: Uses a relative path to point to configuration files, resolved relative to the current JSON file's location.</li> <li><code>\"comma_rel:somefile.txt\"</code>: Reads a file line by line and produces a single comma-separated string of values (<code>\"line1,line2,...\"</code>). Unlike <code>\"file_rel\"</code>, this will not create a JSON list, but rather a flat, comma-delimited string.</li> </ul> <p>These options allow you to keep configuration modular, re-use existing resources, and simplify large or complex pipelines.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#4-advanced-list-expansion","title":"4. Advanced: List Expansion","text":"<p>If you use lists for fields (e.g., multiple signals or time windows), the pipeline automatically expands to cover all combinations (Cartesian product).</p> <p><pre><code>{\n  \"type\": [\"last\", \"avg\"],\n  \"window\": [\"win_from=0;win_to=180\", \"win_from=0;win_to=365\"]\n}\n</code></pre> This generates steps for each type \u00d7 window combination.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#5-reference-lists","title":"5. Reference Lists","text":"<p>At the end of your JSON, you can define reusable value lists, such as drug codes or signals:</p> <p><pre><code>\"diabetes_drugs\": \"ATC_A10,ATC_A11,ATC_A12\"\n</code></pre> Reference these in your pipeline using <code>\"ref:diabetes_drugs\"</code>.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#ready-to-write-your-own","title":"Ready to Write Your Own?","text":"<p>By following this walkthrough, you can confidently define new model JSON files:</p> <ul> <li>Start with the general fields</li> <li>List your pipeline steps in <code>model_actions</code></li> <li>Modularize and reuse with references</li> <li>Expand lists for coverage</li> <li>Define your predictor and parameters</li> </ul> <p>Tip: For a new project, copy and adapt the example above to fit your own signals, features, and model goals.</p>"},{"location":"Infrastructure%20Library/MedModel%20json%20format.html#further-reading","title":"Further Reading","text":"<ul> <li>Rep Processors Practical Guide</li> <li>Feature Generator Practical Guide</li> <li>FeatureProcessor practical guide</li> <li>MedPredictor practical guide</li> <li>PostProcessors Practical Guide</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/index.html","title":"InfraMed Library page","text":"<p>Git Link:\u00a0https://github.com/Medial-EarlySign/MR_Libs</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/index.html#general","title":"General","text":"<p>The InfraMed Library is a non-sql data structure tailored to hold medical records data. The main goal is to provide an efficient way to get a vector of time signals (date/time, value) for a pair of patient and signal type (pid , sid). Main toolsets available in the library:</p> <ul> <li>Converting a data set to the InfraMed Repository format. (see here)</li> <li>Adding a signal to the data set, fixing a signal</li> <li>Reading a data set or a part of it to memory</li> <li>Get the vector of signals for (pid, sid) , date/time sorted</li> <li>Create a \"by-pid\" transposed way of keeping the data</li> <li>Get all signals for a pid \"by-pid\"</li> <li>Free/Lock/Unlock/Load mechanisms for signals</li> <li>Work with dictionaries\u00a0</li> <li>Manage versions of data for a signal</li> <li>Option to load data in memory to create a repository \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/index.html#the-inframed-data-model-in-5-sentences","title":"The InfraMed data model in 5 sentences.","text":"<ul> <li>each patient has one or more signal vectors.</li> <li>each signal is of a specific type.</li> <li>In general any type can be added (see more in the MedSignals page) but a signal is composed of a constant number of time channels (0 or more) and a constant number of value channels (0 or more).</li> <li>Except for the (new) rep in memory features the library assumes the data is for read-only and updated once in a very long time, so the efficiency of creating a repository is less important.</li> <li>The library allows an extremely fast access for a signal vector given a query of a pid + signal_id (or signal name), and this is its main usage.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/index.html#general-classes","title":"General Classes","text":"<p>It is recommeded to read and understand the following pages explaining the main classes used in the InfraMed library:</p> <ul> <li>MedRepository , MedPidRepository : most used classes packing options to read repositories and query them.</li> <li>MedConvert : class used when creating a new repository.</li> <li>MedSignals: class used to handle signal files and signal properties, also contains the unified way of accessing signals.</li> <li>MedDictionary , MedDictionarySections : classes used to read dictionaries and use them More Advanced:</li> <li>PidDynamicRec : class with an advanced option to hold versions of the same signal.</li> <li>InMem repository : the InMemRepData class and its usage in a MedRepository to load data into it. Even More... (internal important classes and methods):</li> <li>IndexTable : class to read data for a specific signal on a subset (or all) pids, with a memory efficient fast index.</li> <li>MedSparseVec : a general key,value datastructure. Memory efficient and fast. The baseline for many of the indexes used in a MedRepository. \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html","title":"Generic (Universal) Signal Vectors","text":""},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#medrepository-signaltypes","title":"MedRepository SignalTypes","text":"<p>Legacy signal types were previously referenced by numeric IDs-this approach is now deprecated: <pre><code>string GenericSigVec::get_type_generic_spec(SigType t)\n{\n    switch (t) {\n    case T_Value: return \"V(f)\";                    //  0\n    case T_DateVal: return \"T(i),V(f)\";             //  1\n    case T_TimeVal: return \"T(l),V(f),p,p,p,p\";     //  2\n    case T_DateRangeVal: return \"T(i,i),V(f)\";      //  3\n    case T_TimeStamp: return \"T(l)\";                //  4\n    case T_TimeRangeVal: return \"T(l,l),V(f),p,p,p,p\"; //  5\n    case T_DateVal2: return \"T(i),V(f,us),p,p\";     //  6\n    case T_TimeLongVal: return \"T(l),V(l)\";         //  7\n    case T_DateShort2: return \"T(i),V(s,s)\";        //  8\n    case T_ValShort2: return \"V(s,s)\";              //  9\n    case T_ValShort4: return \"V(s,s,s,s)\";          // 10\n    case T_CompactDateVal: return \"T(us),V(us)\";    // 11\n    case T_DateRangeVal2: return \"T(i,i),V(f,f)\";   // 12\n    case T_DateFloat2: return \"T(i),V(f,f)\";        // 13\n    case T_TimeRange: return \"T(l,l)\";              // 14\n    case T_TimeShort4: return \"T(i),p,p,p,p,V(s,s,s,s)\"; //15\n    default:\n        MTHROW_AND_ERR(\"Cannot get generic spec of signal type %d\\n\", t);\n    }\n    return 0;\n}\n</code></pre></p> <p>When defining C structs, compilers may add padding bytes to align fields for optimal memory access. For certain types (like T_TimeVal, T_TimeRangeVal, T_DateVal2, and T_TimeShort4), padding was added to maintain compatibility with older C structures. For example, T_TimeRangeVal corresponds to the GSV string \"T(l,l),V(f),p,p,p,p\", meaning each record contains two <code>long long</code> time channels, one <code>float</code> value channel, and four padding bytes.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#gsv-declarations-in-signal-files","title":"GSV Declarations in .signal Files","text":"<p>In repository <code>.signal</code> files, you can specify a signal\u2019s type using either the legacy numeric code or a GSV string specification. If multiple signals share the same type, you can define a type alias to avoid repetition and improve clarity. Define an alias using: GENERIC_SIGNAL_TYPE [type_alias] [GSV_string_spec] (Remember: <code>.signal</code> files are tab-separated.)</p> <p>Example : .signal file example <pre><code>GENERIC_SIGNAL_TYPE mytype0 V(f)\nGENERIC_SIGNAL_TYPE mytype1 T(i),V(f)\nSIGNAL  GENDER  100 16:mytype0  Male=1,Female=2 0\nSIGNAL  BMI 902 16:mytype1  0\nSIGNAL  ICD9_Hospitalization    2300    T(i,i),V(f) 1\nSIGNAL  RBC 1001    1   0\n...\n</code></pre> In this example:</p> <ul> <li>The first two lines define type aliases (<code>mytype0</code>, <code>mytype1</code>).  </li> <li>The next two lines use these aliases for the GENDER and BMI signals.  </li> <li>The following lines show direct type specification and use of a legacy type code.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#using-gsv","title":"Using GSV","text":"<p>The <code>GenericSigVec</code> class implements GSVs, replacing the old USV system. You can still use <code>UniversalSigVec</code> as an alias for <code>GenericSigVec</code> for backward compatibility: <pre><code>typedef class GenericSigVec UniversalSigVec;\n</code></pre> The previous USV implementation is now called <code>UniversalSigVec_legacy</code> and will be removed in the future.</p> <p>Initialization can be done as follows: <pre><code>    GenericSigVec()\n    GenericSigVec(const string&amp; signalSpec, int time_unit = MedTime::Undefined)\n    GenericSigVec(SigType sigtype, int time_unit = MedTime::Undefined)\n    GenericSigVec(const GenericSigVec&amp; other)\n\u00a0\n    void init(const SignalInfo &amp;info)\n    void init_from_spec(const string&amp; signalSpec);\n    void init_from_sigtype(SigType sigtype);\n    void init_from_repo(MedRepository&amp; repo, int sid);\n</code></pre> The string specification replaces the old <code>SigType</code>. You can also initialize from a <code>SignalInfo</code> object for better performance, as it avoids runtime parsing.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#implementation","title":"Implementation","text":"<p>The core logic is in the value getter function: <pre><code>    template&lt;typename T = float&gt;\n    T Val(int idx, int chan, const void* data_) const {\n        auto field_ptr = ((char*)data_) + idx * struct_size + val_channel_offsets[chan];\n        switch (val_channel_types[chan]) {\n        case type_enc::FLOAT32: return (T)(*(float*)(field_ptr));\n        case type_enc::INT16:   return (T)(*(short*)(field_ptr));\n        case type_enc::UINT16:  return (T)(*(unsigned short*)(field_ptr));\n        case type_enc::UINT8:   return (T)(*(unsigned char*)(field_ptr));\n        case type_enc::UINT32:  return (T)(*(unsigned int*)(field_ptr));\n        case type_enc::UINT64:  return (T)(*(unsigned long long*)(field_ptr));\n        case type_enc::INT8:    return (T)(*(char*)(field_ptr));\n        case type_enc::INT32:   return (T)(*(int*)(field_ptr));\n        case type_enc::INT64:   return (T)(*(long long*)(field_ptr));\n        case type_enc::FLOAT64: return (T)(*(double*)(field_ptr));\n        case type_enc::FLOAT80: return (T)(*(long double*)(field_ptr));\n        }\n        return 0;\n    }\n</code></pre></p> <ul> <li>This template function retrieves values (defaulting to <code>float</code>, but other types are supported).</li> <li>It calculates the value\u2019s location using <code>val_channel_offsets</code>.</li> <li>The correct type cast is chosen based on <code>val_channel_types</code>.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#changes-for-gsv-support","title":"Changes for GSV Support","text":"<ul> <li><code>MedConvert</code> now supports GSVs when creating new repositories.</li> <li><code>MedPyExport</code> can export GSVs to Python. Exported field names are now <code>time0</code>, <code>time1</code>, ..., <code>val0</code>, <code>val1</code>, ...</li> <li>Virtual signals now support the new type specification strings.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/Generic%20%28Universal%29%20Signal%20Vectors.html#performance","title":"Performance","text":"<p>Performance testing shows a slight improvement over the old implementation, likely because the new approach avoids C function callbacks, allowing better compiler optimization.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedDictionary.html","title":"MedDictionary","text":"<p>The <code>MedDictionary</code> class provides methods for mapping integer values to strings, which is essential since MedRepository does not support free text and relies on numerical codes for efficiency (e.g., drug names or read codes). This class enables translating these codes back to their textual descriptions.</p> <p>Additionally, MedDictionary supports defining sets of values and assigning them names, which is useful for organizing hierarchical classifications (such as ICD codes, drug categories, or cancer types). Efficient membership testing for these sets is also provided.</p> <p>To handle multiple dictionaries with potentially overlapping numerical codes, MedDictionary uses a sections mechanism.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedDictionary.html#dictionary-file-format","title":"Dictionary File Format","text":"<p>When loading a repository, dictionary files are read with these rules:</p> <ul> <li>Ignore empty lines and lines starting with <code>#</code> (comments).</li> <li>All other lines are tab-delimited.</li> <li>Section definition: <code>SECTION &lt;comma-separated list of section names&gt;</code>   Each section can have multiple names. Typically, a section contains all relevant signal names.</li> <li>Value definition: <code>DEF &lt;numerical int value&gt; &lt;string&gt;</code>   Multiple names can be assigned to the same value. Each set must have a DEF line for its name. Each int value must be unique.</li> <li>Set membership: <code>SET &lt;set name&gt; &lt;member name&gt;</code>   Sets can include other sets, but avoid cyclic definitions.</li> </ul> <p>Example dictionary file: <pre><code># Section definition\nSECTION RC_Diagnosis,Cancer_Location,DM_Registry,HT_Registry,CVD_MI,CVD_HeartFailure,CVD_HemorhagicStroke,CVD_IschemicStroke,CKD_State,DEATH\n\n# Value definitions\nDEF     0       DM_Registry_Non_diabetic\nDEF     1       DM_Registry_Pre_diabetic\nDEF     2       DM_Registry_Diabetic\n\n# Set definitions (each set gets a unique int value)\nDEF     21000   Colon_Cancer\nDEF     21001   CRC_Cancer\nDEF     21002   Stomach_Cancer\nDEF     21003   Rectum_Cancer\n\n# Set memberships\nSET     Colon_Cancer    Digestive Organs,Digestive Organs,Colon\nSET     Stomach_Cancer  Digestive Organs,Digestive Organs,Stomach\nSET     Rectum_Cancer   Digestive Organs,Digestive Organs,Rectum\nSET     CRC_Cancer      Colon_Cancer\nSET     CRC_Cancer      Rectum_Cancer\nSET     CRC_and_Stomach_Cancer  CRC_Cancer\nSET     CRC_and_Stomach_Cancer  Stomach_Cancer\n</code></pre></p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedDictionary.html#meddictionary-vs-meddictionarysections","title":"MedDictionary vs. MedDictionarySections","text":"<ul> <li>MedDictionary: Handles a single dictionary with one namespace for numerical values.</li> <li>MedDictionarySections: Manages multiple dictionaries, each in its own section, with APIs for section management.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedDictionary.html#initializing-dictionaries","title":"Initializing Dictionaries","text":"<p>Dictionaries are automatically initialized when a repository is loaded, using the dictionary files specified in the repository config. To manually initialize, use <code>read(vector&lt;string&gt; &amp;input_dictionary_files)</code> for either MedDictionary or MedDictionarySections. Dictionaries are typically used within a MedRepository, but can be used independently.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedDictionary.html#key-methods","title":"Key Methods","text":"<p>MedDictionary:</p> <ul> <li><code>int id(const string &amp;name)</code>: Get id from name.</li> <li><code>string name(int id)</code>: Get name from id (returns the last name if multiple exist).</li> <li><code>map&lt;int, vector&lt;string&gt;&gt; Id2Names</code>: Maps id to all its names.</li> <li><code>int is_in_set(int member_id, int set_id)</code>: Check if member id is in set.</li> <li><code>int is_in_set(const string&amp; member, const string&amp; set_name)</code>: Same as above, using names.</li> <li><code>int prep_sets_lookup_table(const vector&lt;string&gt; &amp;set_names, vector&lt;char&gt; &amp;lut)</code>: Creates a fast lookup table for set membership.</li> <li><code>int prep_sets_indexed_lookup_table(const vector&lt;string&gt; &amp;set_names, vector&lt;unsigned char&gt; &amp;lut)</code>: Similar, but notes the serial number of the set for each member.</li> </ul> <p>MedDictionarySections:</p> <ul> <li><code>int section_id(const string &amp;name)</code>: Get section id from name.</li> <li><code>vector&lt;MedDictionary&gt; dicts</code>: Access a dictionary by section id and use all MedDictionary methods.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedDictionary.html#code-example","title":"Code Example","text":"<pre><code>// Initialize repository (which initializes MedDictionarySections)\nMedRepository rep;\n\n// Print the name for a drug code\nint section_id = rep.dict.section_id(\"Drug\");\ncout &lt;&lt; \"Name of Drug code \" &lt;&lt; drug_val &lt;&lt; \" is \" &lt;&lt; rep.dict.dicts[section_id].name(drug_val);\n\n// Scan a list of patient IDs for drug usage in specific sets\nvector&lt;int&gt; pids; // Assume populated\nvector&lt;string&gt; drug_sets = {\"ATC_B04A_B__\", \"ATC_C10A_A__\", \"ATC_B01A____\"}; // Example sets\n\n// Prepare lookup table for fast membership testing\nvector&lt;char&gt; lut;\nrep.dict.dicts[rep.dict.section_id(\"Drug\")].prep_sets_lookup_table(drug_sets, lut);\n\nUniversalSigVec usv;\nint drug_sid = rep.sigs.sid(\"Drug\");\n\n// Iterate over patients\nfor (auto &amp;pid : pids) {\n    rep.uget(pid, drug_sid, usv);\n    for (int i = 0; i &lt; usv.len; i++) {\n        int drug_val = (int)usv.Val(i, 0);\n        if (lut[drug_val])\n            cout &lt;&lt; \"Patient \" &lt;&lt; pid &lt;&lt; \" is using drug code \" &lt;&lt; drug_val &lt;&lt; \" at time \" &lt;&lt; usv.Time(i,0); // Print usage\n    }\n}\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html","title":"MedRepository","text":""},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#overview","title":"Overview","text":"<p><code>MedRepository</code> and its extension <code>MedPidRepository</code> are core classes in the InfraMed library, designed to facilitate reading, managing, and querying repository data. Familiarity with these classes is essential for effective use of the library.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#medrepository-vs-medpidrepository","title":"MedRepository vs. MedPidRepository","text":"<p><code>MedPidRepository</code> builds on <code>MedRepository</code> by providing APIs to load a complete patient record (<code>PidRec</code>). This is particularly useful when iterating over patient IDs and needing all associated data for each one.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#initializing-and-loading-data","title":"Initializing and Loading Data","text":"<p>There are several ways to initialize and load data into a <code>MedRepository</code>:</p> <ul> <li> <p>Using <code>read_all()</code>   Loads the repository configuration, a list of signal names (or all signals if the list is empty), and a list of patient IDs (or all if empty). The function loads the full cartesian product of requested pids and signals into memory, making the repository ready for queries.</p> </li> <li> <p>Using <code>init()</code>   Initializes the repository with the configuration file, loading signals, dictionaries, and file locations, but not the actual data. Data can be loaded later using <code>load()</code> and released with <code>free()</code>.</p> </li> <li> <p>Using <code>load()</code> and <code>free()</code> <code>load()</code> loads data for a specific signal and list of pids (or all if the list is empty). <code>free()</code> releases memory for a specific signal, which is useful for handling large datasets.</p> </li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-initializing-and-loading-a-repository","title":"Example: Initializing and Loading a Repository","text":"<pre><code>// Define repository variable\nMedRepository rep;\n\n// Repository configuration file\nstring rep_conf = \"/home/Repositories/THIN/thin.repository\";\n\n// Option 1: Load all signals and pids\nrep.read_all(rep_conf);\n\n// Option 2: Load specific signals and pids\nvector&lt;string&gt; sigs = {\"BDATE\", \"GENDER\", \"Glucose\", \"HbA1C\", \"BMI\", \"Drug\"};\nvector&lt;int&gt; pids = {5000001, 50000002, 10000000};\nrep.read_all(rep_conf, pids, sigs);\n\n// Option 3: Initialize without loading data\nrep.init(rep_conf);\n\n// Load a subset of signals and pids\nrep.load(sigs, pids);\n\n// Free signals from memory\nrep.free(sigs);\nrep.free_all_sigs();\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#accessing-data-in-medrepository","title":"Accessing Data in MedRepository","text":"<p>Once data is loaded, you can access it using the <code>get()</code> function (Old API), which returns a time-sorted vector of values for a specific pid and signal. The result is a void pointer that should be cast to the appropriate signal type. Please use <code>uget()</code> instead and get a generic signal of type <code>UniversalSigVec</code></p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-accessing-data","title":"Example: Accessing Data","text":"<pre><code>// Repository variable\nMedRepository rep;\n\n// Assume repository is initialized and loaded\n\nint pid;\nstring sig;\n\n// Convert signal name to signal ID for performance\nint sid = rep.sigs.sid(sig);\n\n// Output variable for length\nint len;\n\n// Access DateVal signal with deprecated Data Repository format. avoid using.\nSDateVal *sdv = (SDateVal *)rep.get(pid, sig, len);\n// Access Generic signal with variable time and values channels\nUniversalSigVec usv;\nrep.uget(pid, sig, usv);\n\n\nprintf(\"read %d items\\n\", len);\nif (len &gt; 0)\n    printf(\"First item: time %d, val %f. Last item: time %d, val %f\\n\", sdv[0].date, sdv[0].val, sdv[len-1].date, sdv[len-1].val);\n\n// Faster access using signal ID - please use uget\nsdv = (SDateVal *)rep.get(pid, sid, len);\nrep.uget(pid, sid, usv);\n\n// Access a different signal type - please use uget\nSVal *sv = (SVal *)rep.get(pid, \"BDATE\", len);\nrep.uget(pid, \"BDATE\", usv);\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#medpidrepository-initialization-and-usage","title":"MedPidRepository: Initialization and Usage","text":"<p><code>MedPidRepository</code> enables loading data for individual pids into memory, which is useful for scanning large repositories with minimal memory usage. Each thread can hold data for a single patient, rather than all data at once.</p> <p>To use <code>MedPidRepository</code>, ensure the repository is transposed during its creation. After setup, use a <code>PidRec</code> object to store data for a specific patient ID and access signals with either <code>get()</code> or <code>uget()</code>.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-using-medpidrepository-and-pidrec","title":"Example: Using MedPidRepository and PidRec","text":"<pre><code>// Create MedPidRepository and PidRec instances\nMedPidRepository pid_rep;\nPidRec rec;\n\n// Initialize the repository\npid_rep.init(rep_conf_file);\n\n// Load data for a specific patient\npid_rep.get_pid_rec(pid, rec);\n\n// Access a signal using the legacy API\nSDateVal *sdv = rec.get(sig_name, len);\n\n// Access a signal using the recommended API\nUniversalSigVec usv;\nrec.uget(sig_name, usv);\n</code></pre> <p>You can also initialize a <code>PidRec</code> from an existing <code>MedRepository</code> using <code>init_from_rep()</code>. This copies the patient record and allows for dynamic modifications.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#example-initializing-pidrec-from-medrepository","title":"Example: Initializing PidRec from MedRepository","text":"<pre><code>MedRepository rep;\nPidRec rec;\n\n// Repository should be initialized as shown earlier\n\nvector&lt;int&gt; sids; // Leave empty to load all signals\nrec.init_from_rep(&amp;rep, pid, sids);\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedRepository.html#performance-tips","title":"Performance Tips","text":"<ul> <li>Store repositories locally (e.g., <code>/home/Repositories</code> on Linux) for optimal speed.</li> <li>When processing many patient IDs, use signal IDs instead of names for faster lookups:   <pre><code>int pid;\nstring sig_name;\nint sid = rep.sigs.id(sig_name);\nUniversalSigVec usv;\n\nfor (...) {\n    rec.uget(sig_name, usv);\n    // ...process data...\n}\n</code></pre></li> <li>Load only the signals you need</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html","title":"MedSignals / Unified Signals","text":"<p>MedSignals is an internal class in MedRepository that defines signal types and manages <code>.signals</code> files-dictionaries mapping signal names to their properties. Each MedRepository instance has a <code>sigs</code> object for basic <code>.signals</code> operations. All signal definitions are found in <code>MedSignals.h</code>.</p> <p>When initializing a repository, one of the first steps is to read the signals configuration file (specified in the repository config) and set up all signal definitions.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#what-is-a-signal","title":"What is a signal?","text":"<p>A signal type is a class defined in <code>MedSignals.h</code> and acts as a placeholder for signals. Signals represent actual data stored in the repository, each with a specific type. For example:</p> <ul> <li>BYEAR, GENDER, TRAIN: Store a single float value, typically using the <code>SVal</code> type, or as a generic signal with one integer value channel: \"V(i)\"</li> <li>Hemoglobin, Creatinine: Store a date and a float value, using <code>SDateVal</code> or <code>STimeVal</code>, or as a generic signal with one integer time channel and one float value channel: \"T(i),V(f)\"</li> <li>BP: Stores a date and two short values (diastolic and systolic blood pressure), using <code>SDateShort2</code>, or as a generic signal: \"T(i),V(s,s)\"</li> </ul> <p>Each signal has several properties, defined in the <code>SignalInfo</code> class:</p> <ul> <li>name: Signal name</li> <li>type: Type code (from the <code>SigType</code> enum) or a generic type string. You can use <code>get_signal_generic_spec()</code> on <code>UniversalSigVec</code> to get its generic signal string representation.</li> <li>sid: Unique integer signal ID from the signals config file, used for fast indexing and efficient access. Maximum SID is <code>MAX_SID_NUM</code> (currently 100,000).</li> <li>bytes_len: Number of bytes needed to store one element.</li> <li>n_time_channels: Number of time channels.</li> <li>n_val_channels: Number of value channels.</li> </ul> <p>Signals must currently have a fixed size.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#unified-signals-part-i","title":"Unified Signals - Part I","text":"<p>Most medical record signals can be described as having zero or more time channels and zero or more value channels. Examples:</p> <ul> <li>BYEAR: 0 time channels, 1 float value channel</li> <li>Creatinine: 1 integer time channel, 1 float value channel</li> <li>BP: 1 integer time channel, 2 float value channels</li> <li>DM_Registry: 2 integer time channels, 1 int/char value channel</li> </ul> <p>When time channels are integers and value channels are floats, signals can be accessed in a unified way, regardless of their type. This allows for generalized code in components like <code>RepProcessors</code> and <code>FeatureGenerators</code> in MedProcessTools.</p> <p>See Part II for usage examples.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#current-signal-types","title":"Current Signal Types","text":"<p>All signal types are defined in <code>MedSignals.h</code>. Available types include:</p> Signal Type Time Channels Value Channels Placeholders Example Signals SVal 0 1 float BYEAR, TRAIN, GENDER SDateVal 1 1 int : float Hemoglobin, Creatinine STimeVal 1 1 long long : float Creatinine (mimic) SDateRangeVal 2 1 int, int : float DM_Registry STimeRangeVal 2 1 long long, long long : float (mimic) STimeStamp 0 1 : long long (mimic) SDateVal2 1 2 int : float, unsigned short Drugs (MHS) STimeLongVal 1 1 long long : long long (mimic) SDateShort2 1 2 int : short, short BP SValShort2 0 2 : short, short (mimic) SValShort4 0 4 : short, short, short, short (mimic)"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#adding-a-new-signal-type","title":"Adding a New Signal Type","text":"<p>To add a new legacy signal type:</p> <ol> <li>Add an enum value for the type (used for encoding in the signals file).</li> <li>Define the signal class and its methods.</li> <li>Update <code>MedConvert</code> to read the new signal type (add parsing logic if needed).</li> <li>Make sure the new signal class inherits from <code>UnifiedSig</code> for unified signal compatibility.</li> <li>Implement these methods:    - <code>n_time_channels()</code>    - <code>n_val_channels()</code>    - <code>time_unit()</code>    - <code>int Time(int chan)</code>    - <code>float Val(int chan)</code>    - <code>SetVal(chan, _val)</code>    - <code>Set(int *times, float *vals)</code>    - Comparison operators (<code>&lt;</code>, <code>==</code>)    - Output operator (<code>&lt;&lt;</code>)</li> </ol>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#unified-signals-part-ii","title":"Unified Signals - Part II","text":"<p>To work with unified signals, use the <code>UniversalSigVec</code> (<code>usv</code>) class. Instead of the standard <code>get()</code> method, use <code>uget()</code> to load signal data into a <code>usv</code> object. Key features:</p> <ul> <li>Initialize for a specific signal type using only the enum.</li> <li>Use <code>uget()</code> to load data (no copies; uses pointers and virtual functions).</li> <li>Access any time or value channel uniformly across all signal types.</li> </ul> <p>This lets you write type-independent code. Initialization has a performance cost, but repeated use with the same type is efficient.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#example-usage","title":"Example Usage","text":"<pre><code>// Initialize repository and UniversalSigVec\nMedRepository rep;\nUniversalSigVec usv;\n\n// ... initialize repository ...\n\nrep.uget(pid, sid, usv);\n\n// Print all time and value channels for each element\nfor (int i = 0; i &lt; usv.len; i++) {\n    for (int t_ch = 0; t_ch &lt; usv.n_time_channels(); t_ch++)\n        cout &lt;&lt; \"Element \" &lt;&lt; i &lt;&lt; \", time channel \" &lt;&lt; t_ch &lt;&lt; \": \" &lt;&lt; usv.Time(i, t_ch);\n    for (int v_ch = 0; v_ch &lt; usv.n_val_channels(); v_ch++)\n        cout &lt;&lt; \"Element \" &lt;&lt; i &lt;&lt; \", value channel \" &lt;&lt; v_ch &lt;&lt; \": \" &lt;&lt; usv.Val(i, v_ch);\n}\n\n// Usage with PidRec\nMedPidRepository pid_rep;\nPidRec rec;\n\n// ... initialize pid_rep ...\n\npid_rep.get_pid_rec(pid, rec);\nrec.uget(sid, usv);\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/MedSignals%20_%20Unified%20Signals.html#signals-config-file","title":"Signals Config File","text":"<p>For details on the signals file</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html","title":"PidDynamicRec","text":"<p>The <code>PidDynamicRec</code> class provides advanced access to signal data, allowing both reading and modification, as well as maintaining multiple \"versions\" of a signal vector. This is particularly useful when implementing RepProcessors in the <code>MedProcessTools</code> classes, where signals may need to be cleaned, altered, or removed before feature creation.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#why-are-multiple-versions-necessary","title":"Why are multiple versions necessary?","text":"<p>When running a RepProcessor and modifying a signal at timepoint T, data from future timepoints (t &gt; T) might inadvertently influence the change, leading to data leakage during model training and testing. For example, if testing at T_test (where T_test \u2265 T) and using data from t &gt; T_test, future information contaminates the test.</p> <p>The complexity increases when testing at multiple timepoints for a patient (e.g., T_test1 and T_test2, with T_test2 &gt; T_test1). The allowed \"horizon\" for each test is:</p> <ul> <li>For T_test1: t \u2264 T_test1</li> <li>For T_test2: t \u2264 T_test2</li> </ul> <p>Values generated by RepProcessors for t \u2264 T_test1 may differ between these horizons, requiring snapshots (\"versions\") of the data as seen up to each timepoint. Efficient management of these versions is essential for performance and memory usage.</p> <p><code>PidDynamicRec</code> addresses these challenges by enabling efficient versioning.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#version-numbering","title":"Version Numbering","text":"<ul> <li>Version 0: The original data from the repository.</li> <li>Versions 1+: Created and managed using <code>PidDynamicRec</code> tools, typically corresponding to different prediction timepoints.</li> </ul>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#initializing-a-piddynamicrec","title":"Initializing a PidDynamicRec","text":"<p><code>PidDynamicRec</code> inherits from <code>PidRec</code> and is initialized similarly, with the addition of specifying the number of versions to maintain (usually matching the number of prediction timepoints). Use the <code>set_n_versions()</code> method:</p> <pre><code>int init_from_rep(MedRepository *rep, int pid, vector&lt;int&gt; &amp;sids_to_use, int n_versions);\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#reading-from-a-piddynamicrec","title":"Reading from a PidDynamicRec","text":"<p>To read data, specify both the signal and the version:</p> <pre><code>void *get(int sid, int version, int &amp;len);\nvoid *get(string &amp;sig_name, int version, int &amp;len);\nvoid *uget(int sid, int version, UniversalSigVec &amp;_usv);\nvoid *uget(const string &amp;sig_name, int version, UniversalSigVec &amp;_usv);\n</code></pre> <p>Each <code>PidRec</code> maintains a <code>usv</code> object for thread safety and efficiency.</p>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#modifying-versions-in-piddynamicrec","title":"Modifying Versions in PidDynamicRec","text":"<p>The core purpose of <code>PidDynamicRec</code> is to create and manage versions that differ from the original. Initially, all versions reference version 0.</p> <p>Key methods for version management:</p> <pre><code>// Set a version to new data\nint set_version_data(int sid, int version, void *datap, int len);\n\n// Copy original data into a new version\nint set_version_off_orig(int sid, int version);\n\n// Point one version to another's data\nint point_version_to(int sid, int v_src, int v_dst);\n\n// Remove an element from a version\nint remove(int sid, int version, int idx);\n\n// Remove from one version and place in another\nint remove(int sid, int v_in, int idx, int v_out);\n\n// Change an element in a version\nint change(int sid, int version, int idx, void *new_elem);\n\n// Change in one version and copy to another\nint change(int sid, int v_in, int idx, void *new_elem, int v_out);\n\n// Batch update: changes and removals\nint update(int sid, int v_in, vector&lt;pair&lt;int, void *&gt;&gt;&amp; changes, vector&lt;int&gt;&amp; removes);\n\n// Batch update with value channel\nint update(int sid, int v_in, int val_channel, vector&lt;pair&lt;int, float&gt;&gt;&amp; changes, vector&lt;int&gt;&amp; removes);\n</code></pre> <p>Example: Cleaning Glucose Signal</p> <pre><code>MedRepository rep;\nint pid;\n// ... load repository and set pid ...\n\nPidDynamicRec pdr;\nint sid = rep.sigs.sid(\"Glucose\");\nvector&lt;int&gt; sids = { sid };\npdr.init_from_rep(&amp;rep, pid, sids, 1); // Initialize with one extra version\n\nUniversalSigVec usv;\npdr.uget(sid, 0, usv); // Get original data\n\n// Option 1: Create a new vector and set as version 1\nvector&lt;SDataVal&gt; new_glu;\nfor (int i = 0; i &lt; usv.len; i++) {\n    if (usv.Val(i, 0) &gt; 0) {\n        SDataVal sdv;\n        sdv.date = usv.Time(i, 0);\n        sdv.val = (float)((int)usv.Val(i, 0));\n        new_glu.push_back(sdv);\n    }\n}\npdr.set_version_data(sid, 1, &amp;new_glu[0], (int)new_glu.size());\n\n// Option 2: Batch update\nvector&lt;pair&lt;int, float&gt;&gt; changes;\nvector&lt;int&gt; removes;\nfor (int i = 0; i &lt; usv.len; i++) {\n    if (usv.Val(i, 0) &gt; 0) {\n        changes.push_back({i, (float)((int)usv.Val(i, 0))});\n    } else {\n        removes.push_back(i);\n    }\n}\npdr.update(sid, 1, 0, changes, removes);\n\n// Read modified version\npdr.uget(sid, 1, usv);\n// usv now contains the cleaned data\n</code></pre>"},{"location":"Infrastructure%20Library/00.InfraMed%20Library%20page/PidDynamicRec.html#efficiency-version-pointing-and-iteration","title":"Efficiency: Version Pointing and Iteration","text":"<p>Often, multiple versions are identical (e.g., when only considering data up to a given timepoint). To optimize, versions can \"point\" to the same data, only splitting when changes are needed.</p> <p>Mechanisms: 1. Version Pointing: A version can reference another's data, splitting only when modifications occur. 2. Iterators: Use iterators to process blocks of versions sharing the same data.</p> <p>API Examples:</p> <pre><code>class PidDynamicRec : public PidRec {\n    // Point one version to another\n    int point_version_to(int sid, int v_src, int v_dst);\n\n    // Check if two versions share data\n    int versions_are_the_same(int sid, int v1, int v2);\n    int versions_are_the_same(set&lt;int&gt; sids, int v1, int v2);\n    // ...\n};\n\n// Iterator for blocks of identical versions\nclass differentVersionsIterator : public versionIterator {\n    int jVersion;\n    int init();\n    int next();\n    bool done() { return iVersion &lt; 0; }\n    inline int block_first() { return jVersion + 1; }\n    inline int block_last() { return iVersion; }\n};\n</code></pre> <p>Iterating Over Versions Example:</p> <pre><code>_apply(PidDynamicRec&amp; rec, vector&lt;int&gt;&amp; time_points, ... ) {\n    differentVersionsIterator vit(rec, reqSignalIds);\n    for (int iver = vit.init(); !vit.done(); iver = vit.next()) {\n        // Process versions from vit.block_first() to vit.block_last()\n        // All versions in this block point to the same data, optimizing performance\n    }\n}\n</code></pre>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/index.html","title":"Rep Processors Practical Guide","text":"<p>This page intends to list current RepProcessors with explanations on parameters and a json line example. Available components for selection in <code>rp_type</code>:</p> <ul> <li><code>basic_cln</code> : Basic outlier cleaner : learn ditribution and throw extreme values</li> <li><code>nbrs_outlier_cleaner</code> : Neighborhood outlier cleaner</li> <li>configured_outlier_cleaner: Configured outlier cleaner - Wrapper for\u00a0basic_cln with fixed bounderies for each signal</li> <li>rulebased_outlier_cleaner: Rule Based outlier cleaner - Rules based on panels (for example BMI) to remove mismatched values in panels</li> <li>calculator: virtual signals calculator - Virtual signals calculator - linear sum, log, and more..</li> <li>complete: panel completer - Completes signals using panels calculations. searches for signals in exact same time that are relate to calculate (TODO: time window support). for example BMI = Weight/Height^2</li> <li><code>req</code> : check requirement processor</li> <li><code>sim_val</code> : sim val processor\u00a0- When signal has more than one value in same time - which one to choose</li> <li><code>signal_rate</code> : signal rate processor - divide signal value by time diff in 2 time channels that describe the signal period</li> <li><code>combine</code> : combine signals processor - combines 2 signals to 1 signal with same name</li> <li><code>split</code> : split signal processor - splits signal by rule for 2 different signals (to apply different rules for example on each split)</li> <li><code>aggregation_period</code> : create periods signal of categorical signals</li> <li><code>basic_range_cleaner</code> : range cleaner</li> <li><code>aggregate</code> : get signals in or out of a given period signal</li> <li>create_registry : create a virtual signal of a registry to some common medical situations (Diabetes, Hypertension)</li> <li>limit_history : limit or eliminate signals. Needed mainly as a pre processor</li> <li>noiser : Noises the input raw signals, drops values (like \"dropouts\")</li> </ul> <p>[!NOTE] The arguments for each component are not always shown in this wiki. You can either see an examples of common components or look at the code <code>::init</code> function for the arguments of each component.</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/index.html#rep-processors-usage-in-a-medmodel","title":"Rep Processors usage in a MedModel","text":"<p>Rep processors are the first elements running in a MedModel run. They are packed into MultiProcessors, each containing 1 or more rep processors. Each such multi processor contains processors allowed to run in parallel to each other, and indeed the MedModel will parallelize those runs. This is true for both learn and apply stages (mainly for learn, as in apply we anyway parallize on the pids records). Hence once defined it is important to pack the processors in the way that allows maximal parallelism. Example of packing processors when defining them in a json file: <pre><code>// new format example\n\u00a0\n\"model_json_version\": \"2\",\n\"serialize_learning_set\": \"0\",\n\u00a0\n// pack all actions (rp, fg, fp) into model_actions\n  \"model_actions\": [\n// add a group of first rep processors (will run first and in parallel)\n    { \"action_type\": \"rp_set\",\n      \"members\": [\n        { \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"ALT\", \"Na\" ] }, //Not used in LGI\n        { \"rp_type\": \"basic_cln\", \"type\": \"iterative\", \"range\": \"range_min=0.0001;range_max=10000\", \"trimming_sd_num\": \"7\",\"removing_sd_num\": \"14\", \"signal\": \"Hemoglobin\"} //USED in LGI\n      ]\n    },\n// add another group (will run after first group and in parallel)\n    { \"action_type\": \"rp_set\",\n      \"members\": [\n        { \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"RBC\", \"WBC\" ] },\n        { \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": \"Creatinine\"}\n      ]\n    }\n\u00a0\n// continue with your json ...\n    ]\n\u00a0\n// old format example\n  \"processes\": {\n// rep processors in group 0 (running first and in parallel)\n    \"process\" : { \"process_set\": \"0\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"ALT\", \"Na\" ] },\n    \"process\" : { \"process_set\": \"0\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": \"Hemoglobin\" },\n// rep processors in group 1 (running after first and in parallel)\n    \"process\" : { \"process_set\": \"1\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": [ \"RBC\", \"WBC\" ] },\n    \"process\" : { \"process_set\": \"1\", \"rp_type\": \"basic_cln\", \"type\": \"quantile\", \"range\": \"range_min=0.100;range_max=6500.000\",  \"signal\": \"Creatinine\" }\n\u00a0\n// continue with your json ....\n}\n</code></pre> </p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/index.html#running-with-a-pre-processor","title":"Running with a pre processor","text":"<p>The pre processor mechainsm allows one to define new rep processors to be added to a ready model, in order to be run before all the pre processors of the model. This allows for example limiting the history of a model, or deleting signals in an elegant way when searching for signal importance and minimal requirements.\u00a0 A pre processor hence, will typically be one that has an empty learn() and relies only on user given parameters, and is used only at apply time. The most convenient way to use rep_processors is using a json defining them and the\u00a0add_pre_processors_json_string_to_model API in MedModel. Example json: <pre><code># example : limiting some signals to tests done at a window of 1 year before prediction time\n{\n        \"pre_processors\" : [ {\"rp_type\" : \"history_limit\" , \"signal\" : \"ref:signals\", \"win_from\" : \"0\" , \"win_to\" : \"365\"} ] ,\n        \"signals\" : [\"Hemoglobin\", \"MCV\", \"MCH\"]\n}\n</code></pre> \u00a0 The Flow App has an option to get predictions of a model with an added pre_processors file. Use the following: <pre><code>Flow --rep data.repository --get_model_preds --f_model my.model --f_samples test.samples --f_pre_json pre.json --f_preds output.preds\n</code></pre></p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/index.html#create-registy-processor","title":"Create Registy Processor","text":"<p>Creates a registry for the chosen medical condition. Currently implemented are hypertension and diabetes.</p> <ul> <li>name : \"create_registry\" Rough registry definition:</li> </ul> <ol> <li>Diabetes:<ol> <li>Diabetes : one of:<ol> <li>2 tests within 2y of Glucose above 125, or HbA1C above 6.5 (the second one)\u00a0\u00a0</li> <li>1 test of Glucose above 200, or HbA1C above 8.0</li> <li>A diagnostic code</li> <li>Starting point of using diabetes drugs.</li> </ol> </li> </ol> </li> <li>PreDiabetes:  <ol> <li>Non Diabetic</li> <li>1 Glucose test above 110, or 1 HbA1C above 5.7</li> <li>2 Glucose tests within 2y above 100 (the second), but still not diabetic.</li> <li>Not taking diabetes drugs</li> </ol> </li> <li>Healthy   <ol> <li>Non Diabetic</li> <li>Non PreDiabetic</li> <li>Normal range Glucose/HbA1C tests period.</li> </ol> </li> <li>HyperTension:\u00a0</li> </ol> <p>Example: Example<pre><code>    {\n      \"rp_type\":\"create_registry\",\n      \"registry\":\"ht\",\n      \"names\":\"my_HT_Registry\",\n      \"ht_systolic_first\": \"1\",\n      \"ht_drugs\":\"list_rel:registries/ht_drugs.full\",\n      \"ht_identifiers\":\"list_rel:registries/ICD10/hyper_tension.2020.desc\",\n      \"chf_identifiers\":\"list_rel:registries/ICD10/heart_failure_events.2020.desc\",\n      \"mi_identifiers\":\"list_rel:registries/ICD10/mi.2020.desc\",\n      \"af_identifiers\":\"list_rel:registries/ICD10/AtrialFibrilatioReadCodes.2020.desc\",\n      \"ht_chf_drugs\":\"ATC_C03_____\",\n      \"ht_dm_drugs\":\"ATC_C09A____,ATC_C09C____\",\n      \"ht_extra_drugs\":\"ATC_C07A_A__,ATC_C07A_B__\",\n      \"signals\":\"BP,DIAGNOSIS,Drug,BDATE,_v_DM_Registry\"\n    },\n</code></pre></p> <ol> <li>The code (RepCreateRegistry) uses the following definitions -\u00a0<ol> <li>High blood pressure -\u00a0Diastolic BP over 90 or Systolic BP over 140 (for people younger than 60) or 150 (for people 60 or older)</li> <li>Drugs - Relevant drugs are divided into 4 groups. The first is always indicative of HT. The second is indicative of HT unless there are other indication of CHF. The third indicative of HT unless there are other indicaiton of Diabetes. The last is indicative of HT uless other indications of CHF, Diabetes or MI </li> </ol> </li> <li>An individual is considered non hyper-tensive, as long as there are no tests showing high blood pressure, no Read (ICD9/10) codes and no drugs indicative of hypertension</li> <li>Conditions for HT positive:   <ol> <li>The first appearance of HT diagnosis (note - until 3/3/25 we waited for 2nd indication)</li> <li>Two consequtives high BP tests, without normal BP test between them</li> <li>Drug indication after high BT test</li> <li>Two drug indications, less than\u00a0'ht_drugs_gap' days apart</li> <li>With just one bad test / drug indication - the status is 'gray' however considered as not HT</li> </ol> </li> <li>After first HT positive =&gt; always HT positive</li> <li>Proteinuria: <ol> <li>Goes over a list of urine tests</li> <li>For the categorial ones: each has the categories that match states of normal, medium, or severe proteinuria.   </li> <li>For the value based tests : ranges are given for the normal, medium and proteinuria stages.</li> <li>The code goes over all the given tests, and in each day there's a test records if it was normal (0) , medium (1) , or severe (2) .</li> <li>If there are several tests in the same day, the worst of them is considered as the result to use.</li> </ol> </li> <li>The output is a DateVal signal with times and values of 0,1,2 matching normal, medium, severe states (note this is DIFFERENT from the 0-4 proteinuria states we used in the past and which are calculated in the Diabetes registries calculator.</li> <li>CKD: <ol> <li>Goes over eGFR and Proteinuria states and calculates at each time point the CKD state (0-4) based on the last known values of eGFR and Proteinuria.</li> <li>In many cases it would be recommended to have a double layered calculation in which:<ol> <li>Layer 1 creates the virtual signals needed for eGFR and Proteinuria</li> <li>Layer 2 calculates the CKD levels based on Layer1 results.</li> </ol> </li> </ol> </li> </ol> <p>parameters: general:</p> <ul> <li>registry : \"dm\" for siabetes, \"ht\" for hypertension</li> <li>names : the name/names of the virtual signals created by the processor, these will hold the actual registry signal.</li> <li>signals : the signals the rep depends on, in case of working with slightly different signal names than the defaults<ul> <li>defaults for dm :\u00a0\"Glucose\",\"HbA1C\",\"Drug\",\"RC\"</li> <li>defaults for ht :\u00a0\"BP\",\"RC\",\"Drug\",\"BYEAR\",\"DM_Registry\"</li> <li>defaults for proteinuria: all relevant rine tests :\u00a0\"Urine_Microalbumin\", \"UrineTotalProtein\" , \"UrineAlbumin\" , \"Urine_dipstick_for_protein\" , \"Urinalysis_Protein\" , \"Urine_Protein_Creatinine\" , \"UrineAlbumin_over_Creatinine\"</li> <li>defaults for ckd : in ckd it is always reccomended to create the proteinuria signal and then use it, see the examples below.</li> </ul> </li> <li>time_unit : of repository (can rely on default though)</li> <li>registry_values : the names of the registry values created in a dictionary (first will be 0, second 1, and on....) diabetes related:</li> <li>dm_drug_sig ,\u00a0dm_diagnoses_sig ,\u00a0dm_glucose_sig ,\u00a0dm_hba1c_sig : the names for the matching signals, defaults are respectively Drug , RC , Glucose , HbA1C</li> <li>dm_drug_sets : the drug sets to be used as defining a diabetic patient.</li> <li>dm_diagnoses_sets : the set of codes for diabetes</li> <li>dm_diagnoses_severity : 3 or 4 : 3 means a diagnoses needs more supporting evidence (such as bad glucose tests) to decided diabetic, 4 means the code is enough on its own. hypertension related:</li> <li>ht_identifiers: Read (ICD9/10) codes for hypertension</li> <li>chf_idientifiers, mi_identifiers, af_identifiers: Read (ICD9/10) codes for CHF, MI and AF retrospectively</li> <li>ht_drugs: Drugs sets for hyper-tenstion</li> <li>ht_chf_drugs, ht_dm_drugs, ht_extra_drugs: Drugs sets for hyper-tension unless there is other indication of CHF, Diabetes and CHF/Diabetes/MI, retrospectively</li> <li>ht_drugs_gap: See above for details proteinuria related:</li> <li>urine_tests_categories : a listing of the urine tests to use, each with a bit static if it is numeric or not, and then categories or ranges for the signal for the normal, medium and severe states.\u00a0</li> <li>Example:\u00a0 \"Urine_Microalbumin:1:0,30:30,300:300,1000000/UrineTotalProtein:1:0,0.15:0.15,0.60:0.60,1000000/UrineAlbumin:1:0,30:30,300:300,1000000/Urine_dipstick_for_protein:0:Urine_dipstick_for_protein_normal:Urine_dipstick_for_protein_medium:Urine_dipstick_for_protein_severe\"</li> <li>Note the usage of the '/' separator between signals, the use of ':' between the five fields for each signal, and the use of ',' within the ranges or categories fields. ckd related:</li> <li>ckd_egfr_sig : the name of the signal holding the eGFR</li> <li>ckd_proteinuria : the name of the signal holding the proteinuria signal (the 3 levels one\u00a0!!!) \u00a0 Json Examples:</li> </ul> <p><pre><code>//\n// creating a diabetes registry\n//\n{\"rp_type\": \"create_registry\", \"registry\" : \"dm\", \"names\" : \"_v_DM_Registry\",\n                          \"dm_drug_sets\" : \"list:/nas1/UsersData/avi/MR/Tools/Registries/Lists/diabetes_drug_codes.full\",\n                     \"dm_diagnoses_sets\" : \"list:/nas1/UsersData/avi/MR/Tools/Registries/Lists/diabetes_read_codes_registry.full.striped\"}\n\u00a0\n//\n// creating a hyper tension registry\n// It is important to clean and handle simultanous values beforehand\n// note that default lists of codes are in MR_Tools/Registries/Lists/\n//\n{\"action_type\":\"rep_processor\",\"rp_type\":\"basic_outlier_cleaner\",\"range_min\":\"0.001\",\"range_max\":\"100000\",\"val_channel\":\"0\",\"signal\":\"BP\"},\n{\"action_type\":\"rep_processor\",\"rp_type\":\"basic_outlier_cleaner\",\"range_min\":\"0.001\",\"range_max\":\"100000\",\"val_channel\":\"1\",\"signal\":\"BP\"},\n{\"action_type\":\"rep_processor\",\"rp_type\":\"sim_val\",\"type\":\"min\",\"signal\":\"BP\"},\n{\"action_type\":\"rep_processor\",\"rp_type\":\"create_registry\",\"registry\":\"ht\",\"names\":\"my_HT_Registry\"},\n//\n// creating a proteinuria registry\n//\n{\"action_type\":\"rep_processor\",\"rp_type\":\"create_registry\",\"registry\":\"proteinuria\",\"names\":\"_v_Proteinuria_State\"},\n\n//\n// creating a CKD registry, note we use the previously defined proteinuria registry\n//\n{\"action_type\":\"rep_processor\",\"rp_type\":\"create_registry\",\"registry\":\"ckd\", \"names\" : \"_v_CKD_State\" , \n                \"signals\" : \"_v_Proteinuria_State,eGFR_CKD_EPI\" ,\n                \"ckd_egfr_sig\" : \"eGFR_CKD_EPI\" , \n                \"ckd_proteinuria_sig\" : \"_v_Proteinuria_State\"},\n</code></pre> </p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Cleaners%20Json%20Examples.html","title":"Cleaners Json Examples","text":"<p>Examples for creating Jsons with default cleaner. An Example of cleaner can be found in git:\u00a0full_rep_processors.json \u00a0 full_cleaners is based on:</p> <ul> <li>configured simple cleaner with strict\u00a0boundaries\u00a0for each signal\u00a0</li> <li>sim_val - when signal appears with the same time and different value which value to take? (from my observations in THIN - taking the first value is better)</li> <li>panels calculation and remove mismatches of biological rules. For example wrong calculation of BMI Notes:</li> <li>There is a problem with\u00a0Cholesterol_over_HDL\u00a0signal in the loading process,\u00a0\u00a0so using full_cleaner is not recommended for now with the rule of the\u00a0Cholesterol_over_HDL activated (rules 15,17)</li> <li>You can now use Flow with \"pids_sigs_print\" mode or Yaron print program to print pids and signals after rep_processors. It may be\u00a0useful\u00a0for cleaners, virtual signals and more. I had created a\u00a0different\u00a0print (not in Flow) that shows the difference between 2 run modes of rep_processings (or compare run with rep processing to no rep processing at all) and prints the removed rows with \"[REMOVED]\" in each removed row to see what happened. for more information contact me. \u00a0 <pre><code>{\n  \"model_json_version\": \"2\",\n  \"serialize_learning_set\": \"0\",\n  \"model_actions\": [\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"time_channel\":\"0\",\n          \"clean_method\":\"confirmed\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\"\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        },\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"val_channel\":[\"0\", \"1\"],\n          \"clean_method\":\"confirmed\",\n          \"signal\": [\"BP\"]\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"sim_val\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\",\n          \"type\":\"first\",\n          \"debug\":\"0\"\n\n        }\n\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"rule_cln\",\n          \"addRequiredSignals\":\"1\",\n          \"time_window\":\"0\",\n          \"tolerance\":\"0.1\",\n          \"calc_res\":\"0.1\",\n          \"rules2Signals\":\"../settings/ruls2Signals.tsv\",\n          \"consideredRules\":[ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\" ] \n          //,\"verbose_file\":\"/tmp/panel_cleaning.log\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> \u00a0 This was tested with Cleaner Program that checks for filtered stats with examples. here are the filtered stats on THIN: Stats of simple cleaner To create this table with full examples: <pre><code>Flow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --rep_processor_print --sigs /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/all_rules_sigs.list --max_examples 10 --seed 0 --f_output /tmp/test.log --cleaner_path /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/only_configure.json \n</code></pre> \u00a0 Simple Filtering - only Configure Rules for stricted bounderies</li> </ul> Signal TOTAL_CNT TOTAL_CNT_NON_ZERO TOTAL_CLEANED CLEAN_PERCENTAGE CLEAN_NON_ZERO_PERCENTAGE TOTAL_PIDS PIDS_FILTERED PIDS_FILTERED_NON_ZEROS PIDS_FILTER_PERCENTAGE PIDS_FILTER_NON_ZERO_PERCENTAGE comment eGFR_MDRD 30992945 30976157 30258567 2.37% 2.32% 5328855 427719 414278 8.03% 7.77% Remove filter - not needed UrineCreatinine 2603228 2602550 2556325 1.80% 1.78% 694781 17213 17210 2.48% 2.48% Maybe problem in units that can be solved - checking PlasmaViscosity 1258162 1257236 1237837 1.62% 1.54% 459295 8899 8147 1.94% 1.77% Maybe problem in units that can be solved - checking Transferrin 386039 386034 382521 0.91% 0.91% 232202 2709 2704 1.17% 1.16% OK HDL_over_nonHDL 14593886 14580124 14459042 0.92% 0.83% 3414705 77777 77335 2.28% 2.26% OK eGFR_CKD_EPI 30992945 30992374 30763090 0.74% 0.74% 5328855 171719 171715 3.22% 3.22% Remove filter - not needed Height 18860856 18780829 18714169 0.78% 0.35% 9334026 124126 57892 1.33% 0.62% Maybe problem in units that can be solved - checking CA125 252667 252550 251745 0.36% 0.32% 193979 599 598 0.31% 0.31% OK BP 90295429 89497237 89264282 1.14% 0.26% 9410580 525587 178924 5.59% 1.90% bugfix to work on each channel MCHC-M 25816227 25786164 25719334 0.38% 0.26% 5459883 56481 38450 1.03% 0.70% OK BMI 35211327 35194623 35118884 0.26% 0.22% 8293631 73201 59596 0.88% 0.72% OK Phosphore 4571174 4570766 4561223 0.22% 0.21% 1866755 3057 2683 0.16% 0.14% Lymphocytes% 24656284 24652334 24610137 0.19% 0.17% 5312408 26240 26119 0.49% 0.49% Neutrophils% 24740548 24736537 24695583 0.18% 0.17% 5321777 25181 25046 0.47% 0.47% INR 8505951 8496622 8488159 0.21% 0.10% 455102 5643 4012 1.24% 0.88% PDW 384442 383910 383555 0.23% 0.09% 109155 772 772 0.71% 0.71% PlasmaAnionGap 21530 21530 21513 0.08% 0.08% 5491 15 15 0.27% 0.27% WBC 26610249 26593657 26572901 0.14% 0.08% 5554159 25789 12033 0.46% 0.22% FreeT3 506119 505466 505091 0.20% 0.07% 205428 877 314 0.43% 0.15% RBC 25905761 25876600 25857697 0.19% 0.07% 5471987 28169 27732 0.51% 0.51% Ca 7173055 7172113 7167232 0.08% 0.07% 2582464 4826 3942 0.19% 0.15% Mg 182248 182218 182094 0.08% 0.07% 107908 134 108 0.12% 0.10% T4 475830 473666 473368 0.52% 0.06% 213799 2235 277 1.05% 0.13% Hematocrit 25862482 25836512 25822132 0.16% 0.06% 5463443 27793 10406 0.51% 0.19% TEMP 1801163 1786361 1785441 0.87% 0.05% 964098 12126 917 1.26% 0.10% Digoxin 93861 93617 93584 0.30% 0.04% 43469 215 212 0.49% 0.49% SerumAnionGap 59666 59661 59640 0.04% 0.04% 22779 24 24 0.11% 0.11% K+ 28662323 28620055 28610424 0.18% 0.03% 5174781 35073 6931 0.68% 0.13% MPV 3443266 3442461 3441344 0.06% 0.03% 980181 1361 978 0.14% 0.10% Bicarbonate 3175248 3174729 3173852 0.04% 0.03% 741895 1332 844 0.18% 0.11% Cholesterol 18909856 18888280 18883203 0.14% 0.03% 3882355 22236 4367 0.57% 0.11% Albumin 23700344 23664696 23658553 0.18% 0.03% 4850137 25065 4754 0.52% 0.10% Na 29000033 28945701 28940315 0.21% 0.02% 5195665 26686 5191 0.51% 0.10% Iron_Fe 613710 613514 613409 0.05% 0.02% 363871 284 102 0.08% 0.03% Weight 39402271 39199410 39192705 0.53% 0.02% 9808932 167114 6298 1.70% 0.06% NonHDLCholesterol 14588087 14587707 14585866 0.02% 0.01% 3413836 1912 1909 0.06% 0.06% RandomGlucose 710956 701691 701603 1.32% 0.01% 423583 6726 87 1.59% 0.02% Platelets_Hematocrit 3400998 3399872 3399528 0.04% 0.01% 972680 1193 1191 0.12% 0.12% Hemoglobin 27748929 27701627 27698933 0.18% 0.01% 5689777 24331 2523 0.43% 0.04% LDL 12668050 12639848 12638813 0.23% 0.01% 3124704 20626 965 0.66% 0.03% MCV 26246642 26230251 26228319 0.07% 0.01% 5510085 12313 1833 0.22% 0.03% CO2 262289 261884 261866 0.16% 0.01% 44949 216 18 0.48% 0.04% Glucose 16484078 16472703 16471686 0.08% 0.01% 4466133 9937 928 0.22% 0.02% Amylase 355012 354296 354275 0.21% 0.01% 275843 685 20 0.25% 0.01% CorrectedCa 6448360 6447552 6447174 0.02% 0.01% 2381369 1087 357 0.05% 0.01% Triglycerides 14035783 14004268 14003515 0.23% 0.01% 3295899 19785 707 0.60% 0.02% LDH 223522 218968 218958 2.04% 0.00% 102148 3316 10 3.25% 0.01% ALKP 24501543 24491562 24490462 0.05% 0.00% 4975550 8773 844 0.18% 0.02% PULSE 5607620 5597890 5597659 0.18% 0.00% 2221767 7933 224 0.36% 0.01% Cl 6074637 6068936 6068789 0.10% 0.00% 1205272 4050 146 0.34% 0.01% Protein_Total 15053134 15051906 15051694 0.01% 0.00% 3350737 1336 188 0.04% 0.01% FreeT4 8375885 8347195 8347096 0.34% 0.00% 2572146 19968 99 0.78% 0.00% AST 5017954 5016921 5016887 0.02% 0.00% 1391446 972 33 0.07% 0.00% Platelets 26572350 26551880 26551731 0.08% 0.00% 5546115 15154 133 0.27% 0.00% ALT 20504083 20485612 20485528 0.09% 0.00% 4431602 12872 82 0.29% 0.00% MCH 25858658 25846546 25846464 0.05% 0.00% 5463374 7628 82 0.14% 0.00% B12 3015306 3013860 3013860 0.05% 0% 1607426 1212 0 0.08% 0% RDW 4969565 4969446 4969563 4.02E-07 0.00% 1579652 2 2 0.00% 0.00% Urea 22375296 22373145 22373971 0.01% 0.00% 4367623 1291 1289 0.03% 0.03% Monocytes% 24411657 24387478 24388588 0.09% 0.00% 5276761 13670 13512 0.26% 0.26% VitaminD_25 352787 352738 352775 0.00% -0.01% 228066 12 12 0.01% 0.01% Fibrinogen 196551 196504 196550 0.00% -0.02% 143822 1 1 0.00% 0.00% HDL_over_LDL 12596626 12590998 12594958 0.01% -0.03% 3118377 1509 1507 0.05% 0.05% Urine_Dipstick_pH 136097 136047 136091 0.00% -0.03% 64581 6 6 0.01% 0.01% Transferrin_Saturation_Index 252126 251899 251984 0.06% -0.03% 162901 134 134 0.08% 0.08% Ferritin 3729644 3728208 3729625 0.00% -0.04% 1809350 18 18 0.00% 0.00% Bilirubin 23598709 23588476 23598656 0.00% -0.04% 4904303 50 50 0.00% 0.00% Sex_Hormone_Binding_Globulin 133505 133441 133503 0.00% -0.05% 110127 2 2 0.00% 0.00% Lymphocytes# 24916931 24903489 24916729 0.00% -0.05% 5337938 159 159 0.00% 0.00% HbA1C 7597400 7592635 7597179 0.00% -0.06% 1520278 163 160 0.01% 0.01% TIBC 151135 151012 151114 0.01% -0.07% 97981 21 21 0.02% 0.02% Prolactin 407790 407328 407663 0.03% -0.08% 297447 91 91 0.03% 0.03% Cholesterol_over_HDL 15035230 15018683 15031577 0.02% -0.09% 3421173 3150 2923 0.09% 0.09% Follic_Acid 2703458 2701120 2703458 0% -0.09% 1472362 0 0 0% 0% HDL_over_Cholesterol 14670753 14656530 14669390 0.01% -0.09% 3420769 1342 1331 0.04% 0.04% Neutrophils# 24970648 24946553 24970234 0.00% -0.09% 5346509 383 378 0.01% 0.01% FSH 1045438 1044297 1045438 0% -0.11% 688285 0 0 0% 0% Testosterone 388036 387595 388027 0.00% -0.11% 292007 9 9 0.00% 0.00% LDL_over_HDL 12606025 12591423 12605655 0.00% -0.11% 3118883 357 349 0.01% 0.01% Globulin 8009111 7999593 8008795 0.00% -0.12% 1896822 254 253 0.01% 0.01% Creatinine 31070691 31033058 31069711 0.00% -0.12% 5331326 753 732 0.01% 0.01% eGFR 16442849 16422354 16442849 0% -0.12% 3426403 0 0 0% 0% Monocytes# 24612290 24579520 24612015 0.00% -0.13% 5298350 234 219 0.00% 0.00% CK 942771 938846 940828 0.21% -0.21% 481006 1367 1364 0.28% 0.28% Uric_Acid 1151516 1147621 1150405 0.10% -0.24% 655507 1073 1068 0.16% 0.16% Erythrocyte 7239836 7222198 7239830 8.29E-07 -0.24% 2647537 6 5 0.00% 0.00% Lithium 274155 273378 274113 0.02% -0.27% 21344 26 26 0.12% 0.12% GGT 9354116 9324434 9354066 0.00% -0.32% 2369564 49 49 0.00% 0.00% UrineAlbumin 1366696 1361871 1366696 0% -0.35% 391018 0 0 0% 0% GFR 3225746 3213478 3225746 0% -0.38% 882940 0 0 0% 0% LUC 1330513 1324624 1330144 0.03% -0.42% 324604 250 215 0.08% 0.07% HDL 14818204 14749529 14813285 0.03% -0.43% 3431910 3956 2632 0.12% 0.08% UrineTotalProtein 386928 384724 386787 0.04% -0.54% 179141 115 115 0.06% 0.06% Reticulocyte 69816 69418 69814 0.00% -0.57% 49484 2 2 0.00% 0.00% UrineAlbumin_over_Creatinine 2451211 2431361 2451204 0.00% -0.82% 678233 7 7 0.00% 0.00% PSA 1926651 1906549 1926420 0.01% -1.04% 660296 159 157 0.02% 0.02% Serum_Oestradiol 394083 389241 394083 0% -1.24% 274758 0 0 0% 0% PFR 5264955 5164019 5264358 0.01% -1.94% 1355995 569 535 0.04% 0.04% TSH 16173524 15848163 16172909 0.00% -2.05% 4501457 528 450 0.01% 0.01% LuteinisingHormone 847445 830210 847445 0% -2.08% 585366 0 0 0% 0% Rheumatoid_Factor 461205 450087 460648 0.12% -2.35% 364740 494 492 0.14% 0.13% Progesterone 233686 227831 233686 0% -2.57% 156019 0 0 0% 0% CRP 5980408 5820488 5980374 0.00% -2.75% 2320376 30 30 0.00% 0.00% Urine_Protein_Creatinine 324747 314833 324741 0.00% -3.15% 151145 6 6 0.00% 0.00% Urine_Epithelial_Cell 299537 287827 299537 0% -4.07% 145624 0 0 0% 0% Eosinophils% 24273861 22801564 24270972 0.01% -6.44% 5260185 2185 1951 0.04% 0.04% Eosinophils# 24445262 22942339 24445176 0.00% -6.55% 5281030 70 59 0.00% 0.00% Urine_Microalbumin 1386074 1269197 1386072 0.00% -9.21% 375904 2 0 0.00% 0% Basophils# 23695244 14449541 23692629 0.01% -63.97% 5194395 2007 1665 0.04% 0.03% Basophils% 23660542 9994176 23648885 0.05% -136.63% 5193897 9473 3998 0.18% 0.08% NRBC 2113870 3939 2113869 4.73E-07 -53565.10% 733525 1 0 0.00% 0% <p>Examples of filtered row log from program: EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20090210\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 12225\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100204\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 14970\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110304\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 15219\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120319\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 15607\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20130221\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 14.8 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20131115\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 13 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20141124\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0\u00a013055975\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20160122\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11134171\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20091215\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6694\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11134171\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20111115\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20080424\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3686\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20090918\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4630\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100611\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 13229\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110106\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 13349\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110404\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3275\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0 \u00a0\u00a06835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20111202\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 5063\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120315\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3616\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120628\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 5262\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20130306\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 2.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20140221\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4.3 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20150527\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 6835336 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20160822\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3.6 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 8685466 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20091113\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 11823\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20081124\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 5452\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100125\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 6526\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20100624\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4616\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20110715\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3074\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20120615\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3674\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20130611\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 4.2 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20140607\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 14408958\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 UrineCreatinine Time\u00a0\u00a0\u00a0 20140722\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 3 STATS\u00a0\u00a0 UrineCreatinine TOTAL_CNT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2603228 TOTAL_CNT_NON_ZERO\u00a0\u00a0\u00a0\u00a0\u00a0 2602550 TOTAL_CLEANED\u00a0\u00a0 2556325 CLEAN_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.80172%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_NON_ZERO_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.77614%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_PIDS\u00a0\u00a0\u00a0 694781\u00a0\u00a0 PIDS_FILTERED\u00a0\u00a0 17213\u00a0\u00a0 PIDS_FILTERED_NON_ZEROS 17210\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 2.47747%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 2.47704%\u00a0 1. Height- looks like there is factor 100 sometimes EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20041111\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20050302\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060522\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060607\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060607\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0\u00a0[REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20071109\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17008937\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20090617\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19980408\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 17\u00a0\u00a0\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20040209\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20061212\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20091203\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20111122\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5044235 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20121113\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 173 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11310017\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19930902\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 150 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 11310017\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19990422\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 12\u00a0\u00a0\u00a0\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20031118\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20031118\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 16500\u00a0\u00a0 [REMOVED] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20040809\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 158 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20041120\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 165 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20060901\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 158 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 5188073 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 20071010\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 159 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 10759614\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19930602\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 168 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 10759614\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 Height\u00a0 Time\u00a0\u00a0\u00a0 19980225\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [REMOVED] STATS\u00a0\u00a0 Height\u00a0 TOTAL_CNT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 18860856\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_CNT_NON_ZERO\u00a0\u00a0\u00a0\u00a0\u00a0 18780829\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_CLEANED\u00a0\u00a0 18714169\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.777732%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_NON_ZERO_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.354936%\u00a0\u00a0\u00a0\u00a0 TOTAL_PIDS\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a09334026 PIDS_FILTERED\u00a0\u00a0 124126\u00a0 PIDS_FILTERED_NON_ZEROS 57892\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 1.32982%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 0.620225% 1. MCHC-M \u2013 looks like factor 10 problem: EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20010622\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 357\u00a0\u00a0\u00a0\u00a0 [*] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20060711\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090505\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 19950502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 330\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 19950605\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 326\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20021114\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20050328\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20070111\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20070201\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20071030\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20071112\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20080422\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.7 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20081030\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.8 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090611\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090715\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 33.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20091230\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.6 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100106\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 30 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100126\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100204\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.6 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100225\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.2 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100301\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.7 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100308\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 31.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 16311924\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100315\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 29.5 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 15517308\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20040511\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 313\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 15517308\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20071022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 32.7 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 19991116\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 326\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20050808 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value\u00a0\u00a0 34.1 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20051026\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.4 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17224984\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20051222\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.8 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20040728\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value\u00a0\u00a0 33.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090522\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 396\u00a0\u00a0\u00a0\u00a0 [] EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090623\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 35 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20090918\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 35.3 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20091123\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 34.9 EXAMPLE pid\u00a0\u00a0\u00a0\u00a0 17141004\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Signal\u00a0 MCHC-M\u00a0 Time\u00a0\u00a0\u00a0 20100319\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Value\u00a0\u00a0 370\u00a0\u00a0\u00a0\u00a0 [***] STATS\u00a0\u00a0 MCHC-M\u00a0 TOTAL_CNT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 25816227\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0TOTAL_CNT_NON_ZERO\u00a0\u00a0\u00a0\u00a0\u00a0 25786164\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_CLEANED\u00a0\u00a0 25719334\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.375318%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CLEAN_NON_ZERO_PERCENTAGE\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.25917%\u00a0\u00a0\u00a0\u00a0\u00a0 TOTAL_PIDS\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5459883 PIDS_FILTERED\u00a0\u00a0 56481\u00a0\u00a0 PIDS_FILTERED_NON_ZEROS 38450\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 1.03447%\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PIDS_FILTER_PERCENTAGE\u00a0 0.704228% \u00a0 Stats of Full (with panels check) <pre><code>Flow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --rep_processor_print --sigs \"Cholesterol_over_HDL,HDL_over_Cholesterol,UrineAlbumin,HDL,UrineAlbumin_over_Creatinine,UrineCreatinine,LDL,Cholesterol,LDL_over_HDL,HDL_over_LDL,Platelets_Hematocrit,MPV,NonHDLCholesterol,HDL_over_nonHDL,MCV,RBC,Hematocrit,Platelets,MCH,MCHC-M,Protein_Total,Hemoglobin,Height,Albumin,Basophils#,Eosinophils#,Monocytes#,Lymphocytes#,Neutrophils#,NRBC,WBC,BMI,Weight,UrineTotalProtein,ALKP,ALT,Amylase,AST,B12,Basophils%,Bicarbonate,Bilirubin,Ca,CA125,CK,Cl,CO2,CorrectedCa,Creatinine,CRP,Digoxin,eGFR,Eosinophils%,Erythrocyte,Ferritin,Fibrinogen,Follic_Acid,FreeT3,FreeT4,FSH,GFR,GGT,Globulin,Glucose,HbA1C,INR,Iron_Fe,K+,LDH,Lithium,LUC,LuteinisingHormone,Lymphocytes%,Mg,Monocytes%,Na,Neutrophils%,PDW,PFR,Phosphore,PlasmaAnionGap,PlasmaViscosity,Progesterone,Prolactin,PSA,PULSE,RandomGlucose,RDW,Reticulocyte,Rheumatoid_Factor,Serum_Oestradiol,SerumAnionGap,Sex_Hormone_Binding_Globulin,T4,Testosterone,TIBC,Transferrin,Transferrin_Saturation_Index,Triglycerides,TSH,Urea,Uric_Acid,Urine_Dipstick_pH,Urine_Epithelial_Cell,Urine_Microalbumin,Urine_Protein_Creatinine,VitaminD_25,TEMP\" --max_examples 10 --seed 0 --f_output /tmp/test.log --cleaner_path_before /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/configure_sim_val.json --cleaner_path /server/Work/Users/Alon/UnitTesting/examples/general_config_files/Cleaner/full_cleaners.json \n</code></pre></p> Signal TOTAL_CNT TOTAL_CNT_NON_ZERO TOTAL_CLEANED CLEAN_PERCENTAGE TOTAL_PIDS PIDS_FILTERED PIDS_FILTERED_NON_ZEROS PIDS_FILTER_PERCENTAGE PIDS_FILTER_NON_ZERO_PERCENTAGE Comment UrineAlbumin 1364175 1359350 1301695 4.58% 391018 33583 33206 8.59% 8.49% \u00a0Urine_panel, lot of real errors UrineAlbumin_over_Creatinine 2444786 2424936 2382427 2.55% 678233 33467 30981 4.93% 4.57% \u00a0Urine_panel, lot of real errors UrineCreatinine 2550757 2550079 2488398 2.44% 694781 33467 33438 4.82% 4.81% \u00a0Urine_panel, lot of real errors Cholesterol_over_HDL 14653976 14653976 14375111 1.90% 3421173 115517 115517 3.38% 3.38% there is problem in load of wrong source HDL_over_LDL 12594284 12588656 12368827 1.79% 3118377 129667 128275 4.16% 4.11% Platelets_Hematocrit 3398224 3398224 3337846 1.78% 972680 19763 19763 2.03% 2.03% MPV 3438309 3438309 3377931 1.76% 980181 19763 19763 2.02% 2.02% HDL 14751464 14682789 14546126 1.39% 3431910 114884 98162 3.35% 2.86% HDL_over_Cholesterol 14665993 14651770 14467583 1.35% 3420769 86917 86067 2.54% 2.52% there is problem in load of wrong source Cholesterol 18855175 18855175 18649837 1.09% 3882355 114884 114884 2.96% 2.96% NonHDLCholesterol 14585464 14585084 14428102 1.08% 3413836 83548 83450 2.45% 2.44% LDL_over_HDL 12605642 12591040 12550215 0.44% 3118883 27352 26925 0.88% 0.86% LDL 12633401 12633401 12577976 0.44% 3124704 37293 37293 1.19% 1.19% MCV 26166367 26166367 26084584 0.31% 5510085 44007 44007 0.80% 0.80% HDL_over_nonHDL 14459035 14445273 14414026 0.31% 3414705 19387 18580 0.57% 0.54% RBC 25832962 25803801 25758846 0.29% 5471987 38602 34538 0.71% 0.63% Hematocrit 25784644 25784644 25725376 0.23% 5463443 29769 29769 0.54% 0.54% Platelets 26511344 26511344 26450966 0.23% 5546115 19763 19763 0.36% 0.36% MCH 25791202 25791202 25735104 0.22% 5463374 36564 36564 0.67% 0.67% MCHC-M 25697538 25697538 25655308 0.16% 5459883 25802 25802 0.47% 0.47% Protein_Total 15030698 15030698 15014686 0.11% 3350737 10857 10857 0.32% 0.32% Hemoglobin 27672196 27672196 27648170 0.09% 5689777 16426 16426 0.29% 0.29% Albumin 23630785 23630785 23614773 0.07% 4850137 10857 10857 0.22% 0.22% Basophils# 23647655 14401952 23636707 0.05% 5194395 9378 3885 0.18% 0.07% Eosinophils# 24411167 22908244 24400219 0.04% 5281030 9378 7428 0.18% 0.14% Monocytes# 24561221 24528451 24550273 0.04% 5298350 9378 9178 0.18% 0.17% Lymphocytes# 24824132 24810690 24813184 0.04% 5337938 9378 9326 0.18% 0.17% Neutrophils# 24907730 24883635 24896782 0.04% 5346509 9378 9272 0.18% 0.17% NRBC 2113731 3800 2112819 0.04% 733525 646 503 0.09% 0.07% WBC 26543214 26543214 26532266 0.04% 5554159 9378 9378 0.17% 0.17% BMI 35030317 35030317 35017055 0.04% 8293631 12869 12869 0.16% 0.16% UrineTotalProtein 385952 383748 385824 0.03% 179141 125 91 0.07% 0.05% Height 18664187 18664187 18664187 0% 9334026 0 0 0% 0% Weight 39068717 39068717 39068717 0% 9808932 0 0 0% 0% ALKP 24458720 24458720 24458720 0% 4975550 0 0 0% 0% ALT 20471720 20471720 20471720 0% 4431602 0 0 0% 0% Amylase 353637 353637 353637 0% 275843 0 0 0% 0% AST 5013392 5013392 5013392 0% 1391446 0 0 0% 0% B12 3011237 3011237 3011237 0% 1607426 0 0 0% 0% Basophils% 23647760 9981394 23647760 0% 5193897 0 0 0% 0% Bicarbonate 3171108 3171108 3171108 0% 741895 0 0 0% 0% Bilirubin 23542527 23532294 23542527 0% 4904303 0 0 0% 0% Ca 7095160 7095160 7095160 0% 2582464 0 0 0% 0% CA125 252115 251998 252115 0% 193979 0 0 0% 0% CK 939752 935827 939752 0% 481006 0 0 0% 0% Cl 6064395 6064395 6064395 0% 1205272 0 0 0% 0% CO2 261631 261631 261631 0% 44949 0 0 0% 0% CorrectedCa 6440715 6440715 6440715 0% 2381369 0 0 0% 0% Creatinine 31019168 30981535 31019168 0% 5331326 0 0 0% 0% CRP 5976368 5816448 5976368 0% 2320376 0 0 0% 0% Digoxin 92754 92510 92754 0% 43469 0 0 0% 0% eGFR 16433805 16413310 16433805 0% 3426403 0 0 0% 0% Eosinophils% 24269187 22796890 24269187 0% 5260185 0 0 0% 0% Erythrocyte 7233824 7216186 7233824 0% 2647537 0 0 0% 0% Ferritin 3722680 3721244 3722680 0% 1809350 0 0 0% 0% Fibrinogen 196069 196022 196069 0% 143822 0 0 0% 0% Follic_Acid 2700849 2698511 2700849 0% 1472362 0 0 0% 0% FreeT3 504917 504917 504917 0% 205428 0 0 0% 0% FreeT4 8340984 8340984 8340984 0% 2572146 0 0 0% 0% FSH 1044260 1043119 1044260 0% 688285 0 0 0% 0% GFR 3223523 3211255 3223523 0% 882940 0 0 0% 0% GGT 9337150 9307468 9337150 0% 2369564 0 0 0% 0% Globulin 7992922 7983404 7992922 0% 1896822 0 0 0% 0% Glucose 16291642 16291642 16291642 0% 4466133 0 0 0% 0% HbA1C 7510232 7505467 7510232 0% 1520278 0 0 0% 0% INR 8402552 8402552 8402552 0% 455102 0 0 0% 0% Iron_Fe 610270 610270 610270 0% 363871 0 0 0% 0% K+ 28586751 28586751 28586751 0% 5174781 0 0 0% 0% LDH 218679 218679 218679 0% 102148 0 0 0% 0% Lithium 273406 272629 273406 0% 21344 0 0 0% 0% LUC 1328863 1322974 1328863 0% 324604 0 0 0% 0% LuteinisingHormone 844893 827658 844893 0% 585366 0 0 0% 0% Lymphocytes% 24605312 24601362 24605312 0% 5312408 0 0 0% 0% Mg 181832 181832 181832 0% 107908 0 0 0% 0% Monocytes% 24385874 24361695 24385874 0% 5276761 0 0 0% 0% Na 28917933 28917933 28917933 0% 5195665 0 0 0% 0% Neutrophils% 24692458 24688447 24692458 0% 5321777 0 0 0% 0% PDW 383011 382479 383011 0% 109155 0 0 0% 0% PFR 5127271 5026335 5127271 0% 1355995 0 0 0% 0% Phosphore 4557642 4557642 4557642 0% 1866755 0 0 0% 0% PlasmaAnionGap 21500 21500 21500 0% 5491 0 0 0% 0% PlasmaViscosity 1237193 1237193 1237193 0% 459295 0 0 0% 0% Progesterone 233310 227455 233310 0% 156019 0 0 0% 0% Prolactin 406527 406065 406527 0% 297447 0 0 0% 0% PSA 1924332 1904230 1924332 0% 660296 0 0 0% 0% PULSE 5527058 5527058 5527058 0% 2221767 0 0 0% 0% RandomGlucose 700394 700394 700394 0% 423583 0 0 0% 0% RDW 4960073 4959954 4960073 0% 1579652 0 0 0% 0% Reticulocyte 69336 68938 69336 0% 49484 0 0 0% 0% Rheumatoid_Factor 460276 449158 460276 0% 364740 0 0 0% 0% Serum_Oestradiol 393299 388457 393299 0% 274758 0 0 0% 0% SerumAnionGap 59530 59525 59530 0% 22779 0 0 0% 0% Sex_Hormone_Binding_Globulin 132646 132582 132646 0% 110127 0 0 0% 0% T4 473023 473023 473023 0% 213799 0 0 0% 0% Testosterone 386501 386060 386501 0% 292007 0 0 0% 0% TIBC 149863 149740 149863 0% 97981 0 0 0% 0% Transferrin 382006 382006 382006 0% 232202 0 0 0% 0% Transferrin_Saturation_Index 251706 251479 251706 0% 162901 0 0 0% 0% Triglycerides 13994934 13994934 13994934 0% 3295899 0 0 0% 0% TSH 16164536 15839175 16164536 0% 4501457 0 0 0% 0% Urea 22350867 22348716 22350867 0% 4367623 0 0 0% 0% Uric_Acid 1149741 1145846 1149741 0% 655507 0 0 0% 0% Urine_Dipstick_pH 136044 135994 136044 0% 64581 0 0 0% 0% Urine_Epithelial_Cell 299102 287392 299102 0% 145624 0 0 0% 0% Urine_Microalbumin 1379741 1262864 1379741 0% 375904 0 0 0% 0% Urine_Protein_Creatinine 321517 311603 321517 0% 151145 0 0 0% 0% VitaminD_25 349534 349485 349534 0% 228066 0 0 0% 0% TEMP 1779120 1779120 1779120 0% 964098 0 0 0% 0%"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Full%20Rep%20Processor.html","title":"Full Rep Processor","text":"<p>This Page will describe and demonstrate a full rep processor json with all out capabilities:</p> <ul> <li>Cleaners - basic and rules</li> <li>Panel Compelition</li> <li>Common Virtual Signals: for example eGFR</li> <li>Registries \u00a0 The json can be found in git under\u00a0https://github.com/Medial-EarlySign/AM_Lung/blob/main/configs/training/full_rep_processors.json</li> </ul> <p><pre><code>{\n  \"model_json_version\": \"2\",\n  \"serialize_learning_set\": \"0\",\n  \"model_actions\": [\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"time_channel\":\"0\",\n          \"clean_method\":\"confirmed\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\"\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        },\n        {\n          \"rp_type\":\"conf_cln\",\n          \"conf_file\":\"../settings/cleanDictionary.csv\",\n          \"val_channel\":[\"0\", \"1\"],\n          \"clean_method\":\"confirmed\",\n          \"signal\": [\"BP\"]\n          //,\"verbose_file\":\"/tmp/cleaning.log\"\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"sim_val\",\n          \"signal\":\"file:../settings/all_rules_sigs.list\",\n          \"type\":\"first\",\n          \"debug\":\"0\"\n\n        }\n\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"rule_cln\",\n          \"addRequiredSignals\":\"1\",\n          \"time_window\":\"0\",\n          \"tolerance\":\"0.1\",\n          \"calc_res\":\"0.1\",\n          \"rules2Signals\":\"../settings/ruls2Signals.tsv\",\n          \"consideredRules\":[ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\" ] \n          //,\"verbose_file\":\"/tmp/panel_cleaning.log\"\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"complete\",\n          \"sim_val\":\"remove_diff\",\n          \"config\":\"../settings/panel_completer.cfg\",\n          \"panels\":[\"red_line\", \"white_line\", \"platelets\", \"lipids\", \"egfr\", \"bmi\", \"gcs\"]\n        }\n      ]\n    },\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"create_registry\", \n          \"registry\":\"dm\", \n          \"names\":\"_v_DM_Registry\",\n          \"dm_drug_sets\":\"list:../settings/registries/diabetes_drug_codes.full\",\n          \"dm_diagnoses_sets\":\"list:../settings/registries/diabetes_read_codes_registry.full.striped\"\n        },\n        {\n          \"rp_type\":\"create_registry\",\n          \"registry\":\"ht\",\n          \"names\":\"my_HT_Registry\"\n        },\n        {\n          \"rp_type\":\"create_registry\",\n          \"registry\":\"proteinuria\",\n          \"names\":\"_v_Proteinuria_State\"\n        },\n        {\n          \"action_type\":\"rep_processor\",\n          \"rp_type\":\"create_registry\",\n          \"registry\":\"ckd\",\n          \"names\":\"_v_CKD_State\",\n          \"signals\":\"_v_Proteinuria_State,eGFR_CKD_EPI\",\n          \"ckd_egfr_sig\":\"eGFR_CKD_EPI\",\n          \"ckd_proteinuria_sig\":\"_v_Proteinuria_State\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> to include use: $MR_ROOT/Projects/Resources/configs/include_jsons/full_rep_processors.inc.json</p> <p>Debug &amp; Print repository after applying rep_processors: <pre><code>#prints signal _v_CKD_State after applying full_rep_processors\nFlow --rep /home/Repositories/THIN/thin_2018/thin.repository  --pids_sigs_print_raw  --model_rep_processors $MR_ROOT/Projects/Resources/configs/full_model_jsons/full_rep_processors.json --sigs _v_CKD_State\n\u00a0\nRead 0 signals, 0 pids :: data  0.000GB :: idx  0.000GB :: tot  0.000GB\nRead data time 0.126360 seconds\nMedModel:: init model from json file [/nas1/UsersData/alon/MR/Projects/Resources/configs/full_model_jsons/full_rep_processors.json]:\nread 126 lines from: /nas1/UsersData/alon/MR/Projects/Resources/configs/full_model_jsons/../settings/all_rules_sigs.list\nread 126 lines from: /nas1/UsersData/alon/MR/Projects/Resources/configs/full_model_jsons/../settings/all_rules_sigs.list\nNOTE: no [predictor] node found in file\nMedRepository: read config file /home/Repositories/THIN/thin_2018/thin.repository\nMedRepository: reading signals: Urinalysis_Protein,BYEAR,eGFR_CKD_EPI,GENDER,UrineTotalProtein,UrineCreatinine,UrineAlbumin_over_Creatinine,Urine_dipstick_for_protein,Urine_Protein_Creatinine,Urine_Microalbumin,UrineAlbumin,Creatinine,\nRead 12 signals, 17030409 pids :: data  0.523GB :: idx  0.205GB :: tot  0.728GB\nRead data time 1.730095 seconds\n#Print# :: Done processed all 17030409. Took 215.3 Seconds in total\npid     signal_name     description_name        description_value       ...\n5000001 _v_CKD_State    Time_ch_0       20111201        Val_ch_0        4\n5000002 _v_CKD_State    Time_ch_0       20051206        Val_ch_0        4\n5000002 _v_CKD_State    Time_ch_0       20080508        Val_ch_0        4\n5000002 _v_CKD_State    Time_ch_0       20120516        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20080630        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20120626        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20131125        Val_ch_0        4\n5000003 _v_CKD_State    Time_ch_0       20170531        Val_ch_0        4\n5000009 _v_CKD_State    Time_ch_0       20050908        Val_ch_0        4\n</code></pre> Debug the changes of rep_processors (for example use cleaner and see before\\after): <pre><code>#Compares repository from state after cleaner_path_before  to repository state after cleaner_path\n#cleaner_path_before - if empty will compare to repository without rep_processors\nFlow --rep /home/Repositories/THIN/thin_2018/thin.repository --rep_processor_print --sigs Hemoglobin --cleaner_path_before \"\" --cleaner_path $MR_ROOT/Projects/Resources/configs/full_model_jsons/full_rep_processors.json --max_examples 5 --f_output /tmp/repositry_after_rep_processors.log\n\u00a0\nhead /tmp/repositry_after_rep_processors.log\nEXAMPLE pid     10419961        Signal  Hemoglobin      Time    20050310        Value   14.8    [REMOVED]\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19961001        Value   12.1\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19970115        Value   12.4\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19970519        Value   12.8\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    19981022        Value   12.1\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20010123        Value   13.7\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20010831        Value   14.8\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20011025        Value   14.2\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20030116        Value   8.9\nEXAMPLE pid     21605408        Signal  Hemoglobin      Time    20040528        Value   12.3\n...\nSTATS   Hemoglobin      TOTAL_CNT       30787105        TOTAL_CNT_NON_ZERO      30735288        TOTAL_CLEANED   30464195        CLEAN_PERCENTAGE        1.04885%        CLEAN_NON_ZERO_PERCENTAGE       0.882025%\n       TOTAL_PIDS      6168092 PIDS_FILTERED   241868  PIDS_FILTERED_NON_ZEROS 164564  PIDS_FILTER_PERCENTAGE  3.92128%        PIDS_FILTER_PERCENTAGE  2.66799%\n</code></pre> in the above example we can see 5 examples of patients that has at least 1 change from the original signal - we can see that the hemoglobin value of\u00a0 10419961 is being filtered. The last line in output file is summary stats on the signal Hemoglobin compared to the original one. :\u00a0</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/History%20Limit%20repo%20processor.html","title":"History Limit repo processor","text":"<p>A rep processor that allows to limit the history of a signal to a given time range relative to the prediction time point. There is also an option to virtually erase the signal completely so when the model will try to access the signal it will be empty. Mainly used as a pre processor in model apply to see how history impact performance.</p> <ul> <li>name : \"limit_history\" or \"history_limit\" parameters:</li> <li>signal - signal name time_channel - the time channel to limit by</li> <li>win_from , win_to - the time window to select</li> <li>delete_sig : if 1 : delete the signal from record.</li> <li>rep_time_unit , win_time_unit : global by default, otherwise as stated. Example json lines: <pre><code># limit Hemoglobin and Creatinine to tests done up to one year before the prediction time\n{\"rp_type\" : \"history_limit\" , \"signal\" : [\"Hemoglobin\" , \"Creatinine\"] , \"win_from\" : \"0\" , \"win_to\" : \"365\"}\n\u00a0\n# delete GENDER and Hemoglobin signals from each record\n{\"rp_type\" : \"history_limit\" , \"signal\" : [\"GENDER\", \"Hemoglobin\"], \"delete_sig\" : \"1\" }\n\u00a0\n</code></pre> Example as pre processor: A pre processor json file that limits history for RDW: \u00a0 <pre><code>{\n        \"pre_processors\" : [ {\"rp_type\" : \"history_limit\" , \"signal\" : [\"RDW\"], \"win_from\" : \"0\" , \"win_to\" : \"365\"} ] ,\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20print%2C%20how%20many%20outliers%20were.html","title":"How to print, how many outliers were","text":"<p>print_clear_info.json <pre><code>{\n  \"changes\": [\n       {\n                        \"change_name\":\"Turn on verbose cleaning\",\n                        \"object_type_name\":\"RepBasicOutlierCleaner\",\n                        \"json_query_whitelist\": [],\n                        \"json_query_blacklist\": [],\n                        \"change_command\": \"print_summary=0.00001;print_summary_critical_cleaned=0.00001\",\n                        \"verbose_level\":\"2\"\n           }\n     ]\n}\n</code></pre> \u00a0 Full flow command that uses this when model is applied: <pre><code>Flow --get_model_preds --rep $REP --f_samples $SMAPLES_PATH --f_model $MODEL_PATH --f_preds $OUTPUT_PREDS\u00a0--change_model_file print_clear_info.json\n</code></pre></p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html","title":"How to remove signal during admissions","text":""},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#motivation","title":"Motivation","text":"<p>When the repository includes info from inpatients, we might want to remove lab, measurements and drugs from admission periods, as it might be biased by the temporarily medical situation. Note that we are likely to keep diagnosis, as we are typically interested in chronic issues.</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#example","title":"Example","text":"<p>In the following example:</p> <ul> <li>We use**** rep processor</li> <li>We have **** signal in the repository</li> <li>We set get_values_in_range to 0 (will keep just input from inside of timeslots of ranges_sig_name) , as default is 1 (outside timeslots)</li> <li>We need to define signal appropriate ****</li> <li>The signal_options:<ul> <li>Can have a list of input&amp;output, all of them must have the same\u00a0****</li> <li>The default output_name is combination of the signal_name and ranges_sig_name - in this example (if we have not wrote output_name=BP), it would have been BP_ADMISSION. However, keeping the original name is continent when this rep processor is added in the beginning of existing process.\u00a0 \u00a0 <pre><code>{\n    \"action_type\": \"rp_set\",\n    \"members\": [\n    {\n        \"rp_type\":\"basic_range_cleaner\",\n        \"ranges_sig_name\":\"ADMISSION\",\n        \"time_channel\":\"0\",\n        \"range_time_channel\":\"0\",\n        \"get_values_in_range\":\"0\",\n        \"range_operator\":\"all\",\n        \"output_type\":\"T(i),V(s,s)\",\n        \"signal_options\": [ \n            \"signal_name=BP;output_name=BP\"\n        ]\n    }\n]}\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#remove-just-from-predict","title":"Remove just from predict","text":"<p>When a model was trained without the need to remove admissions (repository without inpatient data), but we apply the model on a repository with inpatient info, we need to remove admissions before predict, and this pre processor is not integral part of the model. We would run: <pre><code>Flow --get_model_preds --rep $REP --f_model $MODEL --f_samples $SAMPLES --f_preds $PREDS --f_pre_json $REMOVE_ADMISSIONS\n</code></pre> \u00a0where the REMOVE_ADMISSIONS json is as above, covered with <pre><code>{\n\"pre_processors\":[\nadd the json here\n]\n}\n</code></pre></p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20remove%20signal%20during%20admissions.html#important","title":"Important","text":"<p>The new REMOVE_ADMISSIONS pre-processor should come before the original model pre-processors executed. Otherwise, it might be a problem. For instance, if the original model pre-processors are calculating virtual signal or creating registry, they would use the data during admissions (before the REMOVING). To avoid that, the default in Flow is that the new pre-processor comes first. If, from any reason you want it differently, set the parameter add_rep_processor_first to False. If we have panel completer in our json, after the basic_range_cleaner, it might 'return' the signal during admission, if we have not cleaned all relevant signals. For instance, removing just Hemoglobin is not enough as the panel completer add it back using other signals.</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20rename%20signal.html","title":"How to rename signal","text":"<p>When a model is trained on repository A and test on repository B, then we might face signal naming issue. Example:</p> <ul> <li>model A:\u00a0ICD9_Diagnosis</li> <li>model B: DIAGNOSIS \u00a0To resolve we adjust the model with the following json <pre><code>adjust_model --preProcessors rename_signal_diag.json --skip_model_apply 1 --learn_rep_proc 0 --inModel ${IN_MODEL} --out ${OUT_MODEL} \n</code></pre> While rename_siganl_diag.json is: <pre><code>{ \"pre_processors\" : [\n   {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"filter_channels\",\n          \"signal\":\"DIAGNOSIS\",\n          \"output_name\":\"ICD9_Diagnosis\",\n          \"signal_type\":\"T(i),V(i)\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Howto%20write%20RepProcessor.html","title":"How to Write a RepProcessor","text":"<p>RepProcessors in MedModel follow a defined lifecycle. Below is the typical sequence of method calls and their purpose:</p> <ol> <li> <p>Constructor    - Initializes the RepProcessor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the processor.</p> </li> <li> <p>Initialization    - During learning: <code>init(map&lt;string, string&gt;&amp; mapper)</code> parses arguments from a key-value map.    - During application:      Arguments were loaded from disk (based on <code>ADD_SERIALIZATION_FUNCS</code>).</p> </li> <li> <p>fit_for_repository(MedPidRepository)    - Adapts the processor to the repository (e.g., creates virtual signals if needed).    - During learning:</p> <ul> <li><code>get_required_signal_names()</code>    Identifies which signals are needed for processing.</li> <li><code>filter()</code>  Determines if the processor should be applied, based on whether it affects any required signals. The list of affected signals is stored in the <code>aff_signals</code> variable. You can override the <code>filter</code> logic if needed.  </li> </ul> </li> <li> <p>Virtual Signal Management    - <code>add_virtual_signals()</code>      Lists virtual signals to generate and their types.    - <code>register_virtual_section_name_id()</code>      Registers categorical virtual signals in the dictionary.</p> </li> <li> <p>Signal ID Setup    - <code>set_affected_signal_ids(MedDictionarySections)</code>      Defines output signal IDs.    - <code>set_required_signal_ids(MedDictionarySections)</code>      Defines input signal IDs.    - <code>set_signal_ids(MedSignals)</code>      Sets input/output signal settings (often overlaps with above).</p> </li> <li> <p>Final Initialization    - <code>init_tables(MedDictionarySections, MedSignals)</code>      Finalizes processor settings using repository data.</p> </li> <li> <p>Attribute Initialization    - <code>init_attributes()</code>      Sets up additional processor attributes in MedSamples. For example store fields to document outlier cleaning</p> </li> <li> <p>Signal Requirement</p> <ul> <li><code>get_required_signal_names()</code>   (May be called again) Ensures all required signals are fetched.</li> </ul> </li> <li> <p>Application</p> <ul> <li><code>conditional_apply(PidDynamicRec, MedIdSamples)</code>   Applies processor logic to patient data in memory. Uses <code>PidDynamicRec</code> which is editable in-memory repository for a single patient. It also protects us from changing data for other patients.</li> </ul> </li> <li> <p>Summary</p> <ul> <li><code>make_summary()</code>   Generates a summary after processing (e.g., outlier percentages).   Useful for parallel execution and feature generation.</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Howto%20write%20RepProcessor.html#steps-to-implement-a-repprocessor","title":"Steps to Implement a RepProcessor:","text":"<ol> <li>Create a new <code>.h</code> file for your class and a corresponding <code>.cpp</code> file that includes the header. In the header, include <code>\"RepProcess.h\"</code>.</li> <li>Set up default values in <code>init_defaults()</code> or the constructor. For example, set <code>processor_type</code> using <code>RepProcessorTypes</code> (optional).</li> <li>Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</li> <li>Set up serialization:    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of the <code>.h</code> file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify only the parameters that need to be stored on disk after learning. Do not include temporary or repository-specific variables.</li> <li>Configure key variables for pipeline integration:    - Assign <code>virtual_signals_generic</code> after <code>init</code> if your processor creates virtual signals.    - Set <code>req_signals</code> to define required/input signals. This helps manage dependencies and ensures prerequisite processors run first. You can set this in <code>init_tables</code> or after <code>init</code>.    - Set <code>aff_signals</code> to specify output/affected signals, aiding pipeline dependency tracking. This can also be set in <code>init_tables</code> or after <code>init</code>.</li> <li>Override necessary functions as needed:    - <code>register_virtual_section_name_id</code> (for virtual categorical signals)    - <code>init_tables</code> (for initializing temporary variables using the repository - both in learn\\apply)    - <code>set_required_signal_ids</code>, <code>set_affected_signal_ids</code> (for custom signal ID logic; usually, using <code>aff_signals</code> and <code>req_signals</code> is sufficient)    - <code>fit_for_repository</code> (for repository-specific adjustments, e.g., virtual signal checks) (optional).    - <code>_learn</code> (override only if learning logic is needed; default is empty)    - <code>_apply</code> (main logic for applying the processor)    - <code>print</code> (optional for debugging)    - Any other required virtual functions</li> <li>Register your new RepProcessor in <code>RepProcess.h</code>:    - Add a new type to <code>RepProcessorTypes</code> before <code>REP_PROCESS_LAST</code>. In the documentation comment, specify the name in <code>rep_processor_name_to_type</code> for Doxygen reference.</li> <li>Register your new RepProcessor in <code>RepProcess.cpp</code>:    - Add your class to <code>rep_processor_name_to_type</code>    - Add your class to <code>RepProcessor::new_polymorphic</code>    - Add your class to <code>RepProcessor::make_processor(RepProcessorTypes processor_type)</code></li> </ol>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Noiser.html","title":"Noiser","text":"<p>This rep processor induces random noise into lab signals.\u00a0 This is divided into 3 kinds of noise, which can be used in tandem: 1. time_noise (-1&lt;int) - for noise size t, lowers the date by random value sampled from\u00a0 uniform_int(0, t).\u00a0 2. value_noise (0.0&lt;float)\u00a0- for noise size v, the rep processor first calculates the std of the lab signal across all patients. Then, it adds to each value of this lab a random noise, sampled from gaussian(0, v*std). 3. drop_probability (0.0&lt;float&lt;1.0) - for noise size d, each lab signal will be randomly dropped with probability d. In addition, one can truncate the resulting values to n digits by using truncation=n - important to truncate, as we are dealing with randomly sampled floats. In apply_in_test, if 0 it will apply noise in train only. If 1, will apply noise also in test. \u00a0 The rep processor is defined as such: <pre><code>{ \"pre_processors\" : [\n{\n\"action_type\":\"rp_set\",\n\"members\":[\n{\n\"rp_type\":\"noiser\",\n\"truncation\":\"2\",\n\"time_noise\":\"_TIME_NOISE_\",\n\"value_noise\":\"_VALUE_NOISE_\",\n\"drop_probability\":\"_DROP_NOISE_\",\n \"apply_in_test\":\"_ON_APPLY_STR_\",\n\"signal\": [\"RBC\",\"MPV\",\"Hemoglobin\",\"Hematocrit\",\"MCV\",\"MCH\",\"MCHC-M\",\"Platelets\",\"Neutrophils%\",\"Lymphocytes%\",\"Monocytes%\", \"WBC\",\"Eosinophils#\",\"Eosinophils%\",\"Basophils%\",\"Basophils#\",\"Neutrophils#\",\"Lymphocytes#\",\"Monocytes#\", \"RDW\"]\n} \n]\n}\n] }\n</code></pre> \u00a0 We are currently using this processor in two capacities. The first is to take a trained model and apply it on noised data, to see how much the model is sensitive to noise at prediction. This is much cheaper (adjust_model), and is incorporated into the autotests as\u00a015.test_noise_sensitivity_analysis.py (see Development kit). The second is to noise a model at training time,\u00a0to see how much the model is sensitive to noise at training. For this purpose, see\u00a0U:\\Itamar\\MR\\Projects\\Shared\\test_noiser\\train_experiment\\example_experiment. The file create_preds_train.py is a python script calling shell commands, to test crc_model.json in same directory. The trained models and preds subdirectories contain outputs of the test, including analysis for noising just time, just value, or just drop probability.\u00a0</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Rep%20Calculator.html","title":"Rep Calculator","text":"<p>Rep processor to calculate things: \u00a0 examples/rep_processor_calc.json Full parameter list:\u00a0https://Medial-EarlySign.github.io/MR_LIBS/classRepCalcSimpleSignals.html \u00a0 The input signals is given with comma separated string in \"signals\" parameter - for example: \"\"</p> <ul> <li>calculator - the calculator type, list of types can be found in next section</li> <li>output_signal_type - string type of output signal. For example \"T(i),V(f)\" to generate signal with 1 time channel of type int and 1 value channel of type float. This is the default</li> <li>max_time_search_range\u00a0 -integer that specify what is the maximal time gap to construct the virtual signal based on all signals. The date of the new signal will be the latest date.\u00a0</li> <li>signals_time_unit - the time unit to use in\u00a0max_time_search_range\u00a0(default Days)</li> <li>names - the name of the virtual signal</li> <li>time_channel - integer that specify which time channel to use in all input signals (default 0)</li> <li>work_channel -\u00a0integer that specify which value channel to use in all input signals (default 0). (If signal has less channels, will use last channel. not common usage)</li> <li>calculator_init_params - additional arguments string based on \"calculator\" parameter. since it's can be multiple arguments, you need to escape the string with \"{}\" and put arguments inside of brackets as in the examples. \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Rep%20Calculator.html#calculator-type-calculator-parameters","title":"Calculator type - \"calculator\" parameters","text":"<p>All \"calculator\" parameter options can be found in here\u00a0https://Medial-EarlySign.github.io/MR_LIBS/classSimpleCalculator.html:</p> <ul> <li>sum - linear combination of multiple signals\u00a0 \u00a0 res := b0 + sum_sigma(i=1..N){ factor[i] * input[i]},\u00a0 \u00a0 where b0 is a bias<ul> <li>calculator_init_params - can receive \"b0\" - to specify constant bias argument + \"factors\" which is comma separated numbers that correspond each input signal (default is list of ones)</li> </ul> </li> <li>log - calculates log on signal</li> <li>ratio - divides signals, accepts \"factor\" as final factor after dividing (default 1),.\u00a0\u00a0res := factor * V1^power_mone / V2^power_base<ul> <li>calculator_init_params - \u00a0\"power_mone\", \"power_base\" which default value is 1, and \"factor\" which the default is also 1</li> </ul> </li> <li>multiply -multiply of signals.\u00a0res := b0 * pie_multiply(i=1..N) {input[i]^powers[i]}\u00a0<ul> <li>calculator_init_params \"b0\" and \"powers\" which is comma separated numbers that correspond each input signal (default is list of ones)</li> </ul> </li> <li>kfre -\u00a0 Implements calculation of 3,4 or 8-variable Kidney Failure Risk Equations (KFRE). \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0The code can be found under \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Libs/Internal/MedProcessTools/MedProcessTools/RepProcess.h \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Libs/Internal/MedProcessTools/MedProcessTools/RepProcess.cpp</li> <li>empty - dummy virtual signal to create empty signal</li> <li>exists -\u00a0res := in_range_val if signal exists otherwise out_range_val<ul> <li>calculator_init_params\u00a0 \u00a0- \"out_range_val\", \"in_range_val\"</li> </ul> </li> <li>range -\u00a0A simple Range check that return \"in_range_val\" if within range and returns \"out_range_val\" if outside range. Accepts also \"min_range\", \"max_range\"<ul> <li>calculator_init_params - \"in_range_val\", \"out_range_val\", \"min_range\", \"max-range\"</li> </ul> </li> <li>set -\u00a0\u00a0res := \"in_range_val\" if is in set otherwise \"out_range_val\"<ul> <li>calculator_init_params\u00a0 \u00a0- \"out_range_val\", \"in_range_val\", \"sets\" or \"sets_file\" to specify list of codes of file path to read codes.\u00a0</li> </ul> </li> <li>eGFR - calculates eGFR from Creatinine, Gender, Age, based on CKD_EPI or MDRD equations.\u00a0<ul> <li>calculator_init_params\u00a0 -\u00a0You can pass \"mdrd\" to control if to use MDRD or CKD_EPI equation. You can also pass \"ethnicity\". 0 -for white, \"1\" for black \u00a0</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Rep%20Calculator.html#examples","title":"Examples:","text":"<p><pre><code>{\n                    \"rp_type\":\"calc_signals\",\n                    \"calculator\":\"ratio\",\n                    \"names\":\"PaO2_over_FiO2\",\n                    \"signals\":\"Art_PaO2,FiO2\",\n                    \"max_time_search_range\":\"180\",\n                    \"signals_time_unit\":\"Minutes\",\n                    \"calculator_init_params\":\"{factor=100}\",\n                    \"unconditional\":\"0\"\n}\n{\n                    \"rp_type\":\"calc_signals\",\n                    \"calculator\":\"set\",\n                    \"names\":\"Ventilation_proc\",\n                    \"signals\":\"PROCEDURE\",\n                    \"max_time_search_range\":\"0\",\n                    \"signals_time_unit\":\"Minutes\",\n                    \"calculator_init_params\":\"{sets=Invasive_Ventilation,Non-invasive_Ventilation}\",\n                    \"unconditional\":\"0\"\n},\n{\n                    \"rp_type\":\"calc_signals\",\n                    \"calculator\":\"range\",\n                    \"names\":\"Ventilation_1\",\n                    \"signals\":\"Peak_Insp_Pressure\",\n                    \"max_time_search_range\":\"0\",\n                    \"signals_time_unit\":\"Minutes\",\n                    \"calculator_init_params\":\"{min_range=0;max_range=1000}\",\n                    \"unconditional\":\"0\"\n },\n</code></pre> \u00a0 Run Flow with those signals (print them): <pre><code>#Flow with rep_processors: pids arg can be omitted to print all pids\nFlow --rep /home/Repositories/MIMIC/Mimic3/mimic3.repository --model_rep_processors $MR_ROOT/Projects/Resources/examples/rep_processor_calc.json --pids_sigs_print --sigs \"PaO2_over_FiO2,Art_PaO2,FiO2\" --pids 100000027\n\u00a0\npid     signal_name     description_name        description_value       ...\n100000027       PaO2_over_FiO2  Time_ch_0       142142154|21700404-19:54        Val_ch_0        147\n100000027       PaO2_over_FiO2  Time_ch_0       142142160|21700404-20:00        Val_ch_0        210\n100000027       PaO2_over_FiO2  Time_ch_0       142142997|21700405-09:57        Val_ch_0        134\n100000027       PaO2_over_FiO2  Time_ch_0       142143060|21700405-11:00        Val_ch_0        67\n100000027       PaO2_over_FiO2  Time_ch_0       142143120|21700405-12:00        Val_ch_0        67\n100000027       PaO2_over_FiO2  Time_ch_0       142143134|21700405-12:14        Val_ch_0        57\n100000027       PaO2_over_FiO2  Time_ch_0       142143180|21700405-13:00        Val_ch_0        57\n100000027       PaO2_over_FiO2  Time_ch_0       142144290|21700406-07:30        Val_ch_0        74\n100000027       PaO2_over_FiO2  Time_ch_0       142144335|21700406-08:15        Val_ch_0        80\n100000027       PaO2_over_FiO2  Time_ch_0       142146240|21700407-16:00        Val_ch_0        98.5714\n100000027       Art_PaO2        Time_ch_0       142142154|21700404-19:54        Time_ch_1       142142154|21700404-19:54        Val_ch_0        147\n100000027       Art_PaO2        Time_ch_0       142142599|21700405-03:19        Time_ch_1       142142599|21700405-03:19        Val_ch_0        89\n100000027       Art_PaO2        Time_ch_0       142142997|21700405-09:57        Time_ch_1       142142997|21700405-09:57        Val_ch_0        67\n100000027       Art_PaO2        Time_ch_0       142143134|21700405-12:14        Time_ch_1       142143134|21700405-12:14        Val_ch_0        57\n100000027       Art_PaO2        Time_ch_0       142143846|21700406-00:06        Time_ch_1       142143846|21700406-00:06        Val_ch_0        52\n100000027       Art_PaO2        Time_ch_0       142143900|21700406-01:00        Time_ch_1       142143900|21700406-01:00        Val_ch_0        57\n100000027       Art_PaO2        Time_ch_0       142144007|21700406-02:47        Time_ch_1       142144007|21700406-02:47        Val_ch_0        63\n100000027       Art_PaO2        Time_ch_0       142144148|21700406-05:08        Time_ch_1       142144148|21700406-05:08        Val_ch_0        74\n100000027       Art_PaO2        Time_ch_0       142144335|21700406-08:15        Time_ch_1       142144335|21700406-08:15        Val_ch_0        80\n100000027       Art_PaO2        Time_ch_0       142145347|21700407-01:07        Time_ch_1       142145347|21700407-01:07        Val_ch_0        86\n100000027       Art_PaO2        Time_ch_0       142146228|21700407-15:48        Time_ch_1       142146228|21700407-15:48        Val_ch_0        69\n100000027       Art_PaO2        Time_ch_0       142146662|21700407-23:02        Time_ch_1       142146662|21700407-23:02        Val_ch_0        71\n100000027       Art_PaO2        Time_ch_0       142147017|21700408-04:57        Time_ch_1       142147017|21700408-04:57        Val_ch_0        89\n100000027       Art_PaO2        Time_ch_0       142148289|21700409-02:09        Time_ch_1       142148289|21700409-02:09        Val_ch_0        64\n100000027       FiO2    Time_ch_0       142142085|21700404-18:45        Time_ch_1       142142085|21700404-18:45        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142142100|21700404-19:00        Time_ch_1       142142100|21700404-19:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142142148|21700404-19:48        Time_ch_1       142142148|21700404-19:48        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142142160|21700404-20:00        Time_ch_1       142142160|21700404-20:00        Val_ch_0        70\n100000027       FiO2    Time_ch_0       142142340|21700404-23:00        Time_ch_1       142142340|21700404-23:00        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142142400|21700405-00:00        Time_ch_1       142142400|21700405-00:00        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142142820|21700405-07:00        Time_ch_1       142142820|21700405-07:00        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142142850|21700405-07:30        Time_ch_1       142142850|21700405-07:30        Val_ch_0        50\n100000027       FiO2    Time_ch_0       142143060|21700405-11:00        Time_ch_1       142143060|21700405-11:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143120|21700405-12:00        Time_ch_1       142143120|21700405-12:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143180|21700405-13:00        Time_ch_1       142143180|21700405-13:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143360|21700405-16:00        Time_ch_1       142143360|21700405-16:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142143435|21700405-17:15        Time_ch_1       142143435|21700405-17:15        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142144290|21700406-07:30        Time_ch_1       142144290|21700406-07:30        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142144560|21700406-12:00        Time_ch_1       142144560|21700406-12:00        Val_ch_0        100\n100000027       FiO2    Time_ch_0       142144800|21700406-16:00        Time_ch_1       142144800|21700406-16:00        Val_ch_0        70\n100000027       FiO2    Time_ch_0       142146000|21700407-12:00        Time_ch_1       142146000|21700407-12:00        Val_ch_0        70\n100000027       FiO2    Time_ch_0       142146240|21700407-16:00        Time_ch_1       142146240|21700407-16:00        Val_ch_0        70\n</code></pre></p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html","title":"Virtual Signals","text":"<p>Sometimes, it's necessary to dynamically add new signals to a repository, assigning them unique names and types, and enabling data insertion. This approach is especially useful when calculating features for specific problems, as storing features or intermediate results as new signals often simplifies the process.</p> <p>Example: If eGFR is not available in your repository (or you need a different calculation method), and you want to generate features like last, min, max, or average values across various time windows, the best solution is to create a virtual signal for eGFR. You can then calculate its values in memory, add them to the repository, and use this signal like any other for feature generation.</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html#defining-virtual-signals","title":"Defining Virtual Signals","text":"<p>To create a virtual signal, use the <code>insert_virtual_signal</code> API from <code>MedSignals.h</code>:</p> <ul> <li><code>int insert_virtual_signal(const string &amp;sig_name, int type)</code> or <code>int insert_virtual_signal(const string &amp;sig_name, const string&amp; signalSpec)</code> with generic string definition<ul> <li><code>sig_name</code>: The new signal's name (must be unique within the repository)</li> <li><code>type</code>: The signal's data type or signalSpec for generic signal definition</li> </ul> </li> </ul> <p>This function validates the input and sets up the necessary internal structures.</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html#adding-data-to-a-virtual-signal","title":"Adding Data to a Virtual Signal","text":"<p>There are two main methods:</p> <ol> <li> <p>In-memory mode:    The repository operates entirely in memory, allowing you to add or remove data freely. Refer to the relevant documentation for details.</p> </li> <li> <p>Dynamic records:    Use the dynamic records API (see PidDynamicRec) to modify or add signal data. This method is used in the MedProcessTools library and is described below.</p> </li> </ol> <p>In MedProcessTools, rep processor stages run first, each with <code>learn</code> and <code>apply</code> methods. For virtual signals that only require calculation (not learning), you simply need to compute and insert the new signal. More complex cases involve learning parameters or models, which are then used for signal calculation. The most advanced scenarios involve virtual signals needed during the learning stage of another processor or feature generator, which are not fully supported yet.</p> <p>During the apply stage, the library creates a <code>PidDynamicRec</code> for each patient, containing all necessary time points and signals for future processing. If the required virtual signals are already defined, you only need to add data to them:</p> <ul> <li>Calculate the time and value channels for your virtual signal at all relevant time points.</li> <li>Note: If dynamic record versions differ, generate a separate data set for each version.</li> <li>Use one of these APIs in <code>PidDynamicRec</code>:<ul> <li><code>int set_version_data(int sid, int version, void *datap, int len)</code><ul> <li><code>sid</code>: Signal ID (retrieve using <code>rep.sigs.sid(string name)</code>)</li> <li><code>version</code>: Target version</li> <li><code>datap</code>: Pointer to an array of the signal's type (e.g., <code>vector&lt;sigType&gt; data;</code> and use <code>&amp;data[0]</code>)</li> <li><code>len</code>: Number of items</li> </ul> </li> <li><code>int set_version_universal_data(int sid, int version, int *_times, float *_vals, int len)</code><ul> <li><code>sid</code>: Signal ID</li> <li><code>version</code>: Target version</li> <li><code>_times</code>, <code>_vals</code>: Arrays for time and value channels; if multiple channels exist, arrange them sequentially for each item</li> <li><code>len</code>: Number of items</li> </ul> </li> </ul> </li> </ul> <p>If multiple versions should share the same data, you can link them using:</p> <ul> <li><code>int point_version_to(int sid, int v_src, int v_dst);</code></li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/Virtual%20Signals.html#creating-a-rep-processor-for-virtual-signals","title":"Creating a Rep Processor for Virtual Signals","text":"<p>You can either write a processor from scratch or use the <code>RepCalcSimpleSignals</code> processor, which simplifies many common cases.</p> <p>From scratch: 1. Each <code>RepProcessor</code> has a <code>virtual_signals_generic</code> vector (defined in the base class) to store all virtual signals it creates. Initialize this vector in the processor's <code>init()</code> function. The model will call <code>add_virtual_signals</code> at the appropriate time. 2. Implement <code>init()</code>, <code>init_tables()</code>, <code>learn()</code>, and <code>apply()</code> as needed. 3. When creating a new virtual signal:    - Loop over dynamic record versions    - Calculate the signal from the dynamic record    - Insert it using either <code>set_version_data</code> or <code>set_version_universal_data</code></p> <p>Using RepCalcSimpleSignals: The <code>RepCalcSimpleSignals</code> processor acts as a wrapper for simple virtual signal calculations. You can implement a new <code>SimpleCalculator</code> and integrate it via <code>RepCalcSimpleSignals::make_calculator</code>. Refer to the code for several straightforward examples.</p>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20create%20an%20empty%20signal/index.html","title":"How to create an empty signal","text":"<p>When a model is trained on repository A and test on repository B, then we might face missing signal. Example:</p> <ul> <li>model A: has RDW signal</li> <li>model B: do not have RDW signal \u00a0To resolve we adjust the model with the following json <pre><code>adjust_model --preProcessors add_empty_signal.json --skip_model_apply 1 --learn_rep_proc 0 --inModel ${IN_MODEL} --out ${OUT_MODEL}\n</code></pre> \u00a0 While add_empty_signal.json is: <pre><code>{ \"pre_processors\" : [\n   {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"calc_signals\",\n          \"calculator\":\"empty\",\n          \"names\":\"RDW\",\n          \"signals\":\"BDATE\" \n      }\n      ]\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20Library/01.Rep%20Processors%20Practical%20Guide/How%20to%20create%20an%20empty%20signal/Howto%20create%20a%20signal%20with%20constant%20value.html","title":"Howto create a signal with constant value","text":"<p>In some cases you want to create \"signal\" with constant value, for example, Race, all population are \"White\". You need to add this block: <pre><code>{ \"pre_processors\" : [\n  {\n    \"action_type\":\"rp_set\",\n    \"members\":[\n        {\n            \"rp_type\":\"calc_signals\",\n            \"calculator\":\"exists\",\n            \"names\":\"Race\",\n            \"signals\":\"MEMBERSHIP\",\n            \"max_time_search_range\":\"0\",\n            \"signals_time_unit\":\"Minutes\",\n            \"calculator_init_params\":\"{in_range_val=1}\",\n            \"unconditional\":\"0\"\n        }\n    ]\n  }\n] }\n</code></pre> The input signal in : \"signals\" should contain signal with \"time channel\" you can't pass BDATE or GENDER (we might change the code to support this). the\u00a0 in_range_val=\"1\" contains the output of the signal, 1 for all records.\u00a0 If you want to use this as dictionary, you will need to specify a dictionary with the mapping of the value \"1\" into the desired string.\u00a0 In our example, that we want to generate \"Race=White\" for all, we will need to add this dict to the repository (TODO: in the future we might configure the dictionary virtually and skip that). <pre><code>SECTION Race\nDEF 1   White\n</code></pre> and add this dicts to the repository</p>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html","title":"Feature Generator Practical Guide","text":"<p>Main page for describing Feature Generators We have and how to use them.</p>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#specific-fgs","title":"Specific FGs\u00a0:","text":"<ul> <li>Basic : <code>basic</code> : generating a wide range of simple (powerful) features such as last, min, max, avg, etc...</li> <li>Age : <code>age</code> : generating age , taking into account time units and the birth year / date.</li> <li>Singleton : <code>singleton</code> :\u00a0take the value of a time-less signal</li> <li>Gender : <code>gender</code> : special case of Singleton</li> <li>Binned LM : <code>binnedLM</code> :\u00a0creating linear model for esitmating feature in time points</li> <li>Smoking : unified_smoking : Smoking feature generation</li> <li>Range : \"range\" : features related to range signals (such as some registries)</li> <li>Drug Intake : \"drugIntake\" : coverage of prescription times</li> <li>Alcohol : \"alcohol\" : very THIN related : convertin raw alcohol THIN signals to features</li> <li>Model : \"model\" : using a pre trained MedModel to generate features as its predictions.</li> <li>Time : \"time\" :\u00a0creating sample-time features (e.g. differentiate between times of day, season of year, days of the week, etc.)</li> <li>Attributes : \"attr\" : creating features from Samples attributes (allows \"loading\" of more data in the Samples file).</li> <li>Signal Dependency\u00a0: \"category depend\" : Select categorial features with correlation to the outcome.</li> <li>Embedding : \"embedding\" : Use a pre trained embedding model to generate features. \u00a0 Some more in depth information on Feature Generators \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#what-is-a-feature-generator","title":"What is a Feature Generator?","text":"<p>Formally, within our infrastructure a Feature Generator is running after all the rep processors. Each Feature Generator can create a constant number of features for all the given time points to a specific pid given in the dynamic record of the pid, after all rep processors had been applied to it. The features are then added to the right position in the MedFeatures object for the model. In subsequent stages after all FGs were run, Features Processors applied to the MedFeatures matrix will be run (such as imputers, normalizers, etc), and the matrix will be ready for training/prediction. Several points to remember, esp. when writing a new FG:</p> <ul> <li>Parallelism : Feature Generators are parallelized on pids in apply stages. So when writing one it should be:<ul> <li>Thread Safe : avoid static variables in FG, and if you do use them : guard them.</li> <li>No need to work hard to parallelize inside as it's already very efficiently parallelized from above.</li> </ul> </li> <li>Empty Learn Stage : many FGs are like that : that is the simplest, you don't need to do anything here.</li> <li>Learn Stage : There's an issue here as the FG needs to get all the Samples and the Rep and do the FG training process. The problem is :<ul> <li>Remeber you may need to apply the rep processors needed for the FG.<ul> <li>Since Rep Processors are currently running on dynamic recs, this may become an issue and needs careful coding.</li> <li>If you are only using signals not affected by previous rep processors, you can indeed work directly with the Samples and Rep.</li> </ul> </li> <li>Learn stage is NOT parallelized, hence you should take care of parallelizing it in your Feature Generator code.</li> </ul> </li> <li>Each FG must fill in the required repository signals list in req_signals. This should be filled in first init() time , and serialized (for actual apply). \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#feature-generator-init-from-parameters","title":"Feature Generator init from parameters","text":"<p>Several things happen in the init() routine:</p> <ul> <li>parameters are parsed</li> <li>names are created (use set_names() for that)</li> <li>req_signals are generated. \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#feature-generator-learnapply-stage-sequence","title":"Feature Generator Learn/Apply Stage sequence","text":"<ul> <li>Model will first scan to find out which generators need to be run (using MedModel::get_applied_generators()). Only those passing will be actually used.</li> <li>As part of the init_all procedure, the following will be called for each generator:</li> <li>set_signal_ids : allowing generator to translate signal names to ids using the rep signals.</li> <li>init_tables : allowing generator to initialize any internal table that requires knowing the dictionaries of the repository.</li> <li>Model will collect all required signals, hence every FG must implement the\u00a0get_required_signal_names() function or fill the req_signals vector</li> <li>The easiest way to ensure this is to make sure the req_signals vector is filled in init() . Make sure you add req_signals to the serialization.</li> <li>If this is done, the default\u00a0get_required_signal_names() is enough as it will simply add the signals in req_signals.</li> <li>In learn stage: The learn() is called, the user is responsible to apply the rep processors of the model inside the specific learn routine.</li> <li>In apply stage</li> <li>optional prepare() function getting MedFeatures, rep and samples is called. This is sometimes needed (for example in the Model FG).</li> <li>features is initialized and get_p_data() is called for each generator (usually the default one is good enough)</li> <li>for each pid , all (required) rep processors are invoked by their order, and then all the (reuired) generators. Parallelism is on the pids. \u00a0  </li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#signal-dependency-fg-category_depend","title":"Signal Dependency FG (\"category_depend\")","text":"<p>Using Signal Dependency to generate categorical signal. chi-square statistical test for independency between outcome and appearance of the category value for Gender+Age stratas. It selects the top k categories with the best p value and Lift.  It also works on sets and hierarchical categories like ATC, ICD10\u00a0 Feature Generator Arguments:</p> <ul> <li>\"signal\" - the categorical signal in repository</li> <li>\"win_from,win_to,time_unit_win\" - time window arguments to define time window</li> <li>\"regex_filter\" - filter categories bt name - for example take only \"ATC\" drug codes. use \"ATC_.*\"</li> <li>\"min_age,max_age,age_bin\" - Age strata for the statistical test. Also uses gender</li> <li>\"min_code_cnt\" - filters categories below this count of apearences</li> <li>\"fdr\" - p value filter</li> <li>\"lift_below,lift_above\" - filters on Lift values of category on average</li> <li>\"max_depth,max_parents\" - hierarchical\u00a0arguments, how many parents to take for each category value (in the example till 5 parentes), and maximal number of parents in that depth</li> <li>\"take_top\" - how many features to create, based on the categories. sorted by P value, lift and chi-square score by this order</li> <li>\"stat_test\" - chi-square or mcnemar (TODO add support for Cochran\u2013Mantel\u2013Haenszel statistics, fisher excat test). mcnemar is not exactly mcnemar because our data is not pairwise matched, but manipulated test to mimic this behaivor. from my experience it's more robust and gives better results</li> <li>\"verbose\" - if 1 prints the taken categories with Lift, total_count and some stats</li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#example-of-using-categorydependencygenerator","title":"Example of using\u00a0CategoryDependencyGenerator:","text":"Config Example<pre><code>{\n    \"action_type\":\"feat_generator\",\n    \"fg_type\":\"category_depend\",\n    \"signal\":\"RC\",\n    \"win_from\":\"0\",\n    \"win_to\":\"1825\",\n    \"time_unit_win\":\"Days\",\n    \"regex_filter\":\"ICD10_CODE:.*\",\n    \"min_age\":\"18\",\n    \"max_age\":\"90\",\n    \"age_bin\":\"5\",\n    \"min_code_cnt\":\"1000\",\n    \"fdr\":\"0.01\",\n    \"lift_below\":\"0.8\",\n    \"lift_above\":\"1.2\",\n    \"stat_metric\":\"mcnemar\",\n    \"max_depth\":\"5\",\n    \"max_parents\":\"10\",\n    \"use_fixed_lift\":\"0\",\n    \"verbose\":\"1\",\n    \"take_top\":\"100\"\n}\n</code></pre> <p>Running train MedModel with this+Age+Gender on death from Flu model with AUC=0.92 compared to Age+Gender only with AUC=0.88 \u00a0</p>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/index.html#todo","title":"TODO:","text":"<ul> <li>Use Stratas instead of fixed:Age,Gender</li> <li>Improve logic for filterHirarchy - to use Entropy or better measure to filter parent\\child\u00a0</li> <li>profilling - improve speed</li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/BasicFeatGenerator.html","title":"BasicFeatGenerator","text":"<p>This feature generator is used to create features for most common signals with 1 time channel and for a specific value channel (for example lab test like Hemoglobin). This feature calculate some stats in time window. It can be last value, mean value, slope, last_time in days of last test, etc. Here is an example block to define this feature generator - you should put this under \"model_actions\" element and after all the rep_processors and before feature_processors: <pre><code>{\n    \"fg_type\": \"basic\",\n    //ELEMENT initialization of BasicFeatGenerator \n}\n</code></pre> \u00a0 Full example on numeric values: <pre><code>{\n    \"fg_type\": \"basic\",\n    \"type\": [ \"last\", \"max\", \"min\", \"avg\", \"slope\", \"last_time\", \"last2\" ],\n    \"window: [ \"win_from=0;win_to=365\" ], //defines time window. I used list in here to be able to use this feature generator on multiple features. we can also use seperatly \"win_from\":\"0\", \"win_to\":\"365\"\n    \"signal\": [ \"Hemoglobin\", \"WBC\", \"Glucose\" ], //The signal to operate on\n    \"val_channel\":\"0\", //if signal has more than 1 channel (for example BP) we can specify on each value channel to work. Deafult is 0\n    \"tags\": \"labs,numeric,need_imputer\" //we can specify \"tags\" for this group of feature to later refer all of them. For example, do imputations for all features with \"need_imputer\" tag\n}\n</code></pre> Categorical example: <pre><code>{\n    \"fg_type\": \"basic\",\n    \"type\": \"category_set\" , //The operator, can also be category_set_count to store the count\n    \"window: [ \"win_from=0;win_to=365\" ], //defines time window. I used list in here to be able to use this feature generator on multiple features. we can also use seperatly \"win_from\":\"0\", \"win_to\":\"365\"\n    \"signal\": \"Drug\" , //The signal to operate on\n    \"val_channel\":\"0\", //if signal has more than 1 channel (for example BP) we can specify on each value channel to work. Deafult is 0\n    \"sets\": \"ATC_A10B_A02,ATC_A10B_A01\", //defined code or list of codes (comma separated) to construct the \"set\". You can also refer to codes in file with \"comma_rel:$FILE_PATH_OF_CODES\"\n    \"in_set_name\": \"Diabetes_drug\", //Optional argument to give name to this set. Otherwise a default of concatinating codes from \"sets\" will be created as the name of the feature\n    \"tags\": \"categorical\" //we can specify \"tags\" for this group of feature to later refer all of them. For example, do imputations for all features with \"need_imputer\" tag\n}\n</code></pre> \u00a0 \u00a0 For full list of arguments, please refer to: https://Medial-EarlySign.github.io/MR_LIBS/classBasicFeatGenerator.html \u00a0 For full list of \"type\" params: https://Medial-EarlySign.github.io/MR_LIBS/FeatureGenerator_8h.html Here are some common operators/types:</p> <ul> <li>\"last\" - take last value in the time window</li> <li>\"max\" - take maximal value</li> <li>\"last_time\" - take last time in days of the test in time window (in days)</li> <li>\"category_set\" - for categorical signal. boolean result. Will test if has value part of \"sets\" parameter in the defined time window</li> <li>\"category_set_count\" - for categorical signal. numeric result. Will count how many times found a value that is part of \"sets\" parameter in the defined time window \u00a0 For full json model format refer to\u00a0MedModel json format</li> </ul>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/Howto%20write%20Feature%20Generator.html","title":"How to Write a Feature Generator","text":"<p>A Feature Generator is a processing unit that takes raw input signals directly from a data repository or EMR. Its process has two main stages:</p> <ul> <li>It runs all relevant rep processors to pre-process the input signals. This prepares the data before it can be used to generate new features. This is being called by the infrastructure.</li> <li>It calls the generate function, which receives this pre-processed, patient-specific data and produces the final output.</li> </ul> <p>Feature Generators in MedModel follow a specific sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the Feature Generator object.</p> </li> <li> <p>init_defaults()    - Sets default values for the generator. please update <code>generator_type</code> to hold genertor type</p> </li> <li> <p>Initialization    - During learning: <code>init(map&lt;string, string&gt;&amp; mapper)</code> parses parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      Please make sure to update <code>req_signals</code> as required input signals for the feature generator      please set <code>tags</code> variable     - During application:      Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</p> </li> <li> <p>fit_for_repository(MedPidRepository)    - Adapts the generator to the repository, e.g., modifies logic if certain signals are missing.</p> </li> <li> <p>Signal Requirements and Setup    - <code>get_required_signal_ids()</code>      Returns the list of required signal IDs for learning or applying the generator.    - <code>set_required_signal_ids(MedDictionarySections)</code>      Stores required signal IDs using dictionary sections.    - <code>set_signal_ids(MedSignals)</code>      Stores required signal IDs using signal objects.    - <code>init_tables(MedDictionarySections)</code>      Initializes tables and stores needed signal IDs using dictionary sections.    - <code>set_names</code> - stores the output names of the feature generator - please override.</p> </li> <li> <p>Feature Filtering    - <code>filter_features()</code>      Determines if this generator is needed (e.g., after feature selection). Returns <code>true</code> if the generator should be kept. Uses by default <code>names</code> variable set by <code>set_names</code> to check if the feature generator is needed and if one of his output names is needed in the pipeline.</p> </li> <li> <p>Signal Names    - <code>get_required_signal_names()</code>      Returns all signal names needed to run this generator.</p> </li> <li> <p>Learning Phase    - <code>learn()</code>      Performs learning logic (called only during training).</p> </li> <li> <p>Preparation    - <code>prepare()</code>      Prepares features, attributes, and allocates space.</p> </li> <li> <p>Output Initialization</p> <ul> <li><code>get_p_data()</code>   Initializes the address for the generator\u2019s output (useful for parallelism).</li> </ul> </li> <li> <p>Feature Generation</p> <ul> <li><code>generate()</code>   Generates the feature for each sample. The infrastructure already execuated all relavent rep processors for the desired input signals the feature generator is using. </li> </ul> </li> <li> <p>Summary</p> <ul> <li><code>make_summary()</code>   Summarizes results after generation (e.g., collects statistics across all data).</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/Howto%20write%20Feature%20Generator.html#steps-to-implement-a-feature-generator","title":"Steps to Implement a Feature Generator","text":"<ol> <li> <p>Create Class Files    - Make a new <code>.h</code> header and <code>.cpp</code> source file for your feature generator class. Include <code>\"FeatureGenerator.h\"</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Exclude temporary or repository-specific variables.</p> </li> <li> <p>Signal and Table Setup    - Implement or override (if needed):</p> <ul> <li><code>set_names</code> Update feature generator output features</li> <li><code>get_required_signal_ids()</code> and <code>get_required_signal_names()</code> - only if needed. The deafult is to use <code>req_signals</code></li> <li><code>set_required_signal_ids(MedDictionarySections)</code>  - only if needed. The deafult is to use <code>req_signals</code></li> <li><code>set_signal_ids(MedSignals)</code> - only if needed to do more setup. </li> <li><code>init_tables(MedDictionarySections)</code></li> <li><code>get_required_signal_categories</code> - if the feature generator uses categorical signals - this will need to list all \"required\" categorical values the feature generator is using</li> </ul> </li> <li> <p>Feature Filtering    - Overide (if needed) <code>filter_features()</code> if your generator should be skipped under certain conditions (e.g., after feature selection). The default is to use <code>names</code> to identify if the feature generator is needed.</p> </li> <li> <p>Learning and Preparation    - Implement <code>learn()</code> for training logic (if needed).    - Implement <code>prepare()</code> to allocate resources and set up attributes.</p> </li> <li> <p>Feature Generation    - Implement <code>generate()</code> to produce the feature for each sample.    - Implement <code>get_p_data()</code> if your generator supports parallel output.</p> </li> <li> <p>Summary    - Implement <code>make_summary()</code> to collect and report statistics after feature generation.</p> </li> <li> <p>Register Your Feature Generator in header file in <code>FeatureGenerator.h</code>    - register a new type in <code>FeatureGeneratorTypes</code> before <code>FTR_GEN_LAST</code> In the documentation comment, specify the name in <code>FeatureGeneratorTypes</code> for Doxygen reference. </p> </li> <li> <p>Register Your Feature Generator in cpp file <code>FeatureGenerator.cpp</code>    - Add your type conversion to <code>ftr_generator_name_to_type</code>    - Add your class to <code>FeatureGenerator::new_polymorphic</code>    - Add your class to <code>FeatureGenerator::make_processor(FeatureGeneratorTypes generator_type)</code></p> </li> </ol> <p>Tip: Follow the structure and naming conventions used in existing feature generators for consistency and easier maintenance.</p>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/Unified_Smoking.html","title":"Unified Smoking","text":"<p>The smoking feature process all raw smoking information signal into unified features. </p>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/Unified_Smoking.html#input-signals","title":"Input signals","text":"<ul> <li><code>Smoking_Status</code> - categorical Smoking Status: Current, Never, Former, Unknown, Never_or_Former</li> <li><code>Smoking_Intensity</code> - How many cigarettes per day patient reports smoking in reported date. (optional signal, will use it only if exists)</li> <li><code>Smoking_Duration</code> - How many years the patient reported smoking in the reported date. (optional signal, will use it only if exists)</li> <li><code>Pack_Years</code> - Calculation of cigarettes packs per day multiply by year (Smoking_Duration * Smoking_Intensity/20). (optional signal, will use it only if exists)</li> <li><code>Smoking_Quit_Date</code> - If patient stopped smoking, the date reported stopped smoking. (optional signal, will use it only if exists)</li> </ul> <p>All input signals has 1 time channel of reported date and value channel with the value.</p>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/Unified_Smoking.html#method","title":"Method","text":"<ol> <li>The Smoking feature generator creates a patient timeline of smoking information based on all available input signals.  For example, no <code>Smoking_Status</code> was provided in certain date but <code>Smoking_Intensity</code> was reported so the patient is marked as <code>Current_Smoker</code> within that time.<ul> <li>The smoking generator breaks the timeline based on the input signals. For example. Patient inputs were: <pre><code>20180101 - Reported Current Smoker\n20210101 - Report Former smoker with Quit date of 20200101\n20210301 - Reported Current smoker again\n20250101 - Reported Former Smoker without quit date\n</code></pre> The feature generator will create a time line with: <pre><code>Assuming age 20 =&gt; 20200101 : Was Smoker\n20200101 =&gt; 20200701 (middle between 20200101 to 20210301): Was Former smoker\n20210101 =&gt; 20230201 (middle between 20210301 to 20250101) : Was Smoker\n20230201 =&gt; today: Former Smoker\n</code></pre> In a case we don't have any reported smoking date, first smoking indication is \"Former smoker\" it uses a simple linear model (based on age reported former smoker) to estimate \"exact\" quit date.</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/02.Feature%20Generator%20Practical%20Guide/Unified_Smoking.html#available-features-outputs","title":"Available features (Outputs)","text":"<ul> <li>One-hot smoking status: Current_Smoker, Ex_Smoker, Never_Smoker, Unknown_Smoker.</li> <li><code>Smoking_Intensity</code>: The average (median) daily smoking amount reported up to the time of prediction.</li> <li><code>Pack_years</code>: A measure of cumulative tobacco exposure, calculated either from reported pack-years or by multiplying Smoking Years by the Smoking Intensity.</li> <li><code>Smok_Days_Since_Quitting</code>: measures the number of days that have passed since the patient stopped smoking.<ul> <li>Current Smokers are assigned 0 days.</li> <li>Never Smokers are assigned patient age in days</li> <li>The exact quit date is given in the input signal <code>Smoking_Quit_Date</code></li> <li>If the exact quit date is missing, we estimate it using one of two methods:<ul> <li>Midpoint Estimate: If the patient's record shows they switched from being a \"Current Smoker\" to an \"Ex-Smoker,\" we assume they quit exactly halfway between those two reporting dates.</li> <li>Model Estimate: If there's no clear reporting window to use, a linear model (trained on typical quit ages) is applied to estimate the most likely quit date based on the age when the patient reported quitting.</li> </ul> </li> </ul> </li> <li><code>Smoking_Years</code>: The estimated total duration the patient has smoked, derived from a complete timeline of their smoking statuses (including the quit date calculation).</li> </ul> <p>Please specify the requested feature output in the field <code>smoking_features</code> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/index.html","title":"FeatureProcessor practical guide","text":"FTR_PROCESS_NORMALIZER <p>\"normalizer\" to create\u00a0FeatureNormalizer</p> FTR_PROCESS_IMPUTER <p>\"imputer\" to create\u00a0FeatureImputer</p> FTR_PROCESS_DO_CALC <p>\"do_calc\" to create\u00a0DoCalcFeatProcessor</p> FTR_PROCESS_UNIVARIATE_SELECTOR <p>\"univariate_selector\" to create\u00a0UnivariateFeatureSelector</p> FTR_PROCESSOR_MRMR_SELECTOR <p>\"mrmr\" or \"mrmr_selector\" to create\u00a0MRMRFeatureSelector</p> FTR_PROCESSOR_LASSO_SELECTOR <p>\"lasso\" to create\u00a0LassoSelector</p> FTR_PROCESSOR_TAGS_SELECTOR <p>\"tags_selector\" to create\u00a0TagFeatureSelector</p> FTR_PROCESSOR_IMPORTANCE_SELECTOR <p>\"importance_selector\" to create\u00a0ImportanceFeatureSelector</p> FTR_PROCESSOR_ITERATIVE_SELECTOR <p>\"iterative_selector\" applies bottom-up or top-down iteration for feature selection. Creates\u00a0IterativeFeatureSelector</p> FTR_PROCESS_REMOVE_DGNRT_FTRS <p>\"remove_deg\" to create\u00a0DgnrtFeatureRemvoer</p> FTR_PROCESS_ITERATIVE_IMPUTER <p>\"iterative_imputer\" to create\u00a0IterativeImputer</p> FTR_PROCESS_ENCODER_PCA <p>\"pca\" to create\u00a0FeaturePCA</p> FTR_PROCESS_ONE_HOT <p>\"one_hot\" to create\u00a0OneHotFeatProcessor\u00a0- make one-hot features from a given feature</p> FTR_PROCESS_GET_PROB <p>\"get_prob\" to create\u00a0GetProbFeatProcessor\u00a0- replace categorical feature with probability of outcome in training set</p> FTR_PROCESS_PREDICTOR_IMPUTER <p>\"predcitor_imputer\" to create\u00a0PredictorImputer</p> FTR_PROCESS_MULTIPLIER <p>\"multiplier\" to create\u00a0MultiplierProcessor\u00a0- to multiply feature by other feature</p> FTR_PROCESS_RESAMPLE_WITH_MISSING <p>\"resample_with_missing\" to create\u00a0ResampleMissingProcessor\u00a0- adds missing values to learn matrix</p> FTR_PROCESS_DUPLICATE <p>\"duplicate\" to create\u00a0DuplicateProcessor\u00a0- duplicates samples in order to do multiple imputations.</p> FTR_PROCESS_MISSING_INDICATOR <p>\"missing_indicator\" to create\u00a0MissingIndicatorProcessor\u00a0- creates a feature that indicates if a feature is missing or not</p> FTR_PROCESS_BINNING <p>\"binning\" to create\u00a0BinningFeatProcessor\u00a0- binning with one hot on the bins</p> <p>- \"remove_deg\" - removes features that most of the time are same value (or missing)\u00a0 <pre><code>{\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n          \"fp_type\": \"remove_deg\",\n          \"percentage\": \"0.999\"\n        }\n      ]\n  }\n</code></pre> - \"normalizer\" - to normalize features: <pre><code> {\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n          \"fp_type\": \"normalizer\",\n          \"resolution_only\":\"0\",\n          \"resolution\":\"5\",\n          \"tag\":\"need_norm\",\n          \"duplicate\":\"1\"\n        }\n      ]\n    }\n</code></pre>   tag + duplicate=1, results in wrapping this feature processor with MultiFeatureProcessors that iterates over all features and filter out features by \"tag\" to apply this normalization processing. I tagged all the required relevant features with tag \"need_norm\" - \"imputer\" by strata of age+sex+other features. Take median,common, average, sample, etc:</p> <p><pre><code>{\n\"action_type\": \"fp_set\",\n\"members\": [\n{\n\"fp_type\": \"imputer\",\n\"strata\": \"Age,0,80,10:Gender,1,2,1\",\n\"moment_type\":\"common\",\n\"tag\":\"need_imputer\",\n\"duplicate\":\"1\"\n}\n]\n}\n</code></pre> - \"do_calc\" - calculator of some features from others. For example \"or\" on features, can also \"sum\" features <pre><code>{\n        \"action_type\": \"fp_set\",\n          \"members\": [\n            {\n              \"fp_type\": \"do_calc\",\n              \"calc_type\": \"or\",\n              \"source_feature_names\": \"Current_Smoker,Ex_Smoker\",\n              \"name\":  \"Ex_or_Current_Smoker\"\n            }\n        ]\n    }\n</code></pre></p> <ul> <li>importance_selector - selection of features based on most important features in a model that is trained on the data.\u00a0</li> <li>iterative_selector - please use the tool to do it, it takes forever!!\u00a0Iterative Feature Selector</li> <li>resample_with_missing - used in training to \"generate\" more samples with missing values and increasing data size (not doing imputations, that</li> </ul> <pre><code>{\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n             \"fp_type\": \"resample_with_missing\",\n             \"missing_value\":\"-65336\",\n             \"grouping\":\"BY_SIGNAL_CATEG\",\n             \"selected_tags\":\"labs_numeric\",\n             //\"removed_tags\":\"\",\n             \"duplicate_only_with_missing\":\"0\",\n             \"add_new_data\":\"3000000\",\n             \"sample_masks_with_repeats\":\"1\",\n             \"limit_mask_size\":\"2\",\n             \"uniform_rand\":\"0\",\n             \"use_shuffle\":\"0\",\n             \"subsample_train\":\"3000000\",\n             \"verbose\":\"1\"\n        }\n      ]\n    }\n</code></pre> <p>No need for duplicate, it is scanning the features by it's own and operating on \"selected_tags\". add_new_data- - how many new data points to add. grouping is used to generate masks of missing values in groups and not feature by features. This is another feature processor job). similar to data augmentation in imaging</p> <ul> <li> <p>\"binning\" - binning feature value - can be specified directly the cutoffs or using some binning_method, equal width, minimal observations in each bin, etc. <pre><code>{\n    \"action_type\": \"fp_set\",\n    \"members\": [\n        {\n            \"action_type\": \"feat_processor\",\n            \"fp_type\": \"binning\",\n            \"bin_sett\": \"{bin_cutoffs=0,1,15,30,100}\",\n            \"one_hot\": 0,\n            \"tag\": \"Smok_Pack_Years_Last\", \n            \"duplicate\": 1\n        }\n    ]\n    }\n</code></pre></p> </li> <li> <p>\"predcitor_imputer\" - much more complicated/smart imputer based on model. Gibbs samplings, masked GAN, univariate sampling from features distributions, etc..\u00a0 <pre><code>{\n      \"action_type\": \"fp_set\",\n      \"members\": [\n        {\n          \"fp_type\": \"predictor_imputer\",\n          \"tag\":\"need_imputer\",\n          \"duplicate\":\"0\",\n          \"gen_type\":\"GIBBS\",\n          \"verbose_learn\":\"1\",\n          \"verbose_apply\":\"1\",\n          \"use_parallel_learn\":\"0\",\n          \"use_parallel_apply\":\"0\",\n          \"generator_args\":\"{calibration_save_ratio=0.2;bin_settings={split_method=iterative_merge;min_bin_count=100;binCnt=50};calibration_string={calibration_type=isotonic_regression;verbose=0};predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=1;num_trees=10;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=20;feature_fraction=0.8;bagging_fraction=0.25;bagging_freq=4;is_unbalance=true;num_leaves=80;silent=2};selection_count=200000}\",\n          \"sampling_args\":\"{burn_in_count=20;jump_between_samples=5;find_real_value_bin=1}\"\n        }\n      ]\n    }\n</code></pre> Instead of GIBBS, GAN, can select:</p> </li> <li> <p>RANDOM_DIST\u00a0- random value from normal dist around 0,5 (not related to feature dist)</p> </li> <li>UNIVARIATE_DIST - strata by some features, store distribution in each strata. In apply, find strata and select value randomly from dist</li> <li>MISSING - put missing value</li> <li>GAN -\u00a0generator_args is path to trained model. Please refer to this path to train:\u00a0TrainingMaskedGAN</li> <li>GIBBS - arguments\u00a0<ul> <li>sampling_args -\u00a0<ul> <li>burn_in_count - how many round to do in the start and to ignore them till stablize on reasonable vector.\u00a0</li> <li>jump_between_samples - how many rounds to do before generating new sample. when continuing to iterate in rounds, after several loops we end up with different sample</li> <li>find_real_value_bin - if true will round values to existing values only from feature values. When you try to see if model can discriminate between real data and generated data, the resolution of the feature values is important. tree can detect different between 3 nd 3.000001. If true this will cause 3.00001 to be 3. No good reason why to turn off</li> <li>samples_count - how many samples to extract</li> </ul> </li> <li>generator_args<ul> <li>calibration_save_ratio - what percentage of data to keep for clibration to probability. 0.2 (20%) is good number</li> <li>bin_settings - how to split the feature value into bins. The prediction problem will be multi category to those binned values. For example, hemoglobin last : 13.1-13.3, 13.4-13.6, etc... The target will be what's the probability to have hemoglobin in each value range. after having this, we can sample from this distribution bin value for the feature.\u00a0</li> <li>calibration_string - how to calibarte. keep as isotonic_regression, it's good</li> <li>predictor_type - predictor type for the multi class prediction</li> <li>predictor_args - arguments for the predictor. please pay attention this is multi category prediction! For example objective for lightGBM is \"objective=multiclass\"</li> <li>num_class_setup - since this is multiclass, some predictors requires to setup how many classes there are. This argument controls the name of this parameters. In LightGBM for example it's called \"num_class\"</li> <li>selection_count - down sampling for training those models to speedup</li> </ul> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Charlson.html","title":"Charlson","text":"<p>Json to generate Charlson score. Path: $MR_ROOT/Projects/Resources/examples/Charlson/charlson.pretify.json \u00a0 This is for THIN,\u00a0 input is DIAGNOSIS signal <pre><code>{\n    \"model_json_version\": \"2\",\n    \"serialize_learning_set\": \"0\",\n    \"model_actions\": [\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"age\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"MI\",\n            \"sets\": \"comma_rel:Diseases/MI.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"CHF\",\n            \"sets\": \"comma_rel:Diseases/CHF.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"PVD\",\n            \"sets\": \"comma_rel:Diseases/PVD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"CerebrovascularD\",\n            \"sets\": \"comma_rel:Diseases/CerebrovascularD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Dementia\",\n            \"sets\": \"comma_rel:Diseases/Dementia.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"ChronicPulmD\",\n            \"sets\": \"comma_rel:Diseases/ChronicPulmD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"RheumaticD\",\n            \"sets\": \"comma_rel:Diseases/RheumaticD.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"PepticUlcer\",\n            \"sets\": \"comma_rel:Diseases/PepticUlcer.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"MildLiver\",\n            \"sets\": \"comma_rel:Diseases/MildLiver.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"DiabetesNComplx\",\n            \"sets\": \"comma_rel:Diseases/DiabetesNComplx.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"DiabetesWComplx\",\n            \"sets\": \"comma_rel:Diseases/DiabetesWComplx.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"XPlegia\",\n            \"sets\": \"comma_rel:Diseases/XPlegia.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Renal\",\n            \"sets\": \"comma_rel:Diseases/Renal.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Cancers\",\n            \"sets\": \"comma_rel:Diseases/Cancers.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"SevereLiver\",\n            \"sets\": \"comma_rel:Diseases/SevereLiver.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Metastatic\",\n            \"sets\": \"comma_rel:Diseases/Metastatic.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"HIV\",\n            \"sets\": \"comma_rel:Diseases/HIV.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Leukemia\",\n            \"sets\": \"comma_rel:Diseases/Leukemia.list\"\n        },\n        {\n            \"action_type\": \"feat_generator\",\n            \"fg_type\": \"basic\",\n            \"type\": \"category_set\",\n            \"window\": [\n                \"win_from=0;win_to=18250\"\n            ],\n            \"time_unit\": \"Days\",\n            \"signal\": \"DIAGNOSIS\",\n            \"in_set_name\": \"Lymphoma\",\n            \"sets\": \"comma_rel:Diseases/Lymphoma.list\"\n        },\n        //Processings\n        {\n            \"action_type\": \"fp_set\",\n            \"members\": [\n                //Age binnings\n                {\n                    \"fp_type\": \"binning\",\n                    \"name\": \"Age\",\n                    \"bin_sett\": \"{bin_cutoffs=49,59,69,79;bin_repr_vals=0,1,2,3,4}\",\n                    \"bin_format\": \"%1.0f\",\n                    \"remove_origin\": \"0\",\n                    \"one_hot\": \"1\",\n                    \"keep_original_val\": \"1\"\n                },\n                //substraction of intersecting conditioins:\n                {\n                    \"fp_type\": \"do_calc\",\n                    \"calc_type\": \"and\",\n                    \"source_feature_names\": \"DiabetesNComplx,DiabetesWComplx\",\n                    \"name\": \"Double_Diabetes\"\n                },\n                {\n                    \"fp_type\": \"do_calc\",\n                    \"calc_type\": \"and\",\n                    \"source_feature_names\": \"MildLiver,SevereLiver\",\n                    \"name\": \"Double_Liver\"\n                },\n                {\n                    \"fp_type\": \"do_calc\",\n                    \"calc_type\": \"and\",\n                    \"source_feature_names\": \"Cancers,Metastatic\",\n                    \"name\": \"Double_Cancer\"\n                }\n            ]\n        },\n        {\n            \"action_type\": \"feat_processor\",\n            \"fp_type\": \"do_calc\",\n            \"calc_type\": \"sum\",\n            \"source_feature_names\": \"Age.BINNED_1,Age.BINNED_2,Age.BINNED_3,Age.BINNED_4,MI,CHF,PVD,CerebrovascularD,Dementia,ChronicPulmD,RheumaticD,PepticUlcer,MildLiver,SevereLiver,DiabetesNComplx,DiabetesWComplx,XPlegia,Renal,Cancers,HIV,Leukemia,Lymphoma,Double_Diabetes,Double_Liver,Metastatic,Double_Cancer\",\n            \"weights\": \"1,2,3,4,1,1,1,1,1,1,1,1,1,3,1,2,2,2,2,2,2,6,-1,-1,6,-2\",\n            \"name\": \"Charlson\",\n            \"duplicate\": \"0\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Framingham%20Feature%20Processor.html","title":"Framingham Feature Processor","text":"<pre><code>//Diabetes registry virtual signal (rep processor) to create signal DM_Registry:\n{\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        {\n          \"rp_type\":\"create_registry\", \n          \"registry\":\"dm\", \n          \"names\":\"DM_Registry\",\n          \"dm_diagnoses_sig\":\"DIAGNOSIS\",\n          \"dm_drug_sets\":\"list_rel:registries/diabetes_drug_codes.2020.full\",\n          \"dm_diagnoses_sets\":\"list_rel:registries/diabetes_read_codes_registry.full.striped\",\n           \"signals\":\"Glucose,HbA1C,Drug,DIAGNOSIS\"\n        }\n    ]\n},\n\u00a0\n//Age + Gender input features:\n{ \"action_type\": \"feat_generator\", \"fg_type\": \"age\" },\n{ \"action_type\": \"feat_generator\", \"fg_type\": \"gender\" },\n//DM last value:\n{\n      \"action_type\":\"feat_generator\",\n            \"fg_type\":\"range\",\n            \"type\":\"ever\",\n            \"window\":\"win_from=0;win_to=100000\",\n            \"time_unit\":\"Days\",\n            \"signal\":\"DM_Registry\",\n            \"sets\":\"DM_Registry_Diabetic\"\n},\n//Other signals:\n{    \"feat_generator\":\"basic\",\n    \"type\": \"last\",\n    \"window:  \"win_from=0;win_to=1095\" ,\n    \"signal\": [ \"Cholesterol\", \"HDL\" ], //The signal to operate on\n    \"val_channel\":\"0\"\n},\n{    \"feat_generator\":\"basic\",\n    \"type\": \"last\",\n    \"window:  \"win_from=0;win_to=1095\" ,\n    \"signal\": [ \"BP\" ], //The signal to operate on\n    \"val_channel\":[\"0\", \"1\"]\n},\n\u00a0\n{\n  \"action_type\":\"fp_set\",\n  \"members\": [\n    {\n        \"fp_type\":\"do_calc\",\n        \"calc_type\":\"framingham_chd\",\n        \"source_feature_names\":\"Gender,Age,DM_Registry,Current_Smoker,BP.last.win_0_1095,BP.last.win_0_1095.t0v1,Cholesterol.last.win_0_1095,HDL.last.win_0_1095,Drug.category_set_hypertension_drugs.win_0_1095\",\n        \"name\":\"Framingham_feature_name\"\n    }\n  ]\n}\n</code></pre>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Howto%20write%20Feature%20Processor.html","title":"How to Write a Feature Processor","text":"<p>Feature Processors are components that operate on the feature matrix produced by the Feature Generator. They take a matrix of features as input, process it (e.g., normalization, feature selection, PCA), and output a transformed feature matrix.</p> <p>Feature Processors in MedModel follow a defined sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the Feature Processor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the processor. Be sure to update <code>processor_type</code> to reflect the processor type.</p> </li> <li> <p>Initialization    - During learning:      Implement <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      If your processor affects a single feature, you may want to use <code>feature_name</code> to specify the output feature.    - During application:      Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</p> </li> <li> <p>Repository Setup    - Calls <code>set_feature_name</code> to configure the processor using repository information.</p> </li> <li> <p>Feature Filtering    - Methods like <code>update_req_features_vec</code>, <code>are_features_affected</code>, and <code>filter</code> determine if this Feature Processor is needed for prediction.      If the processor does not affect any required features, it will be skipped.      By default, <code>filter</code> uses <code>feature_name</code> to check if the processor is necessary.</p> </li> <li> <p>select_learn_matrix    - Usually not required. In special cases, you may want to create a copy of the original feature matrix and store it under a different name for use by other processors in the pipeline.</p> </li> <li> <p>Learning Phase    - <code>learn()</code>      Implements any learning logic needed during training.</p> </li> <li> <p>Feature Processing    - <code>apply()</code>      Applies the processor logic to the feature matrix.</p> </li> </ol>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Howto%20write%20Feature%20Processor.html#steps-to-implement-a-feature-processor","title":"Steps to Implement a Feature Processor","text":"<ol> <li> <p>Create Class Files    - Create a new <code>.h</code> header and <code>.cpp</code> source file for your feature processor class. Include <code>\"FeatureProcess.h\"</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Do not include temporary or repository-specific variables.</p> </li> <li> <p>Custom Setup (if needed)    - Implement or override:</p> <ul> <li><code>filter</code> (update logic if your processor affects a specific set of features)</li> </ul> </li> <li> <p>Learning    - Implement <code>learn()</code> for any required training logic.</p> </li> <li> <p>Apply    - Implement <code>apply()</code> to process the features.</p> </li> <li> <p>Register Your Feature Processor in the Header (<code>FeatureProcess.h</code>)    - Add a new type to <code>FeatureProcessorTypes</code> before <code>FTR_PROCESS_LAST</code>. In the documentation comment, specify the name in <code>FeatureProcessorTypes</code> for Doxygen reference.</p> </li> <li> <p>Register Your Feature Processor in the Source (<code>FeatureProcess.cpp</code>)    - Add your type conversion to <code>feature_processor_name_to_type</code>    - Add your class to <code>FeatureProcessor::new_polymorphic</code>    - Add your class to <code>FeatureProcessor::make_processor(FeatureProcessorTypes processor_type)</code></p> </li> </ol> <p>Tip: Follow the structure and naming conventions of existing feature processors for consistency and easier maintenance.</p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/index.html","title":"Embeddings","text":"<p>The Embeddings tool set is designed to allow generation of strong features embedding a very large feature space into a much smaller dimension, and giving the tools to train these Embeddings, and to use them as a feature generator within the InfraStructure. The general plan is the following:</p> <ol> <li>Define the (large) feature set (use the embed_params many options for that, see below)</li> <li>Create x,y matrices to train the embedding : this will also create the .scheme file which is the recipe file of how to create a line in the x matrix - this will be needed later.</li> <li>Train the embedding using Keras (Embedder.py script) : this will also set the dimension of the embedding.</li> <li>Use the .scheme file and the keras layers file to define embedded features to be generated on your prediction problem - the new embedded features will be added to your train/test matrices, the .scheme and layers information will be serialized into your trained model. \u00a0 We will cover the technical details of how to perform each of the steps above, and also take a look at the Embedding WalkThrough Example page to see an actual example. \u00a0</li> </ol>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html","title":"Embeddings WalkThrough Example","text":"<p>In this example we will first create an unsupervised embedding space, and then use it as features for predicting the first CVD MI. All work done here can be found in:\u00a0/nas1/Work/Users/Avi/test_problems/embedding_example The code needed for this is either in MR_LIBS or in MR/Projects/Shared/Embeddings. Quick Jump:</p> <ul> <li>Step 1 : Plan</li> <li>Step 2: Prepare basic lists , and cohort</li> <li>Step 3 : Design and prepare the training dates for embeddings</li> <li>Step 4 : Prepare the embed_params files for x and y</li> <li>Step 5: Create the x and y matrices </li> <li>Step 6: Train the embedding using Keras</li> <li>Step 7 : Testing we get the same embeddings in Keras and Infrastructure</li> <li>Step 8 : Using Embedding In a MedModel</li> <li>Step 9 : Results for MI with/without Embedding , with/without SigDep </li> </ul>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-1-plan","title":"Step 1 : Plan","text":"<p>Our plan is to use the CVD_MI registry, then use TRAIN=1 group for training/cross validation , and TRAIN=2 group for actual validation. We will need to sacrifice some data in order to train our embedding. We do it simply by splitting the TRAIN=1 group into two random groups: 1A and 1B , 1A will be used in training our embedding, 1B will be used in training our model. We will give scores at random times before the outcome (and slice with bootstrap later), say one random sample every 180 days. We will also make sure our samples have at least a BP or LDL or Glucose reading at least 3 years before the sample. This gives us the plan for the outcome training. For the embedding training - we have lots of options, and it is not clear which is the best. In this example we will choose the option of using the same dates and outcome times as in the outcome training group, generate a feature rich large x (source) matrix at the sample times, and a large (but smaller) destiny matrix y for the times (outcome,\u00a0outcome + T) , T = 1y , which will also include our outcome variable. We will then use the embedded layer as features added to the CVD model and see if we see any improvement in perfomance. General Vocabulary of terms we use:</p> <ul> <li>sparse matrix : a matrix in which most values are 0 , and hence can be defined by showing only the few non-zero elements. Very efficient in holding large sets of sparse categorial features. We use the MedSparseMat class to handle these objects. Usually we create such matrices with a prefix followed by a suffix :</li> <li>.smat : the actual sparse matrix, lines of ,, <li>.meta : containing the list of pid,time matching each line</li> <li>.dict : a dictionary giving the names of the features in each column</li> <li>.scheme : the serialized scheme file for this matrix. Allows recreating such matrices with the same rules/dictionaries on a different set of samples.</li> <li>x,y matrices - the marices we train the embedding on. The embedding will start from the (sparse) features in x[i] vector run through a network of several layers, one of which is the embedding layer, and end up in the y[i] (sparse) vector.</li> <li>embed_params : a file containing the parameters (init_from_string format) defining how to generate a sparse matrix, for example which categorial signals to use, on which sets in which time windows, etc.</li> <li>scheme file : a serialized EmbedMatCreator object (from MedEmbed.h) , containing a ready to use object that can be deserialized and used to generate sparse matrices using a predefined set of rules that was learnt.</li> <li>layers file : a file containing a keras embedding model in a format we can read and use in our infrastructure.</li> <li>outcome training : the process of training the actual model for the outcome (in this example cvd_mi)</li> <li>embedding training : the process of preparing the x,y matrices, the scheme files, train a deep learning embedding model in keras, and get the layers file. Scheme and layer files will later be used as a feature generator in the infrastructure.</li>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-2-prepare-basic-lists-and-cohort","title":"Step 2: Prepare basic lists , and cohort","text":"<p><pre><code># creating a cohort file for cvd_mi : first verified mi, censoring cases with other mi's before , and cases with no BP or Glucose or LDL tests\n\u00a0\n/nas1/UsersData/avi/MR/Projects/Shared/Embeddings/Linux/Release/Embeddings --build_example\n\u00a0\n# creating samples for training/testing for TRAIN=1 and TRAIN=2\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --cohort_fname ./cvd_mi.cohort --cohort_sampling \"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=1;min_age=35;max_age=90;min_year=2004;max_year=2017;max_samples_per_id=4\" --out_samples ./train_1.samples\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --cohort_fname ./cvd_mi.cohort --cohort_sampling \"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=2;min_age=35;max_age=90;min_year=2004;max_year=2017;max_samples_per_id=4\" --out_samples ./train_2.samples\n\u00a0\n\u00a0\n# now filter and match on train_1 samples : making sure samples have needed information, and raising the case to control ratio in learning set\n# on the validation set we only apply the filter for needed information\n# generate validate_1 validate_2 subsets\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --filter_and_match --in_samples ./train_1.samples --out_samples ./validate_1.samples --filter_params \"min_sample_time=20040101;max_sample_time=20160101;bfilter=sig_name,BP,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,Glucose,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,LDL,win_from,0,win_to,730,min_Nvals,1;min_bfilter=1\"\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --filter_and_match --in_samples ./train_2.samples --out_samples ./validate_2.samples --filter_params \"min_sample_time=20040101;max_sample_time=20160101;bfilter=sig_name,BP,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,Glucose,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,LDL,win_from,0,win_to,730,min_Nvals,1;min_bfilter=1\"\n\u00a0\nFlow --rep /home/Repositories/THIN/thin_jun2017/thin.repository --filter_and_match --in_samples ./validate_1.samples --out_samples ./learn_1.samples --match_params \"priceRatio=200;maxRatio=10;verbose=1;strata=time,year,1\"\n\u00a0\n# prepare pids groupA (for embedding) and groupB (for model)\nless validate_1.samples | awk '(NR&gt;1){print $2}' | uniq | awk '{print $1, 1+(rand()&lt;0.5)}' &gt; train_pids_groups\nless train_pids_groups | awk '($2==1){print $1}' &gt; pids_group_A\nless train_pids_groups | awk '($2==2){print $1}' &gt; pids_group_B\n# prepare validate_1_A , validate_1_B , learn_1_A , learn_1_B\nintersect.pl validate_1.samples 1 pids_group_A 0 &gt; validate_1_A.samples\nintersect.pl validate_1.samples 1 pids_group_B 0 &gt; validate_1_B.samples\nintersect.pl learn_1.samples 1 pids_group_A 0 &gt; learn_1_A.samples\nintersect.pl learn_1.samples 1 pids_group_B 0 &gt; learn_1_B.samples\n</code></pre> \u00a0 We now intend to use validate_1_A.samples or learn_1_A.samples to design an embedding training, and then use learn_1_B.samples to train a predictor for cvd_mi, once without embeddings signals, and once with, and compare the results (on validate_2_B in CV, and eventually on validate_2).</p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-3-design-and-prepare-the-training-dates-for-embeddings","title":"Step 3 : Design and prepare the training dates for embeddings","text":"<p>To train the embedding we need: 1. x matrix , at some points in time, could be random points, or points similar to how we choose validation or learning samples. We will use the pid, time of validate_1_A.samples for that (~1.6M points, ~660K patients) 2. y matrix : lots of options:</p> <ol> <li>take y to be x : this is exactly learning an autoencoder.</li> <li> <p>simply take the y time to be x time + some translation t from a set T of optional time translations:</p> </li> <li> <p>T =\u00a0{0} (semi-autoencoder) , T={365} (one year forward) , T={730} (two years forward) , T={-365} (one year backwards) etc... </p> </li> <li>T={0,365,730} , all options or random choice of one for each sample. We will use two options: semi-autoencoder (T for y is 0), and option 2b , and randomly set the y time to be 1y or 2y ahead from the sampling point. We keep that in a samples file where the time is the time for x and the outcomeTime is the time for y. Our code later supports this.</li> </ol> <p><pre><code># preparing embedding_1_A.samples (using awk this time)\n# we could have started from learn_1_A samples, but decided to be more general and use lots of data, hence starting from validate_1_A\n\u00a0\nless validate_1_A.samples | awk '(/EVENT/){print}(/SAMPLE/){a=1+(rand()&gt;0.5);a=a*10000;print \"SAMPLE\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$3+a\"\\t\"$6}' &gt; embedding_1_A.samples\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-4-prepare-the-embed_params-files-for-x-and-y","title":"Step 4 : Prepare the embed_params files for x and y","text":"<p>The embed_params file holds all the parameters needed in order to create a sparse matrix for an embedding. For categorial signals we can choose the sets we are interested in (could be a very large number), and a few more settings such as time windows, if to keep as counts, if to shrink, etc. For continous signals we can choose the ranges we are interested in. We can also use a MedModel generated before from a json file and add the features it creates (this option is still in dev/testing) We could use the same embed_params for x and y matrices, but to make things interesting we will use different plans for the x and y matrices. some preparations:</p> <p><pre><code># preparing the list of read codes we'll use to generate features (that's ~97K features !)\nless /home/Repositories/THIN/thin_jun2017/dict.read_codes | awk '(/DEF/ &amp;&amp; substr($3,0,2)==\"G_\"){print $3}' &gt; rc.codes\n\n### preparing the list of atc codes we'll use to generate features (~6k features)\nless /home/Repositories/THIN/thin_jun2017/dict.drugs_defs | awk '(/DEF/ &amp;&amp; substr($3,0,4)==\"ATC_\"){print $3}' | grep -v \":\" &gt; atc.codes\n</code></pre> \u00a0 x_embed_params :</p> <p><pre><code>sigs={\n# sigs are sig=&lt;&gt; format with | between sigs\n# dummy signal is needed to make sure there's at least one entry for each line , helps in matching x,y lines\nsig=dummy;type=dummy|\n# age category with a binning to 10 groups\nsig=BYEAR;type=age;ranges=0,10,20,30,40,50,60,70,80,90,1000;do_shrink=0|\n# gender with 2 categories\nsig=GENDER;type=continuous;ranges=1,2,3;do_shrink=0|\n# RC signals : categorial , use rc.codes list as categories, look at a large (ever) time window, use hierarchy and count each category used\nsig=RC;type=categorial;categories=list:rc.codes;win_from=0;win_to=36500;add_hierarchy=1;do_counts=1|\n# RC signals : categorial same as previous but for a short (1y) time window\nsig=RC;type=categorial;categories=list:rc.codes;win_from=0;win_to=365;add_hierarchy=1;do_counts=1|\n# Drug signals , last 2 years\nsig=Drug;type=categorial;categories=list:atc.codes;win_from=0;win_to=730;add_hierarchy=1;do_counts=1|\n# Adding LDL distribution to bins in last 5 years\nsig=LDL;type=continuous;ranges=0,50,70,100,120,150,200,300,10000;do_shrink=0;win_from=0;win_to=1800;do_counts=1|\n# Adding Glucose distribution to bins in last 5 years\nsig=Glucose;type=continuous;ranges=0,50,70,90,100,110,125,150,200,10000;do_shrink=0;win_from=0;win_to=1800;do_counts=1|\n# Adding HbA1C distribution to bins in last 5 years\nsig=HbA1C;type=continuous;ranges=0,4.0,5.0,5.7,6.0,6.5,7.0,8.0,10.0,10000;do_shrink=0;win_from=0;win_to=1800|\n# Adding Cretinine distribution to bins in last 5 years\nsig=Creatinine;type=continuous;ranges=0,0.5,0.8,1.0,1.2,1.5,2.0,3.0,4.0,10000;do_shrink=0;win_from=0;win_to=1800|\n# Adding BP distribution to bins in last 5 years\nsig=BP;type=continuous;val_chan=0;ranges=0,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,10000;do_shrink=0;win_from=0;win_to=1800|\nsig=BP;type=continuous;val_chan=1;ranges=0,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,10000;do_shrink=0;win_from=0;win_to=1800\n};\n</code></pre> \u00a0 y_embed params: preferring do_counts=0 as easier to build loss function for. <pre><code>sigs={\n# sigs are sig=&lt;&gt; format with | between sigs\n# dummy signal is needed to make sure there's at least one entry for each line , helps in matching x,y lines\nsig=dummy;type=dummy|\n# age category with a binning to 10 groups\nsig=BYEAR;type=age;ranges=0,10,20,30,40,50,60,70,80,90,1000;do_shrink=0|\n# gender with 2 categories\nsig=GENDER;type=continuous;ranges=1,2,3;do_shrink=0|\n# RC signals : categorial same as previous but for a short (1y) time window\nsig=RC;type=categorial;categories=list:rc.codes;win_from=0;win_to=365;add_hierarchy=1;do_counts=0|\n# Drug signals , last 2 years\nsig=Drug;type=categorial;categories=list:atc.codes;win_from=0;win_to=365;add_hierarchy=1;do_counts=0|\n# Adding LDL distribution to bins in last year\nsig=LDL;type=continuous;ranges=0,50,70,100,120,150,200,300,10000;do_shrink=0;win_from=0;win_to=365;do_counts=0|\n# Adding Glucose distribution to bins in last year\nsig=Glucose;type=continuous;ranges=0,50,70,90,100,110,125,150,200,10000;do_shrink=0;win_from=0;win_to=365;do_counts=0|\n# Adding HbA1C distribution to bins in last year\nsig=HbA1C;type=continuous;ranges=0,4.0,5.0,5.7,6.0,6.5,7.0,8.0,10.0,10000;do_shrink=0;win_from=0;win_to=365;do_counts=0|\n# Adding Past/Future Registries: past, 1y ahead, 5y ahead\n# In y matrices it is perfectly OK to peek into the future (!) we only use them for training\nsig=CVD_MI;type=categorial;categories=CVD_MI_Confirmed_Event,CVD_MI_Untimed_Event,CVD_MI_History;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CVD_MI;type=categorial;categories=CVD_MI_Confirmed_Event,CVD_MI_Untimed_Event,CVD_MI_History;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_MI;type=categorial;categories=CVD_MI_Confirmed_Event,CVD_MI_Untimed_Event,CVD_MI_History;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_IschemicStroke;type=categorial;categories=CVD_IschemicStroke_Confirmed_Event,CVD_IschemicStroke_Untimed_Event,CVD_IschemicStroke_History;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CVD_IschemicStroke;type=categorial;categories=CVD_IschemicStroke_Confirmed_Event,CVD_IschemicStroke_Untimed_Event,CVD_IschemicStroke_History;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_IschemicStroke;type=categorial;categories=CVD_IschemicStroke_Confirmed_Event,CVD_IschemicStroke_Untimed_Event,CVD_IschemicStroke_History;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HemorhagicStroke;type=categorial;categories=CVD_HemorhagicStroke_Confirmed_Event,CVD_HemorhagicStroke_Untimed_Event,CVD_HemorhagicStroke_History;win_from=0;win_to=0;do_counts=10000;do_shrink=0|\nsig=CVD_HemorhagicStroke;type=categorial;categories=CVD_HemorhagicStroke_Confirmed_Event,CVD_HemorhagicStroke_Untimed_Event,CVD_HemorhagicStroke_History;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HemorhagicStroke;type=categorial;categories=CVD_HemorhagicStroke_Confirmed_Event,CVD_HemorhagicStroke_Untimed_Event,CVD_HemorhagicStroke_History;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HeartFailure;type=categorial;categories=CVD_HeartFailure_First_Indication;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CVD_HeartFailure;type=categorial;categories=CVD_HeartFailure_First_Indication;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CVD_HeartFailure;type=categorial;categories=CVD_HeartFailure_First_Indication;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=CKD_State;type=categorial;categories=CKD_State_Normal,CKD_State_Level_1,CKD_State_Level_2,CKD_State_Level_3,CKD_State_Level_4;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=CKD_State;type=categorial;categories=CKD_State_Normal,CKD_State_Level_1,CKD_State_Level_2,CKD_State_Level_3,CKD_State_Level_4;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=CKD_State;type=categorial;categories=CKD_State_Normal,CKD_State_Level_1,CKD_State_Level_2,CKD_State_Level_3,CKD_State_Level_4;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=DM_Registry;type=categorial;categories=DM_Registry_Pre_diabetic,DM_Registry_Diabetic;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=DM_Registry;type=categorial;categories=DM_Registry_Pre_diabetic,DM_Registry_Diabetic;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=DM_Registry;type=categorial;categories=DM_Registry_Pre_diabetic,DM_Registry_Diabetic;win_from=-1825;win_to=0;do_counts=0;do_shrink=0|\nsig=HT_Registry;type=categorial;categories=HT_Registry_Hypertensive;win_from=0;win_to=10000;do_counts=0;do_shrink=0|\nsig=HT_Registry;type=categorial;categories=HT_Registry_Hypertensive;win_from=-365;win_to=0;do_counts=0;do_shrink=0|\nsig=HT_Registry;type=categorial;categories=HT_Registry_Hypertensive;win_from=-1825;win_to=0;do_counts=0;do_shrink=0\n};\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-5-create-the-x-and-y-matrices","title":"Step 5: Create the x and y matrices","text":"<p>Finally we are ready for this stage. We will start with our samples file, and create 2 matrices from it, the x will use time, the y will use outcomeTime. The matrices will use the embed rules we defined in the previous step, and we will also shrink them to only contain values that appear at least 1e-3 of the samples (to have roughly at least ~1000 cases of it appearing), and less than 0.75 of the samples , to screen to frequent or too rare columns. Our samples file is the one we prepared before: embedding_1_A.samples (1.6M training points) To actually create the matrices we use the Embeddings project, with the --gen_mat option.</p> <p><pre><code># command line to create x matrix\nEmbeddings --gen_mat --rep &lt;rep&gt; --f_samples ./embedding_1_A.samples --embed \"pFile=x_embed_params\" --min_p 0.001 --max_p 0.75 --prefix x\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#files-created-ls-l","title":"files created (ls -l) :","text":"<pre><code>-rwxrwxrwx 1 root root 6521109879 Feb 13 17:58 x.smat           &lt;---- the sparse matrix. And yes, that is a 6.5GB matrix....\n-rwxrwxrwx 1 root root   40597062 Feb 13 18:00 x.meta           &lt;---- pid,time for each line (same number of lines as in our samples file)\n-rwxrwxrwx 1 root root    1892057 Feb 13 18:00 x.scheme         &lt;---- our serialized x scheme file for this matrix : we will need it when we later create features within a model !\n-rwxrwxrwx 1 root root    1256521 Feb 13 18:00 x.dict           &lt;---- the names of all the columns in the shrunk matrix\n</code></pre>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#and-now-creating-the-y-matrix","title":"and now creating the y matrix","text":"<p><pre><code>Embeddings --gen_mat --rep &lt;rep&gt; --f_samples ./embedding_1_A.samples --embed \"pFile=./y_embed_params\" --min_p 0.001 --max_p 0.75 --prefix y\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-6-train-the-embedding-using-keras","title":"Step 6: Train the embedding using Keras","text":"<p>We are now at a state in which we have our x &amp; y matrices ready. Our goal now is to calculate a deep learning model starting from x[i] and ending in y[i], while flowing through a narrow layer with few (say 100-200) neurons.\u00a0 To do that use the Embedder.py script, or copy it and change what you need inside. The Embedder.py script allows the following:</p> <ol> <li>Train through 3 layers (last is the embedding)\u00a0</li> <li>Train through 5 simetric layers with the middle as the embedding layer.</li> <li>Control parameters of network:<ol> <li>layers sizes</li> <li>l1, l2, dropout regularizers on each layer</li> <li>control of extra weight given to cases in the loss function</li> <li>control leaky ReLU function parameters</li> <li>control noise added to embedding layer in training</li> <li>control number of epochs in training, also allow to continue training of a saved model.</li> <li>savind model in a way that can be later used in the infrastructure (.layers file)</li> <li>test modes to generate predictions/embedding layer results on a set of examples (to allow testing vs. the infrastructure) The Embedder.py script is in the git in .../MR/Projects/Shared/Embeddings/scripts/ \u00a0 <pre><code># example run training a model\npython ./Embedder.py --xfile ./x.smat --yfile ./y.smat --nepochs 5 --dim 400 200 100 --l1 1e-7 0 0 --out_model emodel --train --wgt 10 --dropout 0.0 0.0 0.0 --noise 0.3 --gpu 1 --full_decode\n\u00a0\n# output explained ... : (# lines added to explain)\nUsing TensorFlow backend.\n# full parameter list\n('arguments----&gt;', Namespace(dim=[400, 200, 100], dropout=[0.0, 0.0, 0.0], embed=False, full_decode=True, gpu=[1], in_model='', l1=[1e-07, 0.0, 0.0], l2=[0.0, 0.0, 0.0], leaky=[0.1], nepochs=[5], no_shuffle=False, noise=[0.3], out_model=['emodel'], test=False, train=True, wgt=[10.0], xdim=-1, xfile=['./x.smat'], ydim=-1, yfile=['./y.smat']))\n# using only gpu 1 , not setting this will allow running on all gpus in parallel, or on another (say gpu 0)\nusing gpu  1\nTrain mode\n# reading input matrices (can be quite slow, as matrices can be huge)\n('reading: ', ['./x.smat'], ['./y.smat'])\nreading csv file:  ./x.smat 13:11:18.800850\npreparing sparse mat 13:14:27.758752\nreading csv file:  ./y.smat 13:14:40.032091\npreparing sparse mat 13:15:34.727952\n# printing some basic info on sparse matrices x,y : note for example the ratio of 0 to non 0 in x is 43.1 and in y 48.08 (!), showing how sparse they are.\n('xtrain : ', (1623366, 11933), &lt;1623366x11933 sparse matrix of type '&lt;type 'numpy.float32'&gt;'\n        with 451035611 stored elements in Compressed Sparse Row format&gt;, 'non zero: ', 449412245, 43.10435840038137)\n('ytrain : ', (1623366, 3986), &lt;1623366x3986 sparse matrix of type '&lt;type 'numpy.float32'&gt;'\n        with 136188699 stored elements in Compressed Sparse Row format&gt;, ' non zero: ', 134565333, 48.08621010881012)\n('ORIGDIM----&gt; ', 11933, 3986, -1, -1)\n# and now the actual training results\nRunning model definition\n10.0\n10.0\nTraining on xtrain\nTrain on 1298692 samples, validate on 324674 samples\nEpoch 1/5\n2019-02-20 13:15:51.200332: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-02-20 13:15:52.226714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:81:00.0\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\n2019-02-20 13:15:52.226779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2019-02-20 13:15:52.504954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)\n# each epoch has a line , giving results on train and validation, the validation set is always the last 20% of the data.\n# see below some more explaining on how to understand the measures and results printed.\n1298692/1298692 [==============================] - 249s 192us/step - loss: 0.1734 - w_bin_cross: 0.1637 - espec: 0.0289 - enpv: 0.0039 - ppv: 0.3799 - sens: 0.8203 - val_loss: 0.1578 - val_w_bin_cross: 0.1476 - val_espec: 0.0232 - val_enpv: 0.0037 - val_ppv: 0.4295 - val_sens: 0.8253\nEpoch 2/5\n1298692/1298692 [==============================] - 302s 233us/step - loss: 0.1339 - w_bin_cross: 0.1238 - espec: 0.0240 - enpv: 0.0028 - ppv: 0.4370 - sens: 0.8735 - val_loss: 0.1362 - val_w_bin_cross: 0.1261 - val_espec: 0.0219 - val_enpv: 0.0030 - val_ppv: 0.4558 - val_sens: 0.8564\nEpoch 3/5\n1298692/1298692 [==============================] - 329s 253us/step - loss: 0.1221 - w_bin_cross: 0.1120 - espec: 0.0224 - enpv: 0.0024 - ppv: 0.4584 - sens: 0.8885 - val_loss: 0.1345 - val_w_bin_cross: 0.1245 - val_espec: 0.0214 - val_enpv: 0.0029 - val_ppv: 0.4626 - val_sens: 0.8640\n# note epoch 4 started to climb back in val_loss ... this might be a sign for a need to better regularize the model, as it seems to start entering an overfit state\nEpoch 4/5\n1298692/1298692 [==============================] - 304s 234us/step - loss: 0.1156 - w_bin_cross: 0.1056 - espec: 0.0215 - enpv: 0.0023 - ppv: 0.4706 - sens: 0.8967 - val_loss: 0.1542 - val_w_bin_cross: 0.1442 - val_espec: 0.0306 - val_enpv: 0.0025 - val_ppv: 0.3752 - val_sens: 0.8867\n# last epoch , seems val_loss started to go down again, so we got a better model at the end. Still loss is smaller than val_loss, showing a potential for better regularizing.\nEpoch 5/5\n 1298692/1298692 [==============================] - 333s 257us/step - loss: 0.1111 - w_bin_cross: 0.1011 - espec: 0.0209 - enpv: 0.0021 - ppv: 0.4798 - sens: 0.9024 - val_loss: 0.1213 - val_w_bin_cross: 0.1114 - val_espec: 0.0216 - val_enpv: 0.0024 - val_ppv: 0.4627 - val_sens: 0.8862\n# now follows is the ending in which the output files are created \n\u00a0Writing model to files\n('History: ', &lt;keras.callbacks.History object at 0x6fa5690&gt;, {'val_espec': [0.023174003757131564, 0.021862880609978624, 0.021418394041715545, 0.03055855377804435, 0.021563341807379736], 'val_w_bin_cross': [0.14756845901125903, 0.12613012184193115, 0.12453776885398905, 0.14417843291107432, 0.11137183862486143], 'val_sens': [0.8253292080568654, 0.8563854553455446, 0.8639981839122339, 0.8866606718251948, 0.8861758074101623], 'val_enpv': [0.0037238269219488115, 0.003042069682342851, 0.002891325553422261, 0.0024709458833944, 0.0024423540647124592], 'val_ppv': [0.4295036114702129, 0.4557987501766982, 0.4626011239199966, 0.3751608240688261, 0.4627356967393517], 'enpv': [0.003947158052682541, 0.0027687939362956364, 0.002437750546613879, 0.0022583779117050936, 0.002131666025525145], 'val_loss': [0.15775366971853721, 0.13624100354358956, 0.13452252385947847, 0.15421950637508852, 0.12133861386228599], 'ppv': [0.3798841048923378, 0.43695378672384044, 0.4583713526026837, 0.47056864791872133, 0.4798394423932855], 'w_bin_cross': [0.16372003288450798, 0.12379132172316154, 0.11200136639712739, 0.10560965182129696, 0.10111237579286037], 'sens': [0.8202563503519625, 0.8735020021500306, 0.8885229580472642, 0.8966689669803886, 0.9024332567701953], 'loss': [0.17342430566917966, 0.13392729285731664, 0.1220678161909718, 0.11563198370958765, 0.11113599371116904], 'espec': [0.028870824286393815, 0.02403844574438139, 0.02241259005779445, 0.021534086913695155, 0.020883613924218173]})\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_1 (InputLayer)         (None, 11933)             0\n_________________________________________________________________\ndropout_1 (Dropout)          (None, 11933)             0\n_________________________________________________________________\ndense_1 (Dense)              (None, 400)               4773600\n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 400)               0\n_________________________________________________________________\ndropout_2 (Dropout)          (None, 400)               0\n_________________________________________________________________\ndense_2 (Dense)              (None, 200)               80200\n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 200)               0\n_________________________________________________________________\ndropout_3 (Dropout)          (None, 200)               0\n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               20100\n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 100)               0\n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400\n_________________________________________________________________\ngaussian_noise_1 (GaussianNo (None, 100)               0\n_________________________________________________________________\ndropout_4 (Dropout)          (None, 100)               0\n_________________________________________________________________\ndropout_5 (Dropout)          (None, 100)               0\n_________________________________________________________________\ndense_4 (Dense)              (None, 200)               20200\n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 200)               0\n_________________________________________________________________\ndropout_6 (Dropout)          (None, 200)               0\n_________________________________________________________________\ndense_5 (Dense)              (None, 400)               80400\n_________________________________________________________________\ndense_6 (Dense)              (None, 3986)              1598386\n=================================================================\nTotal params: 6,573,286\nTrainable params: 6,573,086\nNon-trainable params: 200\n_________________________________________________________________\nPrinting model layers\nNLAYERS= 19\nLAYER type=dropout;name=dropout_1;drop_rate=0.000000\nLAYER type=dense;name=dense_1;activation=linear;in_dim=11933;out_dim=400;n_bias=400\nLAYER type=leaky;name=leaky_re_lu_1;leaky_alpha=0.100000\nLAYER type=dropout;name=dropout_2;drop_rate=0.000000\nLAYER type=dense;name=dense_2;activation=linear;in_dim=400;out_dim=200;n_bias=200\nLAYER type=leaky;name=leaky_re_lu_2;leaky_alpha=0.100000\nLAYER type=dropout;name=dropout_3;drop_rate=0.000000\nLAYER type=dense;name=dense_3;activation=linear;in_dim=200;out_dim=100;n_bias=100\nLAYER type=leaky;name=leaky_re_lu_3;leaky_alpha=0.100000\nLAYER type=batch_normalization;name=batch_normalization_1;dim=100\nLAYER type=dropout;name=dropout_4;drop_rate=0.000000\nLAYER type=dropout;name=dropout_5;drop_rate=0.000000\nLAYER type=dense;name=dense_4;activation=linear;in_dim=100;out_dim=200;n_bias=200\nLAYER type=leaky;name=leaky_re_lu_4;leaky_alpha=0.100000\nLAYER type=dropout;name=dropout_6;drop_rate=0.000000\nLAYER type=dense;name=dense_5;activation=linear;in_dim=200;out_dim=400;n_bias=400\nLAYER type=dense;name=dense_6;activation=sigmoid;in_dim=400;out_dim=3986;n_bias=3986\n# that's it !! we succesfully trained the model\n# at the end these are the files created :\n# file created by keras to contain the model structure\n-rwxrwxrwx. 1 root root       6599 Feb 20 13:41 emodel.json\n# the model parameters in keras bin format\n-rwxrwxrwx. 1 root root   78938688 Feb 20 13:41 emodel.h5\n# the model parameters in our simple .layers format that can be read using our infrastructure\n-rwxrwxrwx. 1 root root   62422454 Feb 20 13:41 emodel.layers\n# keeping the command line used to create the model : very useful and important if going to continue training or repeat it\n-rwxrwxrwx. 1 root root        170 Feb 20 13:41 emodel_command_line.txt\n# history of results on train and validation, can be used to draw charts of model training process\n-rwxrwxrwx. 1 root root      35160 Feb 20 13:41 emodel.history\n# you can pick up a model you trained and continue its training. Here we use the emodel trained before to continue and save into emodel2\n# the model will initialize itself from the emodel.json and emodel.h5 files, and then continue training and save to emodel2\npython ./Embedder.py --xfile ./x.smat --yfile ./y.smat --nepochs 5 --out_model emodel2 --in_model emodel --train --gpu 1\n\u00a0\n# skipping ...\n\u00a0\nEpoch 1/5\n1298692/1298692 [==============================] - 294s 227us/step - loss: 0.1073 - w_bin_cross: 0.0975 - espec: 0.0204 - enpv: 0.0020 - ppv: 0.4870 - sens: 0.9074 - val_loss: 0.1181 - val_w_bin_cross: 0.1082 - val_espec: 0.0185 - val_enpv: 0.0026 - val_ppv: 0.5010 - val_sens: 0.8770\nEpoch 2/5\n1298692/1298692 [==============================] - 226s 174us/step - loss: 0.1052 - w_bin_cross: 0.0954 - espec: 0.0202 - enpv: 0.0020 - ppv: 0.4904 - sens: 0.9101 - val_loss: 0.1156 - val_w_bin_cross: 0.1059 - val_espec: 0.0181 - val_enpv: 0.0025 - val_ppv: 0.5089 - val_sens: 0.8804\n# ...\n# we were able to improve a little more.\n\u00a0\n</code></pre> Which is the Embedding layer? The one just before the gaussian noise. We take the third layer, then batch normalize it , so that it is in the N(0,1) distribution on each channel, and then add noise (only in training). This is done in order to make sure our embedding layer gives numbers in a reasonable numerical range, is normalized, and that it is immune to noising each channel, making it a more stable embedding. \u00a0 Some explanation on the model loss and evaluation:</li> </ol> </li> </ol> <ul> <li>Loss function : we treat the y vector as a binary prediction goal, and to the whole problem as predicting together a vector of binary predictions (in the example above a vector of length 3986). The loss we take is the logloss on each channel , summed over all channels. You will note that our last layer before getting to the y layer is using sigmoid as activation into the y layer, hence predicting a probablily for each channel. Since the y vectors are very sparse we multiply by a weight the cases (y[i][j] == 1) , pushing the model to try better to be right on 1 predictions rather than 0 predictions.</li> <li>Evaluation : last 20% of x.y matrices (as they appear in the input files) are always used as evaluation data for the embedding training process, and you see that evaluation after each epoch.</li> <li>Evaluation metrics (all metrics with val_ prefix are the same but on the validation 20% set). All our done at a point which sets &gt;=0.5 predictions as 1 and the others as 0.<ul> <li>loss : overall loss value (as explained above). Big differences between train and validation group hints towards over fit we can try to regilarize (finding the best regularization usually improves results). Note that when using large data sets less regularization is needed. More data is always the best regularizer.</li> <li>w_bin_cross : loss without the regularization terms (l1, l2).</li> <li>ppv : #(true==1 &amp;&amp; prob&gt;=0.5) / # (prob &gt;= 0.5) : the probability of being right when predicting positive (larger is better)</li> <li>sens : #(true==1 &amp;&amp; prob&gt;=0.5) / # (true == 1) : how many of the positives caught when predicting positive (larger is better)</li> <li>enpv : #(true==1 &amp;&amp; prob&lt;0.5) / #(prob&lt;0.5) : the probability for error when predicting negative (lower is better)</li> <li>espec : 1-#(true==0 &amp;&amp; prob&lt;0.5)/#(true == 0) : percentage of true negatives not predicted right out of all negatives (lower is better) \u00a0</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-7-testing-we-get-the-same-embeddings-in-keras-and-infrastructure","title":"Step 7 : Testing we get the same embeddings in Keras and Infrastructure","text":"<p>This is a needed sanity in order to verify the model we trained is indeed the one our infrastructure will use.</p> <p><pre><code># assuming we prepared t_1.samples : a samples file with a single line\n# we first create a sparse mat for this sample, we use the x.scheme file that was created when we first generated the x matrix\nEmbeddings --gen_mat_from_scheme --f_samples ./t_1.samples --f_scheme ../x.scheme --prefix xtest\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#this-created-the-xtestsmat-file","title":"this created the xtest.smat file","text":""},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#we-can-now-check-it-directly-with-keras-using-the-following-line-the-model-will-initialize-from-the-json-and-h35-files","title":"we can now check it directly with keras using the following line (the model will initialize from the .json and .h35 files):","text":"<p><pre><code>python ../Embedder.py --embed --in_model ../emodel --xfile ./xtest.smat  \n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#result-is","title":"result is :","text":"<pre><code>Using TensorFlow backend.\n('arguments----&gt;', Namespace(dim=[400, 200, 100], dropout=[0.0, 0.0, 0.0], embed=True, full_decode=True, gpu=[0], in_model=['../emodel'], l1=[1e-07, 0.0, 0.0], l2=[0.0, 0.0, 0.0], leaky=[0.1], nepochs=-1, no_shuffle=False, noise=[0.3], out_model='', test=False, train=False, wgt=[10.0], xdim=[11933], xfile=['./xtest.smat'], ydim=[3986], yfile='my_y.smat'))\nusing gpu  0\nEmbed mode\n('reading: ', ['./xtest.smat'])\nreading csv file:  ./xtest.smat 18:14:50.269859\npreparing sparse mat 18:14:50.275748\n('ORIGDIM----&gt; ', 11933, 3986, [11933], [3986])\nRunning model definition\n10.0\n10.0\n2019-02-20 18:14:50.625090: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-02-20 18:14:51.702925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:02:00.0\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\n2019-02-20 18:14:51.702988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2019-02-20 18:14:52.051737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\nGenerating an embedding (for testing)\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_1 (InputLayer)         (None, 11933)             0\n_________________________________________________________________\ndropout_1 (Dropout)          (None, 11933)             0\n_________________________________________________________________\ndense_1 (Dense)              (None, 400)               4773600\n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 400)               0\n_________________________________________________________________\ndropout_2 (Dropout)          (None, 400)               0\n_________________________________________________________________\ndense_2 (Dense)              (None, 200)               80200\n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 200)               0\n_________________________________________________________________\ndropout_3 (Dropout)          (None, 200)               0\n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               20100\n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 100)               0\n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400\n=================================================================\nTotal params: 4,874,300\nTrainable params: 4,874,100\nNon-trainable params: 200\n_________________________________________________________________\n[[ 0.5779214  -1.1393576  -1.2145319   0.719882    1.152698    2.7561216\n   4.7062497  -1.9202161   1.4628897  -0.28924894 -5.909066   -3.8524284\n   0.89937544 -1.7450757  -1.6244755   4.2109747   1.8999414   2.4285064\n   2.119122   -1.1817946   2.5119438   2.4076338  -1.6450381   2.5745249\n   1.3517776   5.6222005   3.1432505   0.97051287 -1.9321308   1.6599026\n  -1.135961    4.1045027  -5.8931293  -0.9852152  -0.22657108  3.0653677\n   2.6770768   1.325161   -3.7317533   0.78288555  2.749711   -0.71579885\n  -0.6447115  -0.54217243  2.1220136   0.16115046  1.6993942   4.197215\n  -1.103509   -2.8239236  -2.4093542   1.7677689  -3.7956572   1.0639849\n  -4.50788    -0.5759249   1.5213671  -0.01162529  2.6363235  -2.1107693\n   4.447809    4.9245405   1.1112337   1.6995897   2.810319   -2.2490163\n  -0.57630396  3.6462874   2.2628431   5.652481   -2.4730139   5.1024203\n   3.0555716   2.2935085  -1.472085   -0.3556919   0.70053387  3.6700687\n   4.944693   -6.186803    1.0003986   3.6284337  -4.352747    3.9384065\n  -0.5298457   1.7390456   0.1143899   1.6540909   2.9988813   0.4738245\n  -5.7533255   5.286108    4.581069    1.6037283  -0.42494774  2.3381634\n  -5.95912     2.1251287   5.0158515  -0.31940365]]\n</code></pre> <p>We now want to do the same using our layers file and infrastructure: layer 9 is usually the layer of the Embedding if you didn't change the Embedder.py script to run a different network</p> <p><pre><code>./Embeddings --get_embedding --f_samples ./t_1.samples --f_scheme ../x.scheme --f_layers ../emodel.layers --to_layer 9\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#results","title":"results ...","text":"<p><pre><code>initializing rep /home/Repositories/THIN/thin_jun2017/thin.repository\nRead 0 signals, 0 pids :: data  0.000GB :: idx  0.000GB :: tot  0.000GB\nRead data time 0.102909 seconds\nread_binary_data_alloc [../x.scheme] with crc32 [-63879080]\nread_from_file [../x.scheme] with crc32 [-63879080] and size [1892057]\nMedSamples: reading ./t_1.samples\nWARNING: header line contains unused fields [EVENT_FIELDS,]\n[date]=2, [id]=1, [outcome]=3, [outcome_date]=4, [split]=5,\nread [1] samples for [1] patient IDs. Skipped [0] records\nsorting samples by id, date\nGenerating sparse mat for 1 lines\nReading layers file ../emodel.layers\nApplyKeras: Reading 0 : LAYER   type=dropout;name=dropout_1;drop_rate=0.000000\nApplyKeras: Reading 1 : LAYER   type=dense;name=dense_1;activation=linear;in_dim=11933;out_dim=400;n_bias=400\nApplyKeras: Reading 2 : LAYER   type=leaky;activation=leaky;name=leaky_re_lu_1;leaky_alpha=0.100000\nApplyKeras: Reading 3 : LAYER   type=dropout;name=dropout_2;drop_rate=0.000000\nApplyKeras: Reading 4 : LAYER   type=dense;name=dense_2;activation=linear;in_dim=400;out_dim=200;n_bias=200\nApplyKeras: Reading 5 : LAYER   type=leaky;activation=leaky;name=leaky_re_lu_2;leaky_alpha=0.100000\nApplyKeras: Reading 6 : LAYER   type=dropout;name=dropout_3;drop_rate=0.000000\nApplyKeras: Reading 7 : LAYER   type=dense;name=dense_3;activation=linear;in_dim=200;out_dim=100;n_bias=100\nApplyKeras: Reading 8 : LAYER   type=leaky;activation=leaky;name=leaky_re_lu_3;leaky_alpha=0.100000\nApplyKeras: Reading 9 : LAYER   type=batch_normalization;name=batch_normalization_1;dim=100\nApplyKeras: Reading 10 : LAYER  type=dropout;name=dropout_4;drop_rate=0.000000\nApplyKeras: Reading 11 : LAYER  type=dropout;name=dropout_5;drop_rate=0.000000\nApplyKeras: Reading 12 : LAYER  type=dense;name=dense_4;activation=linear;in_dim=100;out_dim=200;n_bias=200\nApplyKeras: Reading 13 : LAYER  type=leaky;activation=leaky;name=leaky_re_lu_4;leaky_alpha=0.100000\nApplyKeras: Reading 14 : LAYER  type=dropout;name=dropout_6;drop_rate=0.000000\nApplyKeras: Reading 15 : LAYER  type=dense;name=dense_5;activation=linear;in_dim=200;out_dim=400;n_bias=400\nApplyKeras: Reading 16 : LAYER  type=dense;name=dense_6;activation=sigmoid;in_dim=400;out_dim=3986;n_bias=3986\nEmbedding[0] :  0.577931, -1.139361, -1.214531, 0.719889, 1.152678, 2.756100, 4.706246, -1.920236, 1.462890, -0.289272, -5.909092, -3.852452, 0.899393, -1.745082, -1.624470, 4.210994, 1.899885, 2.428522, 2.119110, -1.181803, 2.511931, 2.407662, -1.645038, 2.574553, 1.351748, 5.622195, 3.143245, 0.970485, -1.932137, 1.659895, -1.135962, 4.104524, -5.893116, -0.985202, -0.226589, 3.065342, 2.677109, 1.325170, -3.731724, 0.782854, 2.749709, -0.715786, -0.644691, -0.542146, 2.122007, 0.161158, 1.699387, 4.197234, -1.103530, -2.823920, -2.409330, 1.767769, -3.795663, 1.063999, -4.507883, -0.575921, 1.521367, -0.011617, 2.636321, -2.110771, 4.447805, 4.924530, 1.111231, 1.699584, 2.810309, -2.249007, -0.576356, 3.646280, 2.262861, 5.652493, -2.473031, 5.102398, 3.055582, 2.293494, -1.472100, -0.355688, 0.700531, 3.670066, 4.944662, -6.186786, 1.000359, 3.628438, -4.352755, 3.938388, -0.529864, 1.739041, 0.114385, 1.654087, 2.998851, 0.473810, -5.753366, 5.286112, 4.581021, 1.603754, -0.424938, 2.338176, -5.959126, 2.125135, 5.015850, -0.319395,\n</code></pre> \u00a0 and as can be easily seen we indeed create the same Embedding !! We're good to go and use this embedding in our infrastructure. note we got the same embedding up to ~1e-5 error which is just a numerical error difference. Our embedding is much more stable that this small difference since we trained it with added noise 4 orders of magnitude larger.\u00a0 Another way to test this is to use the Flow --get_json_mat option with the right samples file, and the right json file (see below) \u00a0\u00a0</p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-8-using-embedding-in-a-medmodel-finally","title":"Step 8 : Using Embedding In a MedModel (Finally\u00a0!!)","text":"<p>Once we have the .scheme file and the matching .layers file we can generate features using the \"embedding\" feature generator. This feature generator will generate for each sample it's sparse x line, then run it through the embedding model , and add the layer output as features in the MedFeatures matrix. To use embeddings as features add the following to your json</p> <p><pre><code>    { \"action_type\":    \"feat_generator\", \"fg_type\": \"embedding\", \n                        // the name of the feature will be FTR_&lt;num&gt;.&lt;name_prefix&gt;.col_&lt;embedding column number&gt; , if using several embedding FGs give them different name_prefix names\n                        \"name_prefix\" : \"Semi_AutoEncoder\",\n                        // your x matrix scheme file\n                        \"f_scheme\" : \"/nas1/Work/Users/Avi/test_problems/embedding_example/mats/x.scheme\",\n                        // your layers file\n                        \"f_layers\" : \"/nas1/Work/Users/Avi/test_problems/embedding_example/mats/emodel_auto_xx.layers\",\n                        // layer to use for embedding, typical is 9 if you used the default Embedder.py script\n                        \"to_layer\" : \"9\"},\n</code></pre> </p>"},{"location":"Infrastructure%20Library/03.FeatureProcessor%20practical%20guide/Embeddings/Embeddings%20WalkThrough%20Example.html#step-9-results-for-mi-withwithout-embedding-withwithout-sigdep","title":"Step 9 : Results for MI with/without Embedding , with/without SigDep","text":"<p>We can now easily train models for our problem. We can use learn_1_B as our training samples and validate_1_B as our cross validation set, and validate_2 as an external test set. Doing these tests in 4 flavors:</p> <ul> <li>No categorial signals<ul> <li>We used\u00a0\"last\", \"min\", \"max\", \"avg\", \"last_delta\", \"last_time\" on several time windows\u00a0</li> <li>signals :\u00a0<ul> <li>\"Glucose\", \"BMI\", \"HbA1C\", \"Triglycerides\", \"LDL\", \"Cholesterol\", \"HDL\", \"Creatinine\", \"eGFR_CKD_EPI\", \"Urea\", \"Proteinuria_State\", \"ALT\", \"AST\", \"ALKP\", \"WBC\", \"RBC\", \"Hemoglobin\", \"Hematocrit\", \"RDW\", \"K+\", \"Na\", \"GGT\", \"Bilirubin\", \"CRP\",\"BP\"</li> </ul> </li> </ul> </li> <li>Used also Smoking information, age, gender</li> <li>Using Signal Dependency on Drugs and RC (choosing top 100 codes in each) to select correlated read codes.</li> <li>Using Embeddings:<ul> <li>Embeddings were trained once as an autoencoder (y matrix created at same times of x matrix) and once as a future-encoder in which y matrix was created on times 1 or 2 years a head.</li> <li>Embedding dimension was 100</li> </ul> </li> <li>Using both SigDep and Embeddings together. \u00a0 Predictor used was the same for all cases: lightgbm with slightly optimized parameters. Results are given for the 30-730 time window, ages 40-80 , we compare AUCs</li> </ul> Model CV AUC Test AUC Labs 0.762 0.763 +SigDep 0.782 0.782 +Embeddings 0.785 0.787 +SigDep +Embeddings 0.787 0.789 <p>CI intervals are more or less +- 0.005 Some points:</p> <ul> <li>We see a very minor improvement for using Embeddings.<ul> <li>This is a nice result , as it verifies the whole concept works.</li> <li>There is a trend towards better results even in this model, and when going to 3y, 5y time windows it gets stronger</li> </ul> </li> <li>Since the SigDep option is much simpler also when interprating the models , it is not clear Embeddings are preffered in this problem.</li> <li>On the other hand the Embedding option is quite general and the same set of features could be used in many different models.</li> <li>Also : it may be that finiding/training the right Embedding model will give an even larger boost.</li> <li>interestingly there was also a very small improvement trend when using both together:<ul> <li>Means most of the information captured by both methods is the same, but still each has some small information parts the other doesn't. \u00a0 \u00a0 \u00a0 \u00a0</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html","title":"MedAlgo Library","text":""},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html#general","title":"General","text":"<p>The MedAlgo library (together with its accompanying algorithm libraries) is a general wrapper for several ML algorithms allowing learn , predict and parameters configuration. \u00a0 \u00a0 General Usage Example: \u00a0 Using MedPredictor <pre><code>#include &lt;MedAlgo/MedAlgo/MedAlgo.h&gt;\n\u00a0\n// .... create your train and test matrices ...\nMedMat&lt;float&gt; Xtrain,Ytrain,Xtest,Ytest;\n\u00a0\n// define a MedPredictor pointer\nMedPredicor *predictor;\n\u00a0\n// Create model of choice (here in example linear model)\npredictor = MedPredictor::make_predictor(\"linear_model\");\n\u00a0\n// Initialize parameters for this model (here in example putting rfactor , the 1-ridge , to be 0.9)\npredictor-&gt;init_from_string(\"rfactor=0.9\");\n\u00a0\n// Learn on Train , this will build a model. Here we use MedMat&lt;float&gt; matrices for X and Y\n// There are other API's as well - for example c like API's , please look at the MedPredictor class for more options\npredictor-&gt;learn(Xtrain,Ytrain);\n\u00a0\n// Predict on Test , here we use an API that uses an Xtest MedMat&lt;float&gt; for test, and a vector&lt;float&gt; for predictions\n// There are other API's as well - for example c like API's , please look at the MedPredictor class for more options\nvector&lt;float&gt; preds;\npredictor-&gt;predict(Xtest,preds);\n\u00a0\n// That's it ... now you have the predictions and can test the performance.\n// More on options to test performance and serialize and save models below.\n\u00a0\n\u00a0\n</code></pre></p>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html#predictors-and-their-parameters","title":"Predictors and their parameters","text":""},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html#linear-model","title":"Linear Model","text":"<ul> <li>Use\u00a0MedPredictor::make_predictor(\"linear_model\")</li> <li>Parameters:<ul> <li>rfactor - 1.0: no ridge , closer to 0.0 - stronger ridge. Reccomended: 0.9 for regular runs, 0.3 for highly regularaized runs. Linear Model example <pre><code>MedPredicor *predictor;\npredictor = MedPredictor::make_predictor(\"linear_model\");\npredictor-&gt;init_from_string(\"rfactor=0.9\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html#xgboost","title":"XGBoost","text":"<ul> <li>Use\u00a0MedPredictor::make_predictor(\"xgb\")</li> <li>Parameters<ul> <li>seed</li> <li>booster</li> <li>objective</li> <li>eta - step size in each iteration , should be slow enough to avoid overfitting, and fast enough to get somewhere. If num_round is large, use a small eta and vice versa.</li> <li>num_round - how many trees to build. Running time is linear with this parameter.</li> <li>gamma</li> <li>max_depth - of trees</li> <li>min_child_weight - limiting size of leaves (larger = more regularization)</li> <li>missing_value - you can sign the algorithm what are the missing values (if there are any) in your matrix.</li> <li>lambda</li> <li>alpha</li> <li>scale_pos_weight - allows to fix imbalances in data</li> <li>tree_method \u00a0 XGBoost example <pre><code>MedPredicor *predictor;\npredictor = MedPredictor::make_predictor(\"xgb\");\npredictor-&gt;init_from_string(\"booster=gbtree;objective=binary:logistic;eta=0.05;gamma=1;max_depth=5;num_round=50;min_child_weight=6\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html#qrf","title":"QRF","text":"<ul> <li>Use\u00a0MedPredictor::make_predictor(\"qrf\")</li> <li>Parameters:<ul> <li>ntrees - number of trees to build</li> <li>maxq - max number of quantized cells for a parameter</li> <li>type - one of: binary , regression , categorical_chi2 , categorical_entropy</li> <li>min_node - split only nodes of size above this (larger = more regularization)</li> <li>ntry - how many random features to test in each node. -1 (or 0) is default and means sqrt(num_features), a specific number is the actual requested ntry. (smaller = more regularization)</li> <li>max_samp - how many samples to bag for each tree (total neg+pos). 0 means - bag at the number of input and is default. (smaller = more regularization)</li> <li>n_categ - number of categories. 0/1 for regression , 2 for binary problems, 3 and more for multicategorical data</li> <li>spread - in regression trees nodes with difference from max to min below spread will not split.</li> <li>sampsize - a vector (with , delimeter) stating how many samples to take for each tree from each category. example: sampsize=5000,1000 for a binary problem means bag 5000 neg and 1000 pos for each tree.</li> <li>get_count -\u00a0<ul> <li>0 : avg majority of nodes (less recommended)</li> <li>1 : avg probabilities in nodes (in regression = weighted average of nodes , taking their size into account) - recommended</li> <li>2: \u00a0avg counts in nodes - recommended</li> </ul> </li> <li>get_only_this_categ -\u00a0<ul> <li>-1 : get predictions for all categories one after the other (output size is nsamples*n_categ)</li> <li>0...n_categ-1 : get only the predictions for this categ (output size is nsamples)</li> </ul> </li> <li>learn_nthreads - how many threads to use in learn (use 8 for windows, and 24 for linux servers)</li> <li>predict_nthreads -\u00a0how many threads to use in predict (use 8 for windows, and 24 for linux servers) \u00a0 QRF example <pre><code>MedPredicor *predictor;\npredictor = MedPredictor::make_predictor(\"qrf\");\n\u00a0\n// classification example\npredictor-&gt;init_from_string(\"type=categorical_entropy;ntrees=200;min_node=30;n_categ=2;get_only_this_categ=1;sampsize=15000,5000;learn_nthreads=24;predict_nthreads=24\");\npredictor-&gt;learn(Xtrain,Ytrain);\n\u00a0\n// regression example\npredictor-&gt;init_from_string(\"type=regression;ntrees=200;min_node=100;n_categ=1;spread=0.1;learn_nthreads=24;predict_nthreads=24\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/index.html#gdlm","title":"GDLM","text":"<p>The gdlm package provides algorithms for linear and logistic regression with ridge and lasso regularizations. The solution is via gradient descent.</p> <ul> <li>use \"gdlm\" as the name for the predictor.</li> <li>Parameters:<ul> <li>method: one of full , sgd or logistic_sgd<ul> <li>full : full exact solution to the linear regression problem. Can be slow on huge matrices. Less recommended, but works. Not supporting lasso.</li> <li>sgd : gradient descent solution to the linear problem with least square loss and optional ridge and/or lasso regularizers.</li> <li>logistic_sgd : gradient descent solution to the logistic loss function with optional ridge and/or lasso regularizers.</li> </ul> </li> <li>normalize : 0/1 : use 1 if you want the algorithm to normalize the matrix before the optimization. Note that the algorithms converge only when data is normalized, so use this if data was not prepared normalized.</li> <li>l_ridge : the ridge gamma</li> <li>l_lasso : the lasso gamma (you'll have to play and find the gamma value that works for you. Typically very small values are needed (0.01, 0.001 , etc).</li> <li>max_iter : maximal number of iterations (an iteration is a full epoch through all the data)</li> <li>err_freq : print summary and check stop condition each err_freq iterations</li> <li>batch_size : the batch size for the gradient descent (coefficients are updated after every batch of course)</li> <li>rate : learning rate</li> <li>rate_decay : allow rate to slowly decrease (or stay constant if decay is 1).</li> <li>momentum : for gradient descent</li> <li>stop_at_err : once the relative improvement in loss falls below this value, the optimazation will stop.</li> <li>last_is_bias : leave 0 usually, is there for cases where a bias is given with the x values.</li> <li>nthreads : number of threads for matrix operations. Number of cores (12 in our nodes) is typically a good choice. MedGDLM init examples <pre><code>MedPredictor *predictor;\npredictor = MedPredictor::make_predictor(\"gdlm\");\n\n// classification example with logistic regression , lasso of 0.01 , learning rate of 0.001 and normalization pre running\npredictor-&gt;init_from_string(\"method=logistic_sgd;last_is_bias=0;stop_at_err=1e-4;batch_size=2048;momentum=0.95;rate=0.001;rate_decay=1;l_ridge=0;l_lasso=0.01;err_freq=10;nthreads=12;normalize=1\");\npredictor-&gt;learn(Xtrain,Ytrain);\n\n// same but regression example with least squares and lasso\npredictor-&gt;init_from_string(\"method=sgd;last_is_bias=0;stop_at_err=1e-4;batch_size=2048;momentum=0.95;rate=0.001;rate_decay=1;l_ridge=0;l_lasso=0.01;err_freq=10;nthreads=12;normalize=1\");\npredictor-&gt;learn(Xtrain,Ytrain);\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/Feature%20Importance.html","title":"Feature Importance","text":"<p>The Feature Importance is virtual function of MedPredictor called calc_feature_importance. If you want to support Feature Importance for your predicotr you need to implement the virtual function: calc_feature_importance Some notes:</p> <ul> <li>This function must be called after learn method only! otherwise and exception will be raised.</li> <li>The functions returns the features importance score in vector with same order of the learned features matrix order (sorted by ABC because we use map object). Currently the method is implemented by:</li> <li>QRF - no additional parameters are required to run</li> <li>LightGBM -\u00a0has a parameter \"importance_type\" which has 2 options:\u00a0<ul> <li>\"gain\" -\u00a0the average gain of the feature when it is used in trees (in each split of tree node we have loss gain - averaging that)</li> <li>\"frequency\" -\u00a0the number of times a feature is used to split the data across all trees</li> </ul> </li> <li>XGB - has a parameter \"importance_type\" which has 3 options:\u00a0<ul> <li>\"gain\" -\u00a0the average gain of the feature when it is used in trees (in each split of tree node we have loss gain - averaging that)</li> <li>\"gain_total\" -\u00a0sum of gain the of the feature when it is used in trees (not normalized by number of appearances)</li> <li>\"weight\" -\u00a0the number of times a feature is used to split the data across all trees</li> <li>\"cover\" -\u00a0the average coverage of the feature when it is used in trees. it sums the number of samples in the leaves that in their path have this feature (so this feature was used to split and deciede on this observations). it calc's the average coverage \u00a0 Example run for XGB <pre><code>vector&lt;float&gt; feautres_scores;\nMedPredictor *predictor = MedPredicotr::make_predictor(\"xgb\"); //load trained model\nMedFeatures data; //data.read_from_file(matrix_file); //load dataMatrix or already trained model\n//predictor-&gt;learn(data); //or load trained model, otherwise it will throw exception\n\u00a0\n//now do learn and run again\npredictor-&gt;calc_feature_importance(feautres_scores, \"importance_type=gain\"); //the second argument is additional parameters for feature importance\n//sort and print features\nmap&lt;string, vector&lt;float&gt;&gt;::iterator it = data.data.begin();\nvector&lt;pair&lt;string, float&gt;&gt; ranked((int)feautres_scores.size());\nfor (size_t i = 0; i &lt; feautres_scores.size(); ++i) {\n    ranked[i] = pair&lt;string, float&gt;(it-&gt;first, feautres_scores[i]);\n    ++it;\n}\nsort(ranked.begin(), ranked.end(), [](const pair&lt;string, float&gt; &amp;c1, const pair&lt;string, float&gt; &amp;c2)\n{\n    return c1.second &gt; c2.second;\n});\nfor (size_t i = 0; i &lt; ranked.size(); ++i)\n    printf(\"FEATURE %s : %2.3f\\n\", ranked[i].first.c_str(), ranked[i].second);\n\u00a0\n//QRF- have no additional options - empty string\n//XGB - has parameter - \"importance_type\" with those options: \"gain,weight,cover\"\n//LightGBM  - has parameter \"importance_type\" with those options: \"frequency,gain\". gain is like xgboost and frequency is like weight in xgboost\n\u00a0\n\u00a0\n</code></pre> Using FeatureImportance as Selector: cat /server/Work/Users/Alon/UnitTesting/examples/\"general config files\"/importance_example.json <pre><code>\"process\":{\n            \"process_set\":\"3\",\n            \"fp_type\":\"importance_selector\",\n            \"duplicate\":\"no\",\n            \"numToSelect\":\"10\",\n            \"predictor\":\"qrf\",\n            \"predictor_params\":\"{type=categorical_entropy;ntrees=100;min_node=20;n_categ=2;get_only_this_categ=1;learn_nthreads=40;predict_nthreads=40;ntry=100;maxq=5000;spread=0.1}\",\n            \"importance_params\":\"{}\" //you may pass other parameters for feature importance in here. for example importance_type=gain\n        }\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/Machine%20Learning%20Algorithms%20-%20Parameter%20Tuning%20%28C%2B%2B%20Code%29.html","title":"Machine Learning Algorithms - Parameter Tuning (C++ Code)","text":""},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/Machine%20Learning%20Algorithms%20-%20Parameter%20Tuning%20%28C%2B%2B%20Code%29.html#qrf-regression-tree","title":"QRF Regression Tree:","text":"<p>To Initialize MedQRF predictor you can use MedQRF\u00a0object. You MUST initialize params.type =\u00a0QRF_TreeType::QRF_REGRESSION_TREE. there are 5 different parameters for regression random forest: 1. params.ntrees - which controls the number of trees created in the forest. when this value is big enougth it not suppose to have a big impact making it bigger. the prediction value is the mean average of all trees. theoritically it suppose to converge in big numbers (&gt; 100's) 2. params.maxq- controls how many bins a signal would be divided into (if there are less unique values than this number, it won't do a thing). will be used later for choosing threshold to decide where to split the tree in the signal node. there is a linear shrinkage between min to max value of signal divide by\u00a0maxq number. if bigger number than the runnint time increases, not suppose to increase overfitting \u00a0because it's only searches for split threshold in smaller bin jumps. 3. params.ntry - how many random tries to select signal to split node in the tree .default value is sqrt(number_of_features) 4. params.min_node - split node condition - how many samples should be in node, less than this will not split \u00a0node. default is 100 5. params.spread \u00a0- split node condition - the minimum diff between min and max in signal to split node. for example, if all the signal values in the the tree node only changes(max_value-min_value) less than spread , it will not split the node.\u00a0default value is 0.1. depends on the signal resulotion sometime 0.1 has big meaning and sometime it has small meaning \u00a0 Missing Parameters: 1. spliting function creteria - Gini, info gain? other measures 2. configure special params for special signals. for example in some signals you would like to choose diffrent spread \u00a0</p>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/Machine%20Learning%20Algorithms%20-%20Parameter%20Tuning%20%28C%2B%2B%20Code%29.html#sgd","title":"SGD:","text":"<p>These are the parameters which effects learning_rate in SGD: 1. B - Blocking Value for W in L2 norm - you need to search for a solution in blocked space. esstimate the L2 norm of your parameter optimal solution (W are the parameters of the model) 2. P - max change in derivate. more specificcly maximal change in (f(x+h)-f(x) )/ h. f(x) can be also not diffrential. you can calculate max diff in each signal maximum value- minimum value and divide with h_size which can be small and not important (0.01, 0.1) 3. h_size - the numeric step for calculating derivate 4. T_Steps - the number of steps the SGD will do 5. sample_size - the sample_size for stochastic gradient decend to calculate derievate, need to be not too small (50, 100 are ok to have enougth samples to calculate gradient) - not very important param and it's not effecting learning rate.There is a simple equation that takes all those params into account in yield the learning_rate and esstimate eppsilon error from optimal solution with a good confidence levelIn my code you can ran:learner.set_blocking(B);\u00a0//for projection step when solution is outside boundleaner.set_gradient_params(sample_size, h_size);learner.set_learning_rate(B, P, T_Steps);leaner.output_num = T_Steps\u00a0/ 5;\u00a0// If you want the SGD to output each T_steps/5 rounds the error. if set to 0 which is default won't output anything</p>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/but_why%20-%20feature%20contribs.html","title":"but_why - feature contribs","text":"<p>feature contributions (AKA but_why) is an attempt to explain a specific prediction of a classifier, by providing an interpretable simple model of simple features. We saw 3 works in this field:</p> <ol> <li>LIME - (https://arxiv.org/pdf/1602.04938.pdf) - build a complex model, then build a local simple model (Lasso) for the predictions of the complex model. By local they mean - generate samples, and weight them according to their distance from the interesting sample (patient).</li> <li>tree path interpreter - (https://blog.datadive.net/interpreting-random-forests/) - for tree ensembles such as XGB, follow the decision path for the interesting sample (patient), and accumulate the changes in the outcome mean for each feature.</li> <li>SHAP - (https://arxiv.org/abs/1802.03888) - an improvement over (2), which promises consistency when the impact of a feature stays the same but the structure of the trees changes.\u00a0 Yaron Kinar built LinearizeScore - a local c++ version of LIME, which uses real samples (LIME are using artificial samples which are generated assuming feature independence and normality). we added MedPredictor::calc_feature_contribs, which supports (2) and (3), only for XGB. In Likely we are doing a somewhat strange hybrid solution: we are building a strong XGB model using all the features, then we build a second XGB model with only part of the features (the ones that make medical sense), and we aim to explain the predictions of the strong model. We then use calc_features_contribs for getting the weak model feature contributions, in (2) mode.\u00a0 Note that we currently do not know of a reliable way to quantify which method is better, except in artificial data experiments. Also note that for migrating to yaron's solution, we need the entire repo in memory on our production machine which is currently not feasible.\u00a0 \u00a0</li> </ol>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/index.html","title":"MedPredictor practical guide","text":"<ul> <li>Linear model : type = \"linear_model\" - linear regression using coordinate descent with Ridge factors</li> <li>Linear-SGD : type = \"linear_sgd\" - linear regression using stochastic gradient descent</li> <li>GDLM : type = \"gdlm\" - Generalized linear model using stochastic gradient descent</li> <li>QRF : type = \"qrf\" - Random Forest using quantization for efficency</li> <li>KNN : type = \"knn\" - K Nearest Neighbors</li> <li>MARS : type = \"mars\" - (quite slow) impliminationg of MARS (Multivariate Additive Regression Spline)</li> <li>GBM : type = \"gbm\" - Gradient Boosting Machine\u00a0 - imported public library used in R</li> <li>BP : type = \"BP\" - Back propogation. (Obsolete) version of NN</li> <li>Multi-Class : type = \"multi_class\" - an enevelope for multi-class problems, currently implementing one-vs-all</li> <li>XGBoost : type = \"xgb\" - public implementation of xgboost + internaly added features</li> <li>Lasso : type = \"lasso\" - Lasso linear regression using gradient descent</li> <li>MicNet : type = \"micNet\" - Neural Net</li> <li>Booster : type = \"booster\" - Envelope for boosting models</li> <li>Deep-Bit : type = \"deep_bit\" - Nir's deep bit using Yoav's implementation</li> <li>Light-GBM : type = \"light_gbm\" - public implementation of fast GBM</li> <li>SVM : type = \"svm\" - Support Vector Machine</li> <li>multi-models : type = \"multi_models\" - an envelope for multiple models for different inputs (e.g. - age specific models)</li> <li>vw : type = \"vw\" - an envelope for\u00a0Vowpal Wabbit models</li> <li>TQRF : type = \"tqrf\" - new version of quantized RF (Not ready yet ?)</li> <li>BART : type = \"bart\" - public implementation of BART (Bayesian Additive Regression Trees)</li> <li>MASK: type=\"by_missing_value_subset\" - see here </li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/Howto%20write%20MedPredictor.html","title":"How to Write a MedPredictor","text":"<p>MedPredictor is Classifier/Regressor that inputs the features matrix and output prediction/s.  It is being executed after FeatureProcessors</p> <p>MedPredictor in MedModel follow a defined sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the MedPredictor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the predictor. Be sure to update <code>classifier_type</code> to reflect the predictor type.</p> </li> <li> <p>Initialization    - During learning:      Implement <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      Medial pipeline will initialize <code>features_count</code> and <code>model_features</code> that when we apply the modle we can make sure all and only needed features are inputed to the model.      You might want to set <code>transpose_for_learn</code>, <code>transpose_for_predict</code> - if transpose of features matrix is needed for this type of predictor      You might want to set (legacy and almost not used. Please use Feature Processors) to normalize the inputs: <code>normalize_for_learn</code>, <code>normalize_y_for_learn</code>, <code>normalize_for_predict</code>    - During application:      Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</p> </li> <li> <p>Learning Phase    - <code>learn()</code>      Implements any learning logic needed during training.</p> </li> <li> <p>Applying Phase    - <code>apply()</code>  or <code>predict_single()</code> for single patient (more efficient setup for single patient, some preparation are done prior)      Applies the predictor to calculate the score</p> </li> </ol>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/Howto%20write%20MedPredictor.html#steps-to-implement-a-medpredictor","title":"Steps to Implement a MedPredictor","text":"<ol> <li> <p>Create Class Files    - Create a new <code>.h</code> header and <code>.cpp</code> source file for your predictor class. Include <code>MedAlgo.h</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Do not include temporary or repository-specific variables.</p> </li> <li> <p>Learning    - Implement <code>learn()</code> for any required training logic.</p> </li> <li> <p>Apply    - Implement <code>apply()</code> to calculate the score. If there is more efficient calculate for single call, than also implement <code>predict_single()</code></p> </li> <li> <p>Register Your Predictor in the Header (<code>MedAlgo.h</code>)    - Add a new type to <code>MedPredictorTypes</code> before <code>MODEL_LAST</code>. In the documentation comment, specify the name in <code>MedPredictorTypes</code> for Doxygen reference.</p> </li> <li> <p>Register Your Feature Processor in the Source (<code>MedAlgo.cpp</code>)    - Add your type conversion to <code>predictor_type_to_name</code> dictionary    - Add your class to <code>MedPredictor::new_polymorphic</code>    - Add your class to <code>MedPredictor::make_predictor(MedPredictorTypes model_type)</code></p> </li> </ol>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/MASK%20predictor%20-%20predict%20by_missing_value_subset.html","title":"MASK predictor - predict by_missing_value_subset","text":"<p>When a model uses BMI (for example), and a sample has no BMI, the 'standard' approach is leave the relevant features as NaN or impute (impute is usually better). MASK model takes a different approach - it would score the sample based on a model trained without BMI.</p> <p><pre><code>PARAMS=\"predictor_type=xgb;masks_params=Smoking_Intensity|BMI|Fev1|Hemoglobin,Hematocrit,Platelets,WBC,MCH,Eosinophils%,Neutrophils%,Neutrophils#,RDW,RBC,MCHC-M|ALT,ALKP,Albumin;masks_tw=0,365,365,365,365;predictor_params=${XGB_PARAMS}\"\n</code></pre> A mask is list of signals, separated by \",\". Masks are separated by \"|\". This example has 5 masks:</p> <ul> <li>Mask 0: Smoking_Intensity</li> <li>Mask 1: BMI</li> <li>Mask 2: Fev1</li> <li>Mask 3: Hemoglobin,Hematocrit,Platelets,WBC,MCH,Eosinophils%,Neutrophils%,Neutrophils#,RDW,RBC,MCHC-M</li> <li>Mask 4: ALT,ALKP,Albumin</li> </ul> <p>** **- the code would train 2 power #masks predictors. In this example - 32 predictors, each with parameter as in predictor_params, but with different list of features:</p> <ul> <li>Predictor 0 includes all the features.</li> <li>Predictor 1\u00a0includes all the features but those associated with mask 0.</li> <li>Predictor 2 includes all the features but those associated with mask 1.</li> <li>Predictor 3 includes all the features but those associated with mask 1 and mask 0.</li> <li>...</li> <li>Predictor 10 includes all the features but those associated with mask 3 and mask 1. The mask code supports only these features:</li> <li>Signals with features of type \"last in time window\", e.g. WBC.last.win_0_to_365<ul> <li>365, the time window for looking for missing value in predict, see next, is defined in the parameter mask_tw</li> <li>If WBC is signal in a mask, all WBC features would be dropped (and not just the 'indicator feature' WBC.last.win_0_to_365)</li> </ul> </li> <li>Smoking Intensity - the masks would drop smoking pack years features as well (as one is calculated from the other plus smoking duration) For every Predictor, we run Calibrator, so score is risk, and it makes more sense to compare scores coming from different predictors (in bootstrap after predict)\u00a0 ** **- to chose the right model per sample:</li> <li>For every mask, the code checks if more than 50% of the signals are missing.</li> <li>To check if a feature is missing, we look in features.masks, so predict should look like</li> </ul> <pre><code>Flow --get_model_preds --rep $REP --f_samples $SAMPLES --f_model $MODEL --f_preds $PREDS --change_model_init \"object_type_name=MedModel;change_command={generate_masks_for_features=1}\" \n</code></pre> <ul> <li>Every mask has time window, given in the mask_tw parameter.<ul> <li>For mask 1, BMI, the time window in the example is 365. The code check if\u00a0BMI.last.win0_to_365 is missing.</li> <li>If a mask has more than one feature, e.g. Mask 3 in the example, it would check for 50% missing or more.</li> <li>Smoking Intensity is treated differently - time window is irrelevant and the code check whether the signal exists ever or not.</li> </ul> </li> <li>Based on the relevant masks for each sample, the right predictor is chosen.</li> <li>In the above example, if a sample has no Smoking Intensity ever, and no BMi in the last 365 days, but he has Fev1 in the last 365 days as well as the majority of signals from Mask 4 and 5, then his score would come from predictor 3.</li> <li>Standard output of predict (per prediction batch) includes:<ul> <li>frequency of each mask - % of missing values per mask</li> <li>frequency of predictor used The predictor return scores before and after calibration.</li> </ul> </li> </ul> <ul> <li>How to choose features for masks?<ul> <li>Based on feature importance</li> <li>Grouped by blood panels</li> <li>If Some signals are highly correlated, they should be in the same mask (because if we mask one of them, the sub-model will use the other one)</li> </ul> </li> </ul> <p>We ran two tests:</p> <ul> <li>LungFlag: train on KP, tests mostly on THIN</li> <li>AAA: train and test on Geisinger with cross validation In both cases we saw no significant change from XGB straightforward, with and without calibration\u00a0 Note that for each model we used the same XGB parameters as in the full model - i.e. we haven't run optimization on top of the mask model, as we cannot do it in our current infrastructure and we decided to skip testing it manually. We hoped for better results. But, the good news is the strength of XGB \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/04.MedAlgo%20Library/MedPredictor%20practical%20guide/XGBoost%20added%20features.html","title":"XGBoost added features","text":"<ul> <li>When using gbtree::updater_colmaker::spit_evaluator==ElasticNet, it is possible to add feature specific additive penalties<ul> <li>Parameter format - a string \"fid:valfid:val,...\"</li> <li>When spliting according to id fid, loss-change is decreased by val</li> <li>if a given feature-id is not given in the string, the corresponding penalty is 0</li> <li>In MedXGB intialization, give\u00a0\"feature-name:valfeature-name:val,...\" \u00a0</li> </ul> </li> <li> </li> <li>It is actually XGB standard constraint, but we need some translation to reach the required format</li> <li>Our parameter format:\u00a0monotone_constraints=f1:d1#f2:d2#...</li> <li>Where:<ul> <li>f is part of a unique feature name, and</li> <li>d is a direction: 1 for up and -1 for down</li> </ul> </li> <li>Currently NO defense against raw values/format, and</li> <li>Bad outcome (feature format not recognized by XGB) yield all predictions = 0.5 without warning</li> </ul>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/index.html","title":"PostProcessors Practical Guide","text":"<p>PostProcessors are processors which occours after predicitons. Full PostProcessor code doc:\u00a0https://Medial-EarlySign.github.io/MR_LIBS/classPostProcessor.html Some PostProcessor, for full list of types and json values to put in names please reffer to PostProcessor_Types.</p> <ul> <li>We have now MultiPostProcessor (for parallelism)</li> <li><code>Calibrator</code> - to calibrate scores to probabilities for example</li> <li>ModelExplainer - Several options for ButWhy:<ul> <li><code>tree_shap</code> - The common and our method for explainability based on shapley values. There other \"explainability\" methods, some of them are model agnostic and not only for tree base (but they are more computational expensive)</li> </ul> </li> <li><code>fairness_adjust</code> - Post processings of scores to adjust for fairness between groups. Was deployed in AAA model</li> </ul> <p> \u00a0 PostProcessor API:  \u00a0 ModelExplainer API:  \u00a0 Examples:</p> <ul> <li>Calibration</li> <li>Explainer of prediction \"But Why\" \u00a0 Example for calibration: <pre><code>{\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"calibrator\",\n      \"calibration_type\":\"binning\",\n      \"min_preds_in_bin\":\"200\",\n      \"min_prob_res\":\"0.005\",\n      //\"calibration_samples\":\"\", //on train or give your samples to calibrate on\n      \"verbose\":\"1\"\n}\n</code></pre> Example for explainer: \u00a0 Tree Shapley: <pre><code>{\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"tree_shap\",\n      \"approximate\":\"0\",\n      \"interaction_shap\":\"0\"\n}\n</code></pre> LIME: <pre><code>{\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"lime_shap\",\n      //\"pp_type\":\"shapley\",\n      \"gen_type\":\"GIBBS\",\n      \"n_masks\":\"500\", //how many masks to sample for learn\n      \"generator_args\":\"{kmeans=0;select_with_repeats=0;max_iters=0;predictor_type=qrf;predictor_args={spread=0;type=categorial_entropy;learn_nthreads=40;predict_nthreads=40;ntrees=50;maxq=500;min_node=300;get_only_this_categ=-1};num_class_setup=n_categ;bin_settings={split_method=iterative_merge;min_bin_count=500;binCnt=150};selection_ratio=1.0}\", //when using Gibbs, otherwise give GAN path here\n      \"sampling_args\":\"{burn_in_count=50;jump_between_samples=10;samples_count=1;find_real_value_bin=1;use_cache=0}\" //in GAN not needed\n}\n</code></pre></li> </ul>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html","title":"ButWhy Practical Guide","text":""},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#global-feature-importance","title":"Global Feature Importance","text":"<p>You can compute global feature importance for each model (tree-based predictors like XGBoost, LightGBM, QRF) directly, without extra preparation. Importance can also be calculated for groups of features (e.g., signals).</p>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#option-1-simple-but-redundant","title":"Option 1 - Simple (but redundant)","text":"<p>Use Flow's <code>print_model_info</code> to get feature importance using SHAP values:</p> <p><pre><code>Flow --print_model_info --f_model $MODEL_PATH --rep $REPOSITORY --f_samples $SAMPLES_TO_ANALYSE_SHAPLEY --max_samples $DOWN_SAMPLE_SAMPLES_TO_THIS_COUNT_OPTIONAL_TO_SPEEDUP --importance_param \"importance_type=shap\"\n</code></pre> This command reports SHAP-based feature importance for each feature in tree models.</p>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#option-2-detailed-report","title":"Option 2 - Detailed Report","text":"<p>Use Flow's <code>shap_val_request</code> to generate a report with average outcome/score/SHAP values across different feature value groups:</p> <p><pre><code>Flow --shap_val_request --f_model $MODEL_PATH --rep $REPOSITORY --f_samples $SAMPLES_TO_ANALYSE_SHAPLEY --max_samples $DOWN_SAMPLE_SAMPLES_TO_THIS_COUNT_OPTIONAL_TO_SPEEDUP --group_signals \"\" --bin_method \"split_method=iterative_merge;binCnt=50;min_bin_count=100;min_res_value=0.1\" --f_output $REPORT_STORE_PATH\n</code></pre> This creates a report at the path specified by <code>--f_output</code>. Key arguments:</p> <ul> <li>group_signals: Leave empty to report for each feature. Use:<ul> <li><code>BY_SIGNAL_CATEG_TREND</code>: group by signals, separating value and trend features</li> <li><code>BY_SIGNAL_CATEG</code>: group by signals, combining value and trend features</li> <li>Or provide a tab-delimited file mapping features to groups</li> </ul> </li> <li>bin_method: Controls how features are binned. The recommended value is <code>split_method=iterative_merge;binCnt=50;min_bin_count=100;min_res_value=0.1</code>, which creates up to 50 bins with at least 100 samples per bin and a minimum value resolution of 0.1. Bins are merged greedily to ensure minimum counts in each bin.</li> <li>cohort_filter: Optionally filter samples using a bootstrap cohort file or a cohort string (e.g., <code>Age:40,60;Time-Window:0,365</code>). For filters beyond Age, Gender, or Time-window, specify <code>f_json</code> to define filter features.</li> <li>keep_imputers: If true, model imputers are used to fill missing values during SHAP analysis. By default, imputers are skipped and missing values are binned separately.</li> <li>max_samples: controls maximal samples count and randomly subsample the row to the number if the file is larger. If 0, will do nothing (default: 0) </li> <li>f_model, rep, f_samples: needed inputs for calculation. We need data repository, full model with predictor and samples files to calcualte the feature importance for</li> </ul> <p>Advanced flags (usually not needed):</p> <ul> <li><code>normalize</code>: Normalize SHAP values to percentages (default: 1)</li> <li><code>normalize_after</code>: If 1, normalizes after summing global importance; if 0, normalizes per prediction (default: 1; only applies if <code>normalize</code> is on)</li> <li><code>remove_b0</code>: Remove the baseline/prior score (default: 1). If 0, keeps and prints the baseline (b0), which is the constant added to all predictions.</li> </ul>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#inspecting-the-raw-output","title":"Inspecting the Raw Output","text":"<p>To inspect the first few rows of the output file (<code>shap_val_output.tsv</code>) and transpose them for easier reading:</p> <pre><code>cat shap_val_output.tsv | head -n 4 | awk ' { for (i=1;i&lt;=NF;i++) a[i]=a[i]\"\\t\"$i; } END { for (i in a) {printf(\"%d%s\\n\", i, a[i])} }' | sort -g -k1\n</code></pre> <p>This produces a table like:</p> Feature Age ADMISSION.category_dep_set_Hospital_Emergency_Department.win_0_3650 DIAGNOSIS.category_dep_set_ICD10_CODE:J00-J99.win_0_1825 1 Importance 5.3 4.9 4.15 2 SHAP::Low_Mean 15.4 -4.3 -5.1 3 SHAP::Low_Std 8.6 1.0 1.4 4 SHAP::Low_Prctile10 4.5 -5.6 -6.8 5 SHAP::Low_Prctile50 15.0 -4.3 -5.2 6 SHAP::Low_Prctile90 27.2 -3.1 -3.3 7 FEAT_VAL::Low_Mean 5.27 0 0 9 FEAT_VAL::Low_Prctile0 0 0 0 10 FEAT_VAL::Low_Prctile10 2 0 0 11 FEAT_VAL::Low_Prctile50 6 0 0 12 FEAT_VAL::Low_Prctile90 9 0 0 13 FEAT_VAL::Low_Prctile100 9 0 0 14 SHAP::Medium_Mean -4.9 -1.2 -1.0 ... ... ... ... ... 19 FEAT_VAL::Medium_Mean 44.96 0.3 0.5 21 FEAT_VAL::Medium_Prctile0 40 0 0 25 FEAT_VAL::Medium_Prctile100 50 1 1 26 SHAP::High_Mean 10.3 6.1 3.2 ... ... ... ... ... 31 FEAT_VAL::High_Mean 84.57 1 1 33 FEAT_VAL::High_Prctile0 81 1 1 37 FEAT_VAL::High_Prctile100 90 1 1 ... ... ... ... ... <p>Example (Age column):</p> <ul> <li>Importance: About 5.3% of the average XGBoost raw score (before sigmoid/calibration)</li> <li>SHAP::Low_Mean: 15.4 - average contribution for the \"Low\" age group (positive)</li> <li>FEAT_VAL::Low_Prctile0: 0 - lowest value in \"Low\" bin; FEAT_VAL::Low_Prctile100: 9 - highest value (so \"Low\" = 0-9 years, average 5.27)</li> <li>SHAP::Medium_Mean: -4.9% - negative contribution for 40\u201350 year olds, average 44.96 (Protective)</li> <li>SHAP::High_Mean: 10.3% - positive contribution for 81\u201390 year olds, average 84.57</li> </ul>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#generating-graphs-from-the-report","title":"Generating Graphs from the Report","text":"<p>Use the script <code>feature_importance_printer.py</code> (should be under MR_Scripts <code>Python-scripts/feature_importance_printer.py</code>).</p> <ul> <li>To output a single HTML file with the top 30 features/groups:   <pre><code>feature_importance_printer.py --report_path $REPORT_STORE_PATH --output_path $OUTPUT_PATH --num_format \"%2.3f\" --feature_name \"\" --max_count 30 --print_multiple_graphs 0\n</code></pre></li> <li>To output multiple HTML files (one per feature/group, up to 30):   <pre><code>feature_importance_printer.py --report_path $REPORT_STORE_PATH --output_path $OUTPUT_PATH --num_format \"%2.3f\" --feature_name \"\" --max_count 30 --print_multiple_graphs 1\n</code></pre><ul> <li><code>force_many_graph</code>: Forces scatter (not bar) chart if set</li> <li><code>use_median</code>: Use median instead of mean for feature bins</li> <li><code>contrib_format</code>: Controls SHAP value precision; <code>num_format</code>: feature value precision</li> <li><code>feature_name</code>: If set, outputs only that feature to a single HTML file</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#local-feature-importance-explaining-a-single-prediction","title":"Local Feature Importance \u2013 Explaining a Single Prediction","text":"<p>To use ButWhy for explaining individual predictions, follow these steps:</p> <ol> <li>Add an Explainer PostProcessor to the Model (can be done later with <code>adjust_model</code>)</li> <li>Apply the Model - Use Flow or <code>CreateExplainReport</code> to generate report</li> </ol>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#adding-an-explainer-to-an-existing-model","title":"Adding an Explainer to an Existing Model","text":"<p>For training with an explainer, see the PostProcessors Practical Guide. To add an explainer to an existing model, create a post_processor JSON like:</p> <pre><code>{\n  \"post_processors\": [\n    {\n      \"action_type\": \"post_processor\",\n      \"pp_type\": \"tree_shap\",\n      \"attr_name\": \"Tree_iterative_cov\",\n      \"filters\": \"{max_count=0;sort_mode=0}\",\n      \"processing\": \"{grouping=BY_SIGNAL_CATEG_TREND;iterative=1;group_by_sum=0;learn_cov_matrix=1;zero_missing=0;use_mutual_information=0;mutual_inf_bin_setting={split_method=iterative_merge;min_bin_count=100;binCnt=50;min_res_value=0}}\"\n    }\n  ]\n}\n</code></pre> <p>Add this post processor using <code>adjust_model</code>:</p> <pre><code>adjust_model --rep $REPOSITORY --inModel $F_MODEL_PATH --out $F_MODEL_OUTPUT_PATH_WITH_EXPLANIER --postProcessors $FILE_PATH_TO_POST_PROCESSOR_DEF --samples $SAMPLES_PATH\n</code></pre>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#creating-a-report-for-each-prediction","title":"Creating a Report for Each Prediction","text":"<p><code>CreateExplainReport</code> is part of AllTools:</p> <pre><code>CreateExplainReport --rep $REPOSITORY --samples_path $PATH_TO_SAMPLES_TO_EXPLAIN --model_path $PATH_TO_MODEL --output_path $OUTPUT_PATH_REPORT --take_max 10\n# viewer_url_base: Controls the viewer link (should contain two \"%d\" for pid and prediction time)\n# rep_settings: Controls features shown per group, e.g., rep_settings=\"min_count=2;sum_ratio=0.5\" shows at least 2 features/group and at least 50% total weight\n</code></pre>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/ButWhy%20Practical%20Guide.html#generating-html-reports","title":"Generating HTML Reports","text":"<p>Use <code>explainer_printer.py</code> (in MR_Scripts under <code>Python-scripts/explainer_printer.py</code>):</p> <p><pre><code>explainer_printer.py --report_path $OUTPUT_PATH_REPORT --predictor_name \"pre2d - optional argument to control graph title\" --filter_pid -1 --max_count 10 --output_path $FOLDER_OUTPUT_PATH_TO_HTMLS\n# This creates an HTML file for each sample. Use --filter_pid to select specific pids. max_count limits the number of HTMLs generated.\n</code></pre> </p>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/Explainers%20%28But%20Why%29.html","title":"Explainers (But Why)","text":"<p>ModelExplainer (code doc) ModelExplainer API:  </p>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/Explainers%20%28But%20Why%29.html#modelexplainer-types","title":"ModelExplainer Types:","text":"<ul> <li>TreeExplainer\u00a0 \u2013 explains model with SHAPLEY implementation for trees (it has 2 additional flags \u2013 do interaction calculation for shap values or use approximate calculation which is faster):<ul> <li>If tree model has its own implementation for shapley (like XGboost, LightGBM) \u2013 will use those methods directly</li> <li>If the model is based on ensemble trees (like QRF) it will convert the model into generic tree model in learn. Than in Apply it will run SHAP algorithm for trees with the given flags. I made the function parallel</li> <li>If it's other MedPredictor \u2013 it will learn other ensemble trees model (for example XGBoost) based on parameters for the predictor. The model will \u00a0be \"proxy model\" for our model and will learn the original model output (regression problem) and will run SHAP algorithm for trees for the proxy model to explain scores It's just backup algorithm for models that are not ensemble trees\u2026</li> </ul> </li> <li>ShapleyExplainer -\u00a0Agnostic SHAP algorithm, may use Gibbs, GAN as generators for the process. It also has sampling parameters for the mask to speed up the calculation. Still very slow implementation \u2013 it's here because we already written it and maybe in the future for some specific research problems it may be useful. Maybe to compare with other methods</li> <li>LimeExplainer -\u00a0\u00a0Agnostic SHAP algorithm, may use Gibbs, GAN as generators for the process. It\u00a0has sampling parameters for the mask to speed up the calculation and fits linear model to the random masks. It's faster the Shapley because the runtime is O(Number_of_masks)generate_samples. In Shapley we do this for each feature again, with and without the specific feature. so Shapley with the same sampling args is slower by 2number of features. the linear fit time is very fast</li> <li>MissingShapExplainer \u2013 Agnostic SHAP algorithm that doesn't use Gibbs or GAN and is much faster\u2026 the Algorithm trains proxy model (with same parameters as original model) on the outcome with added masked samples. masked samples are samples where feature values were removed on the mask and replaced with missing value. It reweights all training samples to match SHAP weights for each mask. The results on the simulated data(which are very simple) looks good for lightGBM model, the runtime is also very good. The idea is that the proxy model can now handle much better missing values and we can just feed the model with missing values instead of generating \"real\" samples. Theoretically for linear\\polynomial kernel it's should work(and works) very good</li> <li>LinearExplainer - Simple Explainer for linear models to return feature_value*coeff. The implementation is generic for all models - for each feature\\group of features, the contribution is calculated as the difference between the model score with the original feature value versus the model score with the feature value set to 0. It's similar to Shapley but much faster - taking mask of all 1's so no need to generate values with Gibbs\\GAN. Very similar implementation to\u00a0MissingShapExplainer\u00a0but without proxy model</li> <li>KNNExplainer\u00a0-An explainer that calculates average score for neighbours of sample in training data, \u00a0when \u00a0neighborhood is calculatesd with and without the tested feature, and the ratio between the two is returned.</li> </ul> Explainer type string to put in json internal support for grouping of features Additional options Advantages run_time TreeExplainer \"tree_shap\" NO Support for Interaction valuesSupoprt for approximate alg - Saabas alg which is faster Accurate very fast!! ShapleyExplainer \"shapley\" YES <p>Accurate (when not sampling)</p><p>Model Agnostic</p> Very Slow, depend heavily in the number of features LimeExplainer\u00a0 \"lime_shap\" YES <p>Accurate (when not sampling)</p><p>Model Agnostic</p> Slow, but can be feasible MissingShapExplainer\u00a0 \"missing_shap\" YES Model Agnostic very fast!! LinearExplainer\u00a0 \"linear\" YES fastest!! KNNExplainer \"knn\" NO <p>May use raw score or thresholded score.</p><p>May set a threshold ifnot given.</p> Model agnostic. fast"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/Explainers%20%28But%20Why%29.html#global-arguments-for-all-explainers","title":"Global Arguments for all explainers:","text":"<ul> <li>filters<ul> <li>max_count - maximal number of features\\groups to keep for explaining\u00a0</li> <li>sum_ratio - the maximal number of features\\groups to\u00a0keep for explaining when considering the sum of top features contributions as ratio from the total\u00a0contributions . in range 0-1, 1 - means take all</li> <li>sort_mode - 0 sort feature\u00a0contributions by applying ABS (no importance for sign), +1 - sort only positive contributions\u00a0, -1 - sort only negative\u00a0contributions\u00a0</li> </ul> </li> <li>processing<ul> <li>learn_cov_matrix - If turned on will use the train_matrix to calculated Covarince matrix and will aggregate the\u00a0contributions for all correlated features\\groups according to covariance\u00a0</li> <li>cov_features - covarince matrix path, If we want to use extrernal covariance matrix instead of calculating on train matrix</li> <li>group_by_sum - If turned on will use External grouping to calculate group\u00a0contribution using sum of each feature in the group. for example in TreeExplainer this is the only way to calculate\u00a0contribution for group, the other explainers has specific special implementation for grouping</li> <li>grouping - a file path for grouping or the keyword \"BY_SIGNAL\" to group each feature by it's signal. If file path is provided the file format is tab-delimited with 2 fields: feature_name_to_search_in_features, group_name</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/FairnessPostProcessor.html","title":"FairnessPostProcessor","text":"<p>PostProcessor on score to constraint fairness between groups</p>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/Howto%20write%20PostProcessor.html","title":"How to Write a PostProcessor","text":"<p>A PostProcessor is a component that takes the feature matrix and prediction results, then applies additional post-processing steps. It is executed after the MedPredictor stage in the pipeline.</p> <p>PostProcessors in MedModel follow a defined sequence of method calls. Here\u2019s the typical lifecycle:</p> <ol> <li> <p>Constructor    - Initializes the PostProcessor object.</p> </li> <li> <p>init_defaults()    - Sets default values for the processor. Make sure to update <code>processor_type</code> to indicate the processor type.</p> </li> <li> <p>Initialization    - During learning:      Implement <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse parameters from a key-value map (using <code>SerializableObject::init_from_string</code>).      If your PostProcessor should operate on a specific subset of training samples, set either <code>use_p</code> or <code>use_split</code>:</p> <ul> <li><code>use_split</code>: Uses the \"split\" stored in MedSamples. All splits (based on patient ID) except the selected one are passed to the full model pipeline; the selected split is reserved for training this PostProcessor.</li> <li><code>use_p</code>: A value between 0 and 1 that determines the proportion of randomly selected patient IDs passed to the PostProcessor. The remainder is processed by the main MedModel pipeline.  This mechanism also supports multiple PostProcessors, each working on a different subset of the data.</li> <li>During application:  Arguments are loaded from disk. Parameters stored via <code>ADD_SERIALIZATION_FUNCS</code> are restored automatically.</li> </ul> </li> <li> <p>Pipeline Integration    - <code>init_post_processor()</code>:      Initializes the PostProcessor using the complete MedModel pipeline, allowing for any necessary adaptations before execution.</p> </li> <li> <p>Learning Phase    - <code>Learn()</code>:      Implements any learning logic required during training.</p> </li> <li> <p>Application Phase    - <code>Apply()</code>:      Applies the post-processing logic to the data.</p> </li> </ol>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/Howto%20write%20PostProcessor.html#steps-to-implement-a-postprocessor","title":"Steps to Implement a PostProcessor","text":"<ol> <li> <p>Create Class Files    - Create a new <code>.h</code> header and <code>.cpp</code> source file for your PostProcessor class. Include <code>PostProcessor.h</code> in your header.</p> </li> <li> <p>Set Default Values    - Implement <code>init_defaults()</code> or set defaults in the constructor.</p> </li> <li> <p>Parameter Initialization    - Override <code>init(map&lt;string, string&gt;&amp; mapper)</code> to parse external parameters.</p> </li> <li> <p>Serialization    - Add <code>MEDSERIALIZE_SUPPORT($CLASS_NAME)</code> at the end of your header file (replace <code>$CLASS_NAME</code>).    - Add <code>ADD_CLASS_NAME($CLASS_NAME)</code> in the public section of your class.    - Use <code>ADD_SERIALIZATION_FUNCS</code> to specify which parameters should be saved after learning. Exclude temporary or repository-specific variables.</p> </li> <li> <p>Pipeline Adaptation (if needed)    - Implement <code>init_post_processor()</code> if your PostProcessor needs to adapt based on the full MedModel pipeline.</p> </li> <li> <p>Define Dependencies and Outputs    - Implement <code>get_input_fields()</code> and <code>get_output_fields()</code> to specify the inputs and outputs of your PostProcessor.  </p> <ul> <li>For features, prefix the name with <code>\"feature:\"</code>.</li> <li>For predictions, use <code>\"prediction:X\"</code> (where X is the prediction index, usually 0).</li> <li>For other sample effects, use <code>\"attr:\"</code>, <code>\"str_attr:\"</code>, or <code>\"json:\"</code> as appropriate.  The MedModel pipeline uses this information to determine if the PostProcessor is required.</li> </ul> </li> <li> <p>Learning    - Implement <code>Learn()</code> for any required training logic.</p> </li> <li> <p>Apply    - Implement <code>Apply()</code> to perform the post-processing.</p> </li> <li> <p>Register Your PostProcessor in the Header (<code>PostProcessor.h</code>)    - Add a new type to <code>PostProcessorTypes</code> before <code>FTR_POSTPROCESS_LAST</code>. In the documentation comment, specify the name in <code>PostProcessorTypes</code> for Doxygen reference.</p> </li> <li> <p>Register Your PostProcessor in the Source (<code>PostProcessor.cpp</code>)</p> <ul> <li>Add your type conversion to <code>post_processor_name_to_type</code></li> <li>Add your class to <code>PostProcessor::new_polymorphic</code></li> <li>Add your class to <code>PostProcessor::make_processor(MedPredictorTypes model_type)</code></li> </ul> </li> </ol> <p>Tip: Follow the structure and conventions of existing PostProcessors for consistency and easier integration into the MedModel framework.</p>"},{"location":"Infrastructure%20Library/05.PostProcessors%20Practical%20Guide/MultipleImputations.html","title":"MultipleImputations","text":"<p>The main idea is to do multiple imputations for missing value as post processor on existing model.\u00a0 The steps are:</p> <ul> <li>Apply model till feature processor imputers (if model has one). If model doesn't have imputer complete apply till the end before predictor. The main idea is that we can't just use the model imputer (it's not stochastic and we need to replace it) so the postprocessor locate the imputer if exists and \"replaces\" with stochastic one.\u00a0</li> <li>Duplicate each row 100 times (parameter) for different imputation in each line. There is also additional parameter \"batch\" to control the maximal memory needed for Apply the post processor - mainly set to 10K, so we will end up with 1M rows in each batch that each 100 rows are duplicates of the same row and will be aggregated later.\u00a0</li> <li>Apply the stochastic imputer given in the post processor argument on all rows - so each row out of the 100 duplication will get different imputations</li> <li> <p>Aggregations, each 100 rows are aggregated - Mean,Median, STD, CI_lower, CI_Upper are extract as attributes in model apply. We can later test if the model original score (that is not impacted) is even inside the confidence interval. You can also control the post processor to override the prediction with mean value/median value of this multiple imputations \u00a0 Example config: <pre><code>{ \n  \"post_processors\": [\n    {\n      \"action_type\":\"post_processor\",\n      \"pp_type\":\"aggregate_preds\",\n      \"force_cancel_imputations\":\"1\",\n      \"use_median\":\"0\",\n      \"resample_cnt\":\"100\",\n      \"batch_size\":\"10000\",\n      \"feature_processor_type\":\"predictor_imputer\",\n      //\"feature_processor_args\":\"{gen_type=UNIVARIATE_DIST;generator_args={strata=Age,0,100,5;min_samples=50};tag=labs_numeric}\"\n      \"feature_processor_args\":\"{gen_type=GIBBS;generator_args={kmeans=0;select_with_repeats=0;max_iters=0;predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=0;num_trees=100;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=50;feature_fraction=0.8;bagging_fraction=0.25;bagging_freq=4;is_unbalance=true;num_leaves=80;silent=2};num_class_setup=num_class;calibration_string={calibration_type=isotonic_regression;verbose=0};calibration_save_ratio=0.2;bin_settings={split_method=iterative_merge;min_bin_count=200;binCnt=100};selection_ratio=1.0;selection_count=500000};sampling_args={burn_in_count=5;jump_between_samples=10;samples_count=1;find_real_value_bin=1};verbose_learn=1;tag=labs_numeric}\"\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>resample_cnt - how many times we duplicate each row</p> </li> <li>force_cancel_imputations - flag to indicate we need to find model original imputer and replace it (otherwise there will be no missing values, the imputer will impute them).</li> <li>batch_size - how many samples in each batch before duplications</li> <li>use_median - if true will replace pred_0 with median</li> <li>feature_processor_type - the feature processor init type - pay attention to use something with stochastic properties, for example \"predictor_imputer\" - you can see more imputer options in\u00a0FeatureProcessor practical guide</li> <li>feature_processor_args - arguments for the feature processors. You can see some explanations in the FeatureProcessor practical guide Run adjust model with this post processor to generate gibbs sampling on missing values in features tagged \"labs_numeric\": <pre><code>adjust_model --rep /home/Repositories/THIN/thin_jun2017/thin.repository --samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/diabetes/train.samples --inModel /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/diabetes/base_model.bin --out /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/diabetes/test_imputer.2.mdl --postProcessors $MR_ROOT/Projects/Shared/Projects/configs/UnitTesting/examples/MultipleImputations/post_processors.multipleimputations.json\n</code></pre> Apply with Flow and store attributes: <pre><code>Flow --get_model_preds --print_attr 1 --rep /home/Repositories/THIN/thin_jun2017/thin.repository --f_samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/diabetes/test.samples --f_preds /server/Linux/alon/pre2d_test.tsv --f_model /server/Work/Users/Alon/But_Why/outputs/Stage_B/explainers/diabetes/test_imputer.2.mdl\n</code></pre> </li> </ul> <p>Example output for pred2d model output: Excel Results There are additional columns: attr_pred.ci_lower,attr_pred.ci_upper,attr_pred.mean,attr_pred.median,attr_pred.std \u00a0</p>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html","title":"AlgoMarkers","text":"<p>AlgoMarkers are wrappers to models or calculators that are able to \"talk\" to other services via the API defined in AlgoMarker.h (the api funcs start with AM_API_... ). The AlgoMarker is a C library and we have a <code>AlgoMarker_Server</code> that exposes the library and model as REST API. This can alos be done easily with python FastAPI, but for minimal deployments, it might be better to use the <code>AlgoMarker_Server</code>. This page describes the following:</p> <ul> <li>How to write a new AlgoMarker</li> <li>Compile the AlgoMarker shared library</li> <li>The MedialInfra AlgoMarker<ul> <li>Configuration file</li> <li>Eligibility rules configuration</li> </ul> </li> <li>How to freeze a MedialInfra AlgoMarker</li> <li>How to test the AlgoMarker shared library </li> </ul>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#how-to-write-a-new-custom-algomarker","title":"How to write a new Custom AlgoMarker","text":"<p>The AlgoMarker base class is in AlgoMarker.h in the AlgoMarker lib which is part of medial research libs git. So to work with it pull it from git first. In order to write a new AlgoMarker one has to go through the following steps:</p> <ol> <li>Write a new class that inherits from AlgoMarker (or from one of its derived classes), and make sure the following virtual functions are all filled:<ul> <li>virtual int Load(const char *config_f) : gets a config file (or NULL if you don't need one), and gets the AlgoMarker into a working state.</li> <li>virtual int Unload()\u00a0\u00a0: releases all allocated memory and closes</li> <li>virtual int AddData(int patient_id, const char signalName, \u00a0int TimeStamps_len, long long TimeStamps, int Values_len, float* Values)<ul> <li>Adds data to the AlgoMarker which later could be used to ask scores for</li> <li>Data is added per pid, for a specific signalName, and then 2 arrays giving the time and value channels. The order in each array is element by element and then each channel for each element.</li> </ul> </li> <li>virtual int ClearData() : Clear all data in the AlgoMarker</li> <li>virtual int Calculate(AMRequest request, AMResponses responses)<ul> <li>Major API : once the AlgoMarker is loaded and has data, one can give it requests and get responses (=results).</li> <li>To handle requests and responses one can check their classes and the example AlgoMarkers already implemented.</li> </ul> </li> </ul> </li> <li>Add a type to your new AlgoMarker in the enum\u00a0AlgoMarkerType</li> <li>Update the\u00a0AlgoMarker::make_algomarker routine to support your new class</li> <li>That's It\u00a0!! compile and ready to go. NO NEED to touch any of the API routines - they only rely on the implementation of the 5 routines in (1).</li> </ol> <p>[!NOTE] In most use cases(all till now), you won't need to do that and write a custom AlgoMarker. You can use TYPE = \"MEDIAL_INFRA\" and our existing implementation supports all what you need. So you can skip this step.</p>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#compile-the-algomarker-library","title":"Compile The AlgoMarker library","text":"<ul> <li>Follow the AlgoMarker Library Setup to build the <code>libdyn_AlgoMarker.so</code> library, or use a pre-built version if available.</li> </ul>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#the-medialinfra-algomarker","title":"The MedialInfra AlgoMarker","text":"<p>The MedialInfra AlgoMarker allows using any model that was trained using Medial MedProcessTools infrastructure. On top of that it allows also for some nice configuration of eligibility testing on input data, and is packaged with a configuration file that's easy to edit and work with.</p>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#configuration-file","title":"Configuration File","text":"<p>When Loading a new MedialInfra AlgoMarker a configuration file is given. The format of the general file is explained in the next example:</p> <p>Configuration File Example<pre><code>#################################################################################\n# MedialInfra AlgoMarker config example file\n#################################################################################\n# comment lines start with #\n# all (non comments) separators are tab\n# type of AlgoMarker\n# shoule be MEDIAL_INFRA for a MedialInfra AlgoMarker, otherwise will fail loading\nTYPE    MEDIAL_INFRA\n# name : One can get the name via the AM_API\nNAME    PRE2D\n# repository file configuration : to enable load of signal names and dictionaries, or optionally also data\nREPOSITORY  thin.repository\n# basic time unit for signals used in the specific marker (typically : Date for in patient, Minutes for out patient)\nTIME_UNIT   Date\n# model file for AlgoMarker , if name does not start with '/' the file position will be relative to the directory in which the config file was at\nMODEL   pre2d.model\n#################################################################################\n# Eligibility\n#################################################################################\n# Following parts are optional: when defining eligibility rules\n# if a file is given it will be used, if \".\" is the file name, then it means this config file contains also the filters definitions.\nINPUT_TESTER_CONFIG .\n\u00a0\n# each filter is in the following format:\n# FILTER    &lt;filter type&gt;|&lt;filter params&gt;|&lt;warning_or_error&gt;|&lt;use_for_max_outliers_flag&gt;|&lt;external_rc&gt;|&lt;internal_rc&gt;|&lt;err_msg&gt;\n# &lt;filter type&gt; : currently always 'simple' or 'attr'\n# &lt;filter params&gt; : see examples below, and/or read documentation. Params should be separated with ';' .\n# &lt;warining_or_error&gt;: values are WARNING or ERROR\n# &lt;use_for_max_outliers_flag&gt;: ACC=1 or ACC=0 : state from which filters to accumulate the overall number of outliers\n# &lt;external_rc&gt; : read code to return in the message case the test did not pass\n# &lt;internal_rc&gt; : internal read code returned (another layer of error codes that is needed)\n# &lt;err_msg&gt; : string - free text that will be returned as the error message in case the test did not pass.\n# TESTER_NAME : mainly for debug prints etc...\nTESTER_NAME pre2d_tester\n\u00a0\n# example for a filter to force a minimal number of results in a certain time window (defualt given in days)\nFILTER  simple|sig=Glucose;win_from=0;win_to=730;min_Nvals=2|ERROR|ACC=0|310|310|Not enough Glucose tests in the last 2 years\n\u00a0\n# example for a filter that forces also a maximal number of results for a signal\nFILTER  simple|sig=GENDER;min_Nvals=1;max_Nvals=1|ERROR|ACC=0|310|310|Missing GENDER or more than 1 GENDER signal\n\u00a0\n# example for a filter that forces values to be in a given list (allowed_values)\nFILTER  simple|sig=GENDER;allowed_values=1,3|WARNING|ACC=0|310|310|WARNING: GENDER Value Not 1 or 3\n\u00a0\n# example for an AGE filter (ages should be &gt;= and &lt;= the given range)\nFILTER  simple|sig=AGE;min_val=50;max_val=60|WARNING|ACC=0|320|320|age not in range 50-60\n\u00a0\n# example for a filter that verifies that the values of a signal are defined in the repository dictionary (important for categorical signals)\nFILTER  simple|sig=Drug;values_in_dictionary=1|ERROR|ACC=0|320|320|Drug code not in dictionary\n\u00a0\n# example for filters that check for a maximal number of outliers\nFILTER  simple|sig=Glucose;win_from=0;win_to=3650;min_val=10;max_val=2000;max_outliers=3|ERROR|ACC=1|321|321|Too many glucose outliers\nFILTER  simple|sig=HbA1C;win_from=0;win_to=3650;min_val=3;max_val=12;max_outliers=3|ERROR|ACC=1|321|321|Too many HbA1C outliers\n# if the model has Glucose_nRem attributes we can create the following rule by testing it directly.\nFILTER  attr|attr_name=Glucose_nRem;max=0|ERROR|ACC=0|321|321|Too many glucose outliers\n\u00a0\n# max outliers allowed when summing over all the ACC=1 filters\nMAX_OVERLALL_OUTLIERS   1\n</code></pre> </p>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#eligibility-rules-configuration","title":"Eligibility rules configuration","text":"<p>Most of the options can be seen in the above example, here are the basics:</p> <ul> <li>Each filter is given in a line :\u00a0FILTER |||||| <li>Currently  is always 'simple' or 'attr' <li> : see examples above and definitions below <li> : one of WARNING or ERROR. A warning will run the test, report the problem if found, but not fail the example and still will give a score for it. <li>: ACC=1 or ACC=0 : state from which filters to accumulate the overall number of outliers (MAX_OVERALL_OUTLIERS) <li> : read code to return in the message case the test did not pass <li> : internal read code returned (another layer of error codes that is neededed <li> : string - free text that will be returned as the error message in case the test did not pass. \u00a0 Filter params is given in a string , separator is ';' and there should be no spaces/tabs. It has many options, see the class SanitySampleFilter to view them formally."},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#parameters-for-simple-filter-type","title":"Parameters for simple filter type","text":"<ul> <li>sig : name of the signal we want to test</li> <li>time_ch , val_ch : time and value channels to test signal with (defaults are 0 and 0)</li> <li>win_time_unit : default Days</li> <li>samples_time_unit: default Date</li> <li>min_val , max_val : min and max allowed values for a signal (&gt;=min &lt;=max) when testing for outliers.</li> <li>win_from, win_to : a time window before and relative to the sample in which we apply the test:<ul> <li>Each of the tests asked for will be done only on the signal results in the given time window.</li> <li>Default is all the tests (infinite window to the past)</li> </ul> </li> <li>min_Nvals, max_Nvals : test min or max number of values (ALL values not just those in a given range if one is given) for the given signal and window.</li> <li>max_outliers : maximal allowed number of outliers for the signal in the given time window</li> <li>min_left : minimal number of results left after throwing the outliers.</li> <li>allowed_values : list of values , seprated by comma (',') . Test verifies that all values (in the given window) are one of the given allowed values.</li> <li>values_in_dictionary : if =1 (default 0) , will test that all the values of the signal are valid values in the repository dictionary. Useful for categorical data such as Drug or RC. A filter can be configured to do one simple test, or several. As explained above many filters can be defined. All filters defined will run on every point (pid,timepoint pair) , and all the filters that did not pass will push their error message into the relevant response messages. For cases in which the sample did not pass a filter defined as ERROR , the AlgoMarker will not generate a score. However for cases which only had warnings, it will. \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#parameters-for-attribute-attr-filter-type","title":"Parameters for attribute ('attr') filter type","text":"<p>To set this filter use 'attr' in filter type (see example above). The use of this filters relies on the fact the model was built with an option to create those attributes and inject them into the resulted MedSamples object at prediction time. The classical use is to let cleaners and testers running in the model to report their results this way. Formally these actions will add an attribute to each prediction, and the filter defined here is able to create rules based on those attributes. Parameters:</p> <ul> <li>name : the name of the attribute to test</li> <li> <p>max : the maximal value allowed. An error or warning will be given for any value larger than this. \u00a0 To make sure a model creates those attributes it is needed to make it do so in its json definition.</p> </li> <li> <p>To a basic cleaner ( \"rp_type\": \"basic_cln\" in json) add :\u00a0\"nrem_attr\": \"nRem\",\"ntrim_attr\": \"nTrim\",\u00a0 \u00a0in order to get an attribute of the number of removed signals, and/or trimmed ones. Note this counts the total number and not in a specific time window....</p> </li> <li>To force a panel use for example:<ul> <li>{\"action_type\":\"rep_processor\",\"rp_type\":\"req\",\"signals\":\"Hemoglobin,RBC,Hematocrit\", \"win_from\": \"0\", \"win_to\": \"365\"},</li> <li>this will count how many of the panel signals are missing in the given time window (relative to prediction point). Note that if placed AFTER cleaners (reccomended) it will test this on the cleaned data that may have some values removed.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#how-to-freeze-a-medialinfra-algomarker-version","title":"How to freeze a MedialInfra AlgoMarker Version","text":"<ol> <li>If using a non frozen libraries version:<ol> <li>Either create a branch with a suitable name for your freeze for the Libs git</li> <li>Tag your version in the branch you want to work with.</li> </ol> </li> <li>It is reccomended to freeze also code for Tools git, and code for releavant projects (for example the Diabetes git project for a diabetes algomarker, etc).</li> <li>When freezing several different gits, make sure to tag all of them with the same tag</li> <li>Document your freeze and branches/tags names.</li> </ol> <ol> <li>Each version that is being compiled with our scripts, contains the git last commit information and date of compilation for both MR_Libs and MR_Tools. No need to tag/create branches or things you might forget. </li> <li>Make sure you have everything prepared: <ol> <li>A model file trained for the AlgoMarker. Better test this very algomarker gives the expected results on your test set, and runs with the frozen libraries version.</li> <li>The repository files you were working with: main needed:</li> <li>the .repository file</li> <li>the .signals file</li> <li>the dictionary files for the signals you are using.</li> </ol> </li> <li>Create a new directory, call it with the agreed upon algomarker+version name</li> <li>Put there: <ol> <li>The algomarker model file</li> <li>The repository files used</li> <li>Prepare an algo marker config file as explained above</li> </ol> </li> <li>Make sure the eligibility rules are the ones you want.</li> <li>Good time to run a small unit-test here to see it loads, runs and gives expected results on some prepared data set.</li> <li>Zip directory.</li> </ol> <p>It is also describred in here </p>"},{"location":"Infrastructure%20Library/AlgoMarkers/index.html#how-to-test-the-algomarker-library","title":"How to test the AlgoMarker library","text":"<p>The AlgoMarker project includes the DllAPITester, which can be used to test the DLL and compare its output to scores given by the Flow app with existing repository. With this way we can run score compare and check that the results using AlgoMarker library and our tools used in the research are equivelent. After all json/format conversions and using this wrapper. </p> <p>DllAPITester Help</p> <p>DllAPITester Help<pre><code>DllAPITester --h\nReading params\nProgram options:\n  --help                                produce help message\n  --rep arg (=/home/Repositories/THIN/thin_mar2017/thin.repository)\n                                        repository file name\n  --samples arg                         medsamples file to use\n  --model arg                           model file to use\n  --amconfig arg                        algo marker configuration file\n  --direct_test                         split to a dedicated debug routine\n  --test_data arg                       test data for --direct_test option\n  --date arg (=20180101)                test date\n  --egfr_test                           split to a debug routine for the simple\n                                        egfr algomarker\n</code></pre> After compiling, it can be used as follows:</p> <p>Example Run<pre><code>./Linux/Release/DllAPITester --rep /home/Repositories/THIN/thin_jun2017/thin.repository --samples /server/Work/Users/Tal/Temp/test.samples --model /server/Products/Pre2D/QA_Versions/1.0.0.1/pre2d.model --amconfig /server/Products/Pre2D/QA_Versions/1.0.0.1/pre2d.amconfig\n</code></pre> The tester will compare the scores given by both methods and will return passed/failed. For example:</p> <p>Example Output<pre><code>...\n#Res1 :: pid 5000529 time 20060308 pred 0.135475 #Res2 pid 5000529 time 20060308 pred 0.135475\n#Res1 :: pid 5000529 time 20060315 pred 0.090177 #Res2 pid 5000529 time 20060315 pred 0.090177\n#Res1 :: pid 5000529 time 20060818 pred 0.111277 #Res2 pid 5000529 time 20060818 pred 0.111277\n#Res1 :: pid 5000529 time 20070907 pred 0.085166 #Res2 pid 5000529 time 20070907 pred 0.085166\n#Res1 :: pid 5000529 time 20080225 pred 0.103451 #Res2 pid 5000529 time 20080225 pred 0.103451\n#Res1 :: pid 5000529 time 20080411 pred 0.075562 #Res2 pid 5000529 time 20080411 pred 0.075562\n#Res1 :: pid 5000529 time 20090324 pred 0.132835 #Res2 pid 5000529 time 20090324 pred 0.132835\n#Res1 :: pid 5000529 time 20110124 pred 0.170343 #Res2 pid 5000529 time 20110124 pred 0.170343\nComparing 99 scores\n&gt;&gt;&gt;&gt;&gt;TEST1: test DLL API batch: total 99 : n_similar 81 : n_bad 0 : n_miss 18\nPASSED\n</code></pre> \u00a0 Another option is running a direct test (note the self explanatory test_data format):</p> <p>Testing a single example directly<pre><code>Linux/Release/DllAPITester --rep /home/Repositories/THIN/thin_jun2017/thin.repository --amconfig /nas1/Products/Pre2D/QA_Versions/dev/pre2d.amconfig --direct_test --test_data \"Glucose:120:20171101;GENDER:1;BYEAR:1988\" --date 20180520\n</code></pre> and get the result :</p> <p>Single test result<pre><code>...\nAlgomarker Pre2D was loaded with config file /nas1/Products/Pre2D/QA_Versions/dev/pre2d.amconfig\nAdding Data: sig Glucose :: vals: 120.000000,  times: 20171101,\nAdding Data: sig GENDER :: vals: 1.000000,  times:\nAdding Data: sig BYEAR :: vals: 1988.000000,  times:\nCreating Request\nBefore Calculate\n...\n...\nShared Messages: 0\nGot 1 responses\nGetting response no. 0\nResponse Messages: 0\nScore 0 Messages: 0\nresp_rc = 0\ni 0 , pid 1 ts 20180520 scr 0.272233\nptr for _scr_type 5288768\n_scr_type Raw\nFinished debug_me() test\n</code></pre> </p>"},{"location":"Infrastructure%20Library/AlgoMarkers/AlgoMarker%20inspection.html","title":"AlgoMarker inspection","text":"<p>How to list all Rules outliers <pre><code>Flow --print_model_info --f_model $MODEL_PATH 2&gt;&amp;1\u00a0 | egrep \"RepRuleBasedOutlierCleaner|BasicOutlierCleaner\"\n</code></pre> How to list all Outliers bounds: <pre><code>Flow --print_model_info --f_model $MODEL_PATH 2&gt;&amp;1 | grep BasicOutlierCleaner | grep -v \"FeatureBasicOutlierCleaner\"\n</code></pre> \u00a0 Print model used signals + categories (some of the signals can turned to be virtual if not exists - for example BMI): <pre><code>Flow --print_model_sig --f_model $MODEL_PATH \n</code></pre></p>"},{"location":"Infrastructure%20Library/AlgoMarkers/Request%20Json%20Format.html","title":"Request Json Format","text":""},{"location":"Infrastructure%20Library/AlgoMarkers/Request%20Json%20Format.html#algomarker-request-format","title":"AlgoMarker Request Format","text":"<p>For complete details, see the AlgoMarker Spec.</p>"},{"location":"Infrastructure%20Library/AlgoMarkers/Request%20Json%20Format.html#required-fields","title":"Required Fields","text":"<p>A valid request to AlgoMarker should include the following fields:</p> <ul> <li>type: Set to <code>\"request\"</code>.</li> <li>request_id: A unique string identifier for the request.</li> <li>export: Specifies the desired output. For standard predictions, use <code>\"prediction\": \"pred_0\"</code> to request the model prediction and store it in the <code>\"prediction\"</code> field of the response. You can also specify request to recieve explainabiltiy output if applicable for example by specifiying <code>\"explainability_output_field_name_for_your_control\": \"json_attr Tree_iterative_covariance\"</code>.</li> <li>load: Set to <code>1</code> to indicate that patient data is included in the request.</li> <li>flag_threshold (optional): If your model supports configurable thresholds, specify the threshold name here.</li> <li>requests: An array of patient requests, each containing:<ul> <li>patient_id: An integer patient identifier.</li> <li>time: The calculation date in <code>YYYYMMDD</code> format. Any data after this date will be ignored from score calculation.</li> <li>data: Patient data, including:<ul> <li>signals: An array of signal objects, each with:<ul> <li>code: The signal name.</li> <li>data: An array of data points, each with:<ul> <li>timestamp: An array of timestamps (e.g., <code>[20250806]</code>). Can be empty for static values.</li> <li>value: An array of values (e.g., <code>[\"14.5\"]</code> or <code>[82]</code>). You can pass string/float, both OK.</li> </ul> </li> <li>unit (optional): The unit for the signal (e.g., <code>\"fL\"</code>).</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/AlgoMarkers/Request%20Json%20Format.html#example-signal-entry","title":"Example Signal Entry","text":"Example Signal Entry<pre><code>{\n    \"type\": \"request\",\n    \"request_id\": \"999ef0f4-1099-4178-8c86-ecbfac6578e2\",\n    \"export\": {\n        \"prediction\": \"pred_0\"\n    },\n    \"flag_threshold\": \"USPSTF_50-80$PR_03.000\",\n    \"load\": 1,\n    \"requests\": [\n        {\n            \"time\": \"20250806\",\n            \"patient_id\": \"1\",\n            \"data\": {\n                \"signals\": [\n                    {\n                        \"code\": \"Hemoglobin\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"14.5\"\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"Hematocrit\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"32\"\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"RBC\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"40000.5\"\n                                ],\n                                \"timestamp\": [\n                                    20230806\n                                ]\n                            },\n                            {\n                                \"value\": [\n                                    \"4.5\"\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"MCV\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    82\n                                ],\n                                \"timestamp\": [\n                                    20250806\n                                ]\n                            }\n                        ],\n                        \"unit\": \"fL\"\n                    },\n                    {\n                        \"code\": \"GENDER\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    \"Male\"\n                                ],\n                                \"timestamp\": []\n                            }\n                        ]\n                    },\n                    {\n                        \"code\": \"BDATE\",\n                        \"data\": [\n                            {\n                                \"value\": [\n                                    19780101\n                                ],\n                                \"timestamp\": []\n                            }\n                        ]\n                    }\n                ]\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"Infrastructure%20Library/AlgoMarkers/Setup%20a%20new%20AlgoMarker.html","title":"Setup a new AlgoMarker","text":""},{"location":"Infrastructure%20Library/AlgoMarkers/Setup%20a%20new%20AlgoMarker.html#algomarker-configuration-guide","title":"AlgoMarker Configuration Guide","text":"<p>A sample packaging script can be found in GastroFlag: https://github.com/Medial-EarlySign/AM_LGI/blob/main/Extended_model/scripts/pack_algomarker.sh</p> <p>The setup script defines variables for the model, repository, reference matrix, deployment path, and more.</p>"},{"location":"Infrastructure%20Library/AlgoMarkers/Setup%20a%20new%20AlgoMarker.html#typical-steps","title":"Typical Steps","text":"<ol> <li> <p>Model Adjustments (Optional but Recommended)</p> <ul> <li>Uses tools <code>Flow</code>, <code>adjust_model</code>, and <code>change_model</code> operations to modify the model as needed. For example:<ul> <li><code>Flow --export_production_model</code>: Enables verbose outlier reporting (useful for production, but may slow down predictions and increase memory usage).</li> <li><code>adjust_model</code>: Perform operations such as adding virtual signals (e.g., \"eGFR\") or adding checks (e.g., ensuring a CBC exists on the prediction date and storing the result as an attribute for eligibility filtering).</li> <li><code>change_model</code>: Enable explainability output for AlgoMarker by setting <code>store_as_json=1</code>.</li> </ul> </li> <li>These steps are optional and depend on your production requirements. If you do not need outlier documentation or explainability, you may skip them.</li> </ul> </li> <li> <p>Copy Model Files</p> <ul> <li>Copy the binary model file to the AlgoMarker directory.</li> <li>Document the original model path and its MD5 checksum (recommended).</li> </ul> </li> <li> <p>Build and Copy the Shared Library</p> <ul> <li>Build the <code>libdyn_AlgoMarker.so</code> shared library as described in AlgoMarker Library Setup.</li> <li>Copy the compiled library to the AlgoMarker folder under \"lib\" folder.</li> </ul> </li> <li> <p>Prepare the Repository Configuration</p> <ul> <li>Generate a repository config file containing only the required signals and dictionaries. This defines the signal names, types, and categorical mappings for AlgoMarker.</li> <li>You can copy the config files from the repository used during model training, but it is recommended to clean them up and remove unused signals. A script is available to assist with this from GastroFlag.</li> </ul> </li> <li> <p>Create the AlgoMarker Configuration File (<code>amconfig</code>)</p> <ul> <li>See the Configuration File section for details and a full example.</li> <li>Keep <code>TYPE</code> as <code>MEDIAL_INFRA</code></li> <li><code>NAME</code> is free text to store name for AlgoMarker and will be returned in discovery call request.</li> <li>The <code>FILTER</code> rows allow you to define eligibility rules and warnings. For testing, you may skip these filters.</li> </ul> <p>Example snippet: <pre><code>#################################################################################\n# MedialInfra AlgoMarker config example file\n#################################################################################\nTYPE    MEDIAL_INFRA\nNAME    PRE2D\nREPOSITORY  rep/pre2d.repository\nTIME_UNIT   Date\nMODEL   resources/pre2d.model\n# Eligibility filters and other settings follow...\n</code></pre></p> <ul> <li>Filters can enforce rules such as minimum/maximum number of results, allowed values, age ranges, dictionary validation, and outlier limits.</li> <li>See the full example in Eligibility rules configuration section</li> <li><code>REPOSITORY</code> and <code>MODEL</code> refer to file paths: <code>REPOSITORY</code> is the path to the repository config file (used in Step 4), and <code>MODEL</code> is the path to the model binary (used in Step 2). </li> </ul> <p>It's recommended to use relative paths and place both files in the same folder hierarchy for simplicity.</p> </li> <li> <p>Generate Explainability Config (Optional)</p> <ul> <li>If your model supports explainability, create a configuration file (e.g., <code>resources/explainer.cfg</code>) to control display settings for each group, such as which signals to show and how many values to fetch.</li> </ul> </li> <li> <p>Create Leaflet/Cutoff Threshold Table (Optional)</p> <ul> <li>Generate a cutoff threshold table config file (e.g., from <code>bootstrap_app</code> output) if you want to store thresholds in the AlgoMarker.</li> </ul> </li> <li> <p>Copy Test Script Template (Optional)</p> <ul> <li>Optionally, include a script template for testing AlgoMarker deployments.</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html","title":"The new DllAPITester","text":""},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#the-dllapitester-is-a-testing-tool-for-algomarkers-that-contains-several-useful-options","title":"The DllAPITester is a testing tool for AlgoMarkers that contains several useful options:","text":"<ul> <li>Testing results on a sample set via the infrastructure and via the AlgoMarker</li> <li>Testing via the AlgoMarker library in the infrastructure or via the .so compiled for it.</li> <li>Generating json examples from data.</li> <li>Testing on json examples.</li> <li>Generating dictionaries for AlgoMarkers.</li> <li>Testing also using the newer json requests and json responses. It is a must use tool whenever packing a new algomarker. In the following explanations we will assume one has an AlgoMarker with a model inside, a repository to test on, and a samples file to work with. \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#general-app-parameters","title":"General App Parameters","text":"<p>The App is in ../MR/Libs/Internal/AlgoMarker/Linux/Release/DllAPITester , to compile simply compile the AlgoMarker directory (smake_rel). Major parameters: \u00a0</p> parameter comment rep repository to use samples samples file model the model file (typically the one in the AlgoMarker wrap) amconfig the algomarker (config) to work with amlib the actual .so to use when calling AM_API calls. Optional. Without it will use the AM_API as is currently in the infrastructure. json_dict A list of dictionaries to load into the AlgoMarker when loading it (prior to actual usage of loading data and getting results) am_res_file (optional) The predictions file of the AlgoMarker as generated by calling the AlgoMarker direct_csv (optional) The full feature matrix generated by running through the infrastructure am_csv (optional) The full feature matrix generated by running through the AlgoMarker single Run tests one by one rather than in batch. Slower. Some modes, such as generating json outputs work only in single mode. out_jsons if given will generate a file with a list of jsons (in a long array) that contain all the data needed to give a prediction. These can be used for direct tests in the AlgoAnalyzer. in_jsons if given will take input data from the given jsons. jreq input json request (could contain also data) jresp get output as a json response create_jreq generate a request for a given samples file (will need also jreq_defs, optionally the --add_data_to_jreq flag) and an output file in --jreq_out)"},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#score-compare-testing-results-of-an-algomarker-vs-results-directly-from-the-infrastructure-and-much-more","title":"Score Compare: Testing results of an AlgoMarker vs. results directly from the infrastructure, and much more","text":"<p>This test is currently testing the prediction only (for single prediction models). Not testing other accompanied infomation that could be on top in json responses. It is the major test to be done, to run it: DllAPITester --rep  --samples  --model  --amconfig  This will so a batch run and test the whole samples, producing a summary report. In many cases you would need to add: <ul> <li>--json_dict  : if you require additional dictionaries to be loaded. <li>--single : if you wish to do the test in single mode, one by one, mimicking how it is used by AlgoAnalyzer.</li> <li>optional output files:</li> <li>--amlib  : point to an .so file (typically the one in the AlgoMarker library). If given , the run will use the API from the library, as would happen in real life usage. Without it the API as is currently in the infrastructure will be used. <li>--out_jsons  : generating json examples out of this run. <li>--am_csv  , --direct_csv  : when needing the actual feature matrix via the AlgoMarker or via the infrastructure (or both). <li>--am_res_file : the predictions as generated by the AlgoMarker. \u00a0 An example of a full run example with some options on: <pre><code>DllAPITester --rep /home/Repositories/KPNW/kpnw_apr20/kpnw.repository --samples ./nwp_100.samples --model /nas1/Products/COVID19/QA_Versions/dev_20200608/COVID19-Comp-Flag-2020-05-04.model --amconfig /nas1/Products/COVID19/QA_Versions/dev_20200608/COVID19-Comp-Flag-2020-05-04.amconfig --json_dict /nas1/Work/AlgoMarkers/COVID19/Avi/DIAGNOSIS.txt,/nas1/Work/AlgoMarkers/COVID19/Avi/Drug.txt,/nas1/Work/AlgoMarkers/COVID19/Avi/ADMISSION.txt --single --am_res am.preds --out_jsons n100.jsons\n</code></pre> This example will test the covid predictor on some samples, on the requested repository, but also load some dictionaries prior to that, do the run in single mode, and output both the predictions into a file, and a file containing jsons for all the samples.</li>"},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#generating-json-outputs","title":"Generating json outputs","text":"<p>To do that simply run a score compare run with the out_jsons file requested. You need to be in single mode for this option to work. You can set up some parameters for the creation (optional):</p> <ul> <li>--accountId  :\u00a0 accountId for output json <li>--calculator  :\u00a0 calculator name for output json <li>--units  : list of units to add for requested signals (example: BMI,kg/m^2,Weight,kg,Height,cm,Pack_Years,pack*years,Smoking_Intensity,cigs/day,Smoking_Quit_Date,date,Smoking_Duration,years) <li>--scoreOnDate : flag, use to have that field in output jsons. \u00a0 The jsons will be created with a request_id field \"req_id\" of the form : req__ , coding the actual time for prediction that it was made for."},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#running-on-input-json-examples","title":"Running on input json examples","text":"<p>Sometimes it is useful to generate a run on a given file of jsons, in order to recreate a bug or a prediction on specific cases. This can be done by simply using the --in_jsons option (if given , then you don't need a repository and samples). Currently this option runs over the jsons in the given file one by one (as in single mode), and prints the score to each. It doesn't compare it to anything. You can generate a preds file for all the given jsons using the --preds_json  option. You can also generate the matrix using the --am_csv option. However this currently will simply write the matrix for the last json given in the file (could be improved in future)."},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#generating-dictionaries-for-algomarkers","title":"Generating Dictionaries for AlgoMarkers","text":"<p>AlgoMarkers with categorial signals may need accompanying dictionaries to adjust them to the actual dictionaries in the AlgoMarker. Those dictionaries can be created in anyway and manner as long as they are in the correct format and meaning at the end. The DllAPITester allows generating such dictionaries for a given AlgoMarker and repository. To do that one must supply a config file. The config file is composed of tab delimited lines with 3 fields:   IN means : the value is an input categorial value OUT means : an output value, a valid value to send to the AlgoMarker. There are 2 possible formates for the output, for the one used by the AlgoAnalyzer use --simple_dict option. Example run: <pre><code>DllAPITester --rep /home/Repositories/KPNW/kpnw_apr20/kpnw.repository --dicts_config ../../../NWP/covid_testing/dev_20200504/full_dicts/dicts.config --out_json_dict ./dict.json --simple_dict\n</code></pre>"},{"location":"Infrastructure%20Library/AlgoMarkers/The%20new%20DllAPITester.html#working-with-input-json-requests-and-output-json-responses","title":"Working with input json requests and output json responses","text":"<p>The newer APIs in the library allow for a new way of generating outputs. It is possible now to load data from a rep+samples or in_jsons (As shown above), and then use a json request (\u2013jreq option) for inputs, and get out a json response\u00a0(\u2013jresp). More than that: requests can contain actual data for the patients and if that is used then one can skip loading the data from a repository and samples or in_jsons. To use these options simply do a usual run, but then add a valid --jreq  and an output --jresp . The DllAPITester contains also options to generate such json requests with/without data. (TBD: detailed description of the json request input options, and the structure of the output json response)."},{"location":"Infrastructure%20Library/DataRepository/index.html","title":"DataRepository","text":"<ul> <li>How to load a new DataRepository, please follow ETL Tutorial. It is recommanded to use this tool. It contains testing and final formatting that makes the loading easier.</li> <li>How to load a new DataRepository without the ETL Tool Loading a DataRepository<ul> <li>Explanation on Data Repository Signal config files (definition of the data scheme, name of signals and types)</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/index.html#etl-tool-todos","title":"ETL Tool - TODOs","text":"<ul> <li>Extend tests<ul> <li>Add more numeric test, compare dists to MHS,THIN percentiles. test resolution</li> </ul> </li> <li>parallel the processing</li> <li>Join with BDATE to extract age for stats/filters?</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html","title":"Loading a New Repository","text":""},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#best-practices","title":"Best Practices","text":"<ul> <li>Keep it Simple: Load signals with minimal preprocessing. Handle outliers and clean data within the model itself, not during the ETL stage. This approach simplifies implementation, reduces ETL errors, and results in a more robust model. Future implementations will be easier with a straightforward ETL process.</li> <li>Ensure that all signals are in the correct units - each signal is expected to use the appropriate measurement unit.</li> <li>It is recommended to separate your code into two parts:<ul> <li>Code for fetching the data  </li> <li>Code for processing the data  </li> </ul> </li> </ul> <p>This makes the code easier to read and helps clarify what minimal processing was applied to each signal.</p> <p>You can use your own tools and methods to create \u201cloading files\u201d (described below), or use our ETL tool to both build the ETL process and test the results. More details here: ETL Tool You can start from this code example and change it for your own needs</p> <p>Steps to create loading files on your own:</p>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#step-1-prepare-load-files","title":"Step 1: Prepare Load Files:","text":"<p>In this step you will need to create ETL loading files for each signal.</p>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#data-file-format","title":"Data File Format","text":"<ul> <li>Files are TAB-delimited, no header.</li> <li>Columns:   1. Patient ID   2. Signal name   3. Time channel 0 value   4. ...   5. Time channel i value   6. Value channel 0 value   7. ...   8. Value channel i value   9. Any additional columns are ignored</li> </ul> <p>Notes:</p> <ul> <li>A single file can contain multiple signals (different values in the \"signal name\" column).</li> <li>Rows should be sorted by patient ID and the first time channel (if present).</li> <li>The number of time/value channels used depends on the signal definition.</li> <li>For categorical value columns, use strings without spaces (use underscores <code>_</code> instead). A suitable dictionary is required for conversion to numeric values.</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#example-blood-pressure-signal","title":"Example: Blood Pressure Signal","text":"<p>(1 time channel, 2 value channels; extra columns are ignored)</p> <pre><code>5000001 BP 20030324 60.0 100.0 246..00-1005010500 null value\n5000001 BP 20061218 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20071008 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20090309 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20100802 60.0 90.0 246..00-1005010500 null value\n5000001 BP 20120125 67.0 90.0 246..00-1005010500\n</code></pre>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#step-2-prepare-configuration-file-load_config_file","title":"Step 2: Prepare Configuration file <code>load_config_file</code>","text":"<p>The configuration file uses a delimited format with the following settings:</p> <ul> <li>DESCRIPTION: Description of the repository (ignored by loader)</li> <li>RELATIVE: Use relative paths (set to 1)</li> <li>SAFE_MODE: If 1, stops on critical errors (set to 1)</li> <li>MODE: Use value 3</li> <li>PREFIX: Prefix for data file names stored in OUTDIR</li> <li>CONFIG: Output path for the repository config file (contains dictionaries, signals, etc.)</li> <li>SIGNAL: Path to signal definitions (Signal file format)</li> <li>FORCE_SIGNALS: Comma-separated list of required signals (e.g., BYEAR,GENDER); patients missing these are dropped</li> <li>DIR: Path for config files and data files (relative if RELATIVE=1)</li> <li>OUTDIR: Output directory for the repository</li> <li>DICTIONARY: Path(s) to dictionary files for categorical data</li> <li>DATA: Path(s) to data files</li> <li>LOAD_ONLY: (Optional) Comma-separated list of signals to load; others are ignored (useful for partial updates)</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#example-configuration-file","title":"Example Configuration File","text":"<pre><code># Example file - lines starting with \"#\" are comments\nDESCRIPTION     THIN18 data - full version\nRELATIVE        1\nSAFE_MODE       1\nMODE            3\nPREFIX          data/thin_rep\nCONFIG          thin.repository\nSIGNAL          thin.signals\nFORCE_SIGNALS   GENDER,BYEAR\nDIR             /nas1/Temp/Thin_2018_Loading/rep_configs\nOUTDIR          /nas1/Work/CancerData/Repositories/THIN/thin_2018_2\nDICTIONARY      dicts/dict.drugs_defs\nDICTIONARY      dicts/dict.bnf_defs\nDATA            ../demo2/GENDER\nDATA            ../demo2/BYEAR\nDATA            ../demo2/BDATE\n</code></pre> <p>After you have completed and prepered all loading files please execute:</p>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#step-3-load-the-repository","title":"Step 3: Load the Repository","text":"<pre><code>Flow --rep_create --convert_conf $PATH_TO_CONFIG_FILE --load_err_file $OPTIONAL_FILE_PATH_TO_STORE_ERRORS\n# Optional: Control error thresholds with --load_args, e.g.:\n# --load_args \"check_for_error_pid_cnt=0;allowed_missing_pids_from_forced_ratio=0.05;max_bad_line_ratio=0.05;allowed_unknown_catgory_cnt=50\"\n</code></pre>"},{"location":"Infrastructure%20Library/DataRepository/Load%20new%20repository.html#step-4-generate-reverse-index","title":"Step 4: Generate Reverse Index","text":"<p>After a successful load, generate the reverse index:</p> <pre><code>Flow --rep_create_pids --rep $REPOSITORY_PATH\n</code></pre>"},{"location":"Infrastructure%20Library/DataRepository/Medical%20vocabulary%20mappings.html","title":"Medical vocabulary mappings","text":"<p>There are several domains of medical knowledge/onthologies that is getting updated over time - for example diagnosis and medications. We have several tools for generating dictionaries for those vocabularies. \u00a0</p>"},{"location":"Infrastructure%20Library/DataRepository/Medical%20vocabulary%20mappings.html#updating-medical-vocabulary","title":"Updating Medical Vocabulary","text":"<p>We have several scripts under TOOLS git repository - MR_Tools/DictUtils: Steps to update medications:</p> <ol> <li>Download update vocabulary from:\u00a0https://athena.ohdsi.org/search-terms/start</li> <li>Select for example Drugs, Rx Norm + ATC</li> <li>Copy and extract the files into:\u00a0/nas1/Work/Data/Mapping/Medications - Or to equivalent folder for Diagnosis or Procedures, etc</li> <li>Run the script to extract those files into our dicts format with mapping from RX Norm to ATC -\u00a0MR_Tools/DictUtils/ontologies_scripts/RX_to_ATC.new.py </li> <li>generate_rx_codes() - generates definitions for RX Norm + ATC from \"CONCEPT.csv\" to MR_Tools/DictUtils/Ontologies/RX/dicts, MR_Tools/DictUtils/Ontologies/ATC/dicts:\u00a0</li> <li>generate_rx_maps() - generate mapping from RX Norm to ATC in\u00a0MR_Tools/DictUtils/Ontologies/RX/dicts</li> <li>add_atc_hir() - Creates the hierarchy for ATC codes</li> <li>create_atc_syn() - generate synonm dicts for ATC to include codes in old format of ATC_ABB_CDD instead of ATC:ABBCDD Jupyter notebook with some test on the raw files of OHDSI:\u00a0http://node-02:9000/notebooks/alon-internal/Medications_mapping.ipynb \u00a0 (get_rxnorm_dicts.py - is old script of different data source + RX_to_ATC.py)</li> </ol>"},{"location":"Infrastructure%20Library/DataRepository/Medical%20vocabulary%20mappings.html#todos","title":"TODOs:","text":"<ul> <li>Need to complete code for NDC - current code:\u00a0MR_Tools/DictUtils/ontologies_scripts/NDC_to_ATC.py</li> <li>SNOMED -\u00a0MR_Tools/DictUtils/ontologies_scripts/get_snomed.py - but it's based on different data, that requires registration and to send reports+regulations... Need to switch to OHDSI\u00a0</li> <li>ICD10, ICD9 -\u00a0MR_Tools/DictUtils/ontologies_scripts/icd*\u00a0 scripts \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Medical%20vocabulary%20mappings.html#loading-categorical-signals-with-those-dictionaries","title":"Loading categorical signals with those dictionaries:","text":"<p>After having base dictionaries with mappings, when loading new signal we can use those dictionaries and - search for missing codes, define them, use internal dictionary codes. The library is located in:\u00a0MR_Tools/RepoLoadUtils/common/dicts_utils.py \u00a0 You can use\u00a0create_dict_generic to \"merge\" your signal codes with existing known ontology (for example ICD10) - it will define the missing codes for you and print how many are they? They are also stored in a different dictionary, so they will be easily located, what codes are missing. If it's too much, you can consider updating the ontology dictionaries. Function inputs:</p> <ul> <li>cfg - Configuration object with \"work_dir\" that points to workdir path + \"dict_folder\" that points to path with the Generic medial vocabularies - for example \"MR_Tools/DictUtils/Ontologies/RX/dicts\"<ul> <li>work_dir points to path that includes those subfolders:<ul> <li>FinalSignals - With Prepared signals to load - input path for reading the signals</li> <li>rep_configs/dicts - where we are going to store dictionaries for loading process. The output path of this function</li> </ul> </li> <li>dict_folder - base path for searching dicts. You can pass it as empty string '' - But than you will need to specify full paths in\u00a0def_dicts, sets_dicts</li> </ul> </li> <li>def_dicts - list of dictionaries names that are located under\u00a0dict_folder . Contains \"DEF\" commands</li> <li>set_dicts - list of dictionaries names that are located under\u00a0dict_folder. Contains \"SET\" commands. We currently support Vocabulary dictionaries that has only \"DEF\" commands and \"SET\" command and you need to separate them, to use that tool. Till now the dicts are separated, so there is no concern.</li> <li>signal - name of the signal</li> <li>data_files_prefix - name of the data file in\u00a0FinalSignals that needs to be scanned for codes. Mainly we name the file as the name of the signal it contains, so it will be many times the same as signal</li> <li>header - the header of the datafile - since FinalSignals doesn't contains headers</li> <li> <p>to_use_list - list of columns from \"header\" that contains our codes for merging with the known dicts from \"def_dict + set_dicts\" \u00a0 Example usage of diagnosis loading in Optum - it contains additional 2 categorical columns: <pre><code>def_dicts=['dict.icd9dx', 'dict.icd10']\nset_dicts=['dict.set_icd9dx', 'dict.set_icd10', 'dict.set_icd9_2_icd10']\nheader=['pid','signal' ,'diag_date', 'diagnosis_code', 'diagnosis_status', 'diag_source']\ncategorical_cols=['diagnosis_code', 'diagnosis_status', 'diag_source']\ncreate_dict_generic(cfg, def_dicts, set_dicts, 'DIAGNOSIS', 'DIAGNOSIS', header, categorical_cols)\n</code></pre> \u00a0 Additional function for internal dicts is generate_dict_from_codes. Function inputs:</p> </li> <li> <p>map_df - path or dataframe with 2 columns, first column is the codes, the second column is description.\u00a0</p> </li> <li>outpath - where to store output dictionary</li> <li>min_code - from which code number to start the DEF This function construct dict with 2 rows for each code - the first is the \"code\" and the second row is the description. For example, we might pass \"ATC_XYZ \\t description\" - it will generate \"DEF 1 ATC_XYZ\" and \"DEF 1 description\".\u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Signals%20file%20format.html","title":"Repository Signals File Format","text":"<ul> <li>Lines beginning with <code>#</code> are comments.</li> <li>Each line in the file defines either:<ul> <li>A Generic Signal Type Use <code>GENERIC_SIGNAL_TYPE</code> prefix to declare a new signal type. The format is: <code>GENERIC_SIGNAL_TYPE [type_alias] [GSV_string_spec]</code> (Three tab-separated fields)<ul> <li><code>GENERIC_SIGNAL_TYPE</code>: Marks the line as a signal type definition.</li> <li><code>type_alias</code>: A short name for the signal type.</li> <li><code>GSV_string_spec</code>: The full specification is a single, comma-separated string. All T() (Time) channels must be listed before all V() (Value) channels.  For example: <code>T(i),V(f)</code>.   <ul> <li>Time channels use <code>T(X)</code>, where <code>X</code> is a comma-separated list of basic types (see Types).  </li> <li>Example: <code>T(i)</code> means one integer time channel; <code>T(i,i)</code> means two integer time channels (can also be written as <code>T(i),T(i)</code>).  </li> <li>Value channels follow after time channels with comma separator between. A signal can include no time channels or no value channels.</li> </ul> </li> </ul> </li> <li>A Signal Definition Use <code>SIGNAL</code> to define a signal. Each line should have at least four fields, defaults are used for missing fields.<ul> <li><code>SIGNAL</code>: Marks the line as a signal definition.</li> <li><code>signal_name</code>: The name of the signal.</li> <li><code>signal_unique_code</code>: A unique numeric code for the signal within the repository.</li> <li><code>signal_type</code>: Specify the type directly as a <code>GSV_string_spec</code> (e.g., <code>T(i),V(f)</code> for one integer time channel and one float value channel), or refer to a previously defined type alias using <code>16:type_alias</code>. The 16 is required for the parser to use Custom types and not old and deprecated data type structured.</li> <li><code>comment</code>: Free text, ignored by the system. Can be empty. Usually we document the signal group liek demographic,labs,cbc. Sometimes multiple tags are provided.</li> <li><code>categorical_bitmask</code>: A bitmask written left to right with one digit for each value channel indicating which value channels are categorical. <code>1</code>=categorical, <code>0</code>=numeric. (e.g., <code>10</code> means the first of two value channels is categorical, the second is numeric). Default is <code>0</code> for all value channels.</li> <li><code>units</code> (optional): Units for the signal, separated by <code>|</code> for multiple channels.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Signals%20file%20format.html#timevalue-channel-types","title":"Time/Value Channel Types","text":"<p>Here is a complete list of supported types:</p> <ul> <li><code>u</code> prefix to specify unsigned number and controls the <code>_is_signed</code></li> <li><code>c</code> - character, 8 bit integer that can hold 256 unique numbers</li> <li><code>s</code> - short, 16 bit that can hold 65536 unique numbers </li> <li><code>i</code> - integer, 32 bits, can hold 4B unique numbers </li> <li><code>l</code> - long, 64 bits, can hold 1.8*10^19 unique numbers </li> <li><code>f</code> - float 32 bits</li> <li><code>d</code> - double float 64 bits</li> <li><code>D</code>- long double</li> </ul> <p>For source code please refer to <code>MedSignals.h</code> <code>type_enc::encode</code></p>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Signals%20file%20format.html#example","title":"Example","text":"<pre><code># Tab-delimited format\n\n# Legacy signals definition (Used only for ColonFlag AlgoMarker)\nSIGNAL  GENDER      100     0       Male=1,Female=2    1  \nSIGNAL  BP          920     8    systolic,diastoloc    00    mmHg|mmHg\nSIGNAL  Hemoglobin  1000    1     cbc   0   mg/dL\nSIGNAL  RC          2309    1       Med All ReadCodes ^[0-9A-HJ-NP-U] .no I,O,V,W,X,Y    1\nSIGNAL  Drug        2400    8       Drugs: date, drug code, duration in days, use proper    10    days\n\n# Newer signals definition:\nGENERIC_SIGNAL_TYPE   VInt    V(i)\nGENERIC_SIGNAL_TYPE   LabT1V1    T(i),V(f)\nGENERIC_SIGNAL_TYPE   CategoricalT1V1    T(i),V(i)\nGENERIC_SIGNAL_TYPE   LabT1V2    T(i),V(f,f)\nGENERIC_SIGNAL_TYPE   Date2Val_i_f    T(i),V(i,f)\n\nSIGNAL  GENDER      100     16:VInt       Male=1,Female=2    1  \nSIGNAL  BP          920     16:LabT1V2    systolic,diastoloc    00    mmHg|mmHg\nSIGNAL  Hemoglobin  1000    16:LabT1V1     cbc   0   mg/dL\nSIGNAL  RC          2309    16:CategoricalT1V1       Med All ReadCodes ^[0-9A-HJ-NP-U] .no I,O,V,W,X,Y    1\nSIGNAL  Drug        2400    16:Date2Val_i_f       Drugs: date, drug code, duration in days, use proper    10    days\n</code></pre> <p>A full list of deprecated signal types by numeric code</p>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html","title":"Repository Viewers","text":"<p>Repository viewers provide a simple graphical interface for viewing patient signals. The system uses a C++ backend server (built with Boost) and the plotly.js library for interactive charts, tables, heatmaps, and more.</p>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#how-it-works","title":"How It Works","text":"<ul> <li>The viewer sends a request to the server.</li> <li>The server responds with an HTML page via POST.</li> <li>The page is rendered with all graphics.</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#compiling","title":"Compiling","text":"<ul> <li>Part of MES Tools installation</li> <li>The main application is <code>SimpleHttpServer</code></li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#simplehttpserver-parameters","title":"SimpleHttpServer Parameters","text":"<ul> <li><code>rep</code>: Repository to use.</li> <li><code>plotly_config</code>: Configuration file (see below); usually, the default is sufficient.</li> <li><code>server_dir</code>: Directory for server files (default is usually fine).</li> <li><code>address</code>: Server IP (use <code>ip addr show</code> to find yours.</li> <li><code>port</code>: Server port (avoid 80, 8080, 7090, 8090, 7990; ensure your chosen port is free).</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#configuration-file","title":"Configuration File","text":"<ul> <li>Example and definitions: <code>MR_LIBS/Internal/MedPlotly/MedPlotly/BasicConfig.txt</code></li> <li>Contains default parameters and panel definitions.</li> <li>Panels become plots and can contain multiple signals.</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#key-parameters","title":"Key Parameters","text":"<ul> <li><code>JSDIR</code>: Directory for JavaScript files.</li> <li><code>JSFILES</code>: Main plotly.js file.</li> <li><code>NULL_ZEROS</code>: If 1, skips zero values in plots (useful for outliers).</li> <li><code>LOG_SCALE</code>: If 1, uses logarithmic axis scaling.</li> <li><code>WIDTH</code>/<code>HEIGHT</code>: Default panel size.</li> <li><code>BLOCK_MODE</code>: If 1 (recommended), arranges graphs in a line if space allows.</li> <li><code>SIG &lt;sig_name&gt; &lt;parameters&gt;</code>: Override defaults for specific signals.</li> <li><code>DRUG_GROUP</code>: Defines drug groups for the drugs heatmap (see config examples).</li> <li><code>PANEL</code>: Defines a panel with name, title, signals, and optional size/params.</li> <li><code>VIEW</code>: Lists default panels to show.</li> <li><code>REP_PROCESSORS</code>: JSON file for MedModel to configure repository processors.</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#example-configurations","title":"Example Configurations","text":"<ul> <li><code>BasicConfig</code>: For THIN, AppleTree repositories.</li> <li><code>MHSConfig</code>: For Maccabi, KP.</li> <li><code>RambamConfig</code>: For Rambam repository.</li> <li><code>MimicConfig</code>: For Mimic repository.</li> </ul> <p>You can create or modify configs as needed. The MedPlotly library parses these files and generates plotly inputs based on panel definitions and user requests.</p>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#running-the-viewers","title":"Running the Viewers","text":"<p>We wrapped this application to support multiple DataRepositories if needed. Please see those scripts:</p> <ul> <li>Script location: <code>MR_Scripts/Bash-Scripts/run_viewer.sh</code> (included in your PATH).</li> <li>To edit server/port list: <code>MR_Scripts/Python-scripts/viewers_config.py</code></li> </ul> <pre><code>viewers start\n# To stop all viewers:\nviewers stop\n</code></pre> <ul> <li>Viewers run detached from your SSH session (they continue if your session ends).</li> <li>No output is printed to your screen.</li> <li>You can control error logs location</li> </ul>"},{"location":"Infrastructure%20Library/DataRepository/Repository%20Viewers.html#viewer-features","title":"Viewer Features","text":"<ul> <li>Enter a PID number and press send.</li> <li>Mark a specific date (drawn as a vertical black line on graphs).</li> <li>Some viewers allow specifying a date range (useful for Rambam and Mimic3 viewers).</li> <li>Each graph is interactive (zoom, pan, hover, etc.).</li> <li>Select panels/signals from the list or add them in the signal charts box (space, semicolon, or newline separated).</li> </ul> <p>This flexibility lets you view signals not included in the default</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html","title":"MedProcessTools Library","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#data-handlers","title":"Data Handlers:","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#medregistry-handling-a-registry-individuals-dated-outcome-a-more-generic-format-of-medcohort","title":"MedRegistry:\u00a0Handling a registry - individuals + (dated) outcome (A more generic format of MedCohort)","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#wrapper-class-for-medregistry-and-additional-labeling-policy-arguments-like-time-window-to-define-labels-from-registry-for-each-sample-time","title":": Wrapper class for MedRegistry and additional labeling policy arguments (like time window) to define labels from registry for each sample time","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#handling-of-creating-medsamples-from-medregistry","title":": Handling of creating MedSamples from MedRegistry","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#medcohort-handling-a-cohort-individuals-dated-outcome-follow-periods","title":"MedCohort: Handling a cohort - individuals + (dated) outcome + follow-periods","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#medsamples-handling-samples-individual-dated-outcome-time-points","title":"MedSamples: Handling samples - individual + (dated) outcome + time-points","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#medfeatures-handling-a-features-matrix-samples-features-information","title":"MedFeatures: Handling a features matrix : samples + features information","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#serializableobject-base-class-for-handling-serializations","title":"SerializableObject:\u00a0 Base class for Handling Serializations","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#data-processors","title":"Data Processors:","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/index.html#analysis-processors","title":"Analysis Processors:","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedBootstrap.html","title":"MedBootstrap","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedBootstrap.html#example-usage","title":"Example Usage","text":"<p>Usage example of <code>MedBootstrap</code> within C++ code</p> <pre><code>#define TEMP_PATH \"/tmp/\"\nvoid run_bootstrap_on_samples(const string &amp;samples_path, map&lt;string, map&lt;string, float&gt;&gt; &amp;res, const string &amp;inc_file = \"\") {\n    MedSamples smp;\n    if (smp.read_from_file(samples_path) &lt; 0)\n        MTHROW_AND_ERR(\"Couldn't read file %s\", samples_path.c_str());\n    // Example: create configuration file for cohorts\n    ofstream fw(TEMP_PATH \"bootstrap_new.params\");\n    // Format: COHORT_NAME&lt;TAB&gt;COHORT_DEFINITION\n    // COHORT_DEFINITION is a list of parameters with min,max range, separated by ';'\n    fw &lt;&lt; \"Time_Window:0-365\" &lt;&lt; \"\\t\" &lt;&lt; \"Time-Window:0,365\" &lt;&lt; endl;\n    fw &lt;&lt; \"Time_Window:0-365,Age:40-80\" &lt;&lt; \"\\t\" &lt;&lt; \"Time-Window:0,365;Age:40,80\" &lt;&lt; endl;\n    fw.close();\n    map&lt;string, vector&lt;float&gt;&gt; additional_info;\n    MedBootstrap boot(\"sample_ratio=1.0;sample_per_pid=1;loopCnt=500;filter_cohort=\" + TEMP_PATH + \"bootstrap_new.params\");\n    string addition = \"\";\n    if (!inc_file.empty())\n        addition = \";inc_stats_text=\" + inc_file;\n    boot.roc_Params = ROC_Params(\"working_point_FPR=0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,10;\" +\n        \"working_point_SENS=5,10,20,30,40,50,60,70,80,90;score_resolution=0.0001;score_bins=0\" + addition);\n    if (addition.empty())\n        res = boot.booststrap(smp, additional_info);\n    else\n        res = boot.booststrap(smp, \"/home/Repositories/THIN/thin_mar2017/thin.repository\");\n}\n</code></pre> <p>An application using <code>MedBootstrap</code> is available here: bootstrap_app</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedBootstrap.html#running-bootstrap-with-a-custom-measurement-function","title":"Running Bootstrap with a Custom Measurement Function","text":"<pre><code>// bootstrap_params: initialization string for bootstrap parameters\n// cohort_file: file containing cohort definitions\n// final_feats: MedFeature matrix or MedSamples (if only filtering Age, Gender, TimeWindow)\n\n// The measurement function uses Lazy_Iterator to iterate over label, pred, weight in the bootstrap loop.\n// Additional arguments can be passed in \"params\" (e.g., ROC working points).\n// The function returns the measurements.\nmap&lt;string, float&gt; calc_acc(Lazy_Iterator *iterator, int thread_num, Measurement_Params *params) {\n    map&lt;string, float&gt; res;\n    float pred_val, label, weight;\n    double total_cnt = 0, sum_prd = 0;\n    while (iterator-&gt;fetch_next_external(thread_num, label, pred_val, weight)) {\n        total_cnt += weight != -1 ? weight : 1;\n        sum_prd += (pred_val == label) * (weight != -1 ? weight : 1);\n    }\n    total_cnt += weight != -1 ? weight : 1;\n    sum_prd += (pred_val == label) * (weight != -1 ? weight : 1);\n    sum_prd /= total_cnt;\n    res[\"ACCURACY\"] = sum_prd;\n    return res;\n}\n\nMedBootstrapResult b;\nb.bootstrap_params.init_from_string(bootstrap_params);\nb.bootstrap_params.parse_cohort_file(cohort_file);\nMeasurementFunctions boot_function = calc_acc;\nb.bootstrap_params.measurements_with_params = { pair&lt;MeasurementFunctions, Measurement_Params *&gt;(calc_acc, NULL) }; // Pass NULL if no extra parameters\nb.bootstrap(final_feats);\nb.write_results_to_text_file(\"/tmp/results.csv\");\n</code></pre>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedBootstrap.html#cohorts-file-format","title":"Cohorts File Format","text":"<p>The cohorts file can be defined in two ways:</p> <ol> <li> <p>Single Cohort per Line</p> <ul> <li>Format: <code>COHORT_NAME&lt;TAB&gt;PARAMETERS_DEF</code></li> <li><code>COHORT_NAME</code> is a string for the cohort name.</li> <li><code>PARAMETERS_DEF</code> is: <code>PARAMETER_NAME:MIN_RANGE,MAX_RANGE;...</code> (parameters separated by <code>;</code>).</li> <li>The cohort is the intersection (AND) of all parameter ranges. There is a single tab between the name and the definition.</li> <li>Example: <pre><code>1 year back &amp; age 40-80    Time-Window:0,365;Age:40,80\n</code></pre>     This creates a cohort called \"1 year back &amp; age 40-80\" and filters records with (Time-Window &gt;= 0 and &lt;= 365) and (Age &gt;= 40 and &lt;= 80).</li> </ul> </li> <li> <p>Multiple Cohorts (Cartesian Product)</p> <ul> <li>Format: <code>MULTI&lt;TAB&gt;PARAMETERS_DEF&lt;TAB&gt;...PARAMETERS_DEF&lt;TAB&gt;</code></li> <li>A line starting with <code>MULTI</code> creates all Cartesian combinations for each parameter definition (each in the next tab).</li> <li><code>PARAMETERS_DEF</code> is as above.</li> <li>Example: <pre><code>MULTI   Time-Window:0,30;Time-Window:30,180   Age:40,60;Age:60,80;Age:40,80   Gender:1,1;Gender:2,2\n</code></pre>     This creates 2\u00d73\u00d72=12 cohorts for each combination of Time-Window, Age, and Gender options. \u00a0</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedBootstrap.html#improvement-ideas","title":"Improvement Ideas","text":"<ul> <li>Add support for more complex conditions in MedBootstrap (e.g., AND/OR logic on parameter ranges).</li> <li>Enable bootstrap calculation on multiple predictions for the same samples. This would reduce runtime when comparing different models or scores, as currently each call randomizes the bootstrap cohort and samples.</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedCohort.html","title":"MedCohort","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedCohort.html#medcohort-is-a-data-structure-with-helpers-to-deal-with-a-cohort-a-list-of-individuals-with-dated-outcomes-and-followup-times","title":"MedCohort is a data structure with helpers to deal with a cohort, a list of individuals with (dated) outcomes and followup times.","text":"<p>MedCohort contatins a vector of basic records (CohortRec), each representing a single period for a specific id (with a corresponding outcome) information. A MedCohort can be sampled to generate MedSamples files according to SamplingParams using one of two fuctions:</p> <ul> <li>int create_sampling_file(SamplingParams &amp;s_params, string out_sample_file)\u00a0:\u00a0Generate samples within cohort times that fit SampleingParams criteria and windows.\u00a0Sample dates are selected randomly for each window of s_params.jump_days in the legal period. </li> <li>int create_sampling_file_sticked(SamplingParams &amp;s_params, string out_sample_file)\u00a0:\u00a0Generate samples within cohort times that fit SampleingParams criteria and windows.\u00a0Sample dates are those with the required signals for each window of s_params.jump_days in the legal period (if existing). A MedCohort can also be used to estimate the age and gender dependent incidence rate. Estimation is done using the following function which according to IncidenceParams:</li> <li>int create_incidence_file(IncidenceParams &amp;i_params, string out_file) :\u00a0Generate an incidence file from cohort + incidence-params.\u00a0Check all patient-years within cohort that fit IncidenceParams and count positive outcomes within the incidence_years_window. IncidenceParams initialization:</li> </ul> Parameter Name Description Default Value incidence_years_window how many years ahead do we consider an outcome? 1 rep Repository configration file None from_year first year to consider in calculating incidence 2007 to_year last year to consider in calculating incidence 2013 gender_mask mask for gender specification (rightmost bit on for male, second for female) 0x3 train_mask mask for TRAIN-value specification (three rightmost bits for TRAIN = 1,2,3) 0x7 from_age minimal age to consider 30 to_age maximal age to consider 90 age_bin binning of ages 5 min_samples_in_bin minimal required samples to estimate incidence per bin 20 <p>SamplingParams initialization:</p> Parameter Name Description Default Value is_continous continous mode of sampling vs. stick to signal (0 = stick) 1 stick_to, stick_to_sigs  comma separated list of signals required at sampling times None take_all in 'stick' mode - take all samples with requrired-signal within each sampling period is selected 0 take_closest <p>in 'stick' mode - take the sample with requrired-signals that is closest to each target sampling-date</p><p>if none of take_all and take_closest is given, a random sample with requrired-signal within each sampling period is selected</p> 0 rep Repository configration file None min_age minimum age for sampling 0 max_age maximum age for sampling 200 gender_mask mask for gender specification (rightmost bit on for male, second for female) 0x3 train_mask mask for TRAIN-value specification (three rightmost bits for TRAIN = 1,2,3) 0x7 min_year first year for sampling 1900 max_year last year for sampling 2100 jump_days days to jump between sampling periods 180 min_days, min_days_from_outcome minimal number of days before outcome for sampling 30 min_case, min_case_years minimal number of years before outcome for cases 0 max_case, max_case_years maximal number of years before outcome for cases 1 min_control, min_control_years minimal number of years before outcome for controls 0 max_control, max_control_years maximal number of years before outcome for controls 10"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedCohort.html#include-file-is-hmrlibsinternalmedutilsmedutilsmedcohorth","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedUtils/MedCohort.h","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedFeatures.html","title":"MedFeatures","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedFeatures.html#medfeatures-is-a-data-structure-with-helpers-for-holding-features-data-as-a-virtual-matrix","title":"MedFeatures is a data structure with helpers for holding features data as a virtual matrix.","text":"<p>MedFeatures is the data container used by MedModel to for holding the matrix used for learning/predicting. A MedFeatures object contains a vector of samples (id + date + outcome + ...), a\u00a0vector of weights (one per sample) and a vector of floats (one value per sample) for each features. Each feature is identified by it's name (a string) Additional metadata per feature includes a FeatureAttr entry as well as a set of tags (string), which is used by FeatureProcess objects ro decide whether to act on the feature.\u00a0</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedFeatures.html#include-file-is-","title":"**Include file is -\u00a0**","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedLabels.html","title":"MedLabels","text":"<p>A class the holds the information on how to label samples and outputs the outcome. It uses the MedRegistryand internal parameters to define the \"labeling\" - for example the relevant time window for the outcome. So you can use the same MedRegistry and just change for example the time window for the outcome. The parameters for the labeling are called LabelParams:</p> <ul> <li>time_from / time_to - the time window defintion for the outcome</li> <li>censor_time_from /\u00a0censor_time_to - time window for the censor registry</li> <li>conflict_method - how to cop with conflicts. If the rules has more the one option:</li> <li>drop - will drop sample. has conflict</li> <li>all - will create a sample for each outcome. In binary usecase, the same sample will appear twice, once as a case and once as control</li> <li>max - take maximal label. If both case/control rules are satisfied take max - which is case</li> <li>last - takes the last matched record</li> <li>label_interaction_mode - please refer to TimeWindowInteraction for more info. Defines the \"rules\" of matching between the patient registry records and the sample time window (defined by time_from, time_to on the MedSample time). If the rule is satsisfied, the registry outcome value is taken into account for labeling the sample. If there is only one matched record (of multiple but with the same vaalue), then this value is selected. Otherwise, uses \"conflict_method\" argument to resolve the conflict</li> <li>censor_interaction_mode - Same format as\u00a0label_interaction_mode but just for censor registry</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedPlot.html","title":"MedPlot","text":"<p>To create general graph please use createHtmlGraph function in MedPlot. to create ROC graphs use plotAUC function. \u00a0 plotAUC** Input:\u00a0**</p> <ul> <li>vector of all models predictions vector(each record is prediction vector result of specific model)</li> <li>vector of the labels</li> <li>vector of the models\\predictors names (same size as the first vector)</li> <li>output direcotry for writing graphs</li> <li> <p>optional - indexes is a mask for selecting rows and may be omitted to select all \u00a0 plotAUC\u00a0Output:** ** the function outputs the following graphs:</p> </li> <li> <p>ROC graph curve</p> </li> <li>PPV graph curve - for each false positive rate - it's PPV value for each model</li> <li>False Positive rate as function of model score - you may see each model scores VS it's false positive rate</li> <li>Label distribution of cases and controls\u00a0 For Each Graph you may search for specific working point in the search button - fill in X value (false positive rate) and it will find you the Y value (Sensitivity or PPV depends on the graph) \u00a0 Example Run: \u00a0 <pre><code>vector&lt;float&gt; preds;\u00a0//from MedModel scores\nvector&lt;float&gt; age_baseline;\u00a0//the age_baseline for each sample\nvector&lt;float&gt; labels;\nplotAUC({age_baseline, preds}, labels, {\"Age_Baseline\" ,\"MedModel\"}, \"~/Auc_Example_Folder\")\n</code></pre> </li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedSamples.html","title":"MedSamples","text":"<p>he MedSamples object is designed to store key fields such as \"label\", \"patient id\", and \"requested prediction time\", along with additional information. Data is stored in a tab-separated file format. The \"pred_0\" field, representing a prediction result for performance analysis, is optional.</p> <ul> <li><code>EVENT_FIELDS</code>: Static field; always set to \"SAMPLE\"</li> <li><code>id</code>: Numeric patient identifier</li> <li><code>time</code> : Requested prediction time; only data prior to this point is used for prediction</li> <li><code>outcome</code>: The label or outcome; can be binary (0/1) or numeric (for regression)</li> <li><code>outcomeTime</code>: Event time if the patient is labeled \"1\", or \"end of followup\" for \"0\". This field is optional; if unused, you can specify a placeholder date like \"19000101\". Useful for filtering by time windows.</li> <li><code>split</code>: Optional field for specifying patient split. Typically, splits are assigned based on patient id in a separate file, so please specify \"-1\" </li> <li><code>pred_0</code> - optional prediction result</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedSamples.html#example-file","title":"Example file","text":"<pre><code>EVENT_FIELDS    id  time    outcome outcomeTime split\nSAMPLE  1   20250101    0   20250820    -1\nSAMPLE  1   20250201    0   20250820    -1\nSAMPLE  2   20250101    1   20250820    -1\n</code></pre> <p>Explain:</p> <ul> <li>Patient 1 has two prediction points (20250101 and 20250201), both labeled \"0\". Follow-up is documented until 20250820.</li> <li>Patient 2 has one prediction point (20250101) with label \"1\". The event date is 20250820.</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedSamples.html#overview","title":"Overview","text":"<p>MedSamples is a data structure and set of helper functions for managing samples-combinations of individuals, time-points (dates), and associated information such as outcomes, outcome times, splits, and predictions.</p> <p>The data is organized in a three-tier hierarchy:</p> <ol> <li>MedSamples: Represents a collection of MedIdSamples for multiple patients</li> <li>MedIdSamples: Represents a set of samples for a specific patient (id).</li> <li>MedSample:  Represents a single sample.</li> </ol>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedSamples.html#notes","title":"Notes:","text":"<ul> <li>It is not strictly enforced that all samples within a <code>MedIdSample</code> share the same id, but many functions may not work correctly otherwise.</li> <li>Different samples for the same id may have different splits, and these may not match the split parameter in <code>MedIdSample</code>. However, it is strongly recommended to maintain a single split per id.</li> </ul> <p>Samples can be read from and written to CSV or binary files. \u00a0</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html","title":"SerializableObject","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#general","title":"General","text":"<p>SerializableObject is a class many classes inherit from. It contains tools for the following:</p> <ul> <li>Serialization via the:<ul> <li>get_size , serialize , deserialize methods.</li> <li>the MedSerialize:: namespace mechanisms including the very useful ADD_SERIALIZATION_FUNCS mechanism</li> </ul> </li> <li>Read/Write objects from/to files: this is a general wrapper on top of the serialization.</li> <li>init_from_string mechanism :<ul> <li>parsing of the init string</li> <li>support the brackets\u00a0{} mechanism for parameters</li> <li>support reading parameters from a file (the pFile option) Together this class provides a powerful and easy way to handle those very needed functionalities. To start using the SerializableObject class, simply inherit from it in the definition.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#serialization","title":"Serialization","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#the-virtual-serialization-methods","title":"The virtual serialization methods","text":"<p>SerializableObject contains the declaration of the three major serialization methods: <pre><code>    // Virtual serialization\n    virtual size_t get_size() { return 0; } ///&lt;Gets bytes sizes for serializations\n    virtual size_t serialize(unsigned char *blob) { return 0; } ///&lt;Serialiazing object to blob memory. return number ob bytes wrote to memory\n    virtual size_t deserialize(unsigned char *blob) { return 0; } ///&lt;Deserialiazing blob to object. returns number of bytes read\n</code></pre> Each inheriting class should implement these methods in order to allow for its serialization. There are 2 main methods to do it:</p> <ul> <li>Directly implementing the methods - recommended only in very complex situations (mainly when using other packages not using this terminology with a need to wrap their internal way to serialize).</li> <li> <p>Using the ADD_SERIALIZATION_FUNCS() macro : this should be the way to go and should always be preffered over any other way. \u00a0 When using the serialzation mechanism of SerializableObject to its full power (using the ADD_SERIALIZATION_FUNCS macro) one automatically gets the following serializations:</p> </li> <li> <p>All needed basic types</p> </li> <li>All supported stl containers (add yours if it is not supported : see below)</li> <li>Recusive on all used classes in variables</li> <li>A pointer to a supported class (but only to single ones, not to arrays allocated like that).</li> <li>Immunity to changes of adding more variables to the serialization - in the sense that you will still be able to read objects serialized before that change.</li> <li>Immuinity to changes of deleting variables from a serialization</li> <li>Immunity to changes of changing the order of variables in the serialization.</li> <li>Correct allocation of derived classes when deserializing a pointer to the base class.</li> <li>Note: not immune to changes in the names of variables. So changing a variable name will break serializations - try to avoid this if possible.</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#add_serialization_funcs","title":"ADD_SERIALIZATION_FUNCS","text":"<p>The ADD_SERIALIZATION_FUNCS() is a macro that adds the needed get_size, serialize and deserialize methods to the class, thus saving us the tedious work of writing it. To use it , simply add ADD_SERIALIZATION_FUNCS to the public side of your class (that inherits from SerializableObject) with the list of the variables you need to serialize. That's it. As simple as that. Any order you like, it is not important. The supported variables are:</p> <ul> <li>Basic types (int, float, double, string, etc...)</li> <li>Stl containers (vector , map , pair, etc..) <li>Other classes that are under SerializableObject and had implemented the serialization methods.</li> <li>T * : pointer to a single allocated (via new) SerializeObject supported class. Note : a single element and not an array (there's no way in c++ to know that size just by seeing the pointer... hence it is recommended to use vector&lt;&gt; when in need of something like that)</li> <li>A recursive combination of the above, for example : vector , map&gt;&gt; , etc... Example : this is all that is needed to be done to serialize some classes : <pre><code>// MedModel serialization line\nADD_SERIALIZATION_FUNCS(rep_processors, generators, feature_processors, predictor, serialize_learning_set, LearningSet)\n\u00a0\n// BasicFeatGenerator serialization line\nADD_SERIALIZATION_FUNCS(generator_type, type, tags, serial_id, win_from, win_to, d_win_from, d_win_to,\n        time_unit_win, time_channel, val_channel, sum_channel, signalName, sets,\n        names, req_signals, in_set_name ,bound_outcomeTime, timeRangeSignalName, timeRangeType)\n</code></pre>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#add_class_name-medserialize_support","title":"ADD_CLASS_NAME , MEDSERIALIZE_SUPPORT","text":"<p>In order to get the full functionality of the serialization process, it is needed to add the following macros for each inheriting class:</p> <ul> <li>ADD_CLASS_NAME(class name) : in the public area of the class : this creates a functions that returns the class name, and is very useful when serializing T * cases, and polymorphic classes.</li> <li>MEDSERIALIZE_SUPPORT(class name) : this should be added in the same h file but outside the class (typically we add it at the bottom of the h file). It is needed in order to connect the class serialization functions to the general recursive serialization methods. Simply add those two simple lines for each new SerializableObject inheritting class you write. Example: <pre><code>class Example : public SerializableObject {\npublic:\n    string name = \"\";\n    vector&lt;int&gt; vec_of_ints = {0,1,2};\n    vector&lt;MedModel *&gt; models;\n\u00a0\n    // ...\n\u00a0\n    // serialization\n    ADD_CLASS_NAME(Example)\n    ADD_SERIALIZATION_FUNCS(name, vec_of_ints, models)\n\u00a0\n}\n\u00a0\n//... later in the same h file\n\u00a0\nMEDSERIALIZE_SUPPORT(Example)\n</code></pre> </li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#polymorphic-classes-support","title":"Polymorphic classes support","text":"<p>If you have a Base class with inheriting classes , all inheriting from SerializableObject, you may have the following issue: Some other class contains an element of : Base * var , but var is allocated dynamically to be one of the derived classes of Base. When serializing var we will use the serialization functions of the derived class, BUT when deserializing if we do nothing var will be newed into a Base element, and hence be Base and not the correct derived class, and we will be using its deserialization function that doesn't match the serialization function we used. To solve that the serilizer of SerializableObject saves the type name before each T * serialization, the name is taken from the derived class, so the derived class name will be used. When deserializing the Base class needs to provide a new_polymorphic function that returns the new to its derived class given its name. So , to summarize:</p> <ul> <li>SerializableObject has a virtual function : void * new_polymorphic(string derived_class_name); by default it returns NULL.</li> <li>Base classes should implement it : it is easy , as it mainly contains lines of the type : if (derived_class_name == string_name_of_derived1) return new derived1; etc...</li> <li>To make implementation of the new_polymorphic function even easier, one can use the\u00a0CONDITIONAL_NEW_CLASS() macro Example : this is the new_polymorphic function of FeatureGenerator <pre><code>//.......................................................................................\nvoid *FeatureGenerator::new_polymorphic(string dname) {\n    CONDITIONAL_NEW_CLASS(dname, BasicFeatGenerator);\n    CONDITIONAL_NEW_CLASS(dname, AgeGenerator);\n    CONDITIONAL_NEW_CLASS(dname, GenderGenerator);\n    CONDITIONAL_NEW_CLASS(dname, SingletonGenerator);\n    CONDITIONAL_NEW_CLASS(dname, BinnedLmEstimates);\n    CONDITIONAL_NEW_CLASS(dname, SmokingGenerator);\n    CONDITIONAL_NEW_CLASS(dname, KpSmokingGenerator);\n    CONDITIONAL_NEW_CLASS(dname, AlcoholGenerator);\n    CONDITIONAL_NEW_CLASS(dname, RangeFeatGenerator);\n    CONDITIONAL_NEW_CLASS(dname, DrugIntakeGenerator);\n    CONDITIONAL_NEW_CLASS(dname, ModelFeatGenerator);\n    return NULL;\n}\n\u00a0\n</code></pre> PreSerialization / PostDeSerialization If you need to run a few commands before the serialization starts you can put them in the pre_serialization method inside your class. This is needed for example when you need to clean some of the serialized variables based on some condition before it starts. In the same manner you can implement in the function post_deserialization operations that are needed to be done after the deserialization. This can be handy and helped convert some models (such as xgb) to use this serialization methods. Example: this is the pre_serialization function in MedModel: <pre><code>// allows for conditional serialization of LearningSet while allowing the use of the ADD_SERIALIZATION_FUNCS macro\nvirtual void pre_serialization() \n{ \n    if (!serialize_learning_set) \n        LearningSet = NULL; /*no need to clear(), as this was given by the user*/ \n}\n</code></pre> </li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#serialization-check-list","title":"Serialization check list","text":"<ol> <li>Inherit from SerializableObject or from a class inheritting from it.</li> <li>add the ADD_CLASS_NAME(class name) macro to your class.</li> <li>add the MEDSERIALIZE_SUPPORT(class name) after your class definition in the same h file.</li> <li>Use the ADD_SERIALIZATION_FUNCS(...) macro to list the variables you need to serialize.<ol> <li>if you can't: maybe the pre_serialization() trick can solve your problem? if so - great, implement it, and use the macro.</li> <li>If you still can't due to a complex case: implement the get_size, serialize, and deserialize methods directly.</li> </ol> </li> <li>If your class is a Base class , make sure to implement the new_polymorphic method for it.</li> <li>If your class is a derived class, make sure the new_polymorphic method of its base class supports your derived class.</li> <li>Avoid changing variable names , as it will break the serialization backward support.</li> <li>Avoid using a variable names size . This is an unclear bug (may be a compiler bug) , but using it makes the compiler think its type is different. Simply don't use it as a serialized variable.</li> </ol>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#tips-for-writing-an-easily-serialized-class-and-a-correct-one","title":"Tips for writing an easily serialized class and a correct one","text":"<ol> <li>Do not use c style arrays such as int * to different sizes using new or malloc. Use vector&lt;&gt; instead.</li> <li>Give good names to variables (so that later they won't be changed) (don't use size as a varialble -&gt; there's a bug when using it)</li> <li>Use basic types, stl, other serialized classes, recursively if you need.</li> <li>If needed you can use T * for a single element (it's ok to have a vector or map of those of course) , as long as T is a class supported by MedSerialize.</li> <li>If needed use pre_serialization and/or post_deserialization options. Very handy when there's an inner 3rd party class with its own serialization function.</li> <li>Make all the efforts to be able to use the ADD_SERIALIZATION_FUNCS as your serialization implementation. It is the most powerful method.</li> </ol>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#how-to-use-add_serialization_funcs-if-my-class-uses-a-forward-declaration","title":"How to use ADD_SERIALIZATION_FUNCS if my class uses a forward declaration?","text":"<p>Sometimes it happens that we can't use ADD_SERIALIZATION_FUNCS in the h file due to for example forward declarations of classes used, and hence this can only be compiled in the c file. If you need that, you have the ADD_SERIALIZATION_HEADERS() in your declaration (this is to allow virtuality in base classes, otherwise not a must), and you can use the\u00a0ADD_SERIALIZATION_FUNCS_CPP(classname, ...) inside your cpp file. It is the same as the ADD_SERIALIZATION_FUNCS macro but you need to add your class name at the start.</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#my-stl-container-is-not-supported-how-can-i-add-it","title":"My stl container is not supported , how can I add it?","text":"<p>Help others and implement it in SerializableObject_imp.h , see there many examples for stl containers support. \u00a0</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#known-bugs","title":"Known BUGS","text":"<ul> <li>DO NOT USE 'size' as a variable ending up in the ADD_SERIALIZATION_FUNCS() list : possibly a compiler bug makes the serializer get the wrong type for it.</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SerializableObject.html#init-from-string","title":"Init from string","text":"<p>The other usage of SerializableObject is the init_from_string parsing you get for free. When you call init_from_string(string init_s) for your SerializableObject class, it will parse init_s to pairs of string &lt;-&gt; string loaded into a map and call your init(map&amp; map) method according to the following rules: <ul> <li>init_s is separated by ;</li> <li>After sepration each part is separated by = : the first element is the variable name, the second is its value (as a string)</li> <li>Should clear whitespaces before and after the variables names and values.</li> <li>If you use var={value} then var will be the variable name and value will be its value BUT you can use ; and = inside value (!!) . This helps when passing parameters to another class via a variable. Value can contain\u00a0{} variables on its own, which is also very useful at times.</li> <li>The pFile=fname is a reserved command: when given , init_from_string will open fname, concatenate its lines to one long string, and feed it to the usual init_from_string() method. This allows to keep parameters in a file.</li> <li>if a param is : var=FILE:fname : the string for the variable will be replaced by a string created by reading the file fname, getting rid of all comment and empty lines, getting rid of each line start/end spaces and eols.</li> <li>if a param is : var=\"LIST:fname\" or var=\"list:fname\" : the variable list will be replaced like when using the \"FILE:\" option, but commas (,) will be added between the string objects in the file (separated by spaces or end-of-lines, or even commas, in the file) \u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedRegistry/index.html","title":"MedRegistry","text":"<p>Code documentation Methods in MedRegistry:</p> <ul> <li>reading the object:\u00a0read_text_file or\u00a0read_from_file to read in text format or in binary format.</li> <li>writing the object: write_text_file\u00a0or\u00a0write_from_file\u00a0to writein text format or in binary format.</li> <li>create_registry\u00a0- an option to create registry records by implementing private function\u00a0**\u00a0to fetch each patient registry records</li> <li>calc_signal_stats\u00a0- a method to create\u00a0contingency table with other signal splited by gender and age groups.</li> <li>create_incidence_file - a method to calc the incidence (also with kaplan meier)\u00a0 \u00a0 A class that holds all registry records on all patients using MedRegistryRecord. very similar to MedCohort, but a more generic class to hold multiple periods for outcome on same patients. for example pregnancy, influenza, kidney stones, sofa scores...\u00a0 Each record consist of those fields in MedRegistryRecord:</li> </ul> Parameter name description pid patient id start_date the start date of the outcome\u00a0 end_date the end date of the outcome \u00a0registry_value <p>\u00a0the registry value. 0 for controls, 1 for cases or other value in more complex cases.</p><p>For example in diabetes it may mark the states from 0-2. 0 - no diabetes, 1- pre, 2- diabetes.</p><p>for each period we create record, or value for SOFA Score</p> <p>Example records for cancer from MedCohort: A patient who is in the cohort from 01.01.2000 till 01.01.2016 and got cancer in 01.01.2012 will be presented by 2 MedRegistryRecords. one period for control outcome period and one period for the case period:</p> <ol> <li>control period:\u00a0start_date=01.01.2000,\u00a0end_date=01.01.2012,\u00a0registry_value=0</li> <li> <p>case period:\u00a0start_date=01.01.2012,\u00a0end_date=01.01.2016,\u00a0registry_value=1 a patient who is always control will create 1 record with the start,end dates of the control period \u00a0 It has several ways to be initialized:</p> </li> <li> <p>by reading from disk - binary format or text format</p> </li> <li>by creating registry using create_registry method. need to implement get_registry_records to handle single patient records.the class have also the ability to create contingency table with other signal:for each Gender,Age_bin - the 4 stats number of the registry with the appearances or not appearances of the signal value</li> </ol>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedRegistry/MedSamplingStrategy.html","title":"MedSamplingStrategy","text":"<p>A Class that controls how to create MedSamplesfrom MedRegistryby sampling methods. it has several subclasses, each has it's own parameters and logic to sample the samples form registry:  Code is documented here. do_sample aruments in MedSamplingRegistry:</p> <ul> <li>const vector&lt;MedRegistryRecord&gt; &amp;registry - the registry for the labeling the samples by the sampler (already initialized with sampling params)</li> <li>MedSamples\u00a0&amp;samples - the samples\u00a0result</li> <li>const vector&lt;\u00a0MedRegistryRecord\u00a0&gt; *censor_registry - optional arg for specifying censoring times for the sampling</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedRegistry/MedSamplingStrategy.html#important-samplers","title":"Important Samplers:","text":"<p>**** -\u00a0can be used by specifying\u00a0MedSamplingRegistry::make_sampler(\"time_window\"). from the updated documentition reffer to doxygen Used to sample for each registry record randomly withing a specific time window. It can define diffrent time windows for cases/controls. Parameters:</p> Parmeter Name Type Description Default Value sample_count int how many samples to sample from each registry record. 1 minimal_time_case int the minimal time to give prediciton before the case outcomeTime 0 maximal_time_case int the maximal time to give predicition before the case outcomeTime 0 minimal_time_control int the minimal time to give prediciton before the control outcomeTime (which marks the last time we know the patient is control) 0 maximal_time_control int the maximal time to give prediciton before the control outcomeTime (which marks the last time we know the patient is control) 0 take_max bool If True will take maximal time window for case/control 0 <p>MedSamplingYearly - can be used by specifying\u00a0MedSamplingRegistry::make_sampler(\"yearly\").\u00a0from the updated documentition reffer to\u00a0doxygen\u00a0 Used to sample from year to year by jumping periodically between sample times for each patient. for sampling in ICU (more generic sampler for sampling in Fixed time, please reffer to MedSamplingFixedTime) The arguments:\u00a0time_from, time_to,\u00a0conflict_method,\u00a0outcome_interaction_mode,\u00a0censor_interaction_mode - are common in almost all samplers.</p> Parmeter Name Type Description Default Value start_year int the start year to sample from 0 - Must be provided end_year int The end year to sample from 0 - Must be provided prediction_month_day int the prediction date for the first year to start sampling 101 - mean 01/01 day_jump int the period of days to jump between each sampling date 0 - Must be provided, the common value should be 365 to jump yearly between sample times back_random_duration int random time to sample backward from the prediction date - adds ability to sample in random times in the year 0 <p>MedSamplingDates - Provides way to list all sampling options for each patient (or general options list) in a text file and sample randomly from those options MedSamplingStick - can sample on sticked signal times (fetches the signal times and uses MedSamplingDates to do the sampling)</p> Parmeter Name Type Description Default Value signal_list string a comma \",\" delimeted list with signals to list all possible sampling times for patient to stick to \"\" Must be provided take_count int how many samples to sample for each patient (inherited from MedSamplingDates). 0 - means take all samples 1 sample_with_filters bool Whether to use the filters as constraints and sample only when valid (may cause bais) or sample totally random and filter later True"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedRegistry/TimeWindowInteraction.html","title":"TimeWindowInteraction","text":"<p>This class defines how two time windows interact (boolean result - Yes or No). Motivation - it is being used to\u00a0decide:</p> <ul> <li>How to label samples based on registry - given outcome registry time range and prediction time range. return True/False if we should label the sample based on the registry record outcome value.</li> <li>Whether or not to censor the sample\u00a0- given outcome registry time range and prediction time range. return True/False if we should censor the sample the object is being initialized by medial::sampling::init_time_window_mode\u00a0function.</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedRegistry/TimeWindowInteraction.html#the-initialization-string-format","title":"the initialization string format","text":"<p>the init strings define the Rules for interaction between sample time window and the registry time window. this is abstraction of the time windows definitions:  \u00a0 The init string has format of \u201clabel_value:Interaction_string|label_value:Interaction_string;\u2026\u201d Can also use\u00a0label_value\u00a0for all labels by specifying \u201call\u201d or just the numeric value: \"0\" for controls and \"1\" for cases. We can specify diffrent rules for cases/controls \u00a0 \u00a0Interaction_string\u00a0has format of \u201ccondition,condition\u201d. The first\u00a0condition\u00a0is for sample from time window interaction with [registry start, registry end] The first\u00a0condition\u00a0is for Sample to time window\u00a0interaction\u00a0with [registry start, registry end] \u00a0 Condition\u00a0is enum with those options:</p> <ul> <li>\u201cbefore_start\u201d \u2013 condition for time to be before registry start\u2022\u201cafter_start\u201d \u2013 condition for time to be after registry start\u2022\u201cwithin\u201d \u2013 condition for time to be after registry start and before registry end</li> <li>\u201cbefore_end\u201d \u2013 condition for time to be before registry end\u2022\u201call\u201d \u2013 no condition, always true</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/MedRegistry/TimeWindowInteraction.html#examples","title":"Examples:","text":"<p>First example -\u00a0Diseases that occurs once and forever like cancer full init string: \u201c0:within,within|1:before_start,after_start\u201d Explain controls rule \"within,within\": samples should by within registry start to end time (which registry defines time range we mark the patient as sure control). also the from time window\u00a0of sample/prediction and the end time window of sample/prediction Explain cases rule \"before_start,after_start\": sample should start before start_time of registry and finish after start_time of registry. in the registry there meaning for end time only start time = outcome time. there is no end_time for cancer and it's not being used Second example for vaccination registry (each vaccination holds for X time) full init string: \u201c0:within,within|1:all,within\u201d Explain controls rule \"within,within\": sample time window should be within all time range of unvaccinated period to be counted. can also specify less\u00a0strict rule by \"before_end,after_start\" and conlifct_method=\"max\"\u00a0 to include controls as patients with some interseciton with sure unvaccinated period and no intersection with vaccination period. Explain\u00a0cases rule \"all,within\": sample time window should finish within registry vaccination period (never mind if started already vaccinated or not vaccinated). can also provide more\u00a0strict rule by providing 3rd argument for intersection rate like \"all,within,0.5-1.0\" to count only samples with at least 50%-100% intersection. \u00a0 \u00a0 \u00a0 \u00a0</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/index.html","title":"RepProcessor","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/index.html#repprocessor-applies-in-place-processing-to-the-information-held-for-a-single-patient-id-a-dynamicpidrec","title":"RepProcessor applies in-place processing to the information held for a single patient-id (a DynamicPidRec)","text":"<p>The main functionalities of a RepProcessor include learning\u00a0the processing parameters from (a subset of) a repository, and applying to a single patient Id (DynamicPidRec) at selected time-points. A processor may act differently at different time points (e.g. a predictor that looks at a range of times, must not consider points after the sample-time) - this is achieved by generating different versions of the affected signals in the DynamicPidRec (a version per time point). If the\u00a0DynamicPidRec already has different versions in the required signals (i.e. it has been processed by other rep_processors), the correct version should be used when learning and applying the processor. RepProcessor is a virtual class. See below for the list of implemeted children classes</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/index.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolsrepprocessh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/RepProcess.h","text":"<p>*RepProcessorTypes* RepBasicRangeCleaner</p> Name Value Class Note multi, multi_processor REP_PROCESS_MULTI RepMultiProcessor A container for a set of processors that can be learned simultanously (e.g. cleaneds of different signals) basic_cln, basic_outlier_cleaner REP_PROCESS_BASIC_OUTLIER_CLEANER RepBasicOutlierCleaner Outliers cleaning (removing and trimming) working on single-values nbrs_cln, nbrs_outlier_cleaner REP_PROCESS_NBRS_OUTLIER_CLEANER RepNbrsOutlierCleaner Outliers cleaning (removing and trimming) working on values and their neighborhoods configured_outlier_cleaner, conf_cln REP_PROCESS_CONFIGURED_OUTLIER_CLEANER RepConfiguredOutlierCleaner Uses configuration file for learning borders from statistics , or just set border according to hard coded values rulebased_outlier_cleaner, rule_cln REP_PROCESS_RULEBASED_OUTLIER_CLEANER RepRuleBasedOutlierCleaner Uses set of coded rules about relation among signals taken simulatneously, and if rule is not met all measurements are removed. aggregation_period REP_PROCESS_AGGREGATION_PERIOD RepAggregationPeriod Creates a virtual signal containing the 'treatment period' for the input signal basic_range_cleaner, range_cln REP_PROCESS_BASIC_RANGE_CLEANER RepBasicRangeCleaner Creates a virtual signal containing only instances of the signal that fall within some instance of the range signal."},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepAggregationPeriod.html","title":"RepAggregationPeriod","text":"<p>The rep processor will only work on\u00a0categorical signals. The rep processor assumes that the input signal has no time range - it only considers the first time channel. It is a logical rep\u00a0processor to run before\u00a0RepBasicRangeCleaner. rp_type - \"aggregation_period\". (required) input_name - the name of the (categorical) signal to process. (required) The\u00a0obvious choice for this is one of the drug signals, ie \"DRUG_PRESCRIBED\".\u00a0 output_name - the name of the resulting virtual signal.(required) sets - the set of values that will be considered as a signal. (required) ie\u00a0\"ATC_C03C_\" or\u00a0\"ATCC03C,ATC_N02A_\" note: [\u00a0\"ATCC03C\",\"ATC_N02A____\"]\u00a0(cross product) is not supported. period - the length of the window to be considered a treatment period - defaults to 0. time_unit_sig - the signal time unit - defaults to the\u00a0global_default_windows_time_unit. time_unit_win\u00a0- the period time unit - defaults to the\u00a0global_default_windows_time_unit. <pre><code>    {\n      \"action_type\": \"rep_processor\",\n      \"rp_type\":\"aggregation_period\",\n      \"input_name\":\"DRUG_PRESCRIBED\",\n      \"output_name\":\"drugs_sets_period\",\n      \"sets\": [\"ATC_C03C____\"],\n      \"period\":\"43200\"\n      },\n</code></pre>  \u00a0 (Code that tests this rep processor:\u00a0U:\\ReutF\\MR\\Projects\\Shared\\check_medication_period_rep_processor)</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepBasicOutlierCleaner.html","title":"RepBasicOutlierCleaner","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepBasicOutlierCleaner.html#repbasicoutliercleaner-is-a-point-wise-cleaner-of-outliers","title":"RepBasicOutlierCleaner\u00a0is a point-wise cleaner of outliers","text":"<p>Each value of a given signal is considered with respect to two ranges. First, the values is compared to the 'Remove' range, and if outside, it is deleted. Otherwise, it is compare to the 'Trim' range and clipped if outside (i.e. - assigned the upper limit if larger, and the lower limit if smaller). The 'Remove' and 'Trim' ranges are learnt from data, using the methods of MedValueCleaner\u00a0</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepBasicRangeCleaner.html","title":"RepBasicRangeCleaner","text":"<p>Does not support range input signals, if a signal has more than one time channel, choose the relevent one. Should be run after\u00a0RepAggregationPeriod \u00a0 rp_type - \"range_cln\". (required) signal_name\u00a0- the name of the signal to process. (required) ie \"Albumin\".\u00a0 ranges_sig_name - the name of the ranges signal to use. (can be the output signal of the\u00a0RepAggregationPeriod processor) output_name - the name of the resulting virtual signal. defaults to a combination of the input and ranges signal. time_channel - the time channel of the signal that will be used to\u00a0decide\u00a0if to keep or discard the signal instance. defaults to 0. output_type - the signal type of the output signal, should be the same as the input signal, defaults to 3 (2 time + value channels). \u00a0 <pre><code>{\n      \"action_type\": \"rep_processor\",\n      \"rp_type\":\"range_cln\",\n      \"signal_name\":\"Albumin\",\n      \"ranges_sig_name\":\"drugs_sets_period\",\n      \"time_channel\":\"0\"\n},\n</code></pre>  (Code that tests this rep processor:\u00a0U:\\ReutF\\MR\\Projects\\Shared\\check_medication_period_rep_processor)</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepConfiguredOutlierCleaner.html","title":"RepConfiguredOutlierCleaner","text":"<p>RepConfiguredOutlierCleaner is a subClass of basicOutlierCleaner. It implements the learn method to define the borders on a signal values, and inherit apply from its parent so that \u00a0values that lie outside those values are removed. As the basic cleaner also has a trimming threshold these thresoholds are set to +-1e98, so no values are trimmed. This cleaner gets in its parameters a name of a csv file that details the way each signal is treated. Another parameter is clean_method which is either \"learned\", \"confirmed\" or \"logical\".\u00a0** \"learned\" means that the borders are determined by calculation of distribution as described in the configuration file for this signal. \"confirmed\" means that learning was done already on part of THIN and bounds \u00a0were set and confirmed by the author (Coby). \"logical\" means that the thresholds are predetermined to the thresholds given in the configuration file. Those thresholds were set by understanding the nature of the signal (for example signal must be positive or percentage must not exceed 100). note: even when confirmed or learned are chosen, values that are outside the logical bounds are removed first. Format of the configuration file: name,logicalL,logicalH,low bound,low dist,high bound,high dist name:name of signal. logicalL: Lower bound for the logical option logicalH: Higher bound for the logical option low bound: The lower bound that was calculated for the confirmed option ( may be set to none- meaning stay with the logical bound). low dist: The probability distribution used in confirmed or should be used in learned for calculation of lower bound. May be norm, lognorm or manual ( that means do not use learned. Value was chosen manually for confirmed mode. high bound: see low bound above. high dist : see low dist above.**</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepConfiguredOutlierCleaner.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolsrepprocessh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/RepProcess.h","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepConfiguredOutlierCleaner.html#repbasicoutliercleaner","title":"RepBasicOutlierCleaner","text":"<ul> <li>**Description:\u00a0**\u00a0A child class of RepBasicOutlierCleaner</li> <li>Inherits from: RepBasicOutlierCleaner</li> <li>Generate new dynamic-version:\u00a0No</li> <li>Members:<ul> <li>string confFileName; \u00a0 The file that holds the cleaning parameters for each signal.</li> <li>string cleanMethod; \u00a0 \u00a0// \"logical\" \"confirmed\" or \"learned\" as explained above.</li> <li>map outlierParams; \u00a0 \u00a0 \u00a0 \u00a0 holds the parameter that were read from confFile. <li>**Implemented methods: \u00a0\u00a0**<ul> <li>Constructors :<ul> <li>inheritted</li> </ul> </li> <li>void init_defaults()\u00a0: init cleaning parameters to default values</li> <li>virtual int init(map&amp; mapper)\u00a0: init cleaning parameters according to map <li>int Learn(MedPidRepository&amp; rep, vector&amp; ids, vector&amp; prev_processor)\u00a0:\u00a0Learn the thresholds for removal of values according to the description above. <li>apply \u00a0is inheritted from basic cleaner.</li>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/RepProcessor/RepRulebasedOutlierCleaner.html","title":"RepRulebasedOutlierCleaner","text":"<p>RepRulebasedOutlier cleaner is a subclass of RepProcessor and MedValueCleaner. This cleaner applies the following rules drawn from Coby's imagination: 1. BMI=Weight/Height^2*1e4 2. MCH=Hemoglobin/RBC*10 3. MCV=Hematocrit/RBC*10 4. MCHC-M=MCH/MCV*100 5. Eosinophils#+Monocytes#+Basophils#+Lymphocytes#+Neutrophils#&lt;=WBC 6. MPV=Platelets_Hematocrit/Platelets 7. UrineAlbumin&lt;=UrineTotalProtein 8. UrineAlbumin_over_Creatinine=UrineAlbumin/UrineCreatinine 9. LDL+HDL&lt;=Cholesterol 10. NonHDLCholesterol+HDL=Cholesterol 11. HDL_over_nonHDL=HDL/NonHDLCholesterol 12. HDL_over_Cholesterol=HDL/Cholesterol 13. HDL_over_LDL=HDL/LDL 14. HDL_over_LDL=1/LDL_over_HDL 15. Cholesterol_over_HDL=Cholesterol/HDL 16. -----canceled rule 16 17. Cholesterol_over_HDL=1/HDL_over_Cholestrol 18. LDL_over_HDL=LDL/HDL 19. Albumin&lt;=Protein_Total 20. FreeT4&lt;=T4*1000 21. NRBC&lt;=RBC 22. CHADS2&lt;=CHADS2_VASC All rules are checked to within 10% tolerance (#define TOLERANCE (0.1)) Rules are checked only if all signals needed for rule implementation exist for the same date. If a rule is checked for a certain date and found to be false (outside the tolerance), all values \u00a0of the signals that are included in the rule will be removed for that date. The cleaner has 2 additional parameters: consideredRules: a string of comma separated integers stating the rules you want to apply to the data. If 0 is included in the list, all rules will be applied. If the string is empty (default), no rule will be applied so the cleaner will actualy do nothing. addRequiredSignals: When set to \"0\" only rules that all the participating signals are in the list of this cleaner will be applied. **\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 When set to \"1\" any rule that includes even one of the named signals will be applied. Signals that are in the rule and are not in the list will be loaded by the cleaner. **\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 : the additional signals that are loaded by this cleaner because \u00a0addRequiredSignals was set to \"1\", may not go through the preprocessing by cleaners that preceed this cleaner, if they are not in the signals list for that cleaner. , - Description:\u00a0**\u00a0A child class of RepProcessor used for point-wise cleaning of outliers according to predefined rules - Inherits from: SampleFilter RepProcessor - Generate new dynamic-version:\u00a0No - Members:** - vector  signalNames; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Names of signals that should be cleaned - vector  signalIds; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Ids of signals to clean - int time_channel = 0; - int val_channel = 0; - MedDictionarySections myDict; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0keeping it will enable us to get ids at apply stage - bool addRequiredSignals=false; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 a flag stating if we want to load signals that are not in the cleaned signal list (see explanation above) - vector consideredRules; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 only rules in this list will be considered in this cleaner (see explanation above) <ul> <li>**Implemented methods: \u00a0\u00a0**<ul> <li>Constructors :<ul> <li>*RepRuleBasedOutlierCleaner()\u00a0*</li> </ul> </li> <li>void init_defaults()\u00a0: init cleaning parameters to default values</li> <li>int init(void processor_params)\u00a0*: init cleaning parameters according to input params</li> <li>virtual int init(map&amp; mapper)\u00a0: init cleaning parameters according to map <li>void\u00a0set_signal_ids(MedDictionarySections&amp; dict)\u00a0: set signalId (actually keep the dictionary for further use in apply).</li> <li>int apply(PidDynamicRec&amp; rec, vector&amp; time_points)\u00a0: apply outliers-cleaning to signal in dynamic-rec at time-points"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/index.html","title":"SampleFilter","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/index.html#samplefilter-filters-a-set-of-samples","title":"SampleFilter filters a set of samples","text":"<p>SampleFIlter takes a MedSamples object and generates a new one (optionally in-place) which contains a subset of the original samples. SampleFilters may (optinally) require a MedRepository to apply the filtering SampleFilter is a virtual class. See below for the list of implemeted children classes</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/index.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":"Name Value Class Note train SMPL_FILTER_TRN BasicTrainFilter Generate a training set test SMPL_FILTER_TST BasicTestFilter Generate a test set outliers SMPL_FILTER_OUTLIERS OutlierSampleFilter Remove outlying outcomes match SMPL_FILTER_MATCH MatchingSampleFilter Perform matching required SMPL_FILTER_REQ_SIGNAL RequiredSignalFilter OBSOLETE, replaced by 'basic' basic SMPL_FILTER_BASIC BasicSampleFilter A range of filtering options"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html","title":"BasicSampleFilter","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#basicsamplefilter-applies-one-or-more-basic-filters-to-the-set","title":"BasicSampleFilter applies one (or more) basic filters to the set","text":"<p>BasicSampleFilter filtering options include:</p> <ul> <li>Allowed time (date) range</li> <li>A list of filters per signal, each specified using BasicFilteringParams<ul> <li>Allowed value-range of specific signals within a time-window</li> <li>Requirement on availability of specific signals - number of values within a time-window</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#include-file-is-hmrlibsinternalmedutilsmedprocesstoolssamplefliterh","title":"Include file is -\u00a0H:/MR/Libs/Internal/MedUtils/MedProcessTools/SampleFliter.h","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#basicfilteringparams-initializing","title":"BasicFilteringParams initializing:","text":"Parameter Name Description Default Value sig name of signal used for filtering None min_val minimal allowed value -1e10 max_val maximal allowed value 1e10 min_Nvals minimal required number of values 1 win_from start of time window to check requirements 0 win_to <p>end of time window to check requirements</p><p>e.g. (win_from=30,win_to=60) means that we check the signal one to two months before sample date</p> 2^30 time_ch time-channel to consider 0 val_ch value-channer to consider 0"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/BasicSampleFilter.html#basicsamplefilter-initializing","title":"BasicSampleFilter initializing:","text":"Parameter Name Description Default Value min_sample_time minimal allowed time (date) 0 max_sample_time maximal allowed time (date) 2^30 win_time_unit time-unit name (e.g. \"date\") Days bfilter <p>BasicFilterParams: list of plus-separated filters.</p><p>Each filter is defined by a \":=\"-separated initializer of BaiscFilteringParams</p> None <p>**</p>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/MatchingSampleFilter.html","title":"MatchingSampleFilter","text":""},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/MatchingSampleFilter.html#matchingsamplefilter-matches-cases-a-control-according-to-a-combination-of-matching-criteria","title":"MatchingSampleFilter matches cases a control according to a combination of matching criteria.","text":"<p>MatchingSampleFilter can use the following matching criteria:</p> <ul> <li>Age</li> <li>Time (e.g. calendar year)</li> <li>Values of signals</li> <li>Gender The optimal matching ratio is determined given a relative cost of losing a single cases versus losing a single controls sample.</li> </ul>"},{"location":"Infrastructure%20Library/MedProcessTools%20Library/SampleFilter/MatchingSampleFilter.html#matchingsamplefilter-initialization","title":"MatchingSampleFilter initialization:","text":"Parameter Name Description Default Value priceRatio the relative cost of losing a single controls sample 100.0 match_to_prior Given directly the prior to match to in each bin. If &lt; 0 won't be used -1 maxRatio the maximal allowed control/case matching ratio 10.0 verbose a verbositry flag (set to &gt;0 to allow logging) 0 strata <p>Definition of a matching strata, possibly more than one, separated by a colon</p><p>Each stratum is comma-separated and can be one of:</p><ul><li>\"age\" or \"age,1\" which 1 stands for bin_size in age</li><li>\"time,time-unit-name,time-resolution\" (e.g. \"time:year,1\")</li><li>\"signal,signal-name,resolution,TimeWindow (in days , takes the last)\" (e.g. \"signal,WBC,0.5,365\")</li><li>\"gender\"</li></ul><p> </p> None"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html","title":"Medial Tools","text":"<p>This page describes the command-line tools and Python helpers that interact with the MES infrastructure. It explains how to prepare data, train and evaluate models, and how to use the available tools for common workflows.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#overview","title":"Overview","text":"<p>Medial EarlySign (MES) provides two main ways to work with EMR data and models:</p> <ol> <li>A Python API that exposes many infrastructure capabilities for loading data, creating sample files, training, and inference.</li> <li>Standalone applications (executables) that implement infrastructure workflows; these are useful when the Python API does not yet cover a specific feature.</li> </ol> <p>When a model is deployed via AlgoMarker, only a small runtime API and shared library are required to produce predictions. Deployment does not require the full training API or a training data repository. Deployed models accepts JSON-formatted patient records. See AlgoMarker for details.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#setup-requirements","title":"Setup requirements","text":"<ol> <li> <p>Use the Python integration where possible - see the Python section. Note that the Python package does not expose every feature. Some capabilities are still only available as executables, but extension of the python library is possible. Follow the Python setup instructions.</p> </li> <li> <p>If you need the command-line tools, follow the installation guide: MES Tools to Train and Test Models. Each tool includes a <code>--help</code> flag that lists its arguments and options and <code>--version</code> flag to output git version it was complied with. Common tools are covered also within this Wiki.</p> </li> <li> <p>To develop your own applications against the infrastructure, see the examples in the Examples of Simple Applications section below.</p> </li> </ol> <p>In most cases, Python plus a few tools will be sufficient - it is uncommon need to write new C++ code.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#tutorial","title":"Tutorial","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#step-1-create-a-data-repository","title":"Step 1 - Create a data repository","text":"<p>First convert your EMR data to the MES repository format. Follow the ETL instructions: ETL tutorial.</p> <p>Note: Use the <code>Flow</code> application to perform the actual loading into a repository. The loading API has not been ported to Python yet, so installing the MES Tools may be necessary. See Flow loading.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#step-2-access-repository-data","title":"Step 2 - Access repository data","text":"<p>You can access repository data in two ways:</p> <ol> <li> <p>Python API</p> <ul> <li>Fetch signals as a Pandas DataFrame - Load Signals in python</li> </ul> </li> <li> <p>Tools and UI</p> <ul> <li>Use <code>Flow</code> to print signal summaries or export data - Flow view signals</li> <li>Open a single patient with the repository viewers UI - Repository viewers</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#step-3-create-medsamples","title":"Step 3 - Create MedSamples","text":"<p>Create a tab-separated MedSamples file describing prediction times and outcomes for each patient: MedSamples format.</p> <p>Use the Python API when possible to generate MedSamples. Each row typically contains a patient identifier, prediction time, outcome (0/1 or numeric for regression), outcome time, and optional split/metadata fields.</p> <p>Suggested process:</p> <ol> <li>Generate candidate samples (patient id + date) without labels.</li> <li>Label and exclude ineligible samples, documenting exclusions for cohort diagrams and validation.</li> </ol> <p>Example (creating samples for date <code>20251011</code>):</p> <pre><code>import med\nrep = med.PidRepository()\n\nrep.read_all(\"/path/to/repository\", [], [\"BDATE\"])\nbdate_sig = rep.get_sig(\"BDATE\").rename(columns={\"pid\": \"id\"})\n\nbdate_sig[\"EVENT_FIELDS\"] = \"SAMPLE\"\nbdate_sig[\"time\"] = 20251011\nbdate_sig[\"outcome\"] = 0\nbdate_sig[\"split\"] = -1\nbdate_sig[\"outcomeTime\"] = 20500101\n\n# Keep fields in the MedSamples order\nbdate_sig = bdate_sig[[\"EVENT_FIELDS\", \"id\", \"time\", \"outcome\", \"outcomeTime\", \"split\"]]\n\nbdate_sig.to_csv(\"/path/to/samples\", index=False, sep=\"\\t\")\n</code></pre> <p>[!NOTE]  Keep in mind that training and test sample sets are often processed differently. For example, training sets may be rebalanced by year to avoid the model learning calendar trends. Those filtering and subsampling choices are part of study design and are under the responsibility of the data scientist to mitigate biases and potential information leakage in training.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#training-a-model","title":"Training a model","text":"<ol> <li>Define the model pipeline and components in a MedModel JSON file: MedModel JSON format.</li> <li>Train the model using either:<ul> <li>Python: Train Model Using Python API</li> <li>Tools:<ul> <li>Training with Flow - simple training run without hyperparameter search</li> <li>Optimizer - grid-search-style training with penalties to balance generalization (note: Optuna can be used externally for more flexible tuning)</li> </ul> </li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#applying-a-model","title":"Applying a model","text":"<p>You can generate predictions or extract the feature matrix produced by a model pipeline.</p> <ul> <li>Apply a model using Python</li> <li>Apply a model with Flow</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#model-evaluation","title":"Model evaluation","text":"<p>Please refer to our suggested Checklist for model evaluation Common evaluation tools and workflows used in MES:</p> <ol> <li>bootstrap_app - bootstrap-based performance analysis with cohort and subgroup filtering. For example, we might want to test performance in different sub-cohorts: time window, age range, etc.</li> <li>Feature importance and post-processing - see Flow post-processors.</li> <li>Explainability - add model explainers as post-processors. See the Explainers Guide and our patent US20240161005A1 for the MES-specific approach. Recognizing that standard Shapley values struggle with high-dimensional, correlated medical data, we developed a specialized extension. This new method was validated by our clinicians in a blinded study against other explainability techniques and was a key component of our award-winning submission to the CMS Health AI Outcome Challenge. The results are published, but some of the process can be seen in the Research tab of this wiki.</li> <li>Covariate-shift / simulation tools - Simulator.</li> <li>Automated checks - AutoTest, a pipeline of tests derived from the Model Checklist.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#other-tools-and-utilities","title":"Other tools and utilities","text":"<ul> <li>Add calibration and calibration tests</li> <li>Fairness extraction: Calculates fairness metrics.</li> <li>Adjust Model: Adds components like rep_processors or post_processors to an existing model. Usefull for example, if we want to noise the input, drop values or other manipulations that we don't want to store in the model pipeline during training.</li> <li>Model Signals Importance: Evaluates the importance of signals in an existing model and measures the impact of including or excluding them on performance.</li> <li>Iterative Feature Selector: Iteratively selects features or groups of features in a top-down or bottom-up manner and generates a report.</li> <li>Change Model: Modifies a model settings that don't requires retraining, such as enabling verbose mode for outliers or limiting memory batch size.<ul> <li>How to Limit Memory Usage in Predict.</li> </ul> </li> <li>TestModelExternal: Compares repositories or samples by building a propensity model to identify differences.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/index.html#examples-of-simple-applications","title":"Examples of Simple Applications","text":"<p>To learn how to create your own applications, clone the MR_Tools repository. Navigate to the <code>MedProcessUtils</code> directory and explore the following examples:</p> <ul> <li><code>learn</code> - Application.</li> <li><code>getMatrix</code> - Application.</li> <li><code>predict</code> - Application.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Calibrate%20model%2C%20and%20calibration%20test.html","title":"Calibrate model, and calibration test","text":"<p>Calibrate a model on one repository, and check calibration on another repository.</p> <p><pre><code>FilterSamples --filter_train 0 --rep $REP --filter_by_cohort \"Time-Window:0,365\" --samples $SAMPLES --output $OUTPUT --json_mat $JSON\n</code></pre> Comments:</p> <ul> <li>Time Window should be 0 to horizon. Thus, 0,365 means calibrated risk for outcome within 1 year. And 0,730 would mean calibrated risk for outcome within 2 years.</li> <li>json_mat is required even though it has no effect (to be removed)</li> <li>filter_train default is 1 =&gt; take just train. As we set <code>0</code> - all samples are taken.</li> </ul> <p>Standard predict for the samples generated previously. <pre><code>Flow --get_model_preds --rep $REP --f_samples $INPUT --f_model $MODEL --f_preds $PREDS_FOR_CALIBRATION \n</code></pre> <pre><code>adjust_model --postProcessors $JSON --rep $REP --samples $PREDS_FOR_CALIBRATION --inModel $MODEL --skip_model_apply 1 --out $OUTPUT \n</code></pre> Comments:</p> <ul> <li>The OUTPUT is a model with calibration</li> <li>The terminal output is 'staircase graph' - bins and calibrated risk (printed on screen and reachable through Flow --print_model_info)\u00a0 Output format example: <pre><code>Succesfully added 1 post_processors\nCreated 44 bins for mapping prediction scores to probabilities\nRange: [0.7224, 2147483648.0000] =&gt; 1.0000 | 10.77%(107736.000000 / 1000000.000000)\nRange: [0.6419, 0.7224] =&gt; 0.2500 | 9.98%(99850.000000 / 1000000.000000)\nRange: [0.4393, 0.6419] =&gt; 0.1123 | 11.70%(116964.000000 / 1000000.000000)\nRange: [0.4373, 0.4393] =&gt; 0.1000 | 4.30%(43022.000000 / 1000000.000000)\nRange: [0.4284, 0.4373] =&gt; 0.0889 | 7.88%(78793.000000 / 1000000.000000)\n...\n</code></pre> \u00a0 The required JSON is: <pre><code>{\n    \"post_processors\": [\n        {\n            \"action_type\":\"post_processor\",\n            \"pp_type\":\"calibrator\",\n            \"calibration_type\":\"isotonic_regression\",\n            \"use_p\":\"0.25\"\n        }\n    ]\n} \n</code></pre> \u00a0 Run the program: <pre><code>TestCalibration --rep ${REP} --tests_file ${TEST} --output ${OUT_PATH}\n</code></pre> The test file has 3 TAB tokens in each line: samples_path, optional model_path to apply on samples and optional split to filter from samples. \u00a0 File with expected risk and actual in validation, for each bin of the calibrated model, e.g., plus some KPIs: <pre><code>probabilty_of_model     Validation_probabilty    cases   total_observations     Diff\n0.0000%                 0.0000%                  0       11                     0.0000%\n0.0046%                 0.0062%                  92      1493236                0.0016%\n0.0103%                 0.0096%                  513     5329513                0.0007%\n0.0158%                 0.0184%                  398     2166035                0.0026%\n\u00a0\n9.9270%                 7.6923%                  2       26                     2.2347%\n10.8825%                2.0000%                  1       50                     8.8825%\ntot_diff(L2)=0.001983 prior=0.002349 prior_loss(L2)=0.003845 R2=0.484125 Num_Bins=108 Calibration_Index=0.000890\n</code></pre> \u00a0 And a graph  </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Compare%20AUC%27s.html","title":"Compare AUC's","text":"<p>As a part of the Lung cancer paper, we were asked to check the statistical significance of the AUC's difference (our model vs. a baseline model). We used a non-parmetric empirical method describe by Delong. The method is decribed In the attached bwloe pdf (Comparing Two ROC Cureves - Paired Design - page 547-6)</p> <p>The tool can be found in MR_Tools <code>MedProcessUtils/CompareRocs</code> \u00a0 There are two modes for running the Tool: 1. Using 2 prediction files (one for each model) Example: Running Example <pre><code>python /server/UsersData/ron-internal/MR/Tools/CompareRocs/compareROCs.py --preds_file_1 /server/Work/Users/Ron/Projects/LungCancer/results/model_27/model_27_test.preds --preds_file_2 /server/Work/Users/Ron/Projects/LungCancer/results/Tammemagi_nsclc_ever_smokers_smoking_intensity/Tammemagi_nsclc_ever_smokers_smoking_intensity_non_linear_test.preds\n</code></pre></p> <p><pre><code>sys:1: DtypeWarning: Columns (1,2,3,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\npreds are equal\nmodel 1: AUC: 0.8647144227120102, Variance: 5.427313956814398e-06, estimated 95% CI: [0.8601482898518649, 0.8692805555721554]\nmodel 2: AUC: 0.823383955909251, Variance: 8.174414229417143e-06, estimated 95% CI: [0.817780133133348, 0.8289877786851539]\nZ - score: 23.081440151744086, p-value: 0.0\n</code></pre> \u00a0 \u00a0 \u00a0 2.  Using 2 bootstrap .Raw files + the requested cohort string Example: <pre><code>python /server/UsersData/ron-internal/MR/Tools/CompareRocs/compareROCs.py --preds_file_1 /server/Work/Users/Ron/Projects/LungCancer/results/model_27/paper/p_value_bs_out_1.Raw --preds_file_2 /server/Work/Users/Ron/Projects/LungCancer/results/model_27/paper/p_value_bs_out_2.Raw --cohort_string Time-Window:270.000-365.000,Age:55.000-80.000,Lung_Cancer_Type.category_set_NotNonSmallCell.win_-3650_3650:-0.500-0.500,NLST_Criterion_min_age_55_max_age_80_pack_years_30:0.500-15.000\n</code></pre></p> <pre><code>preds are equal\nmodel 1: AUC: 0.8011430555038992, Variance: 0.00025990328389302294, estimated 95% CI: [0.7695448837940765, 0.8327412272137219]\nmodel 2: AUC: 0.7436638110941679, Variance: 0.0003739639445801397, estimated 95% CI: [0.7057610422045595, 0.7815665799837763]\nZ - score: 4.470422572292469, p-value: 3.903260102799955e-06\n</code></pre> <p>Comparing_Two_ROC_Curves-Paired_Design.pdf</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Fairness%20extraction.html","title":"Fairness Extraction","text":"<p>To evaluate fairness, we compare Sensitivity and Specificity at specific score cutoffs across different groups. The required tool is available in the SCRIPTS repository, which should already be in your PATH for direct use.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Fairness%20extraction.html#process-overview","title":"Process Overview","text":"<ol> <li>Run bootstrap analysis to generate detailed results, reporting performance at each 1% increment of positive rate (PR) and false positive rate (FPR).</li> <li>Define the base cohort (<code>FAIRNESS_BASE_COHORT</code>) for baseline performance assessment.</li> <li>Specify comparison groups (<code>FAIRNESS_GROUPS</code>) to stratify and compare within the base cohort.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Fairness%20extraction.html#example-usage","title":"Example Usage","text":"<pre><code>#!/bin/bash\nFAIRNESS_BASE_COHORT=\"Age:40,89;Time-Window:90,365;Ex_or_Current_Smoker:0.5,1.5\"\nFAIRNESS_GROUPS=(\"Gender:1,1\" \"Gender:2,2\")\nOUTDIR=/tmp\n\n# Prepare cohorts file\necho \"MULTI;${FAIRNESS_BASE_COHORT}\" | sed 's|;|\\t|g' &gt; ${OUTDIR}/cohorts\nfor fr_grp in \"${FAIRNESS_GROUPS[@]}\"; do\n    echo \"MULTI;${FAIRNESS_BASE_COHORT};${fr_grp}\" | sed 's|;|\\t|g' &gt;&gt; ${OUTDIR}/cohorts\ndone\n\nPREDS_FILE=/nas1/Work/Users/Eitan/Lung/outputs/models2023/EX3/model_63/Test_Kit/bootstrap/TimeWindow.alt/result_win_90_365.preds\nRATES_1_99=\"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99\"\n\n# Run bootstrap analysis\nbootstrap_app --use_censor 0 --sample_per_pid 0 --input ${PREDS_FILE} --rep /nas1/Work/Repositories/KP/kp.repository  --json_model /nas1/Work/Users/Alon/LungCancer/configs/analysis/bootstrap/bootstrap.json --output ${OUTDIR}/bt.fairness.by_pr --cohorts_file ${OUTDIR}/cohorts --working_points_fpr ${RATES} --working_points_pr ${RATES}\n\n# Analyze fairness at 3% and 5% positive rate cutoffs\nfairness_extraction.py --bt_report ${OUTDIR}/bt.fairness.by_pr.pivot_txt --output ${OUTDIR} --bt_cohort \"${FAIRNESS_BASE_COHORT}\" --cutoffs_pr 3 5\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Fairness%20extraction.html#fairness_extractionpy-arguments","title":"<code>fairness_extraction.py</code> Arguments","text":"<ul> <li><code>--bt_report</code>: Path to the bootstrap output file (should include results for multiple PR or FPR cutoffs and all comparison groups).</li> <li><code>--bt_cohort</code>: The base cohort used for baseline filtering. If omitted, the script will use the shortest cohort description found in the bootstrap file.</li> <li><code>--output</code>: Directory for output files.</li> <li><code>--cutoffs_pr</code>: List of PR cutoffs to evaluate (if not available, FPR will be used), based on the baseline cohort.</li> </ul> <p>The script above first runs the bootstrap analysis, then defines the base cohort and comparison groups (e.g., <code>Gender:1,1</code> and <code>Gender:2,2</code>).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Fairness%20extraction.html#output","title":"Output","text":"<p>The tool compares Sensitivity and Specificity for the specified <code>FAIRNESS_GROUPS</code> at the same cutoff (e.g., top 3% and 5% positive rate). It also performs a chi-square statistical test to assess differences between groups.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/FilterSamples.html","title":"FilterSamples","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/FilterSamples.html#overview","title":"Overview","text":"<p>This simple application helps you filter your MedSamples based on bootstrap query language criteria or by simple criteria based on <code>TRAIN</code> signal.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/FilterSamples.html#how-to-use","title":"How to use","text":"<p>Example run:</p> <pre><code>FilterSamples --rep $REPOSITORY_PATH --samples $INPUT_SAMPLES_PATH --output $OUTPUT_SAMPLES_PATH --filter_train $FILTER_TRAIN_VAL\n</code></pre> <ul> <li><code>TRAIN == 1</code>: Training set (70%)</li> <li><code>TRAIN == 2</code>: Test set (20%)</li> <li><code>TRAIN == 3</code>: Validation set (10%)</li> </ul> <p>You can also specify <code>--json_mat</code> and <code>--filter_by_bt_cohort</code> to filter by bootstrap query language. More info on bootstrap query language in here: </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html","title":"Iterative Feature Selector","text":"<p>The Iterative Feature Selector is a powerful tool designed to identify the most impactful signals (or features) within your data. It does this by building a predictive model incrementally, either from the ground up or by eliminating less important signals. This tool is a standalone wrapper for the <code>iterativeFeatureSelector</code> FeatureProcessor, allowing for a more focused and verbose exploration of the feature selection process than is available within a full model pipeline.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#core-concepts","title":"Core Concepts","text":"<p>The tool operates by adding or removing signals based on how they affect a chosen performance metric, such as AUC.  It defines a signal as a collection of related features, which can be selected or removed as a group.  If you prefer to work on individual features, you can use the <code>--work_on_ftrs</code> flag.</p> <p>There are two main iterative methods:</p> <ol> <li>Bottom-Up: Starts with a small set of signals (or none). In each iteration, it tests the remaining signals and adds the one that provides the greatest improvement to the selected metric.</li> <li>Top-Down: Starts with all available signals. In each iteration, it tests the remaining signals and removes the one that causes the least decrease in the selected metric.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#command-line-arguments","title":"Command Line Arguments","text":"<p>The following parameters control the tool's behavior.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#input-data","title":"Input Data","text":"<p>You must provide data using one of the following methods:</p> <ul> <li><code>--inCsv &lt;path&gt;</code>: A single CSV file containing your data matrix.</li> <li><code>--inSamples &lt;path&gt;</code> + <code>--inModel &lt;path&gt;</code>/<code>--inJson &lt;path&gt;</code>: The tool will generate the data matrix from raw samples using an existing model file.</li> <li><code>--rep &lt;path&gt;</code>: The configuration file for the data repository for generating the matrix from inSamples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#output","title":"Output","text":"<ul> <li><code>--progress_file_path</code>: Path for progress output file log that you can track and see performance not only in terminal and wait for program to finish.</li> <li><code>--out &lt;path&gt;</code>: The path for the output file, which will contain the selection report.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#selector-parameters","title":"Selector Parameters","text":"<ul> <li><code>--mode &lt;top2bottom | bottom2top&gt;</code>: The direction of the selection process. Default is <code>bottom2top</code>.</li> <li><code>--predictor &lt;type&gt;</code>: The type of predictor to use in the selection loop (e.g., <code>xgb</code>).</li> <li><code>--predictor_params \"&lt;key1=value1;...&gt;\"</code>: A semicolon-separated string of default parameters for your predictor.</li> <li><code>--predictor_params_file &lt;path&gt;</code>: A file that specifies predictor parameters based on the number of features. Each line should be tab-delimited with the format: <code>min_features max_features predictor_param_string</code>. This file overrides <code>--predictor_params</code>.</li> <li><code>--rate \"&lt;rate_string&gt;\"</code>: Defines the number of signals to add or remove in each iteration. The format is a comma-separated list of <code>bound:step</code> pairs. For example, <code>\"50:1,100:2\"</code> means add/remove 1 signal when the count is between 1-50, and 2 when it's between 51-100.</li> <li><code>--required &lt;signal1,signal2,...&gt;</code>: A comma-separated list of signals that must be included in the model from the start.</li> <li><code>--work_on_ftrs &lt;boolean&gt;</code>: If <code>true</code>, the selector operates on individual features instead of entire signals. Default is <code>false</code>.</li> <li><code>--group_mode</code>: Controls the features grouping. Can be a file path with feature to group name for full control or keyword like <code>BY_SIGNAL_CATEG</code> that aggregates features by their originating source signal</li> <li><code>--numToSelect</code>: stop criteria when to stop. The default is 0. Continue till the end.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#performance-evaluation","title":"Performance Evaluation","text":"<p>The tool uses a bootstrap method for performance evaluation.</p> <ul> <li><code>--msr_params &lt;param_string&gt;</code>: Defines the performance metric. For example, <code>AUC</code> or <code>SENS,FPR,0.2</code>.</li> <li><code>--nfolds &lt;number&gt;</code>: Replaces existing data splits with new folds for cross-validation.</li> <li><code>--folds &lt;0,2,4&gt;</code>: A comma-separated list of folds to use for evaluation. If not provided, all folds are used.</li> <li><code>--verbose &lt;level&gt;</code>: Controls the level of detail printed to the console.</li> <li><code>--bootstrap_params &lt;param_string&gt;</code>: Parameters for bootstrap. e.g. sample_per_id=1 ('/' separated, key value is separated by \":\"). example: \"loopcnt:500/sample_per_id:1\"</li> <li><code>--cohort_params &lt;param_string&gt;</code>: Parameters for defining the booststrap cohort. e.g. \"Age:50,75/Time-Window:0,365\" for filtering the samples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#running-example","title":"Running Example","text":"<p>This command runs a top-down iterative selection process, removing signals that have the least impact on AUC. Age,Gender are required and forced to remain in the model</p> Running Example<pre><code>iterativeSelector --inSamples samples --inJson simple_model.json --out outReport --predictor qrf --predictor_params_file params_iterative_seletcor --nfolds 5 --folds \"0,2,4\" --mode top2bottom --verbose 1 --msr_params AUC --required \"Age,Gender\"\n</code></pre> <ul> <li><code>--predictor</code>: Uses a <code>qrf</code> (Quantile Random Forest) predictor.</li> <li><code>--predictor_params_file</code>: Loads specific predictor parameters from a file based on the number of features.</li> <li><code>--nfolds</code> and <code>--folds</code>: Specifies that the evaluation should use 5 folds, but only folds 0, 2, and 4 will be used for testing.</li> <li><code>--mode top2bottom</code>: Starts with all signals and removes them one by one.</li> <li><code>--required</code>: Ensures Age and Gender are never removed from the model. \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#example-predictor-parameters-file","title":"Example: Predictor Parameters File","text":"<p>The <code>predictor_params_file</code> allows you to change predictor parameters as the number of features changes. Each line is tab-delimited.</p> <pre><code>0 50 ntrees = 200 ; min_node = 300\n51 150 ntrees = 200 ; min_node = 200\n151 200 ntrees = 200 ; min_node = 100\n201 100000 ntrees = 200 ; min_node = 50\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Iterative%20Feature%20Selector.html#output-example","title":"Output Example","text":"<p>The tool provides a verbose output, detailing the changes at each step.  In this Top-Down example, the tool is showing which signal it is removing and the corresponding AUC of the model after the removal.</p> <pre><code>Removing family RBC with AUC_Obs = 0.747491\nRemoving family RDW with AUC_Obs = 0.747476\nRemoving family INR with AUC_Obs = 0.749214\nRemoving family PDW with AUC_Obs = 0.751048\nRemoving family MCV with AUC_Obs = 0.750944\nRemoving family Hematocrit with AUC_Obs = 0.751496\nRemoving family WBC with AUC_Obs = 0.751073\n...\nRemoving family MCHC-M with AUC_Obs = 0.750825\nRemoving family Hemoglobin with AUC_Obs = 0.752080\nRemoving family Platelets with AUC_Obs = 0.741643\nRemoving family MCH with AUC_Obs = 0.688449\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Optimizer.html","title":"Optimizer","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Optimizer.html#overview","title":"Overview","text":"<p>The Optimizer is designed to tune the hyperparameters of your model. It evaluates performance on both the test set and the training set (to assess overfitting and generalization error). A penalty factor, controlled by the <code>generelization_error_factor</code> argument, is applied when selecting the best configuration if the generalization error is high.</p> <p>The Optimizer saves its progress, so if a failure occurs, it can resume from where it left off (unless the override flag is set or output folders are deleted). This is especially useful for long-running processes.</p> <p>Although the main focus is on <code>MedPredictor</code> parameters, you can also specify sample weights and different training samples, and combine these options as needed. The Optimizer is part of the AllTools compilation.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Optimizer.html#arguments","title":"Arguments","text":"<ul> <li><code>--rep</code>: Path to the repository.</li> <li><code>--train_samples_files</code>: File with training samples (one per line). Optionally, two extra columns for case/control values (comma-separated).</li> <li><code>--test_samples_files</code>: File with test samples (one per line). Optionally, two extra columns for case/control values (comma-separated).</li> <li><code>--train_test_samples_single</code> (default: 0): If true, <code>train_samples_files</code> and <code>test_samples_files</code> are used directly as MedSamples (not as file paths). Useful when there is only one MedSamples option for training.</li> <li><code>--splits_file</code>: File specifying splits (IDs separated by spaces).</li> <li><code>--train_cases_labels</code>: Comma-separated values defining cases for training. If empty, no transformation.</li> <li><code>--train_control_labels</code>: Comma-separated values defining controls for training. If empty, no transformation.</li> <li><code>--test_cases_labels</code>: Comma-separated values defining cases for testing.</li> <li><code>--test_control_labels</code>: Comma-separated values defining controls for testing.</li> <li><code>--train_till_time</code> (default: -1): If &gt;0, filters training data up to this time; data after is used for testing.</li> <li><code>--change_to_prior</code> (default: -1): If &gt;0, randomly subsamples cases/controls to reach this prior.</li> <li><code>--down_sample_train_count</code> (default: 0): If not zero, downsamples training set to this count.</li> <li><code>--down_sample_test_count</code> (default: 0): If not zero, downsamples test set to this count.</li> <li><code>--json_model</code>: Path to the model JSON file.</li> <li><code>--matching_strata</code> (default: \"Time,year,1\"): Strata arguments for matching.</li> <li><code>--matching_json</code>: JSON for matching, if required.</li> <li><code>--matching_change_to_prior</code> (default: -1): If &gt;0, matches to this ratio (overrides <code>price_ratio</code>).</li> <li><code>--price_ratio</code> (default: 0): If 0, no matching. If &lt;0, assigns weights. Otherwise, performs matching.</li> <li><code>--nfolds</code> (default: 6): Number of folds for splitting training data.</li> <li><code>--split_sel</code> (default: 0): Number of splits to use for model selection. 0 means all.</li> <li><code>--train_weights_method_file</code>: If set, trains with all weight options listed in this file (one per line).</li> <li><code>--ms_predictor_name</code> (default: xgb): Predictor name for model selection.</li> <li><code>--ms_configs_file</code>: Model selection file. Each parameter is assigned with =. Use commas to specify multiple options for a parameter (see example below).</li> <li><code>--generelization_error_factor</code> (default: 5): Penalty factor for generalization error (overfitting) between train and test. Should be &gt;1.</li> <li><code>--result_folder</code>: Output folder for results.</li> <li><code>--config_folder</code>: Output folder for configuration files.</li> <li><code>--min_age_filter</code> (default: 0): Minimum age filter.</li> <li><code>--max_age_filter</code> (default: 120): Maximum age filter.</li> <li><code>--min_year_filter</code> (default: 0): Minimum year filter for sample time.</li> <li><code>--max_year_filter</code> (default: 3000): Maximum year filter for sample time.</li> <li><code>--use_same_splits</code> (default: 1): If true, uses the same splits for different training samples (assumes all have the same samples).</li> <li><code>--bt_json</code>: JSON file for creating features for bootstrap cohort filtering.</li> <li><code>--bt_cohort</code>: Cohort line for filtering (no support for Multi or name, only filter definition).</li> <li><code>--bootstrap_args</code>: Bootstrap parameters.</li> <li><code>--bootstrap_measure</code> (default: AUC_Obs): Measure for bootstrap.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Optimizer.html#example-lightgbm_modeloptions-file","title":"Example: <code>lightgbm_model.options</code> File","text":"<pre><code>verbose=0\nsilent=2\nnum_threads=15\nnum_trees=200\nmetric=auc\nobjective=binary\nlearning_rate=0.01,0.03,0.05\nlambda_l2=0\nmetric_freq=1000\nmin_data_in_leaf=100,500,1000,2000\nfeature_fraction=0.8,1\nbagging_fraction=0.8\nbagging_freq=5\nmax_bin=250\nboosting_type=gbdt\nmax_depth=0,5,6,7\nmin_data_in_bin=50\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Optimizer.html#output","title":"Output","text":"<p>The Optimizer creates two output folders, set by <code>--result_folder</code> and <code>--config_folder</code>:</p> <ul> <li>config_folder: Stores the final model, cross-validation results, and a text file with the selected parameters.</li> <li>result_folder: Contains detailed results for each cross-validation split and a summary file for hyperparameter tuning. For each configuration, it reports performance on both the test and training splits (to assess overfitting).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html","title":"Simulator","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html#goal","title":"Goal","text":"<p>Simulate expected performance of a frozen model under a new environment (covariate/covariance shift). The simulator lets you specify target population characteristics (age, sex, availability of signals, etc.) and estimates how model performance will change.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html#approach-high-level","title":"Approach (high level)","text":"<p>The simulator reweights or subsamples an existing labeled dataset (where ground-truth and original performance are known) to match a user-defined target population. This is a statistical, not machine-learning, adjustment - conceptually similar to inverse-probability weighting but using an explicit target population definition rather than learned propensities.</p> <p>Example: if the original population age range is uniform 40-80 and the target environment is 50-80, patients aged 40-49 receive zero weight in the estimation and the performance metrics are computed on the reweighted population.</p> <p>This method generalizes to multi-dimensional scenarios (age, sex, signal missingness, etc.) and gives an accurate estimate of expected performance when the target population is well specified.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html#code-location","title":"Code location","text":"<p>The simulator is implemented in the MR_Tools repository under: <code>AlgoMarker_python_API/PopulationAnalyzer</code></p> <p>Slides and documentation:</p> <ul> <li>Brief Slides</li> <li>Full simulator slides</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html#running-the-server","title":"Running the server","text":"<p>From the simulator directory start the UI server:</p> <pre><code>./ui.py\n</code></pre> <p>Default port: 3764. Use the full path to <code>ui.py</code> if you run it from another working directory.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html#adding-a-new-algomarker","title":"Adding a new AlgoMarker","text":"<p>To register a new AlgoMarker in the simulator UI:</p> <ol> <li>Copy an existing AlgoMarker Python file (for example <code>LungFlag.py</code>) into the <code>algomarkers/</code> folder.    - The chosen filename (without <code>.py</code>) will appear in the UI. Filenames may use <code>_SLASH_</code> to show a <code>/</code> in the UI.</li> <li>Create or edit the AlgoMarker config Python file (the module the UI imports) and define the following fields:<ul> <li>am_regions: dict mapping region keys to <code>ReferenceInfo</code> objects (paths to reference matrices, repository paths, CV results, etc.).</li> <li>sample_per_pid: numeric bootstrap parameter (how many samples per patient).</li> <li>default_region: optional default region key.</li> <li>additional_info: short descriptive text shown near the model selector.</li> <li>optional_signals: optional list of InputSignal/InputSignalsExistence objects describing extra input groups (e.g., Smoking, Labs, BMI).</li> <li>model_path: path to the model file used by the simulator.</li> <li>orderdinal: optional integer to order this AlgoMarker in the UI.</li> </ul> </li> </ol> <p>These fields are used by the server to load reference matrices, build cohorts, and run the simulation UI.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Simulator.html#example-configuration","title":"Example configuration","text":"<p>An example (abridged) config is included below. The full example in the original file shows cohort filters, region definitions, and optional signals.</p> Click to expand <pre><code>from models import *\n\nlung_cohorts = [\n    CohortInfo(cohort_name='Ever Smokers Age 50-80', bt_filter=lambda df: (df['age']&gt;=50) &amp; (df['age']&lt;=80)),\n    CohortInfo(cohort_name='Ever Smokers Age 45-80', bt_filter=lambda df: (df['age']&gt;=45) &amp; (df['age']&lt;=80)),\n    CohortInfo(cohort_name='Ever Smokers Age 40-90', bt_filter=lambda df: (df['age']&gt;=40) &amp; (df['age']&lt;=90)),\n    CohortInfo(cohort_name='Ever Smokers Age 55-74', bt_filter=lambda df: (df['age']&gt;=55) &amp; (df['age']&lt;=74)),\n]\n\nus_lung_cohorts = [\n    CohortInfo(cohort_name='Ever Smokers Age 50-80', bt_filter=lambda df: (df['age']&gt;=50) &amp; (df['age']&lt;=80)),\n    CohortInfo(cohort_name='Ever Smokers Age 55-74', bt_filter=lambda df: (df['age']&gt;=55) &amp; (df['age']&lt;=74)),\n    CohortInfo(cohort_name='Ever Smokers Age 45-90', bt_filter=lambda df: (df['age']&gt;=45) &amp; (df['age']&lt;=90)),\n    CohortInfo(\n        cohort_name='USPSTF Age 50-80 (20 pack years, &lt;15 years quit)',\n        bt_filter=lambda df: (df['age']&gt;=50) &amp; (df['age']&lt;=80) &amp;\n                             (df['smoking.smok_pack_years']&gt;=20) &amp;\n                             (df['smoking.smok_days_since_quitting']&lt;=15*365)\n    ),\n]\n\nam_regions = {\n    'US-KP': ReferenceInfo(\n        matrix_path='/nas1/Work/Users/eitan/Lung/outputs/models2023/EX3/model_63/reference_matrices/reference_features_kp.final.matrix',\n        control_weight=20,\n        cohort_options=us_lung_cohorts,\n        default_cohort='USPSTF Age 50-80 (20 pack years, &lt;15 years quit)',\n        repository_path='/nas1/Work/CancerData/Repositories/KP/kp.repository',\n        model_cv_path='/nas1/Work/Users/eitan/Lung/outputs/models2023/EX3/model_63/results',\n        model_cv_format='CV_MODEL_%d.medmdl'\n    ),\n    'UK-THIN': ReferenceInfo(\n        matrix_path='/nas1/Work/Users/eitan/Lung/outputs/models2023/EX3/model_63/reference_matrices/reference_features_thin.final.matrix',\n        cohort_options=lung_cohorts,\n        default_cohort='Ever Smokers Age 55-74',\n        repository_path='/nas1/Work/CancerData/Repositories/THIN/thin_2021.lung/thin.repository'\n    ),\n}\n\nsample_per_pid = 0\ndefault_region = 'UK-THIN'\nadditional_info = 'Time Window 90-365'\n\noptional_signals = [\n    InputSignalsExistence(\n        signal_name='Smoking',\n        list_raw_signals=['Smoking_Duration', 'Smoking_Intensity', 'Pack_Years', 'Smoking_Quit_Date'],\n        tooltip_str='If true, include smoking duration/intensity/pack-years/quit date in inputs.'\n    ),\n    InputSignalsExistence(\n        signal_name='Labs',\n        list_raw_signals=[\n            \"Albumin\", \"ALKP\", \"ALT\", \"Cholesterol\", \"Triglycerides\", \"LDL\", \"HDL\", \"Creatinine\",\n            \"Glucose\", \"Urea\", \"Eosinophils%\", \"Hematocrit\", \"Hemoglobin\", \"MCHC-M\", \"MCH\",\n            \"Neutrophils#\", \"Neutrophils%\", \"Platelets\", \"RBC\", \"WBC\", \"RDW\", \"Protein_Total\",\n            \"Lymphocytes%\", \"Basophils%\", \"Monocytes%\", \"Lymphocytes#\", \"Basophils#\", \"Monocytes#\",\n            \"Eosinophils#\", \"MCV\"\n        ]\n    ),\n    InputSignalsExistence(signal_name='BMI', list_raw_signals=['BMI', 'Weight', 'Height']),\n    InputSignalsExistence(signal_name='Spirometry', list_raw_signals=['Fev1']),\n]\n\nmodel_path = '/earlysign/AlgoMarkers/LungFlag/lungflag.model'\norderdinal = 1\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/TestModelExternal.html","title":"TestModelExternal","text":"<p>TestModelExternal is a tool designed to compare differences between repositories or sample sets when applying a model. It builds a propensity model to distinguish between repositories or samples, revealing differences and enabling straightforward comparison of feature matrices. The main goal is to identify complex patterns when comparing data.</p> <p>You can use this tool to:</p> <ul> <li>Compare feature matrices from different repositories to check model transferability and detect issues in new repositories, such as:<ul> <li>Bugs in data handling, eligibility, or client data extraction</li> <li>Estimating expected model performance in a new repository, even without labels, using the propensity model</li> </ul> </li> <li>Compare samples within the same repository, for example, to analyze data from different years and identify feature differences.</li> </ul> <p>TestModelExternal is part of the MR_Tools repository and can be compiled under AllTools.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/TestModelExternal.html#mode-1-compare-when-both-repositories-are-available","title":"Mode 1: Compare When Both Repositories Are Available","text":"<p>Required arguments:</p> <ul> <li><code>model_path</code>: Path to the binary MedModel to test (required in all modes)</li> <li><code>rep_test</code>: Repository for testing and comparison. In the propensity model, data from this repository is labeled as 1.</li> <li><code>samples_test</code>: Path to MedSamples for the test repository</li> <li><code>output</code>: Directory for output files</li> <li><code>rep_trained</code>: Path to the trained model's repository (or reference repository)</li> <li><code>samples_train</code>: Path to MedSamples from the training repository (ensure the same method/eligibility rules are used in both datasets)</li> <li><code>predictor_type</code>, <code>predictor_args</code>: Parameters for the propensity model to distinguish between repositories</li> <li><code>calibration_init_str</code>: Calibration arguments for the propensity model's post-processor</li> </ul> <p>Optional arguments:</p> <ul> <li><code>smaller_model_feat_size</code>: If &gt; 0, creates an additional smaller propensity model using the top X features</li> <li><code>additional_importance_to_rank</code>: Path to a SHAP report (from \"Flow --shap_val_request\") to rank differences combined with feature importance</li> <li><code>features_subset_file</code>: File to filter features from the MedModel</li> <li><code>fix_train_res</code>: If &gt; 0, sets feature resolution in training to match the test set</li> <li><code>sub_sample_train</code>: Integer to limit the maximum number of training samples (0 = no subsampling)</li> <li><code>sub_sample_test</code>: Integer to limit the maximum number of test samples (0 = no subsampling)</li> <li><code>train_ratio</code>: Train/test split ratio (test set is used to report propensity model performance)</li> <li><code>bt_params</code>: Bootstrap parameters for the propensity model</li> <li><code>binning_shap_params</code>: Parameters for SHAP report analysis on the propensity model</li> <li><code>group_shap_params</code>: Grouping arguments for SHAP analysis</li> <li><code>shap_auc_threshold</code>: If AUC is below this value, SHAP analysis is skipped to save time</li> <li><code>print_mat</code>: If &gt; 0, prints the propensity matrix (0 = labels for train samples, 1 = labels for test samples)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/TestModelExternal.html#mode-2-compare-when-repositories-are-not-on-the-same-machine","title":"Mode 2: Compare When Repositories Are Not on the Same Machine","text":"<p>In this mode, leave <code>rep_trained</code> and/or <code>samples_train</code> empty.</p> <p>Required arguments:</p> <ul> <li><code>model_path</code>: Path to the binary MedModel to test (required in all modes)</li> <li><code>rep_test</code>: Repository for testing and comparison (labeled as 1 in the propensity model)</li> <li><code>samples_test</code>: Path to MedSamples for the test repository</li> <li><code>output</code>: Directory for output files</li> <li><code>strata_json_model</code>: JSON file for creating strata and collecting statistics</li> <li><code>strata_settings</code>: Strata settings for collecting statistics</li> </ul> <p>When comparing to a different repository on another machine, also provide either:</p> <ul> <li><code>train_matrix_csv</code>: A CSV matrix from the reference to compare with</li> <li><code>strata_train_features_moments</code>: File for the reference statistics to compare with the specified path. Created in \"train\" repository and controled with <code>strata_json_model</code>, <code>strata_settings</code></li> </ul> <p>The <code>train_matrix_csv</code> can be created in the reference by generating a feature matrix and is the prefered way when possible</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/TestModelExternal.html#mode-3-compare-different-samples-within-the-same-repository","title":"Mode 3: Compare Different Samples Within the Same Repository","text":"<p>Provide different <code>samples_train</code> and <code>samples_test</code> paths, and use the same values for <code>rep_train</code> and <code>rep_test</code> to indicate the same repository.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/TestModelExternal.html#example-output","title":"Example Output","text":"<p>The tool creates a propensity model and generates a SHAP report for this model. It also produces a <code>compare_rep.txt</code> file, which compares feature averages and standard deviations.</p> <p>You can use the resulting propensity model to assess expected performance when controlling for changes in your variables of interest.</p> <p>See examples usages:</p> <ul> <li>AutoTest - Test Matrix Over Years</li> <li>AutoTest - Compare Flags of Baseline Model vs MES Model</li> <li>AutoTest - Estimate Performance from Propensity Model</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html","title":"adjust_model","text":"<p>This tool, part of the AllTools compilation, is designed to modify and enhance existing models by adding new components to their processing pipelines. It's especially useful for integrating new functionalities without rebuilding the entire model from scratch.</p> <p>Common use cases include:</p> <ul> <li>Adding Post-Calibration: You can add a post-calibration component to an existing model to fine-tune its outputs.</li> <li>Adding Explainability: Easily integrate new modules for model explainability.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#command-line-interface","title":"Command Line Interface:","text":"<p>The primary command for using adjust_model is:</p> <pre><code>adjust_model --rep &lt;repository_path&gt; --inModel &lt;current_model_path&gt; --out &lt;out_model_path&gt; --samples &lt;path_to_samples_for_training_new_components&gt; --preProcessors &lt;pre_processor_addition_json&gt; --postProcessors &lt;post_processor_addition_json&gt; \n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#arguments","title":"Arguments","text":"<ul> <li><code>--rep &lt;repository_path&gt;</code>: The path to the data repository. Required unless you use --inCsv.</li> <li><code>--inModel &lt;current_model_path&gt;</code>: The path to the existing model file you want to modify.</li> <li><code>--out &lt;out_model_path&gt;</code>: The path where the new, adjusted model will be saved.</li> <li><code>--samples &lt;path_to_samples_for_training_new_components&gt;</code>: The path to the samples used for training the new components. Required if <code>--learn_rep_proc</code> is enabled or if <code>--skip_model_apply</code> is disabled.</li> <li><code>--preProcessors &lt;pre_processor_addition_json&gt;</code>: A JSON string or file containing a list of pre-model processors to add to the pipeline. </li> <li><code>--postProcessors &lt;post_processor_addition_json&gt;</code>: A JSON string or file containing a list of post-model processors to add to the pipeline.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#optional-flags","title":"Optional Flags","text":"<ul> <li><code>--inCsv</code>: Provide your feature matrix directly in CSV format. This is an alternative to using a repository and samples.</li> <li><code>--learn_rep_proc</code>: When set to <code>1</code>, this flag triggers the <code>learn</code> function for the new pre-processors. This is essential for components that require training. Note: If you use this flag, you must also provide <code>--rep</code> and <code>--samples</code>. (Default: 0)</li> <li><code>--insert_rep_proc_first</code>: When this flag is enabled, new pre-processors are added at the beginning of the existing pipeline. By default, they are appended to the end. (Default: 0)</li> <li><code>--skip_model_apply</code>: If <code>1</code>, the model's pipeline won't be run to generate a feature matrix and samples. This can be useful for specific operations and can significantly speed up the process. (Default: <code>0</code>)</li> <li><code>--model_changes</code>: This allows you to include instructions for modifying existing components within the model's pipeline. This can save you from having to use a separate change_model tool.</li> </ul> <p>Note: You can use either <code>--preProcessors</code> or <code>--postProcessors</code>, or both.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#json-templates-and-examples","title":"JSON Templates and Examples","text":"<p>To add new components, you'll provide a JSON object defining the new processors.  A \"block\" within this JSON corresponds to a specific processor.  For example, a block might be a Calibrator processor.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#adding-pre-processors","title":"Adding Pre-Processors","text":"<p>Here is a template for the <code>--preProcessors</code> argument. The <code>\"members\"</code> array is where you'll define each processor block.</p> <p><pre><code>{ \n  \"pre_processors\": [\n    {\n      \"action_type\": \"rp_set\",\n      \"members\": [\n        // Your processor blocks go here\n      ]\n    }\n  ] \n}\n</code></pre> </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#adding-post-processors","title":"Adding Post-Processors","text":"<p>Here is a template for the <code>--postProcessors</code> argument.</p> <pre><code>{ \n  \"post_processors\": [\n    // Your processor blocks go here\n  ]\n}\n</code></pre> <ul> <li>Full example of adding calibration</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/adjust_model.html#how-to-change-existing-components","title":"How to change existing components","text":"<p>If you want to change and not add new components, please refere to change_model</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html","title":"model_signals_importance","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html#overview","title":"Overview","text":"<p>The <code>model_signals_importance</code> tool systematically evaluates how specific groups of signals contribute to your model\u2019s performance. It does this by comparing the model\u2019s results with and without selected signals, without retraining the model. The model remains fixed; the tool simply tests the effect of removing signals from the input.</p> <p>This analysis is useful for:</p> <ul> <li>Model Interpretability: Quantifies the impact of each signal or group.</li> <li>Feature Selection: Identifies redundant or low-impact signals that can be removed, simplifying the model and reducing data requirements.</li> <li>Performance Benchmarking: Shows the expected performance drop if a signal becomes unavailable.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html#command-line-parameters","title":"Command Line Parameters","text":"<ul> <li><code>--rep</code>: Path to the data repository.</li> <li><code>--samples</code>: Path to the raw data samples.</li> <li><code>--model_path</code>: Path to the pre-trained model.</li> <li><code>--output</code>: Path to save the output report.</li> <li><code>--skip_list &lt;comma-delimited list&gt;</code>: A comma-separated list of signals to exclude from the analysis, such as Age,Gender. These are typically signals that are always present and not under evaluation.</li> <li><code>--no_filtering_of_existence</code>: Disables the default behavior of filtering the test cohort to include only samples where the signal exists. When this flag is used, performance is measured on all samples, regardless of the signal's presence.</li> <li><code>--features_groups &lt;path&gt;</code>: A file mapping features to logical group names (e.g., all CBC features to the CBC group). This can be a tab-delimited file or a keyword like BY_SIGNAL_CATEG to group features by their originating source.</li> <li><code>--group2signal &lt;path&gt;</code>: (Optional) Path to a tab-delimited file with two columns: group name (from <code>features_groups</code>) and a comma-separated list of signals. This remaps groups to the actual signals to exclude during testing. Can help when the \"features_groups\" defined for ButWhy are not input signals and you want to map them to input signals.</li> <li><code>--measure_regex &lt;string&gt;</code>: A regular expression string (e.g., <code>\"AUC|SENS@FPR_03\"</code>) that specifies which performance metrics to include in the output report.</li> </ul> <p>You can also specify filtering arguments with <code>--bt_filter</code> arguments like we do in FilterSamples</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html#how-it-works","title":"How It Works","text":"<p>The tool uses a flexible two-step process to define which signals to test for importance:</p> <ol> <li>Feature-to-Group Mapping: Reads the <code>--features_groups</code> file to map each feature to a group. Like happens in ButWhy analysis</li> <li>Group-to-Signal Mapping: Uses the <code>--group2signal</code> file to determine the final signals to exclude for each test. If a group name from the <code>features_groups</code> file is found in <code>group2signal</code>, the listed signals are used; otherwise, the group name itself is treated as the signal to test.</li> </ol> <p>This process allows for testing the importance of either individual groups or multiple combined groups at once.</p> <p>For each group defined by this process, the tool performs the following steps:</p> <ol> <li>Cohort Identification: Determines the cohort of samples where the input signal is present within the specified time window. When <code>--no_filtering_of_existence</code> it will just calculate the cohort size and will not filter the samples to that cohort.</li> <li>Performance with Signals: Measures the model's performance on this cohort with all signals included. The result is labeled with a <code>:with_signals</code> suffix.</li> <li>Performance Without Signals: Measures the model's performance on the same cohort after omitting the input signal. This result is labeled with a <code>:no_signals</code> suffix.</li> <li>Difference Calculation: Calculates the performance difference, labeled with a <code>:difference</code> suffix, which quantifies the signal's impact.</li> </ol> <p>All results are stored in a final output table.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html#example-usage","title":"Example Usage","text":"<pre><code>model_signals_importance --model_path my_model.bin --features_groups feature_groups.tsv --group2signal group_mapping.tsv --skip_list Age,Gender --no_filtering_of_existence --time_windows 0,365\n</code></pre> <p>In this example:</p> <ul> <li>The base model is <code>my_model.bin</code>.</li> <li><code>feature_groups.tsv</code> maps features to groups.</li> <li><code>group_mapping.tsv</code> defines which signals to test for each group.</li> <li><code>Age</code> and <code>Gender</code> are skipped.</li> <li>Signals are tested without filtering for their existence in the <code>0,365</code> day window. The report will show how many samples have each signal, but performance is measured on all data, not just those with the signal present. If <code>--no_filtering_of_existence</code> was not given, the performance was assess only with the cohort where the signal presents.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html#output","title":"Output","text":"<p>The tool outputs a report comparing model performance on the full dataset versus performance with each signal group removed. The report typically includes a metric (e.g., AUC) and the performance drop, giving a clear, quantitative measure of each signal\u2019s importance.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/model_signals_importance.html#example-output","title":"Example output","text":"Test Description #samples #Unique_patients #controls #cases AUC_Mean AUC_Lower ACU_Upper SENS@FPR_03_Mean SENS@FPR_03_lower SENS@FPR_03_upper Group:CBC:time_window:365:with_signals 1000000 710053 997015 2985 0.824535 0.817392 0.832693 36.8828 34.9927 38.8223 Group:CBC:time_window:365:no_signals 1000000 710053 997015 2985 0.743143 0.735234 0.751862 14.5724 13.219 15.9436 Group:CBC:time_window:365:difference 1000000 710053 997015 2985 0.0813916 EMPTY EMPTY 22.3104 EMPTY EMPTY <p>For instance, in the example table, removing the CBC signal group resulted in a 0.08 point drop in AUC, highlighting its importance.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Doxygen.html","title":"Doxygen","text":"<p>Automatic HTML Documentation with C++.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Doxygen.html#creating-comments-in-libs","title":"Creating Comments in Libs","text":"<ul> <li>You can create local documentation for your code for MR_LIBS by runnig this:</li> </ul> <pre><code>MR_LIBS/document_code_user.sh\n</code></pre> <p>The documentation will be created in /home/$USER/html/libs/html, please edit the script to change the desired location if needed.  To access, you can just open the index.html file or host the directory with <code>python -m http.server -d /home/$USER/html/libs/html 8000</code> on port 8000</p> <p>The build process of this repository is being executed by runnnig this script: <pre><code>MR_LIBS/document_code_server.sh\n</code></pre> the documentation will be availabe in\u00a0https://Medial-EarlySign.github.io/MR_LIBS</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Doxygen.html#general-use-of-doxygen-tool-for-other-projects","title":"General Use of Doxygen tool for other projects:","text":"<ol> <li>Create a Doxygen configuration file by running (on Linux) to create default config file:</li> </ol> <pre><code>doxygen -g\n</code></pre> <ol> <li>Edit the following lines in the created file Doxyfile:</li> <li>PROJECT_NAME - write project name</li> <li>OUTPUT_DIRECTORY - the output html directory. If empty, docs will be written to html/ in the project directory. For public use, change to /var/www/html/${YOUR_DOCUMENTATION_ROOT_NAME, e.g., \"Libs\"}</li> <li>JAVADOC_AUTOBRIEF = YES</li> <li>OPTIMIZE_OUTPUT_FOR_C = YES</li> <li>QUIET = YES</li> <li>RECURSIVE= YES</li> <li>GENERATE_LATEX = NO</li> <li>The following command will generate html documentation from comments in the code (see next section). Re-run the command if you want newly-added Doxygen comments to be incorporated.</li> </ol> <pre><code>sudo doxygen Doxyfile\n</code></pre> <p>If OUTPUT_DIRECTORY was empty, simply view html/index.html in the project directory with any browser. The public documentation (e.g., for Libs) look for Creating Lib documentation section</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Doxygen.html#how-to-create-documentation-in-code","title":"How to create documentation in code","text":"<ol> <li>An example of documenting class members:</li> </ol> <pre><code>string predictor; ///&lt;the predictor type - same as in the json file: qrf,lightgbm...\n</code></pre> <p>The \"///\" initiates a Doxygen comment and the \"&lt;\" specifies that the comment comes after the element declaration. A second option is to add a comment before the member declaration:</p> <pre><code>\u00a0///the predictor type - same as in the json file: qrf,lightgbm...\nstring predictor;\n</code></pre> <ol> <li>To document a class or a file, put this section before the class declaration:</li> </ol> <p><pre><code>/** ImportanceFeatureSelector(importance_selector) - selector which uses feature importance method for sepcific\n* model to rank the feature importance and select them\n* \n* To Use this selector specify \"importance_selector\"\n*/\n</code></pre> The first paragraph, up to a dot or an empty line, is used for a brief description, which appears at the head of the documentation page. The rest of the comment is placed in a detailed description section that appears further down on the documentation page.</p> <ol> <li>For functions, the following syntax is better: <pre><code>/// &lt;summary&gt;\n/// Compares features created by current instance are compatible to features\n/// &lt;/summary&gt;\n/// &lt;returns&gt;\n/// Null if compatible, otherwise the difference\n/// &lt;/returns&gt;\n</code></pre></li> </ol> <p>For more details, see (external network): https://www.stack.nl/~dimitri/doxygen/manual/docblocks.html</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html","title":"Howto Debug Program in Linux","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html#step-1-compile-your-program-with-debug-symbols-g-flag","title":"Step 1 - compile your program with debug symbols \"-g\" flag","text":"<p>In order to debug program properly, you need to compile the program with debug symbols. to do that you can use \"smake_dbg\" instead of \"smake_rel\" and the new exe will be saved under Linux/Debug. If the debug mode is very slow you can directly edit CMakeList.txt and add \"-g\" flag to the CXX_FLAGS so it will compile with optimizations as in release but you can still debug it</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html#step-2-open-debugger","title":"Step 2 - open debugger","text":"<p>After the compilation has been finished. run</p> <pre><code>gdb EXE_PATH\n</code></pre> <p>or better tool (wrapper of gdb) with \"gui\":</p> <pre><code>cgdb EXE_PATH\n</code></pre> <p>in order to jump between code and command line in the gui - use ESC key to focus on code window (you can than scroll up and down) and \"i\"\u00a0 to return the command prompt</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html#step-3-define-breakpoints-and-usefull-debug-stuff","title":"Step 3 - define breakpoints and usefull debug stuff:","text":"<p>Some usefull commands used to debug (you may shortend the commands, for example instead of \"print\" you may write just \"p\" and \"c\" instead of \"continue\"). remember you have history for your commands even after you close the debugger, so using arrows UP, arrows DOWN to visit history may be usefull.</p> <ul> <li>\"catch th\" - sets breakpoint when first exception is thrown</li> <li>\"b FILE_NAME.cpp:LINE_NUMBER\" - sets breakpoint in line with file. for example: \"b MedModel.cpp:100\" will set breakpoint at line 100 in MedModel.cpp.<ul> <li>hitting just \"b LINENUM\" will set breakpoint in line of current file(of stach trace)</li> <li>Conditional breakpoint - you may specify \"if CONDITION\" in the end of the command to set conditional breakpoint. example:\u00a0\"b MedModel.cpp:100 if pid==5000001\"</li> </ul> </li> <li>\"i b\" - prints all breakpoints you have</li> <li>\"d #NUM_BREAKPOINT\" - deletes breakpoint by number, without number will delete all</li> <li>\"bt\" - prints backtrace of your program to see all program call. example: <pre><code>bt\n#0  medial::io::ProgramArgs_base::parse_parameters (this=this@entry=0x7fffffffd400, argc=argc@entry=1, argv=argv@entry=0x7fffffffd888) at /nas1/UsersData/alon/MR/Libs/Internal/MedUtils/MedUtils/MedUtils.cpp:\n360\n#1  0x00000000004db6d4 in main (argc=1, argv=0x7fffffffd888) at /nas1/UsersData/alon/MR/Tools/Flow/Flow/Flow.cpp:396\n</code></pre></li> <li>\"u\" - go up in the stacktrace (who called the function), \"d\" - do down in the stacktrace (the function the current function called). \"sel #NUM\" - jump to current NUM stack in backtrace. for example \"sel 1\" will jump to first item in stacktrace</li> <li>\"i th\" - info threads prints inforamtion about threads. example: <pre><code>i thr\n  Id   Target Id         Frame\n* 1    Thread 0x7ffff7fcb000 (LWP 14541) \"Flow\" main (argc=1, argv=0x7fffffffd888) at /nas1/UsersData/alon/MR/Tools/Flow/Flow/Flow.cpp:395\n</code></pre></li> <li>\"thr #THREAD_NUM\" - go to different thread for debugging different stack trace in other thread</li> <li>\"r PROGRAM_ARGS\" - to start running the program. you may start the program again at anytime.</li> <li>\"i args\" - prints function arguments</li> <li>\"i locals\" - prints current function scope with all variables</li> <li>\"p VAR_NAME\" - prints variable value. example: <pre><code>p argv[0]\n$2 = 0x7fffffffdc1e \"/server/Linux/alon/bins/Flow\"\n</code></pre></li> <li>\"n\" - next step advance program by 1 command</li> <li>\"s\" - step into - advance program by 1 command but step into if calling new function</li> <li>\"c\" - continue program run</li> <li>\"q\" - to quit debugger</li> <li>stop jump between threads when debugging multiple threads program - \"set scheduler-locking on\"</li> </ul> <p>Special prints of STL library:</p> <p>to access vector (we have old debugger so we can't just access it with\u00a0[]). If we have for example \"vector vec\" variable, to access i index we should use \"vec._M_impl._M_start[i]\" to access this position"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html#bonus-section","title":"Bonus Section","text":"<p>Linux creates crash dump for crashed programs (configuration of which program to run when program crashes can be found here: /proc/sys/kernel/core_pattern). currently uses with abrt tool. the dumps are located in different places (depend on the node, we have for some reason different configuration path for dumps in each node - no good reason). The dumps are deleted when new crash dump is created and when we reach the limit quota for dumps, so don't worry, it won't fill up all our storage space. there is command \"show_crashes.sh\" in BASH_SCRIPTS - that locate those dump location in the current node. to open a crash dump, see backtrace and print variables in the stack use gdb/cgdb to open the dump:</p> <pre><code>sudo cgdb CRASHED_PROGRAM_BIN_PATH CRASH_DUMP_PATH\n# example:\n# $&gt; show_crashes.sh\n# /home/tmp/abrt/ccpp-2019-02-21-09:54:17-13058/coredump  /server/UsersData/alon/MR/Projects/Shared/But_Why/Linux/Release/TryExplain --base_config /server/Work/Users/alon/But_Why/configs/explain_base.cfg\n# $&gt; sudo cgdb /server/UsersData/alon/MR/Projects/Shared/But_Why/Linux/Release/TryExplain /home/tmp/abrt/ccpp-2019-02-21-09:54:17-13058/coredump\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html#change-path-to-different-source-code-folder","title":"Change path to different source code folder","text":"<p>For example: <pre><code>set substitute-path /home/alon-internal/MR_ROOT /nas1/UsersData/Alon/MR\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/General/Howto%20Debug%20Program%20in%20Linux.html#profiler","title":"Profiler","text":"<p>how to debug speed. compile program with -pg flags. g adds debug symbols and p adds gmon.out output for th profiler. after program ends gmon.out output will apear in your current directory. run : <pre><code>gprof PATH_TO_BIN_APP PATH_TO_GMON.OUT | less\n</code></pre></p> <p>set substitute-path /home/foo /tmp/debug/home/foo </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html","title":"Model Validation Checklist","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#data-distribution-and-performance","title":"Data Distribution and Performance","text":"<ul> <li>Analyze sample distribution over time: count controls and cases by year and month.</li> <li>Perform bootstrapping on the validation set (and preferably on a future time set). For the future set, also assess performance on the same patients.<ul> <li>Evaluate performance (AUC and other metrics) across years, months, and time windows.</li> <li>Assess results by age group, sex, and key comorbidities (e.g., diabetes, COPD, CVD).</li> <li>Check minimal membership period and presence/absence of key lab tests, if relevant.</li> </ul> </li> <li>Assess calibration on the same samples used for bootstrapping.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#model-analysis","title":"Model Analysis","text":"<ul> <li>Conduct ButWhy analysis:<ul> <li>Examine global feature importance, with and without grouping signals.</li> <li>Analyze contributions of individual features: for important features, report mean score, outcome, and Shapley value for each value bin.</li> </ul> </li> <li>Evaluate coverage and lift for risk groups at various PR cutoffs. For example, determine the prevalence of COPD patients with hospital admissions and the proportion captured in top x, y, z PR cutoffs.</li> <li>Print feature matrix: report mean and CI/STD for each feature to identify outliers or unreasonable values (can be done on large test/train matrices).</li> <li>Compare matrices across years:<ul> <li>Analyze score distributions over multiple years.</li> <li>Build a propensity model to differentiate between years and identify changing features.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#fairness-and-bias","title":"Fairness and Bias","text":"<ul> <li>Assess fairness and bias:<ul> <li>Without matching: compare across sex, age groups, insurance, race, and socio-demographic factors.</li> <li>With matching: control for important clinical or explanatory features.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#external-and-baseline-validation","title":"External and Baseline Validation","text":"<ul> <li>Validate externally on different datasets.</li> <li>Compare to a simple baseline model: assess not only performance but also which patients are flagged. Use ButWhy analysis to understand population differences.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#sensitivity-and-robustness","title":"Sensitivity and Robustness","text":"<ul> <li>Perform sensitivity analysis:<ul> <li>Add noise to lab values.</li> <li>Shift dates.</li> <li>Remove lab values to simulate missing data.</li> </ul> </li> <li>Ensure the model applies cleaning procedures to all signals.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#applying-to-new-datasets-without-labels","title":"Applying to New Datasets Without Labels","text":"<ul> <li>Compare test matrix to training repository matrix: check feature moments using TestModelExternal or train a propensity model.<ul> <li>Also compare score distributions, both raw and after matching on key factors.</li> </ul> </li> <li>Run ButWhy importance analysis on the test set and compare with the training repository.</li> <li>Report statistics on outliers detected by cleaning procedures.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/index.html#test-kit-for-model-validation","title":"Test Kit for Model Validation","text":"<p>For models in development, external validation with labels, or silent run, see the tools in this repository: https://github.com/Medial-EarlySign/MR_Tools, for example under <code>MR_Tools/AutoValidation</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html","title":"AutoTest","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#motivation","title":"Motivation","text":"<p>AutoTest standardizes and accelerates model validation by providing generic tests and documentation practices. It ensures consistent model quality and simplifies transitions between projects by applying best practices for model testing (e.g., feature importance checks, cleaner verification).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#goals","title":"Goals","text":"<ul> <li>Automate validation of models and data repositories</li> <li>Enable reproducible and standardized testing across environments</li> <li>Support a wide range of validation tasks (fairness, coverage, feature importance, etc.)</li> <li>Provide extensible configuration and resource templates</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#location","title":"Location","text":"<ul> <li>Repository: MR_Tools</li> <li>Path: <code>AutoValidation/kits</code></li> <li>Environment variable: <code>$AUTOTEST_LIB</code> (set to the kits path for convenience)</li> </ul> <p>The <code>kits/</code> directory contains the main test kits and scripts for running validation workflows. Each kit is tailored for a specific validation scenario, such as development, silent run, or external validation after silent run.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#use-cases","title":"Use Cases","text":"<p>AutoTest supports three main scenarios:</p> <ul> <li>Development: Test new models during development.</li> <li>External Silent Run: Run auto tests on datasets without labels/outcomes.</li> <li>External_validation_after_SR: Test datasets with labels/outcomes, extending the silent run with additional analyses and sanity checks.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#creating-a-new-testkit-for-your-model","title":"Creating a New TestKit for Your Model","text":"<p>To set up a new TestKit for your model:</p> <ol> <li>Create an empty directory for your TestKit.</li> <li>Navigate into the directory.</li> <li>Run the generator script:     <pre><code>$AUTOTEST_LIB/generate_tests.sh\n</code></pre>     (<code>AUTOTEST_LIB</code> should point to the cloned MR_Tools repository.)</li> <li>When prompted, select the desired TestKit type: Development, External Silent Run, or External_validation_after_SR. This creates a template directory with example configuration files you can adapt for your model. The <code>Tests</code> directory starts empty; all tests are executed from <code>$AUTOTEST_LIB/$KIT_NAME/Tests</code> unless you override them in your own <code>Tests</code> folder.</li> <li>Configure <code>configs/env.sh</code> and other settings in the <code>configs</code> folder. To override tests for your specific use case, place them in your <code>Tests</code> folder. For generic changes affecting all models, edit the corresponding test in the infrastructure folder (<code>$AUTOTEST_LIB/$KIT_NAME/Tests</code>).</li> <li>Run all tests with:     <pre><code>./run.sh\n</code></pre>     To run a specific test, use:     <pre><code>./run.specific.sh\n</code></pre>     (Without arguments, it lists all available tests and their numbers.)</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#writing-and-executing-tests","title":"Writing and Executing Tests","text":"<p>Each test in the <code>Tests</code> directory receives three arguments:</p> <ol> <li><code>CFG_PATH_OF_CURRENT_KIT</code>: Path to the current kit\u2019s config folder.</li> <li><code>SCRIPT_DIR_OF_INFRA</code>: Path to the AutoTest infrastructure.</li> <li><code>OVERRIDE</code>: Binary flag (0/1) to override previous results.</li> </ol> <p>Tests run sequentially, with unified and per-test logs. After execution, manually verify the output to ensure results are meaningful.</p> <p>Test statuses are tracked in <code>tests_status.log</code>:</p> <ul> <li><code>FINISHED_NEED_VERIFY</code>: Test completed successfully; review and approve/disapprove.</li> <li><code>FINISHED_FAILED</code>: Test completed but failed; rerun after clearing the status.</li> <li><code>FINISHED_VERIFIED</code>: Test completed and verified.</li> <li><code>STOPPED_NEED_VERIFY</code>: Test crashed; review for acceptability.</li> <li><code>STOPPED_FAILED</code>: Test crashed and marked as failed; rerun after clearing the status.</li> <li><code>STOPPED_VERIFIED</code>: Test crashed but marked as OK.</li> </ul> <p>If a test has a <code>*_NEED_VERIFY</code> status, the next run of <code>run.sh</code> will prompt you to approve, disapprove, rerun, or skip the decision.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#configuration","title":"Configuration","text":"<p>Main parameters are defined in <code>configs/env.sh</code>. All tests should use/reuse these arguments. Place external files (e.g., bootstrap JSON) in the config folder for consistency. This makes it easier to reuse parameters across all tests.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#extension","title":"Extension","text":"<p>To add or override tests:</p> <ul> <li>Copy the desired template from <code>Test/Templates</code> (<code>TEMPLATE_TEST.sh</code> for shell, <code>TEMPLATE_TEST.py</code> for Python).</li> <li>Place new or overridden tests in your <code>Tests</code> folder, using the same numeric prefix as the base template to override.</li> <li>Templates specify required arguments from <code>env.sh</code>; reuse and extend as needed.</li> </ul> <p>Guide for writing/adding a new test: Add a new AutoTest</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/index.html#testing-the-testkit","title":"Testing the TestKit","text":"<p>Unit tests for the TestKit are available for LGI in \"Development\", \"External_Silent_Run\", and \"External_validation_after_SR\". See <code>MR_Tools/AutoValidation/test_kit</code> to run the desired TestKit tests.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html","title":"Add a new AutoTest","text":"<p>This page explains how to add a new test to the AutoValidation TestKit. It covers where to place the test, the required metadata, a minimal test template, and examples.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#1-decide-whether-the-test-is-generic-or-specific","title":"1) Decide whether the test is generic or specific","text":"<ul> <li> <p>Generic test: applies to many models and is reusable. Put generic test code under:</p> <p>MR_Tools/AutoValidation/kits/$KIT_TYPE/Tests</p> <p>Replace <code>$KIT_TYPE</code> with the appropriate test kit name (for example, <code>Development</code>, <code>ExternalValidation</code>, etc.).</p> </li> <li> <p>Specific test: applies only to a single model or is an override of a generic test. Place specific tests under the generated testkit path for that model, for example:</p> <p>/path/to/current/testkit_code/Tests</p> <p>(This path is created by <code>$AUTOTEST_LIB/generate_tests.sh</code>.)</p> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#2-get-a-template","title":"2) Get a template","text":"<p>Copy a template from the <code>Templates</code> directory. There are templates for both shell (<code>.sh</code>) and Python (<code>.py</code>) tests. Use the template that matches the test runner you use.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#3-test-contract-what-your-test-should-declare","title":"3) Test contract (what your test should declare)","text":"<ul> <li>REQ_PARAMETERS: a list of required environment/config variables (from <code>configs/env.sh</code>) that must be present when the test runs.</li> <li>DEPENDS: an optional list of other test names this test depends on; the framework will ensure they run first.</li> <li>Exit codes:</li> <li>0 - success</li> <li>1 - missing required parameter</li> <li>2 - the test detected a failure (assertion/internal check)</li> <li>3 - other error or crash</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#4-minimal-python-test-example","title":"4) Minimal Python test example","text":"<p>Save this as a <code>.py</code> test file. It demonstrates the common structure and helper usage.</p> <pre><code>#!/usr/bin/env python\n\n# --- Edit these ---\nREQ_PARAMETERS = ['REPOSITORY_PATH', 'WORK_DIR']  # required environment variables\nDEPENDS = []  # names of tests this test depends on (optional)\n# -------------------\n\nimport os\nimport sys\n\n# argv[1] is the config directory, argv[2] is the base script directory\nsys.path.insert(0, os.path.join(sys.argv[2], 'resources'))\nfrom lib.PY_Helper import *\n\ninit_env(sys.argv, locals())\ntest_existence(locals(), REQ_PARAMETERS, TEST_NAME)\n\n# Write your checks below. Use variables from REQ_PARAMETERS directly (they are injected into locals()).\n\n\n# Example sanity check (replace with your logic)\nimport med\nrep = med.PidRepository()\nrep.read_all(REPOSITORY_PATH, [], ['BDATE'])\nbdate_sig_df = rep.get_sig('BDATE')\n\nif bdate_sig_df['val0'].max() &gt; 19850101:\n    # this indicates too-young patients in dataset (example rule)\n    print('Failure: dataset contains patient with age under expected cutoff')\n    raise Exception(\"Dataset contains patient with age range below 40 years old\")\n    sys.exit(2) # can also use return code \n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#5-tips-for-editing-the-test","title":"5) Tips for editing the test","text":"<ol> <li>Add any required variables to <code>REQ_PARAMETERS</code>. If a required variable is missing at runtime, the test framework will fail with a helpful error.</li> <li>If your test relies on other tests, list them in <code>DEPENDS</code> so they run first.</li> <li>Keep tests small and focused: one logical assertion per test is ideal.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#6-where-to-place-the-file-recap","title":"6) Where to place the file (recap)","text":"<ul> <li>Generic tests: <code>MR_Tools/AutoValidation/kits/$KIT_TYPE/Tests</code></li> <li>Specific tests: the testkit path generated for the model, e.g. <code>/path/to/current/testkit_code/Tests</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Add_a_new_AutoTest.html#7-what-to-document-in-prs","title":"7) What to document in PRs","text":"<ul> <li>Purpose of the test (what it asserts and why).</li> <li>Any new configuration keys added to <code>configs/env.sh</code>.</li> <li>If the test is generic, confirm it has broad applicability and doesn't hardcode model-specific paths.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html","title":"Development Kit","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#overview","title":"Overview","text":"<p>The Development Kit validates newly developed models before integration with AlgoMarker. It ensures your model includes essential components (cleaners, imputers, bootstrap results, etc.) and passes a comprehensive suite of tests using the same dataset as training. For external validation, see the External Silent Run kit.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#goals","title":"Goals","text":"<ul> <li>Ensure model quality and completeness before deployment.</li> <li>Automate validation of key model components.</li> <li>Provide reproducible, standardized testing.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#how-to-use","title":"How to Use","text":"<p>Please refere to Creating a New TestKit for Your Model To run all tests, execute from the created TestKit folder: <pre><code>./run.sh\n</code></pre> Or use <code>run.specific.sh</code> to execute a specific test</p> <p>Review results in your configured output directory.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#configuration","title":"Configuration","text":"<p>Set required parameters in <code>env.sh</code>. If a parameter is missing for a test, that test will be skipped.</p> <ul> <li>REPOSITORY_PATH: Path to your data repository.</li> <li>MODEL_PATH: Path to your trained model.</li> <li>WORK_DIR: Output directory for results.</li> <li>CALIBRATED_MODEL, EXPLAINABLE_MODEL: Optional, for calibration and explainability tests.</li> <li>BT_JSON, BT_COHORT: Bootstrap configuration files.</li> <li>NOISER_JSON, TIME_NOISES, VAL_NOISES, DROP_NOISES: For noise sensitivity analysis.</li> <li>BASELINE_MODEL_PATH, BASELINE_COMPARE_TOP: For baseline comparison.</li> <li>See full parameter list above for details.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#additional-files","title":"Additional Files","text":"<ul> <li>coverage_groups.py: Defines high-risk groups for coverage tests.</li> <li>feat_resolution.tsv: Controls feature resolution for matrix feature tests.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/index.html#test-descriptions","title":"Test Descriptions","text":"<p>Each test in this kit is documented separately:</p> <ul> <li>Test_01 - Train Samples Over Years</li> <li>Test_02 - Test Samples</li> <li>Test_03 - Cleaners</li> <li>Test_04 - Imputers</li> <li>Test_05 - But Why</li> <li>Test_06 - Bootstrap Results</li> <li>Test_07 - Feature Importance</li> <li>Test_08 - Calibration</li> <li>Test_09 - Coverage</li> <li>Test_10 - Matrix Features</li> <li>Test_11 - Matrix Over Years</li> <li>Test_12 - Fairness</li> <li>Test_13 - Model Explainability</li> <li>Test_14 - Noise Sensitivity Analysis</li> <li>Test_15 - Compare to Baseline Model</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html","title":"Test 01: Train Samples Over Years","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#purpose","title":"Purpose","text":"<p>Ensure that training samples are properly distributed across years and months, helping to detect data leakage, temporal bias, or sampling issues in model development.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>TRAIN_SAMPLES_BEFORE_MATCHING</code>: Path to the raw training samples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 1\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Analyzes the distribution of cases and controls by year and by month</li> <li>Flags any unusual patterns, imbalances, or trends in the sample</li> <li>Creates a folder <code>samples_stats.train</code> with:<ul> <li><code>stats.txt</code>: Table and histogram showing how many distinct outcomes each patient has in the samples (e.g., case/control status changes)</li> <li><code>cases_controls_id_histogram.html</code>: Histogram of how many times each patient appears as case/control in the samples</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#output-location","title":"Output Location","text":"<ul> <li>Main log: <code>WORK_DIR/01.test_train_samples_over_years.log</code></li> <li>Additional stats: <code>WORK_DIR/samples_stats.train/</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_01%20-%20test_train_samples_over_years.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Check for balanced distribution of cases/controls across years and months</li> <li>Review histograms for repeated patients and outcome changes</li> <li>Investigate any anomalies, such as unbalanced years or unexpected patient repeats</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html","title":"Test 02: Test Samples","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#purpose","title":"Purpose","text":"<p>Analyze the distribution and integrity of test samples, similar to Test 01, but focused on the evaluation set. This helps ensure the test data is representative and free from temporal or sampling bias.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 2\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Checks the distribution of cases/controls in the test samples by year and month</li> <li>Flags any unusual patterns, imbalances, or trends</li> <li>Creates a folder <code>samples_stats.test</code> with summary statistics and HTML plots</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#output-location","title":"Output Location","text":"<ul> <li>Main log: <code>WORK_DIR/02.test_test_samples_over_years.log</code></li> <li>Additional stats and plots: <code>WORK_DIR/samples_stats.test/</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_02%20-%20test%20samples.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Review distributions for balance and representativeness</li> <li>Use plots and tables to spot anomalies or unexpected patterns</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html","title":"Test 03: Cleaners","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#purpose","title":"Purpose","text":"<p>Verify that all input signals have appropriate data cleaning rules applied before model training and testing. This ensures data quality and consistency throughout the pipeline.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 3\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Checks that every input signal has a defined cleaner rule</li> <li>Flags any signals missing cleaner definitions</li> <li>Fails the test if any required cleaner is missing</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#output-location","title":"Output Location","text":"<ul> <li>Main log: <code>WORK_DIR/03.test_cleaners.log</code> (lists signals missing cleaners)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_03%20-%20test%20cleaners.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>If the log lists missing cleaners, update your configuration to define them</li> <li>The test passes only if all signals have cleaner rules</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html","title":"Test 04: Imputers","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#purpose","title":"Purpose","text":"<p>Ensure that all input signals to the model have defined imputation rules, so missing data is handled consistently and robustly.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 4\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Checks that every signal input to the model has an imputer defined</li> <li>Flags any signals missing imputation rules</li> <li>Fails the test if any required imputer is missing</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#output-location","title":"Output Location","text":"<ul> <li>Main log: <code>WORK_DIR/04.test_imputers.log</code> (lists signals missing imputers)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_04%20-%20test%20imputers.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>If the log lists missing imputers, update your configuration to define them</li> <li>The test passes only if all signals have imputation rules</li> </ul> <p>@@@[PLEASE_COMPLETE]: Add troubleshooting tips and example log output</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html","title":"Test 05: But Why","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#purpose","title":"Purpose","text":"<p>Provide explanations for model predictions using Shapley values, helping users understand which features drive decisions and how feature values influence outcomes.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 5\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Calculates Shapley values for individual predictions</li> <li>Aggregates feature importance statistics</li> <li>Generates visual explanations for both global and local feature effects</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#output-location","title":"Output Location","text":"<p>Results are saved under <code>$WORK_DIR/ButWhy</code>: - <code>Global.html</code>: Global signal importance in the model - <code>Global.ungrouped.html</code>: Global feature importance (ungrouped) - <code>single_features/</code>: Directory containing analysis for each important feature     - For each feature:         - Stratification plots showing how feature values affect model response         - Mean outcome for each feature value (probability of being a case)         - Mean score for each feature value (should align with outcome graph)         - Mean and confidence interval of Shapley value for each feature value (may differ from outcome in some cases)</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_05%20-%20But%20why.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use global plots to identify the most influential features</li> <li>Review single feature analyses to understand how feature values impact predictions and Shapley values</li> <li>Look for expected patterns (e.g., U-shaped risk curves for age in Flu Complications) and ensure the model's logic is reasonable</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html","title":"Test 06: Bootstrap Results","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#purpose","title":"Purpose","text":"<p>Assess model performance stability and reliability using bootstrap resampling. This test estimates variability and confidence intervals for key metrics, ensuring robust and reproducible results.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>PREDS_CV</code> or <code>TEST_SAMPLES</code>: Test samples or cross-validation predictions</li> <li><code>BT_JSON</code>: JSON file for bootstrap analysis</li> <li><code>BT_COHORT</code>: Bootstrap cohort definitions</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 6\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Applies bootstrap analysis to model predictions</li> <li>Calculates performance metrics with variability and confidence intervals</li> <li>Helps verify that results are not due to random chance or overfitting</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#output-location","title":"Output Location","text":"<ul> <li>Results are saved in <code>${WORK_DIR}/bootstrap</code></li> <li>Main output file: <code>bt.pivot_txt</code> (see Bootstrap output legend for details)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_06%20-%20bootstrap%20results.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Review confidence intervals and variability for each metric</li> <li>Use results to assess model robustness and generalizability</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html","title":"Test 07: Feature Importance","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#purpose","title":"Purpose","text":"<p>Assess the importance of each input signal in the model by measuring the impact on performance when the signal is excluded. This approach focuses on signals present in the last year or three years, ensuring that rare but important signals are not undervalued. See model_signals_importance for methodology details.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>BT_JSON_FAIRNESS</code>: JSON file for bootstrap analysis</li> <li><code>FAIRNESS_BT_PREFIX</code>: Bootstrap cohort definition for feature importance focus</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Requires predictions file results from Test_06 (bootstrap results)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 7\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>For each signal, measures the change in model performance (e.g., AUC) when the signal is omitted</li> <li>Focuses analysis on cohorts where the signal exists in the last year/three years</li> <li>Avoids undervaluing rare but important signals</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#output-location","title":"Output Location","text":"<ul> <li>Results file: <code>${WORK_DIR}/ButWhy/feature_importance.sorted_final.tsv</code><ul> <li>For each signal: number of samples with non-missing values, and importance score (impact on AUC if excluded)</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_07%20-%20feature%20importance.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use the sorted importance file to identify key signals driving model performance</li> <li>Check for signals that are rare but have high impact</li> <li>Use results to guide feature selection and model refinement \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html","title":"Test_08 - calibration","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#purpose","title":"Purpose","text":"<p>This test evaluates the calibration performance of a predictive model. Calibration measures how well the predicted probabilities reflect actual outcomes, ensuring the model's probability estimates are reliable and actionable.</p> <p>more about calibration information please refer to Calibrate model, and calibration test</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li>WORK_DIR - output work directory</li> <li>CALIBRATED_MODEL\u00a0- path for calibrated model</li> <li>REPOSITORY_PATH - repository path</li> <li>TEST_SAMPLES_CALIBRATION - test samples for calibration</li> <li>BT_JSON_CALIBRATION - json for bootstrap analysis</li> <li>BT_COHORT_CALIBRATION\u00a0- bootstrap cohort definition to focus on calibration</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 8\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#explanation","title":"Explanation","text":"<p>This test assesses the calibration of the model using calibration curves and calibration metrices like calibration index, R2. It compares predicted probabilities to observed outcomes, typically by binning predictions and plotting the observed mean outcome against expected confidence intervals. Well-calibrated models produce probability estimates that closely match real-world results.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_08%20-%20calibration.html#output","title":"Output","text":"<p>Results are saved in <code>${WORK_DIR}/calibration</code></p> <ul> <li><code>bt.pivot_txt</code>- Bootstrap results of calibration measurement</li> <li>*.html\u00a0 - Calibration graph for each cohort; for each score bin, plots observed mean outcome, expected confidence interval, etc.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html","title":"Test 09: Coverage","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#purpose","title":"Purpose","text":"<p>Verify that the model correctly identifies and flags high-risk groups, ensuring coverage of critical populations as defined by custom rules.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> <li><code>configs/coverage_groups.py</code>: Python file with pandas rules to define high-risk groups</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 9\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Uses custom rules (from <code>coverage_groups.py</code>) to define high-risk cohorts in your data</li> <li>Checks how well the model flags these groups compared to random selection</li> <li>Calculates coverage metrics and lift for each group at multiple score cutoffs</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#output-location","title":"Output Location","text":"<ul> <li>Main log: <code>${WORK_DIR}/09.test_coverage.log</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#example-defining-a-risk-group","title":"Example: Defining a Risk Group","text":"<p>Suppose we want to flag undiagnosed CKD patients with low eGFR (&lt;65): <pre><code># Use getcol to find columns in the feature matrix DataFrame \"df\"\neGFR = getcol(df, 'eGFR_CKD_EPI.last.win_1_360')\n# Define groups in the cohort_f dictionary\ncohort_f['eGFR&lt;65'] = df[eGFR] &lt; 65\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#example-output","title":"Example Output","text":"<pre><code>Cohort eGFR&lt;65 :: Has 19426 patient in cohort out of 605636 (3.2%)\nAnalyze cutoff 1.0% with score 0.51042 =&gt; covered 2246 (11.6%), total_flag 6057 (1.0%), 37.1% has condition in flagged. Lift=11.6\nAnalyze cutoff 3.0% with score 0.28844 =&gt; covered 5906 (30.4%), total_flag 18170 (3.0%), 32.5% has condition in flagged. Lift=10.1\nAnalyze cutoff 5.0% with score 0.19112 =&gt; covered 8761 (45.1%), total_flag 30282 (5.0%), 28.9% has condition in flagged. Lift=9.0\nAnalyze cutoff 10.0% with score 0.09499 =&gt; covered 13650 (70.3%), total_flag 60564 (10.0%), 22.5% has condition in flagged. Lift=7.0\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_09%20-%20coverage.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Review the lift values for each cutoff: high lift means the model flags high-risk patients much more often than random</li> <li>Check the percentage of the cohort covered at each cutoff</li> <li>Use these metrics to validate that the model is useful for identifying key populations</li> </ul> <p>In the example output interpertation: The model targets patients with an eGFR below 65 (a cohort of 19,426 patients, or 3.2% of the total population of 605,636). Using a 1% population cutoff (score\u22650.51042), the model flags 6,057 patients. Within this flagged group:</p> <ul> <li>2,246 patients (or 11.6% of the total eGFR&lt;65 group) are correctly identified.</li> <li>The \"lift\" is 11.6, meaning the flagged patients are 11.6 times more likely to have eGFR&lt;65 than a randomly selected patient.</li> <li>The Precision (or probability of having eGFR&lt;65 in the flagged group) is 37.1% (2,246/6,057). While most flagged patients have eGFR\u226565, over a third have eGFR&lt;65.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html","title":"Test 10: Matrix Features","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#purpose","title":"Purpose","text":"<p>Visualize and analyze the distribution of important model features, helping to detect outliers, unusual values, or \"junk\" data that could affect model performance.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> <li><code>config/feat_resolution.tsv</code>: (Optional) Defines resolution for feature plots</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#depends-on","title":"Depends On","text":"<ul> <li>Test 05: But Why: Uses feature importance to select which features to plot</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 10\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Generates matrix plots for selected features, highlighting their value distributions</li> <li>Uses <code>feat_resolution.tsv</code> (if provided) to control plot resolution</li> <li>Helps identify outliers, missing values, and distribution issues in processed features</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/outputs/features_stats.tsv</code>: Table with statistics for each feature (mean, std, missing value percentage, split by cases/controls)</li> <li><code>${WORK_DIR}/outputs/graphs</code>: Graphs for each feature showing value distributions</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_10%20-%20test%20matrix%20features.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Review feature statistics for reasonable values and balance</li> <li>Inspect graphs for outliers, skewed distributions, or unexpected patterns</li> <li>Use findings to refine feature engineering and improve model robustness</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html","title":"Test 11: Matrix Over Years","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#purpose","title":"Purpose","text":"<p>Analyze how model features change over time, comparing samples from the most recent and least recent prediction dates. This helps detect temporal biases, shifts, or time-sensitive patterns in the data and model.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 11\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Compares feature matrices from the most recent and least recent prediction dates</li> <li>Builds a propensity model to differentiate between samples from different years. Uses TestModelExternal tool for more details.</li> <li>Highlights features that change over time and may introduce bias</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#output-location","title":"Output Location","text":"<ul> <li><code>$WORK_DIR/compare_years/</code><ul> <li><code>Global.html</code>: Most important features in the propensity model (differences between years)</li> <li><code>features_diff/</code>: Graphs comparing each important feature (least recent vs. most recent)</li> <li><code>single_features/</code>: But Why analysis for each important feature in the propensity model</li> <li><code>compare_rep.txt</code>: Text file comparing average values of each feature</li> <li><code>test_propensity.bootstrap.pivot_txt</code>: Propensity model performance metrics</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_11%20-%20test%20matrix%20over%20years.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Review the most important features that differ between years</li> <li>Inspect feature comparison graphs for trends, shifts, or anomalies</li> <li>Use findings to identify and address potential temporal biases in your model</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html","title":"Test 12: Fairness","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#purpose","title":"Purpose","text":"<p>Assess model fairness by comparing performance (e.g., sensitivity) across sensitive groups at the same score cutoff. The goal is to ensure similar probability of being flagged by the model when you are a case, regardless of group (e.g., gender, ethnicity).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> <li><code>BT_JSON_FAIRNESS</code>: Bootstrap JSON for filtering cohorts</li> <li><code>FAIRNESS_BT_PREFIX</code>: Bootstrap cohort definition for fairness testing</li> <li><code>config/fairness_groups.cfg</code>: Defines the groups to compare<ul> <li>Each line: two or more group definitions (bootstrap cohort filter format, separated by <code>|</code>), tab-delimited with display names (also separated by <code>|</code>)</li> <li>Example:     <pre><code>Gender:1,1|Gender:2,2   Males|Females\n</code></pre></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 12\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Uses cohort definitions from <code>fairness_groups.cfg</code> to compare model performance across groups</li> <li>Calculates AUC, sensitivity and specificity, at common cutoffs (5%, 10%)</li> <li>Performs statistical tests (chi-square) to assess significance of differences</li> <li>If unfairness is detected, applies matching (e.g., by age) and re-evaluates fairness</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#output-location","title":"Output Location","text":"<ul> <li><code>$WORK_DIR/fairness/</code><ul> <li><code>fairness_report.tsv</code>: Summary table comparing sensitivity, specificity, and AUC at 5% and 10% cutoffs</li> <li><code>fairness_report.*</code>: Statistical chi-square results for sensitivity differences (one file for each cutoff)</li> <li><code>Graph_fairness</code>: Plots sensitivity vs. score threshold for each group (with confidence intervals)</li> <li><code>graph_matched</code>: Plots after matching (e.g., by age) if fairness is not met</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_12%20-%20fairness.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Look for low chi-square values and similar sensitivity across groups in <code>fairness_report.tsv</code></li> <li>Use <code>Graph_fairness</code> to visually compare sensitivity curves</li> <li>If needed, review matched results in <code>graph_matched</code> to see if fairness improves</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html","title":"Test 13: Model Explainability","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#purpose","title":"Purpose","text":"<p>Explore and interpret model predictions by analyzing real data examples of high-risk patients and identifying the most common reasons for being flagged. The analysis focuses on the top 1000 patients with the highest scores. We want to test both model correctness and our explainability method before production.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>EXPLAINABLE_MODEL</code>: Path to the model with explainability features</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> <li><code>EXPLAIN_JSON</code>: JSON for bootstrap filtering</li> <li><code>EXPLAIN_COHORT</code>: (Optional) Filter to focus on specific explainability samples</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 13\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Uses MES explainability extension to generate explanations for model behavior (method reference)</li> <li>Analyzes the top 1000 high-risk patients to find the most common contributing features</li> <li>Provides statistical summaries and detailed examples for exploration</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#output-location","title":"Output Location","text":"<ul> <li><code>$WORK_DIR/ButWhy/explainer_examples/</code><ul> <li><code>group_stats*.tsv</code>: Summary tables of the most common reasons for high scores (e.g., Smoking, COPD diagnosis, BMI, WBC)</li> <li><code>test_report.*.tsv</code>: Example reports for high-risk patients, showing grouped rows by risk factor from most to least important</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#example-output","title":"Example Output","text":"<p>group_stats*.tsv (summary of common reasons in high-risk patients): <pre><code>Group    Frequency    Percentage    leading_feature_1    feature_frequency_1    leading_feature_2    feature_frequency_2    leading_feature_3\nSmoking    996    99.7    Smoking.Smoking_Years    992    Smoking.Smok_Pack_Years_Max    693    Smoking.Never_Smoker\nICD9_Diagnosis.ICD9_CODE:496    537    53.8    ICD9_Diagnosis.category_dep_set_ICD9_CODE:496.win_0_10950    537    ICD9_Diagnosis.category_dep_set_ICD9_CODE:496.win_0_365    537    \nBMI    405    40.5    BMI.max.win_0_1095    390    BMI.last.win_0_180    358    BMI.max.win_0_180\nWBC    282    28.2    WBC.last.win_0_1095    226    WBC.last.win_0_180    198    WBC.min.win_0_180\nPlatelets    271    27.1    Platelets.last_delta.win_0_1095    193    Platelets.slope.win_0_1095    192    Platelets.min.win_0_180\nAge    200    20    111    1    113    1    116\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#how-to-interpret-this-file","title":"How to Interpret this file","text":"<p>The most influential risk factors identified by the model are listed below, based on how frequently they appear in a patient's top three reasons for a high risk score.</p> <p>We can see that the most important risk factor in that model that repeats itself is Smoking - which appears in 99.7% of the times in top 3 reasons - The leading feature inside is Smoking.Smoking_Years. The next contributer is COPD diagnosis that appears 53.8% of the times in top 3 and than BMI - in 40.5% and then WBC 28.2%</p> <p>test_report.*.tsv (example patient report): <pre><code>pid    time    outcome    pred_0    Tree_iterative_covariance    Code_Description    Specific_Feature_Inside_Group(optional)...\n100192    20100913    1    0.445575    Smoking:=1.51313(27.38%)        Smoking.Smoking_Years(40.13972):=0.94721(64.44%)        Smoking.Never_Smoker( 0):=0.22725(15.46%)\n100192    20100913    1    0.445575    WBC:=0.70804(12.81%)        WBC.min.win_0_180(12.6):=0.151(27.51%)        WBC.last.win_0_1095(17.5):=0.11922(21.72%)        WBC.last.win_0_180(17.5):=0.11428(20.82%)\n... (additional rows for other features)\n\n100192    20100913    1    0.445575    ICD9_Diagnosis.ICD9_CODE:786:=-0.1153(2.09%)    Symptoms_involving_respiratory_system_and_other_chest_symptoms|ICD9_CODE:786    ICD9_Diagnosis.category_dep_set_ICD9_CODE:7866.win_0_365( 0):=-0.03598(72.11%)    \n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#how-to-interpret-this-file_1","title":"How to Interpret this file","text":"<p>This output file uses SHAP values to explain how each variable contributes to a patient's final risk score. For patient 100192, the risk score was 0.445575, which correctly predicted the Case (Outcome=1).</p> <ul> <li>Positive SHAP Values (Risk-Increasing): Indicate features that push the score higher (toward greater risk).</li> <li>Negative SHAP Values (Protective/Risk-Decreasing): Indicate features that push the score lower.</li> </ul> <p>The patient's risk is heavily dominated by two features:</p> <ol> <li>Smoking: This is the main contributor, accounting for 27.38% of the total absolute SHAP value sum with a score of +1.51. The core feature, Smoking_Years, is high at 40.13, confirming a long history of smoking. </li> <li>WBC (White Blood Cell Count): This is the second-largest factor, contributing +0.708 (or 12.81% of the total). The patient's WBC level is elevated (last measured at 17.5), which significantly increases the risk assessment.</li> </ol> <p>Conversely, a specific diagnosis ICD9_Diagnosis.ICD9_CODE:786 (Symptoms_involving_respiratory_system_and_other_chest_symptoms) is shown to be slightly protective with a SHAP value of -0.11. This negative value is due to the patient lacking this diagnosis (feature value of 0) in the preceding 10 years, which mildly reduces the overall calculated risk.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_13%20-%20model%20explainability.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use summary tables to identify the most frequent risk factors among high-risk patients</li> <li>Review individual patient reports to understand which features contribute most to their risk scores</li> <li>Look for expected patterns (e.g., Smoking as a top risk factor in lung cancer) and ensure explanations are clinically meaningful</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html","title":"Test 14: Noise Sensitivity Analysis","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#purpose","title":"Purpose","text":"<p>Evaluate model robustness by testing sensitivity to different types of input noise, including missing values, date shifts, and value perturbations. This helps ensure the model remains stable under real-world data conditions.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to the model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> <li><code>TRAIN_SAMPLES_BEFORE_MATCHING</code>: Path to the training samples</li> <li><code>BT_JSON</code>: Bootstrap JSON for cohort filtering</li> <li><code>BT_COHORT</code>: Bootstrap cohort definition</li> <li><code>NOISER_JSON</code>: Path to noiser JSON config</li> <li><code>TIME_NOISES</code>, <code>VAL_NOISES</code>, <code>DROP_NOISES</code>: Parameters to control the noise levels</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#depends-on","title":"Depends On","text":"<ul> <li>Test 06: Bootstrap Results</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 14\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Applies controlled noise to input data using settings from <code>NOISER_JSON</code>, <code>TIME_NOISES</code>, <code>VAL_NOISES</code>, and <code>DROP_NOISES</code></li> <li>Measures the impact of each type of noise on model predictions and performance</li> <li>Assesses model stability and identifies potential resolution problems</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#output-location","title":"Output Location","text":"<ul> <li><code>$WORK_DIR/test_noiser/results/</code><ul> <li><code>time_analysis.csv</code>: Effect of time noise on model performance at different levels</li> <li><code>value_analysis.csv</code>: Effect of value noise on model performance at different levels</li> <li><code>drop_analysis.csv</code>: Effect of dropping tests/values on model performance at different drop levels</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_14%20-%20noise%20sensitivity%20analysis.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Review CSV files to see how model metrics change as noise increases</li> <li>Use findings to improve model robustness and data preprocessing</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html","title":"Test 15: Compare to Baseline Model","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#purpose","title":"Purpose","text":"<p>Compare your model to a baseline model by evaluating performance, correlation, and which patients are flagged. This analysis highlights differences, improvements, and cases your model captures that the baseline misses.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output directory for results</li> <li><code>MODEL_PATH</code>: Path to your model</li> <li><code>BASELINE_MODEL_PATH</code>: Path to the baseline model</li> <li><code>REPOSITORY_PATH</code>: Path to the data repository</li> <li><code>TEST_SAMPLES</code>: Path to the test samples</li> <li><code>BT_JSON</code>: Bootstrap JSON for cohort filtering</li> <li><code>BT_COHORT</code>: Bootstrap cohort definition</li> <li><code>BASELINE_COMPARE_TOP</code>: Top percentage of predictions to compare (default: 5%)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 15\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Compares top X% of flagged patients between your model and the baseline</li> <li>Calculates performance metrics, overlap scores, and correlation</li> <li>Trains a propensity model to differentiate patients flagged by each model</li> <li>Analyzes which features are used differently by your model vs. baseline</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#output-location","title":"Output Location","text":"<ul> <li><code>$WORK_DIR/ButWhy.baseline/</code>: But Why analysis for baseline model (Global.html, Global.ungrouped.html, single_features)</li> <li><code>$WORK_DIR/bootstrap/bt_baseline_compare.tsv</code>: Bootstrap results comparison</li> <li><code>$WORK_DIR/compare_to_baseline/correlation.txt</code>: Correlation of scores between models</li> <li><code>$WORK_DIR/compare_to_baseline/</code>: Propensity model results<ul> <li><code>Global.html</code>: Most important signals differentiating flagged patients</li> <li><code>single_features/</code>: But Why analysis for each important feature</li> <li><code>compare_rep.txt</code>: Table comparing feature means, std, etc. for flagged patients</li> </ul> </li> <li><code>$WORK_DIR/compare_to_baseline/baseline_coverage_by_mes_$BASELINE_COMPARE_TOP.html</code>: Coverage/intersection of flagged individuals at different cutoffs</li> <li><code>$WORK_DIR/compare_to_baseline/compare_Age_top_flagged.html</code>: Comparison of flagged ages by both models</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/Development%20kit/Test_15%20-%20compare%20to%20baseline%20model.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use overlap and coverage metrics to see how much your model improves on the baseline</li> <li>Review propensity model outputs to understand which features drive differences</li> <li>Check correlation and age comparisons for additional insights</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html","title":"External Silent Run","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#overview","title":"Overview","text":"<p>The External Silent Run toolkit automates validation and analysis for medical models, especially when working with new, unlabeled datasets. It helps ensure your data is ready for model scoring and provides tools to estimate model performance.</p> <p>This documentation guides you through setup, running tests, and interpreting results.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#goals","title":"Goals","text":"<ul> <li>Make sure data is OK for running the model</li> <li>Estimate performances</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#structure","title":"Structure","text":"<ul> <li>configs/: Configuration files and environment setup</li> <li>Tests/: Shell and Python scripts for each validation step</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#how-to-use","title":"How to Use","text":"<p>Refer to Creating a New TestKit for Your Model.</p> <p>To run all tests, execute from your TestKit folder:</p> <pre><code>./run.sh\n</code></pre> <p>To run a specific test:</p> <pre><code>./run.specific.sh\n</code></pre> <p>Review results in your configured output directory.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#available-tests-and-tools","title":"Available Tests and Tools","text":"<ul> <li>Test 01 - Generate Repository</li> <li>Test 02 - Fit Model to Repository</li> <li>Test 03 - Create Samples</li> <li>Test 05 - Compare Repository with Reference Matrix</li> <li>Test 06 - Compare Score Distribution</li> <li>Test 07 - Calc Score KLD</li> <li>Test 08 - Sex Ratio</li> <li>Test 09 - Coverage Special Groups</li> <li>Test 10 - Compare Important Feature</li> <li>Test 11 - Estimate Performances</li> <li>Test 12 - Lab Frequency</li> <li>Test 13 - test_but_why</li> <li>Test 14 - 14.test_model_explainability</li> <li>Test 16 - Sample Dates</li> <li>Test 17 - Estimate Performance from Calibration</li> <li>Test 18 - Analyze Messages</li> <li>Test 19 - Search Missing mappings</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#configuration","title":"Configuration","text":"<p>Set required parameters in <code>env.sh</code>. If a parameter is missing for a test, that test will be skipped.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#data-input-parameters-for-scoring","title":"Data Input Parameters for Scoring","text":"<p>If your data is stored in an AWS S3 bucket, set the following credentials:</p> <ul> <li>AWS_REGION</li> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> <li>AWS_INPUT_PATH: Path to input dataset (\"file_api\" format)</li> <li> <p>AWS_OUTPUT_PATH: Path to output scores generated by AlgoAnalyzer</p> <ul> <li>Example input format:</li> </ul> ID Date Signal Value Unit 1 20251005 Hemoglobin 14.1 mg/dL </li> </ul> <p>If not using AWS S3, set these parameters directly:</p> <ul> <li>SILENCE_RUN_INPUT_FILES_PATH: Path to input dataset (\"file_api\" format)</li> <li>SILENCE_RUN_OUTPUT_FILES_PATH: Path to AlgoAnalyzer output scores<ul> <li>If not available, set to <code>GENERATE</code> to create sample files based on last Hemoglobin tests per patient</li> <li>Use <code>TAKE_JUST_LAST</code> to select only the last Hemoglobin date per patient, or <code>0</code> to analyze all dates</li> </ul> </li> </ul> <p>When using AWS parameters, you do not need to set <code>SILENCE_RUN_INPUT_FILES_PATH</code> or <code>SILENCE_RUN_OUTPUT_FILES_PATH</code>. These files will be downloaded to <code>$WORK_DIR/data</code> for inspection.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#model-parameters","title":"Model Parameters","text":"<ul> <li>ALGOMARKER_PATH: Path to model</li> <li>REFERENCE_MATRIX: Full path to reference matrix for comparison</li> <li>CMP_FEATURE_RES: Comma-separated list of important features and their resolutions (e.g., <code>Age:1,MCH.slope.win_0_1000:0.01</code>)<ul> <li>Feature_Name: Unique string identifying the feature</li> <li>Resolution: Used for plotting value distributions</li> </ul> </li> <li>SCORE_MIN_RANGE: Minimum score range for sex ratio test</li> <li>SCORE_MAX_RANGE: Maximum score range for sex ratio test</li> <li>FILTER_LAST_DATE: Set to <code>0</code> to analyze all dates, or use to filter duplicate runs. If you don't know, keep 0</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/index.html#output-parameters","title":"Output Parameters","text":"<ul> <li>WORK_DIR: Output directory for results</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html","title":"Test 01 - Generate Repository","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#purpose","title":"Purpose","text":"<p>Load input data from the AlgoAnalyzer into a repository for evaluation. This step prepares the dataset for further ETL tests and model validation.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output folder path to process and load the repository</li> <li><code>SILENCE_RUN_INPUT_FILES_PATH</code>: Path to the input data files in \"file_api\" format</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 1\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check the output repository for correct data loading.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#what-this-test-does","title":"What This Test Does","text":"<p>This script loads the new dataset into a structured repository, checks data integrity, and prepares files for further analysis. The repository is the foundation for all subsequent validation steps.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#output-location","title":"Output Location","text":"<ul> <li>Processed dataset: <code>${WORK_DIR}/rep</code></li> <li>ETL log files: <code>${WORK_DIR}/ETL</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#how-to-interpret-results","title":"How to Interpret Results","text":"<p>ETL test results are found in <code>${WORK_DIR}/ETL</code>. For more details, see ETL_WORK_DIR.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#test-results-review","title":"Test Results Review","text":"<p>The main ETL log is <code>${WORK_DIR}/01.generate_repository.log</code>. Use the following approach to review the log:</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#a-high-level-review-of-signal-value-distributions-and-sources","title":"a. High-level review of signal value distributions and sources","text":"<p>Example output:</p> <pre><code>Done testing nulls in signal Hemoglobin\nunit:g/L || KLD (127)= 0.005790, KLD_to_Uniform=0.740022, entory_p=4.105875, grp_cnt=8444, group_counts=1/4\nunit:g/l || KLD (127)= 0.010441, KLD_to_Uniform=0.740022, entory_p=4.105875, grp_cnt=6995, group_counts=2/4\nThere are issues with low range, please have a look (more than factor 3)\n       q  value_0  reference     ratio1    ratio2      ratio\n0  0.001   80.908        7.2  11.237223  0.088990  11.237223\n1  0.010  102.000        9.0  11.333333  0.088235  11.333333\n2  0.100  126.000       11.2  11.250000  0.088889  11.250000\nThere are issues with the median, please have a look (more than factor 2)\n     q  value_0  reference     ratio1    ratio2      ratio\n3  0.5    145.0       13.3  10.902255  0.091724  10.902255\nThere are issues with high range, please have a look (more than factor 3)\n       q  value_0  reference     ratio1    ratio2      ratio\n4  0.900    163.0  15.300000  10.653595  0.093865  10.653595\n5  0.990    178.0  16.799999  10.595239  0.094382  10.595239\n6  0.999    189.0  17.900000  10.558659  0.094709  10.558659\nDone testing values of signal Hemoglobin\n</code></pre> <p>In this example, Hemoglobin data comes from four sources, but only two are large enough for analysis. The first source uses unit <code>g/L</code>, the second <code>g/l</code>. A small KLD value (&lt;&lt; 1) means the source's value distribution is similar to the overall Hemoglobin distribution. You can verify this by reviewing graphs in <code>${WORK_DIR}/ETL/outputs/Hemoglobin</code>, but if the numbers are very small, further review may not be necessary.</p> <p>Lines starting with \"There are issues with\" and the following tables highlight discrepancies:</p> <ul> <li><code>q</code>: Quantile being compared</li> <li><code>value_0</code>: Quantile in current dataset</li> <li><code>reference</code>: Quantile in reference dataset</li> <li><code>ratio1</code>: value_0 / reference</li> <li><code>ratio2</code>: 1 / ratio1</li> <li><code>ratio</code>: max(ratio1, ratio2)</li> </ul> <p>Large ratios (e.g., factor of 10) may indicate mismatched units. The log may suggest how to fix units, such as converting from <code>g/L</code> or <code>g/l</code> to the expected <code>g/dL</code> (as described in the AlgoMarker), and recommend multiplying by 10.</p> <p>Important: If there are mismatches in the input, loading will not fail. Warnings will appear in the log, and it is your responsibility to review and ensure the data is correct.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2001%20-%20Generate%20Repository.html#b-deep-dive-into-important-features","title":"b. Deep dive into important features","text":"<p>Important features for the model are defined in <code>configs/env.sh</code>. For each SIGNAL, detailed outputs include:</p> <ul> <li>Specific test log: <code>ETL/outputs/test.$SIGNAL.log</code></li> <li>Processing logs (e.g., dropped lines): <code>ETL/signal_processings_log/$SIGNAL.log</code></li> <li>Distribution of day, month, year, and value: <code>ETL/signal_processings_log/SIGNAL/batches/</code><ul> <li>If there are multiple batches, an aggregated report appears in <code>ETL/signal_processings_log/SIGNAL</code></li> </ul> </li> </ul> <p>It is recommended to manually check logs and charts for all important features. For example:</p> <ul> <li>Example 1: Monthly distribution of Hemoglobin samples from a dataset prepared in mid-2023. Monthly samples may look suspicious, but the yearly graph shows samples are only from the last year, so more samples in early months are expected.</li> </ul> <p> </p> <ul> <li>Example 2: On the right, a normal distribution of a lab measurement. On the left, unclear 'vibrations'. This may not affect the model, but you should check with the dataset owner to ensure it does not hide a larger issue.</li> </ul> <p> </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html","title":"Test 02 - Fit Model to Repository","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#purpose","title":"Purpose","text":"<p>Take an existing Medial model file and fit (transform) it so it can be applied to a generated repository. This step produces one or more transformed models ready for inference and further validation.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working output folder where repository and model outputs are written</li> <li><code>MODEL_PATH</code>: Path to the input Medial model file to be fitted to the repository</li> <li>Optional: <code>CALIBRATED_MODEL_PATH</code>: Path to an alternative calibrated model to also fit (optional)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 2\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check <code>${WORK_DIR}/model</code> for produced model files and logs.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#what-this-test-does","title":"What This Test Does","text":"<p>This test performs the following high-level actions:</p> <ul> <li>Locates the generated repository under <code>${WORK_DIR}/rep</code> (looks for the <code>.repository</code> file)</li> <li>Creates <code>${WORK_DIR}/model</code> and writes transformed model(s) there</li> <li>Runs the transformation command <code>Flow --fit_model_to_rep</code> (more info on fit_model_to_rep) to:<ul> <li>produce a cleaned model without explainability at <code>${WORK_DIR}/model/model.medmdl</code></li> <li>produce a cleaned version that retains explainability at <code>model.with_explainability.medmdl</code></li> <li>optionally fit a calibrated model if <code>CALIBRATED_MODEL_PATH</code> is provided</li> </ul> </li> <li>Writes action logs and missing-category logs in <code>${WORK_DIR}/model</code></li> <li>Performs simple post-checks to detect missing categories and to verify that the conversion completed successfully by searching the test log for the string \"All OK - Model can be applied on repository\".</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#output-location","title":"Output Location","text":"<ul> <li>Transformed model (without explainability): <code>${WORK_DIR}/model/model.medmdl</code></li> <li>Transformed model (with explainability): <code>${WORK_DIR}/model/model.with_explainability.medmdl</code></li> <li>Calibrated transformed model (if provided): <code>${WORK_DIR}/model/model.calibrated.medmdl</code></li> <li>Action logs: <code>${WORK_DIR}/model/actions.log</code> and <code>actions.with_explanability.log</code> (and <code>.calibrated.log</code> if applicable)</li> <li>Missing categories logs: <code>${WORK_DIR}/model/missing_categ.log</code> (and <code>.with_explanability</code> / <code>.calibrated</code> variants)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#how-to-interpret-results","title":"How to Interpret Results","text":"<p>The script performs basic validation and emits logs. Review the following to ensure the model was fitted correctly:</p> <ul> <li>Missing categories:</li> <li>Check <code>${WORK_DIR}/model/missing_categ.log</code>. Any lines containing <code>MISSING_CODE_VALUE</code> indicate categories referenced by the model but absent from the repository. The test prints a failure message with the number of missing categories if any are found.</li> <li>Conversion actions and errors:</li> <li>Review <code>${WORK_DIR}/model/actions.log</code> and <code>${WORK_DIR}/${TEST_NAME}.log</code> for details of transformations and any warnings or errors.</li> <li>Final success marker:</li> <li>The script looks for the exact text <code>All OK - Model can be applied on repository</code> in <code>${WORK_DIR}/${TEST_NAME}.log</code>. If not found, the test exits with code 2 and prints a message pointing to logs above.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Missing categories (MISSING_CODE_VALUE):<ul> <li>This typically means the model expects categorical values not present in the repository. Check data preprocessing and mapping configuration; consider adding mappings or ensuring training categories exist in the input data.</li> </ul> </li> <li>Conversion actions in <code>actions.log</code>:<ul> <li>Inspect <code>actions.log</code> to see what transformations were applied and whether any replacements, drops, or heuristic fixes were used.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#example-output-snippets","title":"Example output snippets","text":"<p>1) When missing categories are detected, the script will print:</p> <pre><code>Failed has N missing categories - please refer to ${WORK_DIR}/model/missing_categ.log\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script calls <code>Flow --fit_model_to_rep</code> multiple times with different flags to produce model variants. The important flags visible in the script are <code>--cleaner_verbose -1</code>, <code>--remove_explainability</code> (0/1), and log path flags.</li> <li>The test expects the environment to provide <code>WORK_DIR</code>, <code>MODEL_PATH</code>, and optionally <code>CALIBRATED_MODEL_PATH</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If <code>Flow</code> is not found on your PATH, ensure the Medial runtime and tools are installed and available in the environment used by the TestKit. You can also modify PATH in <code>configs/env.sh</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2002%20-%20Fit%20Model%20to%20Repository.html#test-results-review","title":"Test Results Review","text":"<p>Primary logs to review after running this test:</p> <ul> <li><code>${WORK_DIR}/model/actions.log</code></li> <li><code>${WORK_DIR}/model/missing_categ.log</code></li> <li><code>${WORK_DIR}/${TEST_NAME}.log</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html","title":"Test 03 - Create Samples","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#purpose","title":"Purpose","text":"<p>Generate sample cohorts from the prepared repository or from an external samples file. These samples are used by subsequent tests for scoring and evaluation.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where the repository and Samples outputs will be written</li> <li><code>SILENCE_RUN_OUTPUT_FILES_PATH</code>: Either the special value <code>GENERATE</code> (to generate samples from the repository) or a path to an input TSV/CSV file containing sample definitions</li> <li><code>TAKE_JUST_LAST</code>: Applicable when <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> is <code>GENERATE</code>. If set to <code>1</code> will filter and take only most recent Hemoglobin lab test date as candidate for the analysis, otherwise will use all Hemoglobin dates for the analysis.</li> <li><code>FILTER_LAST_DATE</code>: The reference matrix contains multiple dates for each patient. If provided <code>1</code> will filter and take only most recent date for each patient. Might be better analsys if that's what we are doing in the client/in this dataset.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 3\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check <code>${WORK_DIR}/Samples</code> for produced sample files.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#what-this-test-does","title":"What This Test Does","text":"<p>Behavior depends on <code>SILENCE_RUN_OUTPUT_FILES_PATH</code>:</p> <ul> <li>If <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> is <code>GENERATE</code>:<ul> <li>Reads the repository located under <code>${WORK_DIR}/rep</code> (looks for <code>*.repository</code>)</li> <li>Generates a samples file at <code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> <li>Two generation modes exist (controlled by <code>TAKE_JUST_LAST</code> in the script):<ul> <li>If <code>TAKE_JUST_LAST</code> &gt; 0: for each patient take only the last Hemoglobin record</li> <li>Otherwise: take all Hemoglobin records for each patient</li> </ul> </li> <li>Copies the generated file to <code>${WORK_DIR}/Samples/1.all_potential.samples</code></li> </ul> </li> <li> <p>If <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> is a path to an existing file:</p> <ul> <li>The script converts the input file into the internal <code>SAMPLE</code> format, transforming dates from <code>DD-MMM-YYYY</code>-style strings into integer yyyymmdd timestamps and sorting samples.</li> <li>Produces <code>${WORK_DIR}/Samples/test.bf.samples</code> and <code>${WORK_DIR}/Samples/test.bf.orig.preds</code> (original predictions preserved)</li> <li>If <code>${WORK_DIR}/ETL/FinalSignals/ID2NR</code> exists, the script remaps identifiers and writes <code>${WORK_DIR}/Samples/3.test_cohort.samples</code> and <code>${WORK_DIR}/Samples/test.orig.preds</code>. Otherwise it symlinks the generated files.</li> </ul> </li> <li> <p>The script also prepares <code>${WORK_DIR}/ref_matrix</code> either by symlinking <code>REFERENCE_MATRIX</code> or by filtering it to the last date when <code>FILTER_LAST_DATE</code> &gt; 0.</p> </li> <li>Finally it runs <code>samples_by_year.sh</code> on the generated cohort file to produce year-based summaries.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#output-location","title":"Output Location","text":"<ul> <li>Main cohort samples: <code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> <li>All potential samples (copy): <code>${WORK_DIR}/Samples/1.all_potential.samples</code></li> <li>Intermediate samples: <code>${WORK_DIR}/Samples/test.bf.samples</code></li> <li>Original predictions file: <code>${WORK_DIR}/Samples/test.bf.orig.preds</code> and/or <code>${WORK_DIR}/Samples/test.orig.preds</code></li> <li>Reference matrix (symlink or filtered): <code>${WORK_DIR}/ref_matrix</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Verify all generated files are non empty with just headers.</li> <li>Verify there are no errors and the execution finished successfully.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Input file formatting errors:<ul> <li>If the external file isn't the expected format (columns in different order or different date format), the awk parsing and date conversion will produce incorrect times. Confirm column positions and pre-normalize the file if needed.</li> </ul> </li> <li>Missing repository or incorrect <code>*.repository</code> file:<ul> <li>If generation mode is selected but the repository isn't present or contains unexpected schema, the process will fail</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#example-output-snippets","title":"Example output snippets","text":"<p>1) Generated sample header and a sample line:</p> <pre><code>EVENT_FIELDS    id  time    outcome outcomeTime split\nSAMPLE  12345   20230115    0   20990101    -1\n</code></pre> <p>2) When <code>ID2NR</code> mapping is used, final lines preserve the remapped ID as the second column.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script uses several small utilities and conventions from the TestKit (<code>Flow</code>, <code>paste.pl</code>, <code>samples_by_year.sh</code>). Ensure these helper scripts are available on PATH or in <code>configs/env.sh</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2003%20-%20Create%20Samples.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> <li><code>${WORK_DIR}/Samples/test.bf.samples</code></li> <li><code>${WORK_DIR}/Samples/test.orig.preds</code> (when present)</li> <li><code>${WORK_DIR}/ref_matrix</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html","title":"Test 05 - Compare Repository with Reference Matrix","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#purpose","title":"Purpose","text":"<p>Compare the generated repository against a reference matrix to quantify dataset differences and identify features that drive separation. This helps estimate likely impacts on model performance and prioritise follow-up checks.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Output folder path where compare outputs will be written</li> <li><code>MODEL_PATH</code>: Path to the fitted model (the script uses <code>${WORK_DIR}/model/model.medmdl</code> if present)</li> <li><code>REFERENCE_MATRIX</code>: Full path to the reference matrix (feature matrix from model training)</li> <li><code>CMP_FEATURE_RES</code>: Comma-separated list of important features and resolutions (e.g., <code>Age:1,MCH.slope.win_0_1000:0.01</code>). Used to build a restricted separation model limited to important features.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 5\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Outputs appear under <code>${WORK_DIR}/compare</code> and <code>${WORK_DIR}/compare.no_overfitting</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#what-this-test-does","title":"What This Test Does","text":"<p>Using the TestModelExternal workflow, the test performs two comparisons:</p> <ul> <li>A full comparison using all model features (output: <code>${WORK_DIR}/compare</code>).</li> <li>A restricted comparison that uses only the important features derived from <code>CMP_FEATURE_RES</code> (output: <code>${WORK_DIR}/compare.no_overfitting</code>).</li> </ul> <p>For each comparison the test:</p> <ul> <li>Computes per-feature statistics (mean, std, missing counts) and statistical tests (Mann\u2013Whitney U)</li> <li>Trains a separation/propensity model to measure how separable the new dataset is from the reference (reports AUC and related metrics)</li> <li>Produces a Shapley-style feature importance report for the separation model (<code>shapley_report.tsv</code>) and per-feature plots (<code>features_diff</code>)</li> <li>Creates HTML diagnostic reports; the script prepares a local Plotly HTML template and patches script paths so generated HTML is portable within the docs site</li> </ul> <p>The compare run is guarded by timestamp checks and will skip re-running if outputs are newer than inputs, unless overridden by the test environment.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/compare</code> - full comparison (all features)</li> <li><code>${WORK_DIR}/compare.no_overfitting</code> - restricted comparison using features listed in <code>CMP_FEATURE_RES</code></li> </ul> <p>Key artifacts inside each directory:</p> <ul> <li><code>compare_rep.txt</code> - per-feature textual summary and tests</li> <li><code>test_propensity.bootstrap.pivot_txt</code> - separation model / propensity results (AUC etc.)</li> <li><code>shapley_report.tsv</code> - feature importance ranking for the separation model</li> <li><code>features_diff/</code> - plots comparing distributions for top differing features</li> <li><code>ButWhy/</code> - HTML dashboards produced from the shapley report</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#how-to-interpret-results","title":"How to Interpret Results","text":"<p>The goal is to detect distributional shifts that might affect model performance and to identify the features responsible. Use the outputs as follows:</p> <ul> <li><code>compare_rep.txt</code> contains two sections: a summary block and a tabular section with per-feature stats and Mann-Whitney p-values. The tabular section is typically loaded into a DataFrame for analysis.</li> <li>Mann-Whitney p-values point to distributional differences but do not capture all changes (e.g., variance differences with similar medians). Treat p-values as a signal for inspection, not automatic rejection.</li> <li><code>test_propensity*</code> outputs report how separable the datasets are. AUC ~ 0.5 indicates little separation (preferred). Very high AUC (e.g., ~1.0) indicates strong separation and suggests problems for matching-based performance estimation in next steps.</li> <li><code>shapley_report.tsv</code> and <code>features_diff/</code> plots are used to prioritise features for investigation. Always check <code>Age</code> first as many differences can be age-proxies.</li> </ul> <p>When AUC is high, prefer the restricted results in <code>compare.no_overfitting</code> (which isolates important features) to assess likely impact on model performance.</p> <ul> <li><code>AUC_Mean</code> close to 0.5 is desirable - values near 1.0 require investigation.</li> <li>Use <code>shapley_report.tsv</code> to find which features drive separation. If those features are important to the model, further action is needed.</li> <li>Visualize suspect features using <code>features_diff</code> plots to confirm distributional shifts and check for imputation artifacts.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#suggested-code-to-analysis-compare_reptxt","title":"Suggested code to analysis compare_rep.txt","text":"<p>Run the following code to turn compare_rep.txt into readable dataframes.</p> <p><pre><code>f = os.path.join(DIR, 'compare/compare_rep.txt')\nt2 = pd.read_csv(f, sep= '\\r')\nt2 = t2[t2[t2.columns[0]].map(lambda x: x[0:3])=='MAN']\ncut = t2.index.min()\nt21 = pd.read_csv(f, nrows=cut)\nt21['status'] = t21.index.map(lambda x: x[0].split(' ')[0])\nt21['feature'] = t21.index.map(lambda x: x[0].split('::')[0].rstrip().split(' ')[-1])\nt21['TRAIN mean'] = t21.index.map(lambda x: x[0].rstrip().split('mean=')[1])\nt21['TRAIN std'] = t21.index.map(lambda x: x[1].rstrip().split('=')[1])\nt21['TRAIN miss_cnt'] = t21.index.map(lambda x: x[2].rstrip().split('=')[1].split(\"|\")[0])\nt21['TEST mean'] = t21.index.map(lambda x: x[2].rstrip().split('mean=')[1])\nt21['TEST std'] = t21[t21.columns[0]].map(lambda x: x.rstrip().split('=')[1])\nt21['TEST miss_cnt'] = t21[t21.columns[1]].map(lambda x: x.split('|')[0].split('=')[1].rstrip())\nt21['mean_diff_ratio'] = t21[t21.columns[1]].map(lambda x: x.split('mean_diff_ratio=')[1].split('|')[0].rstrip())\nt21['IMP'] = t21[t21.columns[1]].map(lambda x: x.split(' ')[-1])\ncols = ['status', 'feature', 'TRAIN mean', 'TRAIN std', 'TRAIN miss_cnt', 'TEST mean', 'TEST std', 'TEST miss_cnt', 'mean_diff_ratio', 'IMP'] \nt21 = t21[t21.status=='BAD'][cols].reset_index(drop=True)\n\u00a0\nt22 = pd.read_csv(f, skiprows=cut+1, sep='\\t')\n</code></pre> The first dataframe, t21, should be ignored. t21 shows moments, range and missing values count, for every feature, comparing the reference to the new dataset.  The second dataframe, t22, shows the same information (without range), plus Mann Whitney test result.</p> <ul> <li>_1 is for the new dataset</li> <li>_2 is for the reference</li> <li> <p>The Mann-Whitney U Test assesses whether two sampled groups are likely to derive from the same population, but note test limitations - if median and shape are the same for both samples, P_value would be high even for different std/scale.\u00a0  In table t22: t21 shows moments, range, and missing value counts for every feature, comparing reference to new dataset.</p> </li> <li> <p>We need to make sure we don't see low P_value for any important feature to the model, or proxy for such features, i.e.. we may list\u00a0MCH.min.win_0_180 as important feature, and we don't want it or MCH.min.win_0_360 to have low P-value.</p> </li> <li>We need to understand the reasons for the low P-value when happened, in order to better understand the new data set. For instance, in the table above, we see that in the tested dataset RDW was not given close to sample point, probably because it is not part of the panel. As RDW is not an important signal, we can ignore it.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#example","title":"Example","text":"<p>AUC_Mean is 0.98 - very high. However no significant different seen in compare_rep.txt  Here is the Graph of a specific signal with big difference form shapley analysis:</p> <p> We see that the reference has several dominant values and this is due to imputations.</p> <ul> <li>In this case we use as reference a dataset different from the one we use for model training and feature importance.</li> <li>However, the reference dataset has many missing values for the relevant signal and imputations were generated that resulted in a binned, different resolution and distribution of the feature. It is of cource undesired situation, but not neccaraly very bad. The model was performing well in the dataset in spite of those missing values and imputations.</li> <li>Lesson learned is that we need to use as reference the original dataset (test samples only).</li> </ul> <p>The reference matrix was generated again and than the AUC of this comparising dropped significantly.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>A temporary Plotly-based HTML template is created at <code>${WORK_DIR}/tmp/plot_with_missing.html</code> and patched so HTML reports reference <code>js/plotly.js</code> relative to the documentation output.</li> <li><code>CMP_FEATURE_RES</code> is parsed into a feature list (<code>${WORK_DIR}/tmp/feat_list</code>) which the restricted comparison uses.</li> <li>The test uses subsampling limits (<code>--sub_sample_test</code>, <code>--sub_sample_train</code>, <code>--sub_sample_but_why</code>) to bound runtime and memory; these can be tuned in the script.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If <code>TestModelExternal</code> or <code>feature_importance_printer.py</code> are missing, make sure the TestKit resources are available on PATH in <code>configs/env.sh</code>.</li> <li>Slow runs or memory errors: reduce subsample sizes (<code>MAX_SIZE</code> in the script) or increase available memory for the Python process.</li> <li>Very high AUC but no obvious per-feature differences: check whether irrelevant features produce separation (use <code>compare.no_overfitting</code>) or whether imputation/binned reference values are causing artifacts.</li> <li>Broken HTML reports: confirm <code>${WORK_DIR}/tmp/plot_with_missing.html</code> exists and that <code>js/plotly.js</code> is reachable relative to the generated report locations.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2005%20-%20Compare%20Repository%20with%20Reference%20Matrix.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/compare/compare_rep.txt</code></li> <li><code>${WORK_DIR}/compare/test_propensity.bootstrap.pivot_txt</code></li> <li><code>${WORK_DIR}/compare/shapley_report.tsv</code> and <code>${WORK_DIR}/compare/ButWhy/</code></li> <li><code>${WORK_DIR}/compare/features_diff/</code></li> </ul> <p>And for the restricted comparison:</p> <ul> <li><code>${WORK_DIR}/compare.no_overfitting/compare_rep.txt</code></li> <li><code>${WORK_DIR}/compare.no_overfitting/shapley_report.tsv</code> and <code>.../ButWhy/</code></li> </ul> <p>If additional examples or parsing snippets are desired (for example, the Python snippet used historically to parse <code>compare_rep.txt</code>), add them here. @@@[PLEASE_COMPLETE]</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html","title":"Test 06 - Compare Score Distribution","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#purpose","title":"Purpose","text":"<p>Compare model score distributions between the reference dataset (training matrix) and the new test repository. This validates that score moments, cutoffs, and distributional shape are consistent and helps detect shifts that could affect model performance.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code>:</p> <ul> <li><code>WORK_DIR</code>: Working directory where repository, samples, predictions, and compare artifacts are stored</li> <li><code>REFERENCE_MATRIX</code>: Full path to the reference feature matrix (used to recreate reference predictions)</li> <li><code>MODEL_PATH</code>: Path to the fitted Medial model (the test will default to <code>${WORK_DIR}/model/model.medmdl</code>) and depends on Test 02</li> <li>Optional environment controls used by the TestKit: <code>MEMORY_LIMIT</code> (limits model memory usage), and the samples created by Test 03 under <code>${WORK_DIR}/Samples</code> (especially <code>3.test_cohort.samples</code> and <code>1.all_potential.samples</code>)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 6\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check <code>${WORK_DIR}/compare</code> for produced score reports and HTML visualizations.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#what-this-test-does","title":"What This Test Does","text":"<p>High-level flow:</p> <ul> <li>Ensures <code>MODEL_PATH</code> is set (uses <code>${WORK_DIR}/model/model.medmdl</code> by default)</li> <li>Generates model predictions for the full potential sample set (<code>${WORK_DIR}/predictions/all.preds</code>) and for the test cohort (<code>${WORK_DIR}/compare/3.test_cohort.preds</code>) using Flow get_model_preds</li> <li>Converts <code>REFERENCE_MATRIX</code> rows into a predictions-like TSV (<code>${WORK_DIR}/compare/reference.preds</code>) when needed</li> <li>Computes basic score moments (mean, STD) for Reference, Test_Run, and Test_Run.Original (if available) and writes them to <code>${WORK_DIR}/compare/compare_score.txt</code></li> <li>Produces a binned score distribution TSV <code>${WORK_DIR}/compare/score_dist.tsv</code> and an HTML histogram <code>${WORK_DIR}/compare/score_dist.html</code></li> <li>Runs <code>compare_scores.py</code> (helper script) to compute additional statistics and saves its outputs under <code>${WORK_DIR}/compare</code></li> <li>Runs bootstrap analyses (bootstrap_app) to compute score-related performance measures and formats them with <code>bootstrap_format.py</code> into <code>compare_score.txt</code></li> </ul> <p>The script includes timestamp checks and will avoid re-running expensive prediction steps when outputs are up to date.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/compare/compare_score.txt</code> - textual summary with means, stds and formatted bootstrap measures</li> <li><code>${WORK_DIR}/compare/score_dist.tsv</code> - binned score distribution (Reference vs Test_Run)</li> <li><code>${WORK_DIR}/compare/score_dist.html</code> - histogram visualization (Plotly)</li> <li><code>${WORK_DIR}/compare/3.test_cohort.preds</code> and <code>${WORK_DIR}/predictions/all.preds</code> - prediction files used for analysis</li> <li>Bootstrap reports: <code>${WORK_DIR}/compare/bt.*.pivot_txt</code> and related formatted outputs</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Compare the mean and STD lines in <code>${WORK_DIR}/compare/compare_score.txt</code> for <code>Reference</code>, <code>Test_Run</code>, and <code>Test_Run.Original</code>. <code>Test_Run</code> and <code>Test_Run.Original</code> should be nearly identical (exactly identical is expected in most cases). This compares AlgoAnalyzer scores and our execution of scores. Minor difference might exists due to floating point percision implemnetation of AlgoAnalyzer. There might be difference from <code>Reference</code>.</li> <li>If <code>Test_Run</code> and <code>Test_Run.Original</code> differ, the test prints Pearson and Spearman correlations and RMSE (helpful to detect computation mismatches). Small numeric differences may occur in some historical configurations (e.g., LGI).</li> <li>Inspect <code>score_dist.html</code> to visually compare the two score histograms <code>Reference</code> and <code>Test_Run</code>. Large shifts or multimodal changes may indicate distributional drift or population differences.</li> <li>Review bootstrap measures (<code>SCORE@PR</code>) to see changes in relevant score cutoffs (for example, top-percentile performance).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Missing samples/predictions: Ensure Test 03 ran successfully and <code>${WORK_DIR}/Samples/3.test_cohort.samples</code> exists. If not, re-run sample generation.</li> <li><code>Flow</code> not found or model load issues: Verify the Medial runtime and <code>Flow</code> binary are available in PATH and that <code>MODEL_PATH</code> points to a valid model file.</li> <li>Memory errors during prediction: Set <code>MEMORY_LIMIT</code> in <code>configs/env.sh</code> or increase available memory on the host. The script will inject a model-change flag to limit in-memory data if <code>MEMORY_LIMIT</code> is set.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#example-output-snippets","title":"Example output snippets","text":"<p>Contents of <code>${WORK_DIR}/compare/compare_score.txt</code> (example):</p> <pre><code>Test    Mean    STD\nReference   0.1234  0.0456\nTest_Run    0.1240  0.0460\nTest_Run.Original   0.1240  0.0460\n...\n</code></pre> <ul> <li>Score distribution compare: </li> <li>Age distribution compare: </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script patches a Plotly HTML template so the generated <code>score_dist.html</code> references <code>js/plotly.js</code> relative to the docs/site structure.</li> <li>Reference predictions are synthesized from <code>REFERENCE_MATRIX</code> by extracting columns assumed to contain the predicted score (awk transformation used in script).</li> <li><code>compare_scores.py</code> (in <code>resources/lib</code>) performs more detailed numeric comparisons and writes additional logs under <code>${WORK_DIR}/compare</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If HTML plot is blank or doesn't render, confirm <code>${WORK_DIR}/tmp/plotly_graph.html</code> exists and that <code>js/plotly.js</code> is available at the path expected by the patched HTML template.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2006%20-%20Compare%20Score%20Distribution.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/compare/compare_score.txt</code></li> <li><code>${WORK_DIR}/compare/score_dist.html</code> and <code>${WORK_DIR}/compare/score_dist.tsv</code></li> <li><code>${WORK_DIR}/predictions/all.preds</code> and <code>${WORK_DIR}/compare/3.test_cohort.preds</code></li> <li>Bootstrap reports: <code>${WORK_DIR}/compare/bt.*.pivot_txt</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html","title":"Test 07 - Calculate Score Kullback\u2013Leibler Divergence (KLD)","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#purpose","title":"Purpose","text":"<p>Compute the Kullback\u2013Leibler Divergence (KLD) between the reference score distribution and the new test run score distribution. This quantifies how different the two score histograms are and provides related summary metrics used for quick drift detection.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working directory where <code>compare/score_dist.tsv</code> and other compare artifacts are stored</li> <li>The test depends on Test 06 (score distributions) and expects <code>${WORK_DIR}/compare/score_dist.tsv</code> to exist</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 7\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check <code>${WORK_DIR}/compare</code> for the printed KLD summary and related outputs created by Test 06.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#what-this-test-does","title":"What This Test Does","text":"<p>Based on <code>07.calc_score_kld.py</code>:</p> <ul> <li>Reads <code>${WORK_DIR}/compare/score_dist.tsv</code> which contains binned score percentages for <code>Reference</code> and <code>Test_Run</code></li> <li>Converts the two distributions into aligned probability vectors</li> <li>Calls an internal helper <code>calc_kld</code> to compute:<ul> <li>Number of bins</li> <li>KLD between reference and test distributions</li> <li>KLD to a uniform distribution (KLD_to_Uniform)</li> <li>entropy_p (a summary entropy-based metric)</li> </ul> </li> <li>Prints a short one-line summary recommending to review <code>${WORK_DIR}</code> for details</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/compare/score_dist.tsv</code> - input to this test (binned distributions)</li> <li>Script prints a summary line to stdout; check the test log <code>${WORK_DIR}/07.calc_score_kld.log</code> (or the general test runner log) for the KLD output</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>KLD is a non-symmetric measure of how one distribution diverges from another. Higher KLD indicates greater difference.</li> <li><code>KLD_to_Uniform</code> gives a sense of how concentrated the distribution is relative to uniform; very small values mean the distribution is close to uniform. The score distribution is not suppose to be uniform, but it gives you a sense of what is a \"high\" KLD value. </li> <li><code>entropy_p</code> is an entropy-derived metric used as an auxiliary indicator.</li> </ul> <p>Practical guidance:</p> <ul> <li>Small KLD (close to zero) indicates little distributional change in scores.</li> <li>Large KLD (order of magnitude larger than typical baselines for your model) should trigger deeper inspection (compare score histograms and top contributing factors such as age or missing signals).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#example-output-snippet","title":"Example output snippet","text":"<p>Example printed line from the script:</p> <pre><code>KLD (20)= 0.005790, KLD_to_Uniform=0.740022, entory_p=4.105875\n</code></pre> <p>This indicates the computation used 20 bins and returned the three summary metrics.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script uses a small epsilon (1e-4) to avoid log-of-zero issues when computing KLD.</li> <li>It relies on the helper function <code>calc_kld</code> from the project's <code>lib.PY_Helper</code> module which handles bin alignment and normalization.</li> <li>If <code>score_dist.tsv</code> is missing or malformed, the script will fail - ensure Test 06 produced valid binned distributions first.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you get a file-not-found error for <code>score_dist.tsv</code>, re-run Test 06 and confirm the file exists and has <code>Test</code>, <code>score</code>, and <code>Percentage</code> columns.</li> <li>If KLD values are extremely large, inspect the two histograms (<code>score_dist.html</code> from Test 06) to see which score bins differ and whether one distribution contains zero mass for bins present in the other. Consider re-binning or smoothing if needed.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2007%20-%20Calc%20Score%20KLD.html#test-results-review","title":"Test Results Review","text":"<p>Primary items to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/compare/score_dist.tsv</code></li> <li><code>${WORK_DIR}/compare/score_dist.html</code></li> <li>The test runner log containing the printed KLD summary (e.g., <code>${WORK_DIR}/07.calc_score_kld.log</code>)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html","title":"Test 08 - Sex Ratio","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#purpose","title":"Purpose","text":"<p>Assess sex balance (male/female ratio) among flagged/high-score subjects and compare the test run to the reference dataset across score cutoffs. This detects potential sex-based disparities in model outputs.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working directory containing compare artifacts generated by Tests 05 and 06</li> <li><code>SCORE_MIN_RANGE</code>, <code>SCORE_MAX_RANGE</code>: Numeric bounds (0..1) specifying the score range to inspect (the script steps by 0.01 internally)</li> <li>The test depends on Tests 05 and 06 and expects the following files to exist:<ul> <li><code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code></li> <li><code>${WORK_DIR}/compare/rep_propensity.matrix</code></li> <li><code>${WORK_DIR}/compare/3.test_cohort.preds</code></li> <li><code>${WORK_DIR}/compare/reference.preds</code></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 8\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Loads the propensity matrices generated by Test 05 and prediction files generated by Test 06</li> <li>Joins the propensity rows with sample IDs and then with model predictions so every sample has <code>pred_0</code> (score) and <code>Gender</code> fields</li> <li>Builds a list of score cutoffs between <code>SCORE_MIN_RANGE</code> and <code>SCORE_MAX_RANGE</code> stepping by 0.01</li> <li>For each cutoff calculates:<ul> <li><code>flagged_males</code>: number of males in the test set with <code>pred_0</code> &gt;= cutoff</li> <li><code>total_mark</code>: total number of flagged subjects in the test set at that cutoff</li> <li><code>ref_fl_males</code>: number of males in the reference set flagged at that cutoff</li> <li><code>ref_tot_marked</code>: total number of flagged subjects in the reference set at that cutoff</li> <li>Prints the male ratio (percentage) for test and reference and the total flagged counts and rates</li> </ul> </li> </ul> <p>The script prints the summary lines to stdout (captured by the test runner logs).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#output-location","title":"Output Location","text":"<ul> <li>Primary printed output: test runner log (e.g., <code>${WORK_DIR}/08.sex_ratio.log</code>) containing per-cutoff sex-ratio summaries</li> <li>Source tables examined: <code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code>, <code>${WORK_DIR}/compare/rep_propensity.matrix</code>, <code>${WORK_DIR}/compare/3.test_cohort.preds</code>, <code>${WORK_DIR}/compare/reference.preds</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>For each score cutoff, the script prints the male ratio in the flagged subset for Test_Run vs Reference and the total number flagged (absolute and percentage of the dataset).</li> <li>A decreasing male ratio with increasing cutoff is expected if higher scores are less common in males for this model; the opposite trend may indicate bias.</li> <li>Differences between Test_Run and Reference male ratios indicate either demographic differences (population sex skew) - follow up by checking Age and other covariates.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#example-output-line","title":"Example output line","text":"<pre><code>Analyze cutoff of score 0.25000 (test|ref) =&gt; males ratio (60.0%|55.0%), total_flag 200 (5.0%|4.0%)\n</code></pre> <p>This means at cutoff 0.25, 60% of flagged subjects are male in the test set vs 55% in the reference; 200 subjects flagged in the test set representing 5% of the test population while the reference had 4% flagged.</p> <p>Full example: </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>It raises an error if joins fail unexpectedly (indicates missing IDs or mismatched data). Ensure previous tests completed successfully and created the expected files.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing files or join mismatches: verify Tests 05 and 06 completed successfully and that the listed source files exist and have consistent IDs.</li> <li>If results look surprising, inspect the underlying <code>Gender</code> and <code>pred_0</code> columns in <code>${WORK_DIR}/compare/3.test_cohort.preds</code> and <code>${WORK_DIR}/compare/reference.preds</code> to ensure gender coding (e.g., <code>Gender==1</code> for males) and score scaling are as expected.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2008%20-%20Sex%20Ratio.html#test-results-review","title":"Test Results Review","text":"<p>Primary items to inspect after running this test:</p> <ul> <li>The sex ratio summary in the test runner log (e.g., <code>${WORK_DIR}/08.sex_ratio.log</code>)</li> <li><code>${WORK_DIR}/compare/3.test_cohort.preds</code> and <code>${WORK_DIR}/compare/reference.preds</code> </li> <li><code>${WORK_DIR}/compare/rep_propensity.matrix</code> and <code>rep_propensity_non_norm.matrix</code> (for linkage and sample selection details)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html","title":"Test 09 - Coverage Special Groups","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#purpose","title":"Purpose","text":"<p>Evaluate model performance and coverage in predefined special groups (high-risk cohorts). The test compares how many subjects from each special group are flagged at several top-percentile cutoffs in the test dataset vs the reference dataset and reports lift and coverage metrics.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working directory containing compare artifacts and prediction files</li> <li><code>configs</code> Directory must contain <code>coverage_groups.py</code> - a Python file that defines the dictionary <code>cohort_f</code> with boolean filters for each special group.</li> <li>The test depends on Tests 05 and 06 and expects:<ul> <li><code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code></li> <li><code>${WORK_DIR}/compare/rep_propensity.matrix</code></li> <li><code>${WORK_DIR}/compare/3.test_cohort.preds</code></li> <li><code>${WORK_DIR}/compare/reference.preds</code></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 9\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check <code>${WORK_DIR}</code> and <code>${WORK_DIR}/compare</code> for generated logs and outputs.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Loads the test and reference propensity matrices and prediction files and joins them to ensure every sample has <code>pred_0</code> (score) and the linking id <code>str_attr_original_id</code>.</li> <li>Computes cutoff scores corresponding to top positive rates (defaults: 1%, 3%, 5%, 10%) using the test dataset quantiles.</li> <li>Loads <code>coverage_groups.py</code> from <code>configs</code> folder. this file must populate <code>cohort_f</code> with boolean masks (filters) on the DataFrame <code>df</code> to define special groups in pandas.</li> <li>For each defined group the test computes, at each cutoff:<ul> <li>Number covered in the group (how many flagged within the group)</li> <li>Total flagged across the dataset</li> <li>Coverage percentages (group coverage vs group size, total flagged as share of dataset)</li> <li>Lift and, where reference data exists for the same cohort, comparative metrics (reference coverage, lift in reference)</li> </ul> </li> <li>Prints detailed per-cohort per-cutoff summaries to stdout (captured by the test runner logs).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#output-location","title":"Output Location","text":"<ul> <li>Primary printed output: test runner log (e.g., <code>${WORK_DIR}/09.coverage.log</code>) with per-cohort coverage/lift summaries</li> <li>Source inputs inspected: <code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code>, <code>${WORK_DIR}/compare/rep_propensity.matrix</code>, <code>${WORK_DIR}/compare/3.test_cohort.preds</code>, <code>${WORK_DIR}/compare/reference.preds</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>For each special group and each positive-rate cutoff the test prints: the number covered in the cohort, cohort coverage percentage, total flagged and its share of the dataset, the fraction of flagged who belong to the cohort, and the lift (cohort coverage divided by expected share). When reference cohort filters are provided the test prints equivalent reference numbers and a pairwise lift comparison.</li> <li>Large differences in coverage or lift between test and reference can be caused by:<ul> <li>True population-level differences (e.g., different prevalence or age structure)</li> <li>Data issues such as missing values or different coding </li> <li>Model-related behavior that flags different subpopulations</li> </ul> </li> </ul> <p>Interpretation advice:</p> <ul> <li>If a cohort is small (few patients), treat lift estimates as noisy and check absolute counts.</li> <li>If lift is high in the test but not in the reference, investigate whether the flagged cohort is genuinely higher-risk in this population or whether artifacts (imputation, sampling) explain the difference.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#example-output-snippet","title":"Example output snippet","text":"<p>This analysis inspects the model's performance specifically within a high-risk cohort defined as: Age: 70-75 AND Last Hemoglobin (Hgb) &lt; 11.</p> <ol> <li>Population Context</li> </ol> Dataset Size Cohort Prevalence (Age 70-75, Hgb&lt;11) Current Dataset 337,048 1.1% Reference Dataset 31,089 0.3% <p>Key Finding: The current dataset exhibits a significantly higher proportion of this high-risk group (1.1% vs. 0.3%). This suggests the current population has an elevated baseline risk profile compared to the reference.</p> <ol> <li>Model Performance Metrics (Using Cutoff: 0.28065) A decision score cutoff of 0.28065 was chosen to achieve a target \u22481% positive flag rate in the Current Dataset.</li> </ol> Metric Current Dataset Reference Dataset Overall Positive Flag Rate 1.0% (3,371 flagged patients) 0.6% Cohort Coverage 15.5% of patients within the cohort are flagged. 23.1% of patients within the cohort are flagged. Cohort Contribution to flagged 17.6% of all flagged patients belong to this cohort 13.9% of all flagged patients belong to this cohort. Lift for cohort 15.5 41.5 <ol> <li>Interpretation and Observations<ol> <li>Risk Acknowledgment: The model appropriately flags older patients with low hemoglobin. The 0.28065 cutoff results in 15.5% coverage of this high-risk group in the current population.</li> <li>Cutoff Effect: Applying the same cutoff (0.28065) to the healthier Reference Dataset yields a lower overall positive rate (0.6%), which is expected since the cutoff was calibrated on a higher-risk population (the Current Dataset).</li> <li>Lift: The \"Lift\" metric (the ratio of the group's flagging rate to the overall flagging rate) was observed to be higher in the Reference Dataset. This is because the high-risk cohort was rare in the healthier reference population, meaning that belonging to this group had a disproportionately higher impact on being flagged.</li> </ol> </li> </ol> <p>Final Note: While these statistics clearly show the model's behavior and the shifting population risk, there is no clear, defined threshold for what constitutes an unacceptable or \"bad\" result based solely on these performance shifts. Further definition of acceptable drift or performance degradation is required.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The test reads and executes <code>coverage_groups.py</code> from <code>configs</code>. That file must define <code>cohort_f</code> as a dictionary mapping cohort names to boolean masks (expressions referencing <code>df</code>), e.g.:</li> </ul> <p><pre><code># inside coverage_groups.py\ncohort_f['ElderlyHighRisk'] = (df['Age'] &gt; 75) &amp; (df['SomeLab'] &gt; 2.0)\n  ...\n</code></pre> You might use a helper function <code>getcol(df, 'feature_name')</code> to access and exact specific column names from <code>df</code> DataFrame by iterating over the columns as searching for this substring. If multiple matches or no matches are found, an error will be thrown. The available features are features the model uses and it suppose to be sufficient for sub grouping important groups.</p> <ul> <li>Default positive-rate cutoffs used are <code>[1, 3, 5, 10]</code> percent; you can adjust <code>coverage_groups.py</code> behavior or modify the script if needed.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing or malformed <code>coverage_groups.py</code>: the test will fail early with an error instructing you to provide the file. Ensure it defines <code>cohort_f</code> and uses <code>getcol</code> helper if needed.</li> <li>Join mismatches (missing IDs): the script will raise an error if the DataFrame sizes change unexpectedly after joining IDs and predictions - verify that <code>${WORK_DIR}/compare/rep_propensity.matrix</code> has <code>id</code>, <code>outcome</code>, and <code>str_attr_original_id</code> columns and that prediction files use matching <code>id</code> and <code>time</code> fields.</li> <li>Small cohort sizes: when cohort counts are small (e.g., &lt;100), treat percentage/lift estimates as noisy; inspect absolute counts in logs.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2009%20-%20Coverage%20Special%20Groups.html#test-results-review","title":"Test Results Review","text":"<p>Primary items to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/09.coverage.log</code> (or the test runner log capturing stdout)</li> <li><code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code> and <code>${WORK_DIR}/compare/rep_propensity.matrix</code> (to check cohort selection and linking)</li> <li><code>${WORK_DIR}/compare/3.test_cohort.preds</code> and <code>${WORK_DIR}/compare/reference.preds</code> (to inspect <code>pred_0</code> and sample times)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html","title":"Test 10 - Compare Important Feature","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#purpose","title":"Purpose","text":"<p>Compare distributions of the model's most important features between the current (test) dataset and the reference dataset. This test computes summary statistics, missing-value rates, KLD-based similarity measures, and produces per-feature histograms split by gender.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: working folder containing compare artifacts and matrices</li> <li><code>CMP_FEATURE_RES</code>: comma-separated list of feature identifiers with resolution, format: FeatureName:resolution (e.g. \"Hemoglobin:0.1,Age:1\")</li> <li>The test depends on Test 05; expected files are:<ul> <li><code>${WORK_DIR}/compare/rep_propensity.matrix</code></li> <li><code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code></li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 10\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>After run, check <code>${WORK_DIR}</code> and <code>${WORK_DIR}/compare</code> for generated files.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Loads <code>${WORK_DIR}/compare/rep_propensity.matrix</code> and <code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code> into pandas DataFrames.</li> <li>Filters the non-normalized frame to positive outcome rows (used to compute missing-value-before-normalization stats).</li> <li>Parses <code>CMP_FEATURE_RES</code> into two aligned lists: feature names (normalized to remove category prefixes like <code>ICD9_CODE:</code>) and numeric resolution values.</li> <li>For each feature in the list:<ul> <li>Matches the exact column in the matrix (error if ambiguous or missing)</li> <li>Computes missing-value rates for test (outcome==0) and for positive cases using <code>-65336</code> as the sentinel missing value</li> <li>Applies rounding/bucketing according to the provided resolution and computes grouped percentage distributions separately for:<ul> <li>Test_Run:Males (outcome&gt;0 &amp; Gender==1)</li> <li>Test_Run:Females (outcome&gt;0 &amp; Gender==2)</li> <li>Reference:Males (outcome==0 &amp; Gender==1)</li> <li>Reference:Females (outcome==0 &amp; Gender==2)</li> </ul> </li> <li>Writes per-feature bar/histogram HTML into <code>${WORK_DIR}/features_graphs/&lt;Feature&gt;.html</code></li> <li>Computes a KLD score comparing positive vs reference distributions using <code>calc_kld</code> and prints KLD, KLD-to-uniform, and entropy</li> <li>Appends feature-level statistics into <code>${WORK_DIR}/features_stats.tsv</code> with columns:     Feature, Reference_Mean, Reference_Std, Reference_Missing_value_percent, TestRun_Mean, TestRun_Std, TestRun_Missing_value_percent, TestRun_Missing_value_before_norm</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/features_graphs/</code> - one HTML plot per feature (bar charts split by gender and dataset)</li> <li><code>${WORK_DIR}/features_stats.tsv</code> - TSV summary of means, stds and missing rates</li> <li>Standard output / test log (e.g., <code>${WORK_DIR}/10.analyze_features.log</code>) containing per-feature KLD printouts and summary lines</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use <code>features_stats.tsv</code> to quickly spot differences in means, spread, and missing-value rates between the test and the reference.</li> <li>Use the HTML graphs in <code>${WORK_DIR}/features_graphs/</code> to visually compare the binned distributions by gender and dataset.</li> <li>KLD values printed by the script quantify distributional divergence between positive cases and reference; higher KLD indicates larger distributional shift. Compare KLD across features to prioritize investigation.</li> </ul> <p>Interpretation advice:</p> <ul> <li>Large mean or distribution differences together with large KLDs imply features where the test dataset deviates from the reference and that might explain model performance differences.</li> <li>High missing-value rates in the test (or large change in missing rate vs reference) may indicate data collection or ETL problems that need fixing before model deployment.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\"Found X features for Y\" NameError: If the script raises this, the feature name from <code>CMP_FEATURE_RES</code> matched multiple columns or none. Check <code>CMP_FEATURE_RES</code> spelling and column names in <code>${WORK_DIR}/compare/rep_propensity.matrix</code>.</li> <li>Missing <code>rep_propensity</code> files: ensure Test 05 completed successfully and produced <code>${WORK_DIR}/compare/rep_propensity.matrix</code> and <code>_non_norm.matrix</code>.</li> <li>Plot HTMLs referencing <code>plotly.js</code> not displaying: verify <code>${WORK_DIR}/features_graphs</code> contains the HTML files and that <code>../js/plotly.js</code> is present relative to the generated path. If Plotly is not available, copy <code>plotly.js</code> into <code>${WORK_DIR}/compare/js/</code> or change the path in the script.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#example-output-snippets","title":"Example output snippets","text":"<p>1) <code>features_stats.tsv</code> example:</p> <p></p> <p>2) Log file <code>10.analyze_features.log</code></p> <p></p> <p>3) Histogram Graph of features in <code>${WORK_DIR}/features_graphs</code></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The resolution value controls binning: the script rounds values to the nearest multiple of the resolution before grouping.</li> <li>The script writes percentages (not raw counts) into the graphs and normalizes group counts to percentages so we can compare current dataset to reference.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2010%20-%20Compare%20Important%20Feature.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/features_graphs/</code> (HTML per-feature plots)</li> <li><code>${WORK_DIR}/features_stats.tsv</code></li> <li>Test runner log capturing stdout for KLD lines</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html","title":"Test 11 - Estimate Performances","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#purpose","title":"Purpose","text":"<p>Estimate expected model performance on the tested dataset by re-weighting or sampling the reference cohort to match the distribution of important features in the tested dataset and then measuring AUC with bootstrap confidence intervals.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test environment:</p> <ul> <li><code>WORK_DIR</code>: working directory where compare artifacts are stored</li> <li><code>REFERENCE_MATRIX</code>: path to the reference matrix used for matching/sample generation</li> <li><code>CMP_FEATURE_RES</code>: list of important features used elsewhere in the kit (used indirectly by weighting steps)</li> <li>The script depends on output from earlier tests (notably Test 05) and expects:<ul> <li><code>${WORK_DIR}/compare/reference.preds</code></li> <li><code>${WORK_DIR}/compare/rep_propensity.calibrated.model</code> (or produced model at <code>${WORK_DIR}/compare.no_overfitting/rep_propensity.calibrated.model</code>)</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 11\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>Check <code>${WORK_DIR}/compare.no_overfitting</code> for outputs and logs.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Computes propensity/importance weights on the reference matrix with:<ul> <li><code>Flow --get_propensity_weights --f_matrix &lt;REFERENCE_MATRIX&gt; --f_preds &lt;WORK_PATH&gt;/labeled_weights.preds --f_model &lt;PROP_MODEL&gt; ...</code></li> <li>These weights are used to reweight the labeled reference examples to emulate the test distribution.</li> </ul> </li> <li>Runs bootstrap analyses on the original reference (<code>bootstrap_app --input ${WORK_DIR}/compare/reference.preds</code>) producing <code>${WORK_PATH}/bt_reference*</code> outputs.</li> <li>Runs a weighted bootstrap using the labeled weights file (<code>bootstrap_app --input ${WORK_PATH}/labeled_weights.preds --weights_file \"attr:weight\"</code>) to estimate expected performance when reference is transformed to the test distribution. Results are in <code>${WORK_PATH}/bt_reference.estimated*</code>.</li> <li>Calls <code>bootstrap_format.py</code> to produce a human-readable summary table and writes <code>${WORK_PATH}/summary_table.estimated_performance.tsv</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/compare.no_overfitting/summary_table.estimated_performance.tsv</code> - main summary table comparing original and estimated performance</li> <li><code>${WORK_DIR}/compare.no_overfitting/bt_reference.estimated.pivot_txt</code> and related <code>bt_reference*</code> bootstrap output files - raw bootstrap results</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>The primary output is a table showing AUC (and other measures, when enabled) for:<ul> <li>The original reference dataset (baseline)</li> <li>The estimated reference transformed to the test distribution (expected performance)</li> </ul> </li> <li>The table includes point estimates and bootstrap-derived confidence intervals. A drop in AUC from reference to estimated indicates expected degradation due to distributional differences.</li> </ul> <p>Interpretation advice:</p> <ul> <li>Large increases in the confidence interval or substantial drops in AUC suggest that the test dataset is different from the reference in ways that affect model performance.</li> <li>You can reduce variance of the estimate by decreasing the number of important features used for matching (trades off bias vs variance).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing <code>REFERENCE_MATRIX</code> or <code>rep_propensity</code> model: ensure previous tests completed and <code>REFERENCE_MATRIX</code> is available; <code>rep_propensity.calibrated.model</code> must be present under <code>${WORK_DIR}/compare.no_overfitting</code> or <code>${WORK_DIR}/compare</code>.</li> <li><code>Flow</code> not found or failing: verify <code>Flow</code> is on PATH </li> <li>Bootstrap outputs missing or stale: the script re-runs expensive steps only if input files are newer or <code>OVERRIDE</code> is set. if results are stale remove the relevant <code>bt_reference*</code> files and re-run.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#example-output-files","title":"Example output files","text":"<ul> <li> <p><code>${WORK_DIR}/compare.no_overfitting/summary_table.estimated_performance.tsv</code> (example values rendered by the test runner) </p> </li> <li> <p>Detailed bootstrap analysis can be found in <code>${WORK_DIR}/compare.no_overfitting/bt_reference.estimated.pivot_txt</code></p> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2011%20-%20Estimate%20Performances.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script limits extreme weights by <code>--trim_propensity_weight 0 --max_propensity_weight ${MAX_WEIGHT}</code> where <code>MAX_WEIGHT</code> defaults to 100000 in the script.</li> <li>Bootstrap is run with <code>--sample_per_pid 0</code> in this script; if your analysis requires different bootstrap parameters, please edit the script or create environment variable for this.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html","title":"Test 12 - Lab Frequency","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#purpose","title":"Purpose","text":"<p>Analyze lab frequency for signals that back the model's important features. The test produces counts of how many patients had N observations of a given signal (per-signal histogram of patient-level lab counts).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: working directory where repository and output folders live</li> <li><code>CMP_FEATURE_RES</code>: comma-separated list of important features (used to derive the list of relevant signals)</li> <li>A prepared repository under <code>${WORK_DIR}/rep</code>, containing <code>test.signals</code> and <code>test.repository</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute: <pre><code>./run.specific.sh 12\n</code></pre> Or include as part of the full suite: <pre><code>./run.sh\n</code></pre></p> <p>After run, check <code>${WORK_DIR}/signals_cnt/</code> for per-signal TSV results.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Parses <code>CMP_FEATURE_RES</code> to extract signal names. It strips category prefixes like <code>ICD9_CODE:</code>/<code>ICD10_CODE:</code>/<code>ATC_CODE:</code> and excludes some control features (e.g., <code>FTR_</code>, <code>Age</code>, <code>category_</code>, <code>Smoking</code>). It also ensures <code>DIAGNOSIS</code> and <code>Smoking_Status</code> signals are included by default.</li> <li>For each signal it verifies the signal exists in <code>${WORK_DIR}/rep/test.signals</code> and skips signals not present.</li> <li>For present signals it runs:<ul> <li><code>Flow --rep ${WORK_DIR}/rep/test.repository --pids_sigs --sigs &lt;signal&gt;</code> to retrieve patient-signal rows (id, date, ...)</li> <li>An awk pipeline deduplicates per-patient-date entries and counts how many distinct dates each patient has for the signal, then aggregates across patients to produce counts: how many patients had exactly 1 sample, 2 samples, ...</li> </ul> </li> <li>Writes per-signal files: <code>${WORK_DIR}/signals_cnt/&lt;signal&gt;.tsv</code> with rows: signal, count, num_patients</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/signals_cnt/</code> - one TSV per signal named <code>&lt;signal&gt;.tsv</code> containing columns: signal, num_labs, num_patients</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Each per-signal TSV shows how many patients had N lab entries for that signal during the observation window. Compare these distributions across signals or against a reference dataset to find differences in monitoring intensity.</li> <li>Without a reference, raw counts indicate whether certain signals are rarely or frequently measured in this dataset (useful for data quality and expected feature availability).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing <code>test.signals</code> or <code>test.repository</code>: the script checks for the signal list in <code>${WORK_DIR}/rep/test.signals</code> and will skip signals not found. Ensure the repo was created by Test 03 and contains expected files.</li> <li><code>Flow</code> not found or failing: ensure <code>Flow</code> is on PATH.</li> <li>Empty output files: if <code>${WORK_DIR}/rep/test.signals</code> lists the signal but Flow returns no rows, inspect the repository content to ensure the signal has records (search for the exact signal token in <code>test.signals</code>).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#example-output","title":"Example output","text":"<p>A sample <code>${WORK_DIR}/signals_cnt/Hemoglobin.tsv</code> might look like:</p> <pre><code>Hemoglobin  1   10234\nHemoglobin  2   4230\nHemoglobin  3   1231\nHemoglobin  4   512\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script deduplicates by patient-date so multiple entries on the same day count as a single lab for that day.</li> <li>The signal extraction logic excludes some features (e.g., <code>Age</code>, features starting with <code>FTR_</code> or <code>category_</code>) because they are not time-series signals.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2012%20-%20Lab%20Frequency.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/signals_cnt/</code> (per-signal TSVs)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html","title":"Test 13 - But Why (Shapley)","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#purpose","title":"Purpose","text":"<p>Produce Shapley explanations and global feature-importance visualizations for the model using the test cohort. Helps identify which features drive predictions and create per-feature explainability graphs.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#required-inputs","title":"Required Inputs","text":"<ul> <li><code>WORK_DIR</code>: working directory containing repository and model artifacts</li> <li><code>MODEL_PATH</code>: path to the fitted model (expected <code>${WORK_DIR}/model/model.medmdl</code>)</li> <li><code>${WORK_DIR}/rep/test.repository</code> and <code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#how-to-run","title":"How to Run","text":"<p><pre><code>./run.specific.sh 13\n</code></pre> Or run as part of the suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Requests SHAP values from Flow twice: grouped by signal category and ungrouped (full features). Outputs are written to <code>${WORK_DIR}/ButWhy/shapley_grouped.report</code> and <code>${WORK_DIR}/ButWhy/shapley.report</code>.</li> <li>Uses <code>feature_importance_printer.py</code> to generate global HTML reports:<ul> <li><code>${WORK_DIR}/ButWhy/Global.html</code> (grouped)</li> <li><code>${WORK_DIR}/ButWhy/Global.ungrouped.html</code></li> </ul> </li> <li>Generates per-feature HTML files under <code>${WORK_DIR}/ButWhy/single_features/</code> by expanding the ungrouped SHAP report.</li> <li>Patches HTML files to use a local Plotly JS (<code>../js/plotly.js</code>).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/ButWhy/</code> - contains <code>shapley.report</code>, <code>shapley_grouped.report</code>, <code>Global.html</code>, <code>Global.ungrouped.html</code>, and <code>single_features/</code> HTMLs.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li><code>Global.html</code> gives a ranked list of feature groups and their aggregate importance. Use it to identify which signal categories contribute most to the model.</li> <li><code>single_features/</code> contains per-feature SHAP distribution plots useful for detailed investigation.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing model or repository: ensure <code>MODEL_PATH</code> and <code>${WORK_DIR}/rep/test.repository</code> exist.</li> <li><code>Flow</code> failures: run the <code>Flow --shap_val_request</code> command manually to debug arguments (e.g.).</li> <li>Plotly not loading: ensure <code>../js/plotly.js</code> exists relative to the generated HTML; the test rewrites script tags to <code>../js/plotly.js</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2013%20-%20But%20Why%20%28Shapley%29.html#files-to-inspect","title":"Files to inspect","text":"<ul> <li><code>${WORK_DIR}/ButWhy/shapley.report</code></li> <li><code>${WORK_DIR}/ButWhy/shapley_grouped.report</code></li> <li><code>${WORK_DIR}/ButWhy/Global*.html</code></li> <li><code>${WORK_DIR}/ButWhy/single_features/*.html</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html","title":"Test 14 - Model Explainability","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#purpose","title":"Purpose","text":"<p>Generate explainer-based reports and group-level feature summaries for the top predicted test samples. Produces group-&gt;feature mappings and a compact group statistics table.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#required-inputs","title":"Required Inputs","text":"<ul> <li><code>WORK_DIR</code>: working directory with <code>compare/3.test_cohort.preds</code> and model artifacts</li> <li><code>MODEL_PATH</code> and <code>EXPLAINABLE_MODEL_PATH</code> (model with explainability hooks; expected under <code>${WORK_DIR}/model/</code>)</li> <li><code>TOP_EXPLAIN_PERCENTAGE</code>: integer percent (used to select top scoring samples)</li> <li><code>TOP_EXPLAIN_GROUP_COUNT</code>: integer number of top groups to include in final stats</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#how-to-run","title":"How to Run","text":"<p><pre><code>./run.specific.sh 14\n</code></pre> Or run the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Selects the top <code>TOP_EXPLAIN_PERCENTAGE</code> percent highest-scoring unique patients from <code>${WORK_DIR}/compare/3.test_cohort.preds</code> to build explainer examples.</li> <li>Runs <code>CreateExplainReport</code> against the explainer model (<code>EXPLAINABLE_MODEL_PATH</code>) and the selected samples to create <code>${WORK_DIR}/ButWhy/explainer_examples/test_report.tsv</code>.</li> <li>Reformat and aggregate the explainability output to:<ul> <li>Map groups to features and features to signal categories</li> <li>Produce group histograms and a final <code>group_stats_final.tsv</code> containing Group, Frequency, Percentage and leading features</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/ButWhy/explainer_examples/test_report.tsv</code></li> <li><code>${WORK_DIR}/ButWhy/explainer_examples/group_stats_final.tsv</code></li> <li>temporary files under <code>${WORK_DIR}/tmp/</code> used during reformatting</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li><code>test_report.tsv</code> contains detailed per-sample explainer output. Use it to inspect example explanations.</li> <li><code>group_stats_final.tsv</code> summarizes how many examples belong to each explanation group and lists top features per group - useful to prioritize group-level analyses.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If <code>CreateExplainReport</code> fails, run it manually with the selected sample file <code>${WORK_DIR}/ButWhy/explainer_examples/top.samples</code> to inspect errors.</li> <li>If expected groups/features are missing, check <code>TOP_EXPLAIN_PERCENTAGE</code> and <code>TOP_EXPLAIN_GROUP_COUNT</code> values (they control sample selection and report granularity).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2014%20-%20Model%20Explainability.html#files-to-inspect","title":"Files to inspect","text":"<ul> <li><code>${WORK_DIR}/ButWhy/explainer_examples/test_report.tsv</code></li> <li><code>${WORK_DIR}/ButWhy/explainer_examples/group_stats_final.tsv</code></li> <li><code>${WORK_DIR}/tmp/*</code> intermediate mapping files</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html","title":"Test 15 - Features and Flag","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#purpose","title":"Purpose","text":"<p>Compare distributions of important features between flagged and non-flagged samples in both the test and reference datasets. Produces per-feature statistics, per-gender histograms, and ratio plots highlighting where flagged populations differ.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#required-inputs","title":"Required Inputs","text":"<ul> <li><code>WORK_DIR</code>: working directory containing compare matrices and prediction files</li> <li><code>CMP_FEATURE_RES</code>: comma-separated list of feature identifiers with resolution (used to select and bucket features)</li> <li><code>TOP_EXPLAIN_PERCENTAGE</code>: used to compute the flagged cutoff (script computes CUTOFF = 100 - TOP_EXPLAIN_PERCENTAGE)</li> <li>Files expected:</li> <li><code>${WORK_DIR}/compare/rep_propensity.matrix</code></li> <li><code>${WORK_DIR}/compare/rep_propensity_non_norm.matrix</code></li> <li><code>${WORK_DIR}/compare/3.test_cohort.preds</code></li> <li><code>${WORK_DIR}/compare/reference.preds</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#how-to-run","title":"How to Run","text":"<p><pre><code>./run.specific.sh 15\n</code></pre> Or include in the full run: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Reads normalized and non-normalized propensity matrices and prediction files.</li> <li>Parses <code>CMP_FEATURE_RES</code> to extract features and their binning resolution. Rounds feature values to the requested resolution.</li> <li>Splits samples into flagged vs not-flagged using the percentile cutoff derived from test predictions (<code>CUTOFF = 100 - TOP_EXPLAIN_PERCENTAGE</code>).</li> <li>Computes per-feature stats (means, std, missing%) for flagged and not-flagged groups in both test and reference and writes <code>features_and_flag.csv</code>.</li> <li>Produces HTML bar charts per-feature showing percentage distributions for Test_Flagged, Test_Not_Flagged, Ref_Flagged, Ref_Not_Flagged and additional ratio charts (files under <code>${WORK_DIR}/features_and_flag/*.html</code>).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/features_and_flag/features_and_flag.csv</code></li> <li><code>${WORK_DIR}/features_and_flag/&lt;feature&gt;.html</code> and <code>&lt;feature&gt;_ratio.html</code> per-feature visualization files</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use <code>features_and_flag.csv</code> to compare means/stds and missing rates between flagged and not-flagged groups across test and reference.</li> <li>Use per-feature HTMLs to visually inspect whether flagged subjects have different distributions and whether those patterns are consistent with the reference dataset.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Feature name matching errors: the script searches columns by substring and assumes a single exact match; mismatches will raise errors-confirm <code>CMP_FEATURE_RES</code> aligns with column names in the matrices.</li> <li>Missing matrices or predictions: run Tests 03 and 05 first to ensure <code>${WORK_DIR}/compare/*</code> artifacts exist.</li> <li>Plot rendering: HTMLs reference <code>../js/plotly.js</code>; ensure the asset exists or change the path.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2015%20-%20Features%20and%20Flag.html#files-to-inspect","title":"Files to inspect","text":"<ul> <li><code>${WORK_DIR}/features_and_flag/features_and_flag.csv</code></li> <li><code>${WORK_DIR}/features_and_flag/*.html</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html","title":"Test 16 - Sample Dates","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#purpose","title":"Purpose","text":"<p>Quickly summarize the distribution of sample dates in the test cohort. Useful to check temporal coverage and spot data collection gaps.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#required-inputs","title":"Required Inputs","text":"<ul> <li><code>WORK_DIR</code>: working directory containing <code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#how-to-run","title":"How to Run","text":"<p><pre><code>./run.specific.sh 16\n</code></pre> Or include in full run: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Reads <code>${WORK_DIR}/Samples/3.test_cohort.samples</code> and extracts the sample date (the script converts sample <code>time</code> to a year/month integer by <code>int(time/100)</code>).</li> <li>Produces a count of samples per date and writes <code>${WORK_DIR}/samples_stats/by_date.tsv</code>.</li> <li>Generates an HTML plot <code>${WORK_DIR}/samples_stats/by_date.html</code> using <code>plot.py</code> and a local HTML template.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/samples_stats/by_date.tsv</code></li> <li><code>${WORK_DIR}/samples_stats/by_date.html</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use the TSV to see the number of samples collected per month (or per date unit). Look for irregularities such as sudden drops or spikes indicating data collection problems or cohort selection artifacts.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing <code>${WORK_DIR}/Samples/3.test_cohort.samples</code>: ensure Test 03 ran successfully.</li> <li><code>plot.py</code> failures: ensure the plot template <code>${WORK_DIR}/tmp/plotly_graph.html</code> exists or update the command to point to another template.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2016%20-%20Sample%20Dates.html#files-to-inspect","title":"Files to inspect","text":"<ul> <li><code>${WORK_DIR}/samples_stats/by_date.tsv</code></li> <li><code>${WORK_DIR}/samples_stats/by_date.html</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html","title":"Test 17 - Estimate Performance from Calibration","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#purpose","title":"Purpose","text":"<p>Compute model predictions using a calibrated model and run a specialized <code>PerformanceFromCalibration</code> tool to summarize performance metrics derived from the calibrated predictions.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#required-inputs","title":"Required Inputs","text":"<ul> <li><code>WORK_DIR</code>: working directory containing repository and model artifacts</li> <li>Calibrated model path: expected <code>${WORK_DIR}/model/model.medmdl</code> (the script uses <code>CALIBRATED_MODEL_PATH</code>)</li> <li><code>${WORK_DIR}/rep/test.repository</code> and <code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#how-to-run","title":"How to Run","text":"<p><pre><code>./run.specific.sh 17\n</code></pre> Or run within the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Produces predictions using the calibrated model with Flow and writes <code>${WORK_DIR}/compare/test.calibrated.preds</code>.</li> <li>Runs <code>PerformanceFromCalibration --preds &lt;preds&gt; --output &lt;WORK_DIR&gt;/pref_from_calibration/result</code> to analyze calibrated predictions and produce a summary output.<ul> <li>The Flow call may include a memory limit modification if <code>MEMORY_LIMIT</code> is set; the script injects a model change via <code>--change_model_init</code> when applicable.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/compare/test.calibrated.preds</code></li> <li><code>${WORK_DIR}/pref_from_calibration/result</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>The <code>result</code> file contains performance summaries produced by <code>PerformanceFromCalibration</code>. Use it to compare calibrated vs uncalibrated predictions and to estimate expected performance under calibration adjustments.</li> </ul> <p>Performance\u00a0Estimation\u00a0Warning! The performance metrics derived from this cohort analysis are highly sensitive to noise and should be treated with caution.</p> <p>This estimation method relies on the assumption that the model is perfectly calibrated on the test dataset, which may introduce inaccuracies. Since even minor calibration errors can significantly skew the performance estimates, the methodology detailed in Test 11: Estimate Performances is the preferred and more robust approach for determining actual model performance. Anyway, this method is also available. </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li><code>Flow</code> failures or memory issues: if Flow fails due to memory, set <code>MEMORY_LIMIT</code> in <code>env.sh</code> to an appropriate value.</li> <li>Missing calibrated model: ensure <code>${WORK_DIR}/model/model.medmdl</code> exists and is the calibrated variant you expect.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2017%20-%20Estimate%20Performance%20from%20Calibration.html#files-to-inspect","title":"Files to inspect","text":"<ul> <li><code>${WORK_DIR}/compare/test.calibrated.preds</code></li> <li><code>${WORK_DIR}/pref_from_calibration/result</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html","title":"Test 18 - Analyze Messages","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#purpose","title":"Purpose","text":"<p>Summarize textual messages returned by the model runner (AlgoMarker) in the outputs. Useful to spot common error messages or unexpected status codes from the algorithm.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#required-inputs","title":"Required Inputs","text":"<ul> <li><code>WORK_DIR</code>: working directory to place outputs</li> <li><code>SILENCE_RUN_OUTPUT_FILES_PATH</code>: path to the silence-run output file produced by the algorithm, unless set to the special value <code>GENERATE</code> (in which case this test will skip)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#how-to-run","title":"How to Run","text":"<p><pre><code>./run.specific.sh 18\n</code></pre> Or run with the full suite: <pre><code>./run.sh\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>If <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> is not <code>GENERATE</code>, the script scans the provided TSV/CSV of algorithm outputs, counts occurrences of each message (uses the last column as the message), and computes the percentage of total outputs for each message.</li> <li>Outputs the sorted message counts to <code>${WORK_DIR}/outputs/messages.tsv</code>.</li> <li>If <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> equals <code>GENERATE</code>, the test prints a message and skips processing.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#output-location","title":"Output Location","text":"<ul> <li><code>${WORK_DIR}/outputs/messages.tsv</code> - columns: message, count, percent_of_total</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use the TSV to identify the most frequent messages returned by the algorithm. Messages with high frequency may indicate systematic errors or common success markers (e.g., <code>&lt;OK&gt;</code>). Investigate rare but critical error messages.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the script prints \"No output files from AlgoMarker - skips\", ensure <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> is set to the path of the algorithm output file and not <code>GENERATE</code>.</li> <li>If parsing fails, inspect the input file for expected tab-separated columns and ensure the last column contains the message strings.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External%20Silent%20Run/Test%2018%20-%20Analyze%20Messages.html#files-to-inspect","title":"Files to inspect","text":"<ul> <li><code>${WORK_DIR}/outputs/messages.tsv</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/index.html","title":"External Validation (After Silent Run)","text":"<p>The purpose of this test is to analysis the performance when we have outcomes avaialable.</p> <p>This TestKit documents the follow-up tests to run after completing the External Silent Run. Run this kit after performing the External Silent Run to convert the silent-run (no outcomes repository) artifacts into evaluation-ready outputs. This is to make sure no information leakage is done and separate the execution of the score calculation from the labels. It is also possible to load the labels in the silent run and the silent run will just ignore the labels, if we only recieved data with outcomes.</p> <p>Note: The External Silent Run produces repository artifacts and the initial sample files used as inputs to the tests described below. Make sure the External Silent Run completed successfully before running this kit.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/index.html#tests-included","title":"Tests included","text":"<ul> <li>Test 01 - Load Outcome<ul> <li>Purpose: Load outcome/register data into the working repository and prepare the repository structure for downstream tests.</li> </ul> </li> <li>Test 02 - Fit Model to Repository<ul> <li>Purpose: Convert and fit a provided model to the local repository and produce transformed model files (with/without explainability).</li> </ul> </li> <li>Test 04 - Relabel &amp; Create Samples<ul> <li>Purpose: Create an outcome registry, relabel samples using diagnosis codes, drop excluded samples, and filter to the comparison cohort.</li> </ul> </li> <li>Test 05 - Compare Matrices &amp; Feature Analysis<ul> <li>Purpose: Compare the current matrix with a reference matrix, produce Shap/ButWhy explainability reports and feature-level visualizations.</li> </ul> </li> <li>Test 06 - Matrix Feature Statistics &amp; KLD Analysis<ul> <li>Purpose: Produce per-feature statistics, plots and KLD metrics for top Shap-identified features comparing the run with the reference.</li> </ul> </li> <li>Test 07 - Bootstrap Analysis<ul> <li>Purpose: Run bootstrap performance estimation (AUC, sensitivity/precision points, ORs, LIFT) and optional comparator bootstraps.</li> </ul> </li> <li>Test 08 - Age Of Flagged<ul> <li>Purpose: Generate age and temporal analyses for flagged patients and compare distributions across cohorts.</li> </ul> </li> <li>Test 09 - Features With Missing Values Analysis<ul> <li>Purpose: Inspect and visualize missingness patterns for important features and produce stats and KLD measures.</li> </ul> </li> <li>Test 10 - Calibration Test<ul> <li>Purpose: Run calibration checks and bootstrap-based calibration graphs for score-bin calibration across cohorts and time windows.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/index.html#notes","title":"Notes","text":"<ul> <li>Many tests depend on helper utilities, please ensure these are on PATH. Installation </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/index.html#quick-start","title":"Quick start","text":"<p>From the TestKit folder, you can run a single test, e.g.:</p> <pre><code>./run.specific.sh 4\n</code></pre> <p>Or run the full kit:</p> <pre><code>./run.sh\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html","title":"Test 01 - Load Outcome","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#purpose","title":"Purpose","text":"<p>Load outcome/register data into the working repository. This prepares the repository structure and signals needed by later tests (samples generation, model fitting and evaluation).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where the repository and outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Path to the reference / silent run (used for copying inputs when running the silent/external run)</li> <li><code>OUTCOME_INPUT_FILES_PATH</code>: Path(s) or pattern to the raw outcome input files that <code>load_outcomes.py</code> expects</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 1\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Check <code>${WORK_DIR}/rep</code> and <code>${WORK_DIR}/ETL</code> for the loaded repository and ETL artifacts.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Runs the Python ETL loader to parse and import the provided outcome files.</li> <li>If the repository hasn't been marked as loaded (no <code>${WORK_DIR}/rep/loading_done</code>), or when <code>OVERRIDE</code> &gt; 0, it makes <code>${WORK_DIR}/ETL/rep_configs/load_with_flow.sh</code> executable and runs it to build the repository files.</li> <li>Writes a simple marker file <code>${WORK_DIR}/rep/loading_done</code> to indicate successful load.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#output-location","title":"Output Location","text":"<ul> <li>Repository: <code>${WORK_DIR}/rep</code> (look for a <code>*.repository</code> directory)</li> <li>ETL and temporary files: <code>${WORK_DIR}/ETL</code>, <code>${WORK_DIR}/tmp</code></li> <li>Load marker: <code>${WORK_DIR}/rep/loading_done</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Verify <code>${WORK_DIR}/rep</code> contains the expected <code>*.repository</code> directory and related data files.</li> <li>Check <code>${WORK_DIR}/rep/loading_done</code> exists after a successful run.</li> <li>Inspect the stdout/stderr and any logs printed by <code>load_outcomes.py</code> and <code>load_with_flow.sh</code> for errors.</li> </ul> <p>Make sure the ETL was done correction, same as in Test 01 - Generate Repository</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Missing input files or incorrect <code>OUTCOME_INPUT_FILES_PATH</code>:<ul> <li>Ensure input files exist and are in the format expected by <code>load_outcomes.py</code>.</li> </ul> </li> <li>Missing helper scripts or broken ETL config in <code>${FIRST_WORK_DIR}/ETL</code>:<ul> <li>Confirm the ETL folder contains <code>rep_configs/load_with_flow.sh</code> and helper modules used by the Python loader.</li> </ul> </li> <li>Python environment/package errors when running <code>load_outcomes.py</code>:<ul> <li>Ensure the required Python packages are installed in the environment.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2001%20-%20Load%20Outcome.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/ETL</code> (copied ETL configs and scripts)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2002%20-%20Fit%20Model%20to%20Repo.html","title":"Test 02 - Fit Model to Repository","text":"<p>Same as Test 02 - Fit Model to Repository</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html","title":"Test 04 - Relabel &amp; Create Samples","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#purpose","title":"Purpose","text":"<p>Relabel an input samples file with outcome labels derived from the repository's diagnosis registry, produce a cleaned cohort file and filter samples to the requested comparison cohort. This prepares evaluation-ready sample files for downstream bootstrap and comparison tests.</p> <p>Note: This test is distinct from Test 03 - Create Samples from <code>External Slient Run</code> kit which generates a samples cohort from raw inputs or an external file. <code>Test 04</code> assumes a sample cohort has already been created (from <code>Test 03</code>) and focuses on relabeling and filtering by outcome codes.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Path to the reference run (contains Silent Run Samples and model outputs)</li> <li><code>BT_JSON</code>: Path to bootstrap features JSON, default used from <code>${FIRST_WORK_DIR}/json/bootstrap/bt_features.json</code></li> <li><code>COMPARE_COHORT</code>: Cohort identifier used by <code>FilterSamples</code> to select a subset of samples</li> <li><code>CODE_LIST_FILE</code>: File listing diagnosis codes to use (e.g., ICD lists)</li> <li><code>CODE_DIR</code>: Directory containing code lists</li> <li><code>SUB_CODES</code>: Comma-separated list of sub-cohort identifiers</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 4\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Primary output files will be placed under <code>${WORK_DIR}/Samples</code> and <code>${WORK_DIR}/outputs</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Creates an outcome registry with <code>create_registry.py</code> (if missing or <code>OVERRIDE</code> &gt; 0):<ul> <li><code>python ${CURR_PT}/resources/lib/create_registry.py --rep $REP_PATH --signal DIAGNOSIS --output ${WORK_DIR}/Samples/outcome.reg --end_of_data 20230101 --codes_dir ${CODE_DIR} --codes_list ${CODE_LIST_FILE} --sub_codes ${SUB_CODES}</code></li> </ul> </li> <li>Relabels samples using <code>relabel.py</code> (writes dropped samples too):<ul> <li><code>python ${CURR_PT}/resources/lib/relabel.py --registry ${WORK_DIR}/Samples/outcome.reg --samples ${FIRST_WORK_DIR}/Samples/3.test_cohort.samples --output ${OUTPUT} --output_dropout ${WORK_DIR}/Samples/dropped.samples --follow_up_controls 730 --time_window_case_maximal_before 730 --time_window_case_minimal_before 0 --future_cases_as_control 0 --sub_codes ${SUB_CODES}</code></li> </ul> </li> <li>Runs <code>samples_by_year.sh</code> to show distribution by year and month.</li> <li>Computes sample statistics with <code>samples_stats.py</code>.</li> <li>Produces a cleaned sample file and runs <code>FilterSamples</code> to generate <code>${WORK_DIR}/Samples/3.test_cohort.samples</code> filtered by <code>COMPARE_COHORT</code> and the bootstrap JSON (<code>BT_JSON</code>).</li> <li>Runs <code>samples_by_year.sh</code> again on the final cohort file.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#output-location","title":"Output Location","text":"<ul> <li>Relabeled samples: <code>${WORK_DIR}/Samples/relabeled.samples</code></li> <li>Filtered test cohort: <code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> <li>Dropped samples: <code>${WORK_DIR}/Samples/dropped.samples</code> with exclusion reason.</li> <li>Clean intermediate file: <code>${WORK_DIR}/Samples/clean.samples</code></li> <li>Statistics: <code>${WORK_DIR}/samples_stats</code> (path passed to <code>samples_stats.py</code>)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Inspect <code>${WORK_DIR}/Samples/relabeled.samples</code> to confirm samples are labeled with outcome columns and have expected counts.</li> <li>Check <code>${WORK_DIR}/Samples/dropped.samples</code> to see why samples were excluded.</li> <li>Verify <code>${WORK_DIR}/Samples/3.test_cohort.samples</code> exists and matches expected cohort selection.</li> <li>Review <code>${WORK_DIR}/model/04.create_samples.log</code> for other info and statistics</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Missing or incorrect code lists (<code>CODE_LIST_FILE</code> / <code>CODE_DIR</code>):<ul> <li>Ensure code files (ICD lists, etc.) are present and the <code>create_registry.py</code> arguments point to the correct directory.</li> </ul> </li> <li>Input sample file missing or malformed:<ul> <li>The test expects <code>${FIRST_WORK_DIR}/Samples/3.test_cohort.samples</code> or the configured <code>SILENCE_RUN_OUTPUT_FILES_PATH</code> outputs.</li> </ul> </li> <li><code>FilterSamples</code> utility missing or not on PATH:<ul> <li>Ensure the <code>FilterSamples</code> executable is available in PATH </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2004%20-%20Relabel%20%26%20Create%20Samples.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/Samples/relabeled.samples</code></li> <li><code>${WORK_DIR}/Samples/3.test_cohort.samples</code></li> <li><code>${WORK_DIR}/Samples/dropped.samples</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html","title":"Test 05 - Compare Matrices &amp; Feature Analysis","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#purpose","title":"Purpose","text":"<p>Compare the matrix produced by the silent/external run against a reference matrix and generate feature-level difference reports and visualizations (including Shap/ButWhy analyses). The test produces interactive HTML reports (Plotly) and global/single-feature explainability outputs for inspection. This is similar to Silent Run Test (no labels) - 05. Now we loaded the repository and have outcome. The stratification and comparasing can also stratify by the outcome.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>MODEL_PATH</code>: Path to the model used for generating matrices (by default pulled from <code>${FIRST_WORK_DIR}/model/model.medmdl</code>)</li> <li><code>REFERENCE_MATRIX</code>: CSV path to the reference matrix to compare against</li> <li><code>CMP_FEATURE_RES</code>: Comma-separated feature resolution definitions used for feature-subsetting in the no-overfitting run</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 5\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Check <code>${WORK_DIR}/compare</code> and <code>${WORK_DIR}/compare.no_overfitting</code> for results and HTML visualizations.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Calls TestModelExternal to compare the model applied to the test repository against the <code>REFERENCE_MATRIX</code>. Outputs are written to <code>${WORK_DIR}/compare</code>.<ul> <li>Uses sampling limits (<code>MAX_SIZE=500000</code>) and various subsampling parameters to limit memory use.</li> <li>Produces feature-level <code>shapley_report.tsv</code> under <code>${WORK_DIR}/compare</code>.</li> </ul> </li> <li>Runs <code>feature_importance_printer.py</code> to build a global and single-feature HTML visualizations under <code>${WORK_DIR}/compare/ButWhy</code>.</li> <li>Runs a second comparison under <code>compare.no_overfitting</code> with a restricted feature set derived from <code>CMP_FEATURE_RES</code> to check results when removing possibly overfitted features.</li> <li>Fixes plotly.js script tag paths in output HTML files so they link correctly from the final docs location.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#output-location","title":"Output Location","text":"<ul> <li>Primary comparison outputs: <code>${WORK_DIR}/compare</code></li> <li>No-overfitting comparison outputs: <code>${WORK_DIR}/compare.no_overfitting</code></li> <li>Feature explainability HTML reports: <code>${WORK_DIR}/compare/ButWhy</code> and <code>${WORK_DIR}/compare.no_overfitting/ButWhy</code></li> <li>Shapley reports: <code>${WORK_DIR}/compare/shapley_report.tsv</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Open <code>${WORK_DIR}/compare/</code> and use same methodology as in Test 05 in Silent Run</li> <li>Review <code>${WORK_DIR}/compare/compare_rep.txt</code> for a textual summary of the comparison.</li> <li>Use the <code>ButWhy</code> HTMLs to understand global and single-feature contributions and check whether important features behave similarly between the silent run and the reference.</li> <li>The <code>compare.no_overfitting</code> run restricts features based on <code>CMP_FEATURE_RES</code> and can reveal whether top differences are driven by a small set of features.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Large matrices exceed memory or time limits:<ul> <li>Adjust <code>MAX_SIZE</code> and subsample parameters in the script to reduce processing size.</li> </ul> </li> <li>Missing <code>TestModelExternal</code> or <code>feature_importance_printer.py</code> utilities:<ul> <li>Ensure these are available on PATH.</li> </ul> </li> <li>Plotly template or <code>plot.py</code> not found:<ul> <li>Confirm <code>plot.py</code> exists in PATH and that its <code>templates/plot_with_missing.html</code> file is reachable.</li> </ul> </li> <li>Incorrect <code>CMP_FEATURE_RES</code> formatting:<ul> <li>The script expects comma-separated tokens like <code>FeatureA:0.1,FeatureB:10</code>-ensure formatting matches.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2005%20-%20Compare%20Matrices%20%26%20Features.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/compare/compare_rep.txt</code></li> <li><code>${WORK_DIR}/compare/shapley_report.tsv</code></li> <li><code>${WORK_DIR}/compare/ButWhy/Global.html</code></li> <li><code>${WORK_DIR}/compare.no_overfitting/ButWhy/Global.html</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html","title":"Test 06 - Matrix Feature Statistics &amp; KLD Analysis","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#purpose","title":"Purpose","text":"<p>Compute per-feature statistics comparing the silent/external run matrix against a reference matrix and produce plots and Kullback\u2013Leibler divergence (KLD) metrics for the top features derived from a Shap/ButWhy report. The test writes per-feature HTML graphs and a <code>features_stats.tsv</code> summary. This is similar to Silent Run Test (no labels) - 10. Now we loaded the repository and have outcome. The stratification and comparasing can also stratify by the outcome.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Path to the reference run (used to obtain the original model and shapley report)</li> <li><code>MODEL_PATH</code>: Path to the model to be used on the repository (by default taken from <code>${FIRST_WORK_DIR}/model/model.medmdl</code>)</li> <li><code>REFERENCE_MATRIX</code>: CSV path to the reference matrix</li> <li><code>CONFIG_DIR</code>: Directory containing config files such as <code>feat_resolution.tsv</code></li> <li><code>CMP_FEATURE_RES</code>: Comma-separated list used to determine top features and resolutions</li> </ul> <p>Script-level required params (from the script header):</p> <ul> <li><code>REQ_PARAMETERS=['WORK_DIR', 'FIRST_WORK_DIR', 'MODEL_PATH', 'REFERENCE_MATRIX', 'CONFIG_DIR']</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 6\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Check <code>${WORK_DIR}/outputs/graphs</code> and <code>${WORK_DIR}/outputs/features_stats.tsv</code> for results.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Validates that a Shap/ButWhy report exists under <code>${FIRST_WORK_DIR}/ButWhy/shapley.report</code> and fails if not present.</li> <li>Optionally reads a resolution file <code>${CONFIG_DIR}/feat_resolution.tsv</code> to apply trimming and resolution rules for plotting.</li> <li>Reads the top features from the Shapley report (top 25 positive importance features plus <code>pred_0</code>).</li> <li>Loads the repository matrix for <code>${WORK_DIR}/Samples/3.test_cohort.samples</code> using <code>get_matrix()</code> and loads the reference CSV.</li> <li>For each selected feature:</li> <li>Applies trimming/resolution rules if provided.</li> <li>Computes mean, std, and missing rates overall, for cases, and for controls.</li> <li>Writes per-feature HTML bar plots (via <code>plot_graph</code>) to <code>${WORK_DIR}/outputs/graphs</code>.</li> <li>Computes KLD between cases and controls and prints the metric.</li> <li>Summarizes per-feature stats in <code>${WORK_DIR}/outputs/features_stats.tsv</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#output-location","title":"Output Location","text":"<ul> <li>Feature graphs: <code>${WORK_DIR}/outputs/graphs</code> (one HTML per feature)</li> <li>Stats TSV: <code>${WORK_DIR}/outputs/features_stats.tsv</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Open the generated HTML files to visually inspect the distribution of feature values between cases, controls and reference.</li> <li>Use <code>features_stats.tsv</code> to get a tabular summary of mean, std, and missingness across groups.</li> <li>KLD values printed to stdout indicate how separable feature distributions are between cases and controls; higher KLD suggests more divergence.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Missing <code>shapley.report</code> under <code>${FIRST_WORK_DIR}/ButWhy</code>:<ul> <li>Run ButWhy analysis (feature importance) before running this test in Silent Run</li> </ul> </li> <li>Feature name mismatches between the Shap report and repository matrix columns:<ul> <li>Confirm that feature naming conventions match (the script searches by substring match and expects exactly one match per selected feature).</li> </ul> </li> <li>Large reference matrices or memory issues when loading matrices:<ul> <li>Use a machine with sufficient memory or sample/reduce the dataset before running.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#example-output-snippets","title":"Example output snippets","text":"<p>This is similar to Silent Run Test (no labels) - 10, but with stratification by outcome.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2006%20-%20Test%20Matrix%20Features.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/outputs/graphs/*.html</code></li> <li><code>${WORK_DIR}/outputs/features_stats.tsv</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html","title":"Test 07 - Bootstrap Analysis","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#purpose","title":"Purpose","text":"<p>Run bootstrap-based performance estimates and tables for the evaluation cohort, producing pivot tables with AUC, sensitivity/precision operating points, odds ratios and other KPIs. Optionally runs comparator analysis when alternative predictions are provided.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Reference run (contains baseline bootstrap JSON and cohort parameters)</li> <li><code>BT_JSON</code>: Path to bootstrap features JSON (defaults to <code>${FIRST_WORK_DIR}/json/bootstrap/bt_features.json</code>)</li> <li><code>BT_COHORT</code>: Path to bootstrap cohorts params (defaults to <code>${FIRST_WORK_DIR}/json/bootstrap/bt.params</code>)</li> <li><code>SUB_CODES</code>: Comma-separated sub-cohort identifiers (used to run per-sub-cohort bootstraps)</li> <li>Optional: <code>ALT_PREDS_PATH</code> to run comparator bootstrap between predictions</li> <li>Optional: <code>CONTROL_WEIGHT</code> to adjust weighting of controls</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 7\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Check <code>${WORK_DIR}/bootstrap</code> for output pivot tables and reports.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Joins samples (<code>${WORK_DIR}/Samples/relabeled.samples</code>) with predictions from <code>${FIRST_WORK_DIR}/compare/3.test_cohort.preds</code> (or <code>${FIRST_WORK_DIR}/predictions/all.preds</code> if present) to produce <code>${WORK_DIR}/bootstrap/all.preds</code>.</li> <li>Filters to eligible predictions and produces <code>${WORK_DIR}/bootstrap/eligible_only.preds</code>.</li> <li>Sets up arrays of sensitivity, FPR and precision working points used for bootstrap KPIs.</li> <li>Invokes bootstrap_app with the provided <code>BT_JSON</code> and <code>BT_COHORT</code> to generate pivot outputs and per-cohort bootstrap results under <code>${WORK_DIR}/bootstrap/mes.bt*</code> and pivot text files <code>${WORK_DIR}/bootstrap/*.pivot_txt</code>.</li> <li>For each <code>SUB_CODES</code> member, creates per-sub-cohort prediction files and runs bootstrap again (with <code>sample_per_pid=1</code> for sub-cohorts).</li> <li>Runs <code>bootstrap_format.py</code> to aggregate pivot reports into TSV tables like <code>${WORK_DIR}/bootstrap/bt.just_MES.tsv</code>.</li> <li>If <code>ALT_PREDS_PATH</code> is provided, fits a comparator to the cohort and runs an additional bootstrap to compare the model against the alternative predictions, producing <code>${WORK_DIR}/bootstrap/comperator.bt*</code> and aggregated tables <code>${WORK_DIR}/bootstrap/bt.MES_comperator.tsv</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#output-location","title":"Output Location","text":"<ul> <li>Bootstrap pivot and pivot text: <code>${WORK_DIR}/bootstrap/*.pivot_txt</code></li> <li>Aggregated tables: <code>${WORK_DIR}/bootstrap/bt.just_MES.tsv</code>, <code>${WORK_DIR}/bootstrap/bt.MES_comperator.tsv</code> (if comparator run)</li> <li>Per-sub-cohort bootstrap files: <code>${WORK_DIR}/bootstrap/&lt;subcode&gt;.bt</code> and <code>&lt;subcode&gt;.preds</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Open pivot tables (<code>.pivot_txt</code>) and aggregated TSV files to inspect AUC, sensitivity/precision points, ORs, LIFT, and other measures across cohorts and groups.</li> <li>Compare comparator tables to see whether the alternative predictions outperform the model in specific cohorts or overall.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Mismatch between sample IDs and prediction files when joining:<ul> <li>Ensure sample files use the expected ID columns and prediction files have matching ID columns.</li> </ul> </li> <li><code>bootstrap_app</code> or <code>bootstrap_format.py</code> missing or failing:<ul> <li>Confirm these CLI tools are available and their versions are compatible with the <code>BT_JSON</code> being used.</li> </ul> </li> <li>Large sample sizes causing long runtime:<ul> <li>Use subsampling arguments in <code>bootstrap_app</code> (e.g., <code>sample_per_pid</code>, <code>sample sizes</code>) to speed up analysis.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#example-output-snippets","title":"Example output snippets","text":"<p>1) Aggregated bootstrap TSV header (example):</p> <pre><code>Cohort  Measure Value\nAge:50-75   AUC_Mean    0.85123\n</code></pre> <p>2) Console summary after running: </p> <pre><code>Please refer to /path/to/work_dir/bootstrap/bt.just_MES.tsv\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The test computes eligible-only predictions by filtering out lines with missing eligibility fields.</li> <li>Working points for sensitivity, precision and FPR are hard-coded arrays in the script and can be customized as needed.</li> <li>Control weighting can be passed via <code>CONTROL_WEIGHT</code> to adjust bootstrap behaviour for class imbalance.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2007%20-%20Bootstrap%20Analysis.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/bootstrap/mes.bt.pivot_txt</code></li> <li><code>${WORK_DIR}/bootstrap/bt.just_MES.tsv</code></li> <li><code>${WORK_DIR}/bootstrap/*.bt</code> (per-cohort)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html","title":"Test 08 - Age of Flagged Analysis","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#purpose","title":"Purpose","text":"<p>Generate age and temporal analyses for flagged cases/controls across specified cohorts. Produces graphs and statistics for cases and controls stratified by cohorts defined in the <code>COHORTS</code> environment variable.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Reference run (contains rep and bootstrap JSON)</li> <li><code>COHORTS</code>: A pipe-separated list of cohort definitions; each item should be <code>name=features</code> where <code>features</code> is a filter string understood by <code>FilterSamples</code>/bootstrap</li> <li>Optional: <code>ALT_PREDS_PATH</code> for comparator comparisons</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 8\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Outputs are stored under <code>${WORK_DIR}/age_of_flagged</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>If <code>ALT_PREDS_PATH</code> is not set, prints a message noting comparator comparisons will be skipped.</li> <li>For each cohort listed in <code>COHORTS</code>:<ul> <li>Uses <code>FilterSamples</code> to create cohort-specific prediction files <code>${WORK_DIR}/bootstrap/&lt;cohort_name&gt;.preds</code> if not present.</li> <li>Invokes <code>age_of_flagged.py</code> (a Python helper) to compute and plot age distributions and other temporal statistics for the given cohort; outputs go under <code>${WORK_DIR}/age_of_flagged</code>.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#output-location","title":"Output Location","text":"<ul> <li>Per-cohort age analysis: <code>${WORK_DIR}/age_of_flagged</code> (HTML or image outputs and supporting data files)</li> <li>Per-cohort prediction files used as input: <code>${WORK_DIR}/bootstrap/&lt;cohort&gt;.preds</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Open the plots in <code>${WORK_DIR}/age_of_flagged</code> to inspect age distributions among flagged (predicted high-risk) patients and controls.</li> <li>Compare cohort-specific plots to check consistency (e.g., older average age in one cohort may indicate selection bias).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Misformatted <code>COHORTS</code> variable:<ul> <li>Ensure <code>COHORTS</code> contains entries like <code>cohortName=featureFilter</code> separated by <code>|</code>.</li> </ul> </li> <li>Missing <code>FilterSamples</code> or <code>age_of_flagged.py</code> helper scripts:<ul> <li>Ensure the referenced helpers are available under <code>${CURR_PT}/resources</code> or on PATH.</li> </ul> </li> <li>Missing input predictions or samples:<ul> <li>Confirm <code>${WORK_DIR}/bootstrap/eligible_only.preds</code> exists and prediction files referenced by <code>${FIRST_WORK_DIR}</code> are present.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2008%20-%20Age%20Of%20Flagged.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/age_of_flagged/*</code> (per-cohort graphs)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html","title":"Test 09 - Features With Missing Values Analysis","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#purpose","title":"Purpose","text":"<p>Run feature analysis that specifically inspects missingness patterns for a selected set of important features. Computes statistics and plots for features where missing values are informative or prevalent.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Reference run (contains model and rep)</li> <li><code>MODEL_PATH</code>: Path to the model to be used for generating features</li> <li><code>CONFIG_DIR</code>: Directory holding <code>feat_resolution.tsv</code> and similar config files</li> <li><code>CMP_FEATURE_RES</code>: Comma-separated list of feature:resolution tokens used to define the top features and per-feature resolution</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 9\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Outputs are placed under <code>${WORK_DIR}/outputs/features_stats.with_missings.tsv</code> and <code>${WORK_DIR}/outputs/graphs_with_missings</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Loads the repository with <code>med.PidRepository()</code> and initializes it with <code>${WORK_DIR}/rep/test.repository</code>.</li> <li>Loads the model from <code>${FIRST_WORK_DIR}/model/model.medmdl</code>, fits it to the repository, and applies feature generators to the samples (<code>${WORK_DIR}/Samples/3.test_cohort.samples</code>), producing a feature DataFrame.</li> <li>Selects <code>top_features</code> from <code>CMP_FEATURE_RES</code> and applies per-feature resolution adjustments.</li> <li>For each feature:<ul> <li>Computes overall, case, and control means, stds and missingness percentages (missing is expected to be coded as <code>-65336</code>).</li> <li>Writes results to <code>${WORK_DIR}/outputs/features_stats.with_missings.tsv</code>.</li> <li>Generates plots (markers+lines) saved under <code>${WORK_DIR}/outputs/graphs_with_missings</code> (plotly HTML files).</li> <li>Computes KLD between case and control distributions and prints results.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#output-location","title":"Output Location","text":"<ul> <li>Stats TSV: <code>${WORK_DIR}/outputs/features_stats.with_missings.tsv</code></li> <li>Graphs: <code>${WORK_DIR}/outputs/graphs_with_missings</code> (one HTML per feature)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Use the TSV to quickly see which features have high missingness and how that differs between cases and controls.</li> <li>Open HTML graphs to inspect distribution patterns and whether missingness behaves differently across outcome groups.</li> <li>Elevated KLD suggests the feature's distribution differs notably between cases and controls (including missingness patterns).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Failure to load repository or model:<ul> <li>Ensure <code>${WORK_DIR}/rep/test.repository</code> and <code>${FIRST_WORK_DIR}/model/model.medmdl</code> exist.</li> </ul> </li> <li>Feature name mismatches or multiple substring matches:<ul> <li>The script matches features by substring and will fail if it finds zero or multiple matches. Confirm naming conventions.</li> </ul> </li> <li>Missing <code>med</code> Python package or model runtime dependencies:<ul> <li>Ensure the runtime Python environment has the <code>med</code> package and project helpers installed.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>Feature resolution is applied by rounding values to the specified resolution in <code>CMP_FEATURE_RES</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2009%20-%20Features%20With%20Missing.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/outputs/features_stats.with_missings.tsv</code></li> <li><code>${WORK_DIR}/outputs/graphs_with_missings/*.html</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html","title":"Test 10 - Calibration Test","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#purpose","title":"Purpose","text":"<p>Run calibration checks for the model on the evaluation cohort. Produces calibration pivot tables and bootstrap calibration graphs showing model calibration across score bins, time windows, and cohorts.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#required-inputs","title":"Required Inputs","text":"<p>From <code>configs/env.sh</code> and the test invocation environment:</p> <ul> <li><code>WORK_DIR</code>: Working folder where outputs will be written</li> <li><code>FIRST_WORK_DIR</code>: Reference run (used to obtain baseline predictions)</li> <li><code>BT_COHORT_CALIBRATION</code>: Cohorts file for calibration analysis</li> <li><code>BT_JSON_CALIBRATION</code>: Bootstrap JSON for calibration-specific metrics</li> <li>Optional: <code>ALT_PREDS_PATH</code> for comparator predictions</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#how-to-run","title":"How to Run","text":"<p>From your TestKit folder, execute:</p> <pre><code>./run.specific.sh 10\n</code></pre> <p>Or include as part of the full suite:</p> <pre><code>./run.sh\n</code></pre> <p>Check <code>${WORK_DIR}/calibration</code> for calibration outputs and graphs.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#what-this-test-does","title":"What This Test Does","text":"<ul> <li>Joins samples with predictions similar to Test 07 to create <code>${WORK_DIR}/bootstrap/all.preds</code> and filters to <code>${WORK_DIR}/bootstrap/eligible_only.preds</code>.</li> <li>Validates <code>BT_COHORT_CALIBRATION</code> contains yearly breakdowns and warns if not present.</li> <li>Invokes <code>TestCalibration</code> with the provided cohorts and JSON model to produce calibration outputs under <code>${WORK_DIR}/calibration/test_calibration</code> and calibration graphs under <code>${WORK_DIR}/calibration/graphs</code>.</li> <li>Prints a brief numeric summary of mean score and incidence.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#output-location","title":"Output Location","text":"<ul> <li>Calibration outputs: <code>${WORK_DIR}/calibration/test_calibration</code> (tables and pivot files)</li> <li>Calibration graphs: <code>${WORK_DIR}/calibration/graphs</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#how-to-interpret-results","title":"How to Interpret Results","text":"<ul> <li>Open pivot tables and graphs to inspect calibration curves across bins, cohorts, and time windows.</li> <li>Confirm expected calibration behaviour (e.g., predicted probabilities aligned with observed incidence per bin).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#common-failure-modes-and-suggestions","title":"Common failure modes and suggestions","text":"<ul> <li>Missing <code>TestCalibration</code> utility or incompatible flags:<ul> <li>Ensure <code>TestCalibration</code> exists and the JSON/params supplied match the utility's expected schema.</li> </ul> </li> <li>Insufficient sample size for fine-grained binning:<ul> <li>Adjust <code>pred_binning_arg</code> or <code>bt_params</code> to use fewer bins or fewer bootstrap loops.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#example-output-snippets","title":"Example output snippets","text":"<p>TestCalibration tool outputs</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#notes-and-implementation-details","title":"Notes and Implementation Details","text":"<ul> <li>The script uses <code>pred_binning_arg</code> with <code>iterative_merge</code> and parameters tuned to create up to 100 bins with minimum counts; it also runs <code>loopCnt=500</code> bootstrap samples by default.</li> <li>Calibration requires a good spread of predicted scores; if all scores are near a single value, binning will collapse and calibration plots will be uninformative.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Model%20Checklist/AutoTest/External_validation_after_SR/Test%2010%20-%20Calibration.html#test-results-review","title":"Test Results Review","text":"<p>Primary files to inspect after running this test:</p> <ul> <li><code>${WORK_DIR}/calibration/test_calibration</code> (pivot and summary tables)</li> <li><code>${WORK_DIR}/calibration/graphs/*</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/index.html","title":"Python","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Python/index.html#quick-start","title":"Quick Start","text":"<p>We provide three libraries for use:</p> <ol> <li>MedPython: A Python library that integrates with our C library.</li> <li>ETL Library: A pure Python utility designed to assist in creating Data Repositories.</li> <li>AlgoMarker API Server: A pure Python server wrapper for utilizing the AlgoMarker library (limited to predict/apply for implementation setting. Much lighter as opposed to MedPython).</li> </ol> <p>Note: These libraries are not currently available as PyPi packages. To use them, you need to set the <code>PYTHONPATH</code> environment variable to their installation paths. For more information: Setup</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/index.html#pages","title":"Pages","text":"<ul> <li>MedPython<ul> <li>Examples: Usage examples for MedPython.</li> <li>Python Binding Troubleshooting: Guidance for troubleshooting Python bindings in MedPython.</li> <li>Extend and Develop: Instructions for exposing additional C++ APIs to Python.</li> </ul> </li> <li>ETL Library: Refer to the ETL Tutorial for more details.</li> <li>Python AlgoMarker API Server: Documentation for the pure Python FastAPI server of the AlgoMarker library.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/index.html#setup","title":"Setup","text":"<ol> <li>Clone the Git Repositories:<ul> <li>MR_LIBS</li> <li>MR_Tools</li> </ul> </li> <li>Set Up MedPython:    Follow the instructions in Setup MedPython.    The ETL Library and AlgoMarker API Server doesn't requires installation and are pure python code.</li> <li>Configure Environment Variables:    Ensure Python recognizes the libraries by setting the <code>PYTHONPATH</code> environment variable. Replace <code>${MR_LIBS}</code> with the path to the cloned <code>MR_LIBS</code> repository and <code>${MR_TOOLS}</code> with the path to the cloned <code>MR_Tools</code> repository.</li> </ol> <pre><code>export PYTHONPATH=${MR_LIBS}/Internal/MedPyExport/generate_binding/Release/medial-python312:${MR_TOOLS}/RepoLoadUtils/common\n</code></pre> <p>The Python AlgoMarker API Server does not require installation. Simply run the script directly when needed.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html","title":"Examples","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#import-medials-api-in-python","title":"Import Medial's API in Python","text":"<pre><code>import med\n# Optional set to use stdout, usually I'm not running this:\nmed.logger_use_stdout()\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#which-medpython-module-am-i-using","title":"Which MedPython module am I using","text":"<pre><code>print(med.__file__) # Shows file path\nif hasattr(med.Global, 'version_info'):\n    print(med.Global.version_info) # Shows compilation date and git commit hash when compliled \"version\"\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#inspect-available-functions","title":"Inspect Available Functions","text":"<p>Documentation is a work in progress. To inspect available methods:</p> <pre><code>help(med)\nhelp(med.PidRepository)\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#load-a-repository","title":"Load a Repository","text":"<pre><code>rep = med.PidRepository()\nrep.read_all(\n    '/nas1/Work/CancerData/Repositories/THIN/thin_2021/thin.repository',\n    [],\n    ['GENDER', 'DEATH', 'BDATE', 'Albumin']\n)\nprint(med.cerr())\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#iterating-over-a-signal","title":"Iterating over a Signal","text":"<pre><code>## This is not the proper way to work with Python, since Python loops are very slow, however, it's nice as a poc.\n## Don't do this, using python loops is slow - use get_sig to retrieve dataframe\n\nsigname = 'Albumin'\nrep = med.PidRepository()\nrep.read_all('/home/Repositories/THIN/thin_jun2017/thin.repository',[],[signame])\nprint(med.cerr())\nfor pid in rep.pids[20:30]:\n  usv = rep.uget(pid, rep.sig_id(signame))\n  if len(usv)&gt;0:\n    print('\\n\\n')\n    for rec in usv:\n      print(\"Patient {} had {}={:.2} at {}\".format(pid, signame, rec.val(), rec.date()))\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#useful-functions-to-fix-signal-types","title":"Useful functions to fix signal types","text":"<pre><code>def fix_ts_m1900(df, col):\n    import datetime as dt\n    df[col] = dt.datetime(1900,1,1)+pd.TimedeltaIndex(df[col], unit='m')\ndef fix_date_ymd(df, col): df[col] = pd.to_datetime(df[col], format='%Y%m%d')\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#load-some-signals","title":"Load some signals","text":"<p>When using \"get_sig\" function, there is no need to call \"read_all\" before, \"init\" is enough. get_sig, loads the signal automatically from disk if needed and not loaded</p> <pre><code>albumin = rep.get_sig('Albumin')\nalbumin = albumin[albumin['date'] % 100 != 0]\nalbumin.rename(columns={'val': 'Albumin'}, inplace=True)\nfix_date_ymd(albumin, 'date')\ngender = rep.get_sig('GENDER')\ngender.rename(columns={'val': 'Gender'}, inplace=True)\nbdate = rep.get_sig('BDATE')\nbdate['val'] = bdate['val'] + 1\nfix_date_ymd(bdate, 'val')\nbdate.rename(columns={'val': 'BDate'}, inplace=True)\n\u00a0\nmortality = rep.get_sig('DEATH')\nmortality = mortality[(mortality['val'] % 100 &lt;= 31) &amp; (mortality['val'] % 100 &gt; 0)]\nfix_date_ymd(mortality, 'val')\nmortality.rename(columns={'val': 'MortDate'}, inplace=True)\n\u00a0\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#using-lookup-table-in-python","title":"Using Lookup table in python","text":"<ul> <li>When using \"get_sig\" function, there is no need to call \"read_all\" before, \"init\" is enough. get_sig, loads the signal automatically from disk if needed and not loaded.</li> <li>We can use dictionaries to query specific categorical codes and their hierarchies. <ul> <li>For example, by defining \"ICD10_CODE:J00-J99\", we'll capture all codes within this group based on the dictionary's definition of ICD10. </li> <li>This method relies on predefined parent-child pairs for hierarchy and does not use regular expressions. </li> <li>It is not limited to ICD10, ICD9 or specific known code system, but you will need to define the dictionaries correctly and their hierarchies.</li> </ul> </li> </ul> <pre><code># readmissions is data frame with readmitted patients\nrep = med.PidRepository()\nrep.read_all(FLAGS.rep, readmissions.pid.values.astype('int32'), ['ADMISSION','DIAGNOSIS_IP','DIAGNOSIS_OP'])\nadmissions = rep.get_sig('ADMISSION').rename(columns = {'time0':'outcomeTime'})\nreadmissions = readmissions.merge(admissions,on = ['pid','outcomeTime'],how='outer')\n# Handle missing admissions\nreadmissions.loc[((readmissions.outcome==1) | (readmissions.outcome==2) | (readmissions.outcome==3)) &amp; (readmissions.time1.isna()),'time1'] = readmissions.loc[((readmissions.outcome==1) | (readmissions.outcome==2) | (readmissions.outcome==3)) &amp; (readmissions.time1.isna()),'outcomeTime']\n# Read Relevant Codes\nicd9 = pd.read_csv(FLAGS.icd9,header=None,names=['code']).code.values\n# Add Adverse Events for each ICD9 code in icd9 dataframe. It also uses hierarchy defined in the dictionary, for example using \"487\" includes: 487.0, 487.1, 487.8, etc.\nlut = rep.dict.prep_sets_lookup_table(rep.dict.section_id('DIAGNOSIS_IP'),['ICD9_CODE:'+str(x) for x in icd9])\nip_diagnosis = rep.get_sig('DIAGNOSIS_IP',translate=False)\nip_diagnosis = ip_diagnosis[(lut[ip_diagnosis.val0]!=0)]\nop_diagnosis = rep.get_sig('DIAGNOSIS_OP',translate=False)\nop_diagnosis = op_diagnosis[(lut[op_diagnosis.val0]!=0)]\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#load-medmodel-and-apply-predict-on-sample","title":"Load MedModel and apply (predict) on sample","text":"<pre><code>rep_path='' #Path of repositroy\nmodel_file ='' #Path of MedModel\nsamples_file = '' #path of samples or load samples from DataFrame using: samples.from_df(dataframe_object with the right columns)\nprint(\"Reading basic Repository structure for fitting model\")\nrep = med.PidRepository()\nrep.init(rep_path) #init model for first proccesing of \"model.fit_for_repository\"\n\u00a0\nprint(\"Reading Model\")\nmodel = med.Model()\nmodel.read_from_file(model_file)\nmodel.fit_for_repository(rep)\nsignalNamesSet = model.get_required_signal_names() #Get list of relevant signals the model needed to fetch from repository\n\u00a0\nprint(\"Reading Samples\")\nsamples = med.Samples()\nsamples.read_from_file(samples_file)\nids = samples.get_ids() #Fetch relevant ids from samples to read from repository\n\u00a0\nprint(\"Reading Repository\")\nrep.read_all(rep_path, ids, signalNamesSet) #read needed repository data\n\u00a0\n#Apply model:\nmodel.apply(rep, samples)\ndf = samples.to_df()\ndf.to_csv('output_file')\nsamples.write_to_file('write_to_samples_file')\n\u00a0\n#feature matrix exists in - model.features.to_df() . The \"samples\" object now has the scores\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#learn-model-from-json-to-generate-matrix","title":"Learn model from json to generate matrix","text":"<pre><code>rep_path='' #Path of repositroy\njson_model ='' #Path of json\nsamples_file = '' #path of samples or load samples from DataFrame using: samples.from_df(dataframe_object with the right columns)\n\nprint(\"Reading basic Repository structure for fitting model\")\nrep = med.PidRepository()\nrep.init(rep_path) #init model for first proccesing of \"model.fit_for_repository\"\n\nprint(\"Reading Model\")\nmodel = med.Model()\nmodel.init_from_json_file(model_file)\nmodel.fit_for_repository(rep)\nsignalNamesSet = model.get_required_signal_names() #Get list of relevant signals the model needed to fetch from repository\n\nprint(\"Reading Samples\")\nsamples = med.Samples()\nsamples.read_from_file(samples_file)\nids = samples.get_ids() #Fetch relevant ids from samples to read from repository\n\nprint(\"Reading Repository\")\nrep.read_all(rep_path, ids, signalNamesSet) #read needed repository data\n\n#Learn model:\nmodel.learn(rep, samples)\nmodel.features.to_df().write_to_file('write_to_matrix_file')\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#bootstrap-analysis","title":"Bootstrap analysis","text":"<pre><code>import pandas as pd\ndf=pd.read_feather('/nas1/Work/Users/Ilya/Mayo/Feathers/predictions_073.feather')\ndf=df[['true_V', 'prob_V']]\n#Example of Analyzing df with bootstrap\nbt=med.Bootstrap()\nres=bt.bootstrap(df['prob_V'], df['true_V'])\nall_measurment_names=res.keys()\nprint('AUC: %2.3f [%2.3f - %2.3f]'%(res['AUC_Mean'], res['AUC_CI.Lower.95'], res['AUC_CI.Upper.95']))\n#Can convert to dataframe with Measurement and Value columns:\nres_df=res.to_df()\nres_df[res_df['Measurement'].str.startswith('AUC')]\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#bootstrap-analysis-on-samples","title":"Bootstrap analysis on samples","text":"<pre><code>samples=med.Samples()\nsamples.read_from_file('/nas1/Work/AlgoMarkers/Pre2D/pre2d_1_041219/Performance_no_drugs/OnTest/Partial_All_on_OnTest_2_no_drugs_filtered.preds')\nREP_PATH='/nas1/Work/CancerData/Repositories/THIN/thin_jun2017/thin.repository'\nJSON_TO_FILTER='/server/UsersData/alon/MR/Projects/Shared/Projects/configs/Diabetes/configs/bt_features.json'\nCOHORT_FILE='/server/UsersData/alon/MR/Projects/Shared/Projects/configs/Diabetes/configs/bt_params'\nbt=med.Bootstrap()\nres=bt.bootstrap_cohort(samples, REP_PATH,JSON_TO_FILTER, COHORT_FILE)\nres_df=res.to_df()\nres_df\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#print-errors-in-medpython","title":"print errors in medPython","text":"<p><pre><code>print(med.cerr())\n</code></pre> </p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Examples.html#additional-examples","title":"Additional Examples","text":"<ul> <li>See <code>$MR_LIBS/Internal/MedPyExport/examples/MedProcUtils/</code> for more Python implementations. MR_LIBS is git repository</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html","title":"Extending and Developing the Medial C++ API for Python","text":"<p>This document outlines the low-level C++ details required to further develop the Python bindings for our API.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#objectives","title":"Objectives","text":"<p>The goals are to:</p> <ol> <li>Use our API from Python.</li> <li>Employ Python as a \"Glue Language\".</li> <li>Enable prototyping, exploration, and discovery in Python.</li> <li>Ensure interoperability with other frameworks.</li> <li>Minimize maintenance overhead when updating code.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#implementation-approach","title":"Implementation Approach","text":"<p>For maintainability, SWIG is chosen as the binding solution. Other options considered:</p> <ul> <li>Cython: Adds another language to the process.</li> <li>Boost::python: Limited support/resources; requires special definitions for each exported member.</li> <li>ctypes: Direct C calls, but parameter definitions are error-prone and may lack binary compatibility across platforms.</li> </ul> <p>Why SWIG?</p> <ul> <li>Mature project with extensive resources. TensorFlow uses SWIG for example.</li> <li>Handles memory and ownership complexities.</li> <li>Good NumPy support.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#our-implementation-medpyexport","title":"Our Implementation: \"MedPyExport\"","text":"<ul> <li>Located in <code>MR_LIBS/Internal/MedPyExport</code>.</li> <li>Wraps classes and exports only necessary functions for Python.</li> <li>Minimal learning curve for contributors familiar with C++.</li> <li>SWIG interface files (<code>.i</code>) are auto-generated during build.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#notes-pitfalls","title":"Notes &amp; Pitfalls","text":"<ul> <li>Always use <code>std::vector</code> for vectors.</li> <li>Restart Jupyter Notebook after adding new functions/classes to reload wrappers.</li> <li>Use distinct names for NumPy parameters of different types.</li> <li>Method overloading is not supported (may be possible to implement).</li> <li>Avoid \"big-data\" loops; use NumPy/Pandas for vectorized operations.</li> <li>Build system is separate from the main CMake files.</li> <li>No direct Pandas DataFrame conversion yet.</li> <li>SWIG scans only simple C++ headers; keep implementation in <code>.cpp</code> files.</li> <li>Compilation works on both Windows and Linux (tested execution on Linux/Jupyter).</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#build-system","title":"Build System","text":"<ul> <li>Targeted for Linux machines</li> <li>Windows compilation works but does not produce a Python-loadable binary.</li> <li>Uses a specialized CMake file for SWIG integration.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#directory-structure","title":"Directory Structure","text":"<p>Located in <code>MR_LIBS/Internal/MedPyExport</code>:</p> <ul> <li><code>MedPyExport</code>: Source files.</li> <li><code>generate_binding</code>: CMake and binding generation files.<ul> <li><code>MedPython</code>: SWIG interface files.<ul> <li><code>scripts</code>: Helper scripts for compilation.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#key-files","title":"Key Files","text":"<ul> <li><code>make.sh</code>: Bash script to start CMake build.</li> <li><code>MedPython/MedPython.i</code>: Main SWIG interface.</li> <li><code>MedPython/medial-numpy.i</code>: NumPy SWIG interface.</li> <li><code>MedPython/MedPython.h</code>: Entry point for headers scanned by SWIG.</li> <li><code>MedPython/MedPython.c</code>: Entry point for C files scanned by SWIG.</li> <li><code>MedPython/pythoncode.i</code>: Python code for the binding.</li> <li><code>MedPython/scripts/make_apply.py</code>: Script to generate NumPy apply directives.</li> <li><code>MedPython/apply_directives.i</code>: Auto-generated SWIG directives.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#extending-the-code","title":"Extending the Code","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#example-exporting-a-class","title":"Example: Exporting a Class","text":"<pre><code>// .h file for the class 'PidRepository'\n#include \"MedPyCommon.h\"\nclass MedPidRepository;\nclass MPPidRepository {\npublic:\n    MedPidRepository* o;\n    // ...\n};\n</code></pre> <ul> <li>Include <code>MedPyCommon.h</code> for utilities/macros.</li> <li>Exported class names start with <code>MP</code> to avoid conflicts.</li> <li>Use a pointer to the wrapped object.</li> <li>Keep headers simple for SWIG; implementation in <code>.cpp</code> files.</li> </ul> <p>Add all exported headers to <code>MedPyExport.h</code>:</p> <pre><code>#ifndef __MED_PY_EXPORT_H\n#define __MED_PY_EXPORT_H\n#include \"MedPyExportExample.h\"\n#include \"MPPidRepository.h\"\n#include \"MPDictionary.h\"\n// ...\n#endif\n</code></pre> <p>Python Usage:</p> <pre><code>import medpython as med\nrep = med.PidRepository()\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#adding-a-new-class","title":"Adding a New Class","text":"<pre><code>class MPPidRepository {\npublic:\n    MedPidRepository* o;\n    MPPidRepository();\n    ~MPPidRepository();\n    int read_all(const string &amp;conf_fname);\n    string dict_name(int section_id, int id);\n    std::vector&lt;bool&gt; dict_prep_sets_lookup_table(int section_id, const std::vector&lt;std::string&gt; &amp;set_names);\n    // ...\n};\n</code></pre> <ul> <li>Basic types are mapped automatically.</li> <li>Use <code>std::vector</code> for vectors.</li> </ul> <p>Implementation Example:</p> <pre><code>#include \"MPPidRepository.h\"\n#include \"InfraMed/InfraMed/MedPidRepository.h\"\nMPPidRepository::MPPidRepository() : o(new MedPidRepository()) {}\nMPPidRepository::~MPPidRepository() { delete o; }\nint MPPidRepository::read_all(const std::string &amp;conf_fname) { return o-&gt;read_all(conf_fname); }\nstring MPPidRepository::dict_name(int section_id, int id) { return o-&gt;dict.name(section_id, id); }\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#properties","title":"Properties","text":"<p>Implement getter/setter methods with <code>MEDPY_GET_</code> and <code>MEDPY_SET_</code> prefixes:</p> <pre><code>class MPSamples {\n    int MEDPY_GET_time_unit();\n    void MEDPY_SET_time_unit(int new_time_unit);\n};\n</code></pre> <p>Python Usage:</p> <pre><code>&gt;&gt;&gt; s = med.Samples()\n&gt;&gt;&gt; s.time_unit\n1\n</code></pre> <ul> <li>Omit the setter for read-only properties.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#static-const-variables","title":"Static Const Variables","text":"<p>Mapped to class variables in Python:</p> <pre><code>class MPTime {\npublic:\n    static const int Undefined;\n    static const int Date;\n    static const int Years;\n};\n</code></pre> <p>Implementation:</p> <pre><code>const int MPTime::Undefined = MedTime::Undefined;\nconst int MPTime::Date = MedTime::Date;\n</code></pre> <p>Python Usage:</p> <pre><code>print(med.Time.Date)\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#iterators","title":"Iterators","text":"<p>Array Iterator Example:</p> <pre><code>class MPSigVectorAdaptor {\npublic:\n    int __len__();\n    MPSig __getitem__(int i);\n};\n</code></pre> <ul> <li>Class name ends with <code>VectorAdaptor</code>.</li> <li>Implements <code>__len__</code> and <code>__getitem__</code>.</li> </ul> <p>Map Iterator Example:</p> <pre><code>class MPStringFeatureAttrMapAdaptor {\n    std::map&lt;std::string, FeatureAttr&gt;* o;\npublic:\n    int __len__();\n    MPFeatureAttr __getitem__(std::string key);\n    void __setitem__(std::string key, MPFeatureAttr&amp; val);\n    std::vector&lt;std::string&gt; keys();\n};\n</code></pre> <ul> <li>Class name ends with <code>MapAdaptor</code>.</li> <li>Implements <code>__len__</code>, <code>__getitem__</code>, and optionally <code>__setitem__</code>, etc.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#numpy-arrays","title":"NumPy Arrays","text":"<p>Input Arrays:</p> <pre><code>class MPPidRepository {\n    int read_all(string conf_fname, MEDPY_NP_INPUT(int* pids_to_take, int num_pids_to_take));\n}\n</code></pre> <p>In-place Arrays:</p> <pre><code>void MedPyExportExample::numpy_vec_in_out(MEDPY_NP_INPLACE(double* vec, int m));\n</code></pre> <p>Output Arrays:</p> <pre><code>class MPFeatures {\n    void MEDPY_GET_weights(MEDPY_NP_OUTPUT(float** float_out_buf, int* float_out_buf_len));\n};\n</code></pre> <p>Variant Output Example:</p> <pre><code>void getitem(string key, MEDPY_NP_VARIANT_OUTPUT(void** var_arr, int* var_arr_sz, int* var_arr_type)) {\n    // Implementation sets var_arr, var_arr_sz, var_arr_type based on key\n    *var_arr_sz = 0;\n    if (key == \"i\")\n    {\n        *var_arr = (void*)malloc(sizeof(int) * 10);\n        *var_arr_sz = 10;\n        *var_arr_type = (int)MED_NPY_TYPES::NPY_INT;\n        for (int i = 0; i &lt; 10; i++)\n            ((*(int**)var_arr))[i] = i * 5;\n    }\n    else if (key == \"d\")\n    {\n        *var_arr = (void*)malloc(sizeof(double) * 20);\n        *var_arr_sz = 20;\n        *var_arr_type = (int)MED_NPY_TYPES::NPY_DOUBLE;\n        for (int i = 0; i &lt; 20; i++)\n            ((*(double**)var_arr))[i] = i * 2.5;\n    }\n    else if (key == \"f\")\n    {\n        *var_arr = (void*)malloc(sizeof(float) * 15);\n        *var_arr_sz = 15;\n        *var_arr_type = (int)MED_NPY_TYPES::NPY_FLOAT;\n        for (int i = 0; i &lt; 15; i++)\n            ((*(float**)var_arr))[i] = i * 0.33333f;\n    }\n    else if (key == \"n\")\n    {\n        *var_arr = nullptr;\n    }\n}\n</code></pre> <p>Supported NumPy Types:</p> <pre><code>NPY_BOOL, NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT,\nNPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG, NPY_FLOAT, NPY_DOUBLE,\nNPY_LONGDOUBLE, NPY_CFLOAT, NPY_CDOUBLE, NPY_CLONGDOUBLE, NPY_OBJECT,\nNPY_STRING, NPY_UNICODE, NPY_VOID, NPY_DATETIME, NPY_TIMEDELTA, NPY_HALF,\nNPY_NTYPES, NPY_NOTYPE, NPY_CHAR, NPY_USERDEF, NPY_NTYPES_ABI_COMPATIBLE\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Extend%20and%20Develop.html#helper-functions-macros","title":"Helper Functions &amp; Macros","text":"<ul> <li>Docstrings: <code>MEDPY_DOC(function_or_class_name, docstring)</code></li> <li>Ignore for SWIG: <code>MEDPY_IGNORE(...)</code> or <code>#ifdef SWIG ... #endif</code></li> <li>Buffer to Vector: <code>buf_to_vector(int_in_buf, int_in_buf_len, idx);</code></li> <li>Vector to Buffer: <code>vector_to_buf(o-&gt;weights, float_out_buf, float_out_buf_len);</code></li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20AlgoMarker%20API%20Server.html","title":"Python AlgoMarker API Server","text":"<p>This project provides a Python server wrapper for the AlgoMarker C library (<code>libdyn_AlgoMarker.so</code>). The wrapper allows you to interact with AlgoMarker using python code or as a server with a REST API. The server uses python FastAPI.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20AlgoMarker%20API%20Server.html#location","title":"Location","text":"<p>You can find the Python library and a tool for querying AlgoMarkers in the MR_Tools at this folder <code>$MR_Tools/AlgoMarker_python_API</code></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20AlgoMarker%20API%20Server.html#files","title":"Files","text":"<ul> <li>AlgoMarker.py: Main Python wrapper for the C library. The wrapper will use the new JSON API if available, or fall back to the older API for compatibility. Can be also used directly without FastAPI with python code.</li> <li>test_algomarker_lib.py: Utility tool that uses <code>AlgoMarker.py</code> to query AlgoMarker.</li> <li>AlgoMarker_minimal.py: Minimal version of the wrapper with fewer functions.</li> <li>simple_app_example.py: Example app demonstrating usage of the minimal wrapper.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20AlgoMarker%20API%20Server.html#example-usage","title":"Example Usage","text":"<p>Run the utility tool with:</p> <pre><code>python MR_Tools/AlgoMarker_python_API/test_algomarker_lib.py \\\n  --amconfig /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lungflag.amconfig \\\n  --output /tmp/results.txt \\\n  --add_data_json_path /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/data.single.json \\\n  --request_json_path /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/req.single.json \\\n  --amlib /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lib/libdyn_AlgoMarker.so\n</code></pre> <ul> <li><code>--amlib</code> (optional): Specify a custom library path.</li> <li>If your request JSON includes <code>\"data\"</code>, you can run:</li> </ul> <pre><code>python MR_Tools/AlgoMarker_python_API/test_algomarker_lib.py \\\n  --amconfig /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lungflag.amconfig \\\n  --output /tmp/results.txt \\\n  --request_json_path /nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/req.full.json\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20AlgoMarker%20API%20Server.html#simple-usage-example","title":"Simple Usage Example","text":"<pre><code>from AlgoMarker_minimal import AlgoMarker\nimport json\n\nALGOMARKER_PATH = '/nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/lungflag.amconfig'\nREQUEST_JSON = '/nas1/Products/LungCancer/QA_Versions/LungFlag3.NEW.2023-07-26.With_ButWhy/examples/req.full.json'\n\ndef read_text_file(path):\n    with open(path, 'r') as fr:\n        return fr.read()\n\nwith AlgoMarker(ALGOMARKER_PATH) as algomarker:\n    discovery_json = algomarker.discovery()\n    print('$&gt; discovery - first 100 characters:')\n    print(json.dumps(discovery_json, indent=True)[:100])\n    print('$&gt; calculate - first 200 characters:')\n    resp = algomarker.calculate(read_text_file(REQUEST_JSON))\n    print(json.dumps(resp, indent=True)[:200])\n\nprint('All Done')\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20AlgoMarker%20API%20Server.html#running-as-a-fastapi-server","title":"Running as a FastAPI Server","text":"<p>You can also run the wrapper as a FastAPI server. This will wrapper <code>AlgoMarker.py</code> You will need to setup fastapi package with: <pre><code>python -m pip install fastapi\n</code></pre></p> <p>To configure, edit <code>run_server.sh</code>:</p> <ul> <li>Set <code>AM_CONFIG</code> to the path of your AlgoMarker <code>.amconfig</code> file.</li> <li>Set <code>AM_LIB</code> to the path of your compiled library (default is <code>/lib/libdyn_AlgoMarker.so</code> in the same directory as the amconfig file).</li> </ul> <p>After editing, you can run the script.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20binding%20Troubleshooting.html","title":"Python Binding Troubleshooting","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20binding%20Troubleshooting.html#problem-1-import-med-does-not-work","title":"Problem 1: <code>\"import med\"</code> does not work","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20binding%20Troubleshooting.html#possible-cause-medpython-is-not-in-path","title":"Possible Cause: MedPython is not in path","text":"<p>Please make sure and print our PYTHONPATH environment variable: <pre><code>echo $PYTHONPATH\n</code></pre> and make sure it points out to a folder that contains <code>med.py</code> file. If not, please follow Setup</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20binding%20Troubleshooting.html#problem-2-medpython-fails-to-compile","title":"Problem 2: MedPython fails to compile","text":"<p>If you encounter errors like the following:</p> <pre><code>[ 27%] Swig compile medpython.i for python\n:1: Error: Unable to find 'swig.swg'\n:3: Error: Unable to find 'python.swg'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:8: Error: Unable to find 'exception.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:9: Error: Unable to find 'typemaps.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:10: Error: Unable to find 'std_string.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medpython.i:11: Error: Unable to find 'std_vector.i'\n/nas1/UsersData/git/MR/Libs/Internal/MedPyExport/generate_binding/MedPython/medial-numpy.i:3295: Error: Unable to find 'std_complex.i'\n</code></pre> <p>This is likely due to an outdated <code>CMakeCache.txt</code> file containing old SWIG settings. To resolve this, delete the <code>CMakeCache.txt</code> file using the following command (Change MR_LIBS to the folder you clones MR_LIBS into):</p> <pre><code>rm $MR_LIBS/Internal/MedPyExport/generate_binding/CMakeBuild/Linux/Release/CMakeCache.txt\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Python/Python%20binding%20Troubleshooting.html#problem-3-stderr-in-jupyter-notebooks","title":"Problem 3: stderr in Jupyter Notebooks","text":"<p>Some API messages are printed to stderr and may not appear in Jupyter notebooks. To display these messages, use the <code>cerr()</code> utility:</p> <pre><code>med.cerr()\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html","title":"Using the Flow App","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#overview","title":"Overview","text":"<p>The Flow App is a versatile tool with multiple switches, each designed to perform a specific action. Below are its key functionalities:</p> <ul> <li>Load New Repository: Converts raw ETL output files into an efficient, binary, and indexed format compatible with the AlgoMedical library framework.</li> <li>Train a model.</li> <li>Apply a model to generate predictions.</li> <li>Extract feature matrices from the model pipeline.</li> <li>Print specific patient data or signal distributions.</li> <li>Feature Importance with Shapley Values Analysis.</li> <li>Prepare Samples and Get Incidences Using Flow.</li> <li>Fit MedModel to Repository: Adjusts an existing model to fit a new repository. For instance, if a non-critical signal is missing, the \"fit\" operation generates a virtual empty signal to bypass errors, ensuring compatibility. The suggested changes can later be reviewed and validated or corrected. More information inside</li> </ul> <p>GitHub Repository: Flow App Code</p> <p>The Flow App is also compiled as part of AllTools. Refer to the Setup Instructions.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#flow-app-options","title":"Flow App Options","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#general-switches","title":"General Switches","text":"<ul> <li><code>--help</code>: Displays the full help menu.</li> <li><code>--help_</code>: Searches the help menu and displays only the relevant sections matching the search term.</li> <li><code>--rep</code>: Specifies the path to the repository.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#creating-repositories","title":"Creating Repositories","text":"<p>To create a repository using a convert configuration file, use the <code>--convert_conf</code> option:</p> <pre><code>Flow --rep_create --convert_conf ./ICU.convert_config\n</code></pre> <p>To create a by-pid transposed version of a repository, use the following command. This allows faster access to specific patient IDs (pids) and reduces memory consumption significantly:</p> <pre><code>Flow --rep_create_pids --rep ./ICU.repository\n</code></pre> <p>For more details on creating repositories, convert configuration files, and required inputs, refer to Load a New Repository.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#printing-pids-and-signals","title":"Printing PIDs and Signals","text":"<ul> <li>Print all records for all signals for a specific pid using the default API:</li> </ul> <pre><code>Flow --rep ./ICU.repository --printall --pid 200001\n</code></pre> <ul> <li>Print all records for all signals for a specific pid using the by-pid API (faster but requires a by-pid repository):</li> </ul> <pre><code>Flow --rep ./ICU.repository --pid_printall --pid 200001\n</code></pre> <ul> <li>Print all records for a specific signal and pid using the default API:</li> </ul> <pre><code>Flow --rep ./ICU.repository --print --pid 200001 --sig Sepsis\n</code></pre> <ul> <li>Print all records for a specific signal and pid using the by-pid API (faster but requires a by-pid repository):</li> </ul> <pre><code>Flow --rep ./ICU.repository --pid_print --pid 200001 --sig Sepsis\n</code></pre> <ul> <li>Print general statistics for a signal, such as sample counts, gender distribution, average samples per person, and more. This works only for <code>SDateVal</code> type signals and repositories containing <code>GENDER</code> and <code>BYEAR</code> signals:</li> </ul> <pre><code>Flow --rep ./thin.repository --describe --sig Creatinine\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#training-a-model","title":"Training a Model","text":"<p>To train a model, you need the following inputs:</p> <ul> <li><code>REPOSITORY_PATH</code>: Path to the data repository.</li> <li><code>PATH_TO_TRAIN_SAMPLES</code>: Path to MedSamples, a TSV file defining labels for each patient and point in time.</li> <li><code>PATH_TO_JSON_WITH_MODEL_INSTRUCTIONS</code>: Path to the JSON file defining the model architecture. See Model JSON Format.</li> <li><code>PATH_TO_OUTPUT_TO_STORE_MODEL</code>: Path to save the trained model.</li> </ul> <p>Example command:</p> <pre><code>Flow --simple_train --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_json $PATH_TO_JSON_WITH_MODEL_INSTRUCTIONS --f_model $PATH_TO_OUTPUT_TO_STORE_MODEL\n</code></pre> <p>For cross-validation, use the <code>--train_test</code> mode switch. However, this is deprecated. Use the Optimizer instead.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#predictingapplying-a-model","title":"Predicting/Applying a Model","text":"<p>To apply a model, you need the following inputs:</p> <ul> <li><code>REPOSITORY_PATH</code>: Path to the data repository.</li> <li><code>PATH_TO_TRAIN_SAMPLES</code>: Path to MedSamples, defining requested prediction times for each patient. The outcome column is not used during testing.</li> <li><code>PATH_TO_TRAINED_MODEL_BINARY_FILE</code>: Path to the stored model.</li> <li><code>OUTPUT_PATH_TO_STORE_SAMPLES</code>: Path to save the predictions. The output will include a <code>pred_0</code> column in the MedSamples file for each requested prediction date.</li> </ul> <p>Example command:</p> <pre><code>Flow --get_model_preds --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_model $PATH_TO_TRAINED_MODEL_BINARY_FILE --f_preds $OUTPUT_PATH_TO_STORE_SAMPLES\n</code></pre> <p>Pre-processors can be added to the beginning of the model pipeline to manipulate raw signals before they are fed into the model. This allows you to perform operations that don't require training or storage in the model itself, such as simulating the removal or limitation of a specific signal. For more details, see Using Pre Processors</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#creating-a-feature-matrix-for-samples","title":"Creating a Feature Matrix for Samples","text":"<p>To create a feature matrix, use the same inputs as for predicting/applying a model. The output will be a CSV file containing the feature matrix:</p> <pre><code>Flow --get_mat --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_model $PATH_TO_TRAINED_MODEL_BINARY_FILE --f_matrix $OUTPUT_PATH_TO_STORE_MATRIX\n</code></pre> <p>To inspect the training matrix directly from the model JSON, use the following command. The inputs are the same as for model training, but the output is a matrix instead of a model:</p> <pre><code>Flow --get_json_mat --rep $REPOSITORY_PATH --f_samples $PATH_TO_TRAIN_SAMPLES --f_json $PATH_TO_JSON_WITH_MODEL_INSTRUCTIONS --f_matrix $OUTPUT_PATH_TO_STORE_MATRIX\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#how-to-generate-a-matrix-without-imputations","title":"How to Generate a Matrix Without Imputations","text":"<p>To create a matrix that skips imputations and normalizations, add the <code>--stop_step 2</code> argument to your <code>--get_mat</code> command.</p> <p>This setting halts the pipeline after the feature generation step, skipping the \"feature processors\" that would normally handle imputation and normalization. In the resulting matrix, missing data will be marked with the value <code>-65536</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/index.html#print-trained-model-information","title":"Print trained model information","text":"<p>To inspect model pipeline:</p> <p><pre><code>Flow --print_model_info --f_model $MODEL \n</code></pre> You can add <code>--print_json_format 1 --f_output $OUTPUT_JSON</code> and set <code>OUTPUT_JSON</code> to output path with a more detailed information about the model. It is not exactly a json format, but this is textual.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html","title":"Fitting a MedModel to a Repository","text":"<p>The Flow app provides an option called <code>fit_model_to_rep</code>. You can use it as follows:</p> <pre><code>Flow --fit_model_to_rep [other arguments]\n# See all arguments with: Flow --help_ fit_model_to_rep\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#overview","title":"Overview","text":"<p>The purpose of this tool is to take a repository path and a model path as input, and output a new, adjusted model that fits the repository at a specified output path.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#key-considerations","title":"Key Considerations","text":"<ol> <li>Signal names may change over time (e.g., GENDER \u2194 SEX, DIAGNOSIS \u2194 ICD9_Diagnosis, RC). These changes can cause compatibility issues.</li> <li>Sometimes, models require signals that do not exist in the repository. Rather than treating missing signals as empty (which can be misleading), MedModel enforces strict checks to avoid confusion.</li> <li>This tool outputs list of \"suggested\" changes in the model for your review and creates the new adjusted model.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#workflow","title":"Workflow","text":"<ol> <li>The tool reads both the model and the repository, checking for required signals that are missing from the repository.</li> <li>For each missing signal, it attempts to resolve the issue by adding or removing a <code>rep_processor</code> in the model, following a set priority. It starts by search an alternative signal name (for example, if SEX signal is missing and we have GENDER it will use GENDER as SEX, otherwise it will create an emoty signal to mark this signal as missing).</li> <li>It validates all categorical codes used by the model to ensure they are present in the repository.</li> <li>The adjusted model is saved to the specified output file.</li> <li>A log of all transformations made to the model is printed to a file or the screen.</li> <li>A log of any missing codes for each signal is also printed to a file or the screen.</li> <li>If all codes are present and all adjustments are successful, the process completes without errors.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#example-usage","title":"Example Usage","text":"<pre><code>Flow --fit_model_to_rep \\\n  --f_model /nas1/Work/Users/Eitan/Lung/outputs/models2023/EX3/model_63/config_params/exported_full_model.final.medmdl \\\n  --rep /nas1/Work/CancerData/Repositories/THIN/thin_2021.lung2/thin.repository \\\n  --f_output /tmp/1.mdl \\\n  --log_action_file_path /tmp/actions.log \\\n  --log_missing_categories_path /tmp/categ.log \\\n  --cleaner_verbose -1 \\\n  --remove_explainability 1 \\\n  --allow_virtual_rep 0\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#using-production-model-for-testing","title":"Using Production model for Testing","text":"<p>When testing a model sourced from AlgoMarker Production, you'll often want to disable components that are only enabled for production-specific monitoring. The Production model includes features like verbose outlier cleaning and explainability (when applicable).</p> <p>While these features don't affect the model's prediction score, they significantly slow down the testing process. To disable them for quicker testing, use the following command:</p> <pre><code>Flow --fit_model_to_rep --rep &lt;REPOSITORY_PATH_TO_TEST_THE_MODEL&gt; --f_model &lt;PATH_TO_AM_MODEL&gt; --f_output &lt;OUTPUT_PATH_FOR_CHANGED_MODEL&gt; --cleaner_verbose -1 --remove_explainability 1\n</code></pre> <p>Key flags used:</p> <ul> <li><code>--cleaner_verbose -1</code>: Disables verbose output for outlier cleaning.</li> <li><code>--remove_explainability 1</code>: Disables the explainability component.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#argument-descriptions","title":"Argument Descriptions","text":"<ul> <li><code>log_action_file_path</code>: Logs all changes made to the model. If not provided, output is printed to the screen.</li> <li><code>log_missing_categories_path</code>: Logs all missing codes for signals. If not provided, output is printed to the screen.</li> <li><code>cleaner_verbose</code>: Controls verbosity of outlier reporting. Use <code>1</code> for production (verbose), <code>-1</code> for validation (no verbose, faster).</li> <li><code>remove_explainability</code>: Controls if to remove the explainability module from the model. Faster in testing of just the scores</li> <li><code>allow_virtual_rep</code>: Allows use only a repository definition without data (like we have in AlgoMAkrer) for testing model fitting. Some adjustments may be limited, because we don't use actual data.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#sample-output","title":"Sample Output","text":"<pre><code>Signal BUN, median value is 34.200001 in repository\nwrite_to_file [/tmp/1.mdl] with crc32 [273944850]\nread_binary_data_alloc [/tmp/1.mdl] with crc32 [273944850]\nread_from_file [/tmp/1.mdl] with crc32 [273944850] and size [193041186]\ncategorical signal ICD9_Diagnosis is OK\ncategorical signal Smoking_Status is OK\nAll categorical signals are OK!\nAll OK - Model can be applied on repository\n</code></pre> <p>In this example, the model uses Urea, but the repository has BUN. The tool finds the median value of BUN, writes the adjusted model, and verifies it. The output confirms that all categorical signals are valid and the model can be applied.</p> <p>If all codes exist, the <code>/tmp/categ.log</code> file will be empty. If codes are missing, the file will list the signal name and missing categorical value (tab-delimited).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Fit%20MedModel%20to%20Repository.html#example-action-log","title":"Example Action Log","text":"<pre><code>REMOVED_RENAME_FROM_TO  SEX     GENDER\nCONVERT_SIGNAL_TO_FROM_FACTOR   Urea    BUN     0.467290\nEMPTY_SIGNAL    RDW\n</code></pre> <ul> <li>The first line indicates that the model used \"SEX\", but the repository has \"GENDER\". The processor converting GENDER to SEX was removed, so the model now accepts GENDER input signal.</li> <li>The second line shows that a virtual signal \"Urea\" was created from \"BUN\" by multiplying by 0.467290.</li> <li>The third line indicates that \"RDW\" was missing from the repository, so an empty virtual signal was</li> </ul> <p>This is valid transformation of some breaking changes we had in our signals/repositories during the years + empty RDW signal which is acceptable in that case.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html","title":"Split Files","text":"<p>A split file contains a list of pids split into several splits. Keeping the split in a file allows using the exact same split over several runs, or between stages such as creating a matrix with completions and modeling it. Pids not appearing in the split file should always be assumed to not be used. \u00a0</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html#format","title":"Format","text":"<p>Comment lines start with # . File is tab or space delimited. first line: NSPLITS  Followed by tupples:"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html#the-medsplit-class","title":"The MedSplit Class","text":"<p>Defined in MedFeat/MedOutcome.h , this class contains the basic tools to create splits, and read/write them to files.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Split%20Files.html#create-random-splits-for-patients","title":"Create Random Splits for Patients","text":"<p>Inputs:</p> <ul> <li>REPOSITORY_PATH - you will need data to retreive all patients</li> <li>SPLIT_NUMBER - fill in the nubmer of splits you want to create</li> <li>OUTPUT_PATH - output file to store the output split file in format  <pre><code>Flow --create_splits \"nsplits=$SPLIT_NUMBER\" --rep $REPOSITORY_PATH --f_split $OUTPUT_PATH\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html","title":"Using Flow to Prepare Samples and Calculate Incidences","text":"<p>The Flow app provides powerful tools for selecting, filtering, and matching samples based on a cohort. These tools are essential for preparing sample files for training/validation or for matching populations on specific parameters. Flow also supports incidence estimation and generating incidence files for later analysis (e.g., bootstrap). The instructions below are primarily for date-based cases, but can be adapted for other variables.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#cohort-files","title":"Cohort Files","text":"<p>A cohort file defines the patients to use for training/testing, including their entry/exit times and outcome information. Format:</p> <ul> <li>Tab-delimited</li> <li>Lines starting with <code>#</code> are comments</li> <li>Each line contains 5 fields:<ol> <li><code>pid</code>: Patient ID (one line per pid)</li> <li>Entry date to cohort</li> <li>End date in cohort</li> <li>Outcome date: For controls (outcome=0), same as end date; for cases (outcome=1), the event date (must be \u2264 end date)</li> <li>Outcome: Typically 0/1 (control/case), but can be regression or multicategory (currently only a single value is supported)</li> </ol> </li> </ul> <p>Example cohort file: <pre><code># pid 5000009 entered on 20060505, left on 20091023, and had a case outcome at 20091023\n5000009 20060505        20091023        20060605        1.000000\n# pid 5000014 is a control (0.0 outcome), entered 20150107, left 20160929\n5000014 20150107        20160929        20160929        0.000000\n5000017 20110826        20160819        20160819        0.000000\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#creating-a-sample-file-from-a-cohort-file","title":"Creating a Sample File from a Cohort File","text":"<p>Use Flow to generate a sample file:</p> <pre><code>Flow --rep &lt;repository&gt; --seed &lt;random seed&gt; --cohort_fname &lt;cohort file&gt; --cohort_sampling &lt;sampling parameters&gt; --out_samples &lt;output samples file&gt;\n</code></pre> <p>All parameters are self-explanatory except <code>cohort_sampling</code>, which controls how samples are created (for both controls and cases). Key options (with defaults):</p> <ul> <li><code>min_control_years</code> (0): Minimum years before outcome for controls</li> <li><code>max_control_years</code> (10): Maximum years before outcome for controls</li> <li><code>min_case_years</code> (0): Minimum years before outcome for cases</li> <li><code>max_case_years</code> (1): Maximum years before outcome for cases</li> <li><code>is_continous</code> (1): Continuous sampling (1) or on-test (0)</li> <li><code>min_days_from_outcome</code> (30): Minimum days before outcome to sample</li> <li><code>jump_days</code> (180): Days between sampling periods</li> <li><code>min_year</code> (1900), <code>max_year</code> (2100): Year range for sampling</li> <li><code>gender_mask</code> (3): 1=male, 2=female, 3=both</li> <li><code>train_mask</code> (7): Mask for TRAIN value (bits for TRAIN=1,2,3)</li> <li><code>min_age</code> (0), <code>max_age</code> (200): Age range for sampling</li> <li><code>stick_to_sigs</code>: Comma-separated list of signals; only use time points with at least one of these signals</li> <li><code>take_closest</code> (0): Take sample with stick signal closest to each target date</li> <li><code>take_all</code> (0): Take all samples with stick signal in each period</li> <li><code>max_samples_per_id</code> (2^31-1): Max samples per ID</li> <li><code>max_samples_per_id_method</code> ('last'): 'last' or 'rand' (choose last or random samples)</li> </ul> <p>Sample usage:</p> <pre><code># Continuous sampling, random sample every 180 days\n# Controls: 1\u201310 years before end; Cases: up to 2 years before outcome\n# Only TRAIN=1, ages 35\u201390\nSAMPLING_PARAMS1=\"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=1;min_age=35;max_age=90\"\n\n# As above, but on-test and only at Glucose or HbA1C test dates\nSAMPLING_PARAMS1=\"min_control=1;max_control=10;min_case=0;max_case=2;jump_days=180;train_mask=1;min_age=35;max_age=90;is_continous=0;stick_to_sigs=Glucose,HbA1C\"\n\n# Run Flow (replace SAMPLING_PARAMS as needed)\nFlow --rep /home/Repositories/THIN/thin_mar2017/thin.repository --seed 123 --cohort_fname ./pre2d.cohort --cohort_sampling ${SAMPLING_PARAMS} --out_samples ./temp.samples\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#filtering-and-matching-samples","title":"Filtering and Matching Samples","text":"<p>You can filter and/or match samples from an existing samples file (typically created as above):</p> <p><pre><code>Flow --rep &lt;rep&gt; --seed &lt;random_seed&gt; --filter_and_match --in_samples &lt;input samples file&gt; --out_samples &lt;output samples file&gt; --filter_params &lt;filter params&gt; --match_params &lt;match params&gt;\n</code></pre> Filtering is applied first (if specified), then matching (if specified).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#filter-parameters","title":"Filter Parameters","text":"<p>Filtering allows you to select samples within a date range or based on signal values in a window before the sample time (e.g., only samples with Creatinine &lt; 1.1 in the last 2 years).</p> <ul> <li><code>min_sample_time</code> (0): Minimum allowed time (in sample's time unit, usually date)</li> <li><code>max_sample_time</code> ((1&lt;&lt;30)): Maximum allowed time</li> <li><code>win_time_unit</code> (\"Days\"): Time unit for bfilter windows</li> <li><code>bfilter</code>: Filter on a signal; multiple filters allowed. Parameters (comma-separated):<ul> <li><code>sig_name</code>: Signal name</li> <li><code>win_from</code> (0): Window start (relative to sample time, backwards)</li> <li><code>win_to</code> ((1&lt;&lt;30)): Window end</li> <li><code>min_val</code> (-1e10): Minimum allowed value</li> <li><code>max_val</code> (1e10): Maximum allowed value</li> <li><code>min_Nvals</code> (1): Minimum number of signal instances in window</li> <li><code>time_channel</code> (0), <code>val_channel</code> (0): Channels to consider</li> </ul> </li> <li><code>min_bfilter</code>: How many bfilters must pass (default: all)</li> </ul> <p>Examples:</p> <pre><code># Only samples between 20070101 and 20150101\nFILTER1=\"min_sample_time=20070101;max_sample_time=20150101\"\n\n# At least 1 Creatinine test in last 2 years\nFILTER2=\"bfilter=sig,Creatinine,win_from,0,win_to,730,min_Nvals,1\"\n\n# As above, but all Creatinine &lt; 0.9\nFILTER3=\"bfilter=sig,Creatinine,win_from,0,win_to,730,min_Nvals,1,min_val=0,max_val=0.9\"\n\n# Combined: date range, at least one Glucose in last 2Y, all Glucose &lt; 100 in last 5Y, all HbA1C &lt; 5.7 in last 5Y\nFILTER4=\"min_sample_time=20070101;max_sample_time=20101201;bfilter=sig,Creatinine,win_from,0,win_to,730,min_Nvals,1;bfilter=sig,Creatinine,win_from,0,win_to,1825,min_val,0,max_val,100;bfilter=sig,HbA1C,win_from,0,win_to,1825,min_val,0,max_val,5.7\"\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#matching-parameters","title":"Matching Parameters","text":"<p>Matching ensures the ratio of cases to controls is balanced within defined strata (e.g., by year, gender, age, or signal value). The algorithm tries to maximize the number of samples kept, with a weight parameter to prioritize keeping cases when they are rare. The goal is to control and remove information related directly to those variables. A common case is to match by years to remove temporal information the model might gian from difference in cases, controls ratio in certain years.</p> <ul> <li><code>priceRatio</code> (100.0): How many controls to lose per case (suggested: n_controls/n_cases)</li> <li><code>maxRatio</code> (10.0): If optimal ratio &gt; maxRatio, sample less (enrich cases)</li> <li><code>verbose</code> (0): More output</li> <li><code>match_to_prior</code>: Specify target prior directly</li> <li><code>strata</code>: Define stratification (':'-delimited for multiple strata, ','-delimited for parameters):<ul> <li><code>type</code>: time, age, gender, or signal</li> <li><code>signalName</code>: For time: year/month/days; for signal: name; for age/gender: none</li> <li><code>resolution</code>: Bin size</li> </ul> </li> </ul> <p>Examples:</p> <p><pre><code># Match and stratify by year\nMATCH1=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=time,year,1\"\n# Match by year to a prior of 0.1 (10%)\nMATCH1=\"match_to_prior=0.1;maxRatio=4.5;verbose=1;strata=time,year,1\"\n# Match by gender\nMATCH2=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=gender\"\n# Match by age (5-year bins)\nMATCH3=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=age,5\"\n# Match by age, year, and gender together\nMATCH4=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=age,5:time,year,1:gender\"\n# Match by Glucose (bin=10) and HbA1C (bin=1.0)\nMATCH5=\"priceRatio=10;maxRatio=4.5;verbose=1;strata=signal,Glucose,10:signal,HbA1C,1.0\"\n</code></pre> For more details, see: MatchingSampleFilter</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#calculating-incidence-for-a-cohort","title":"Calculating Incidence for a Cohort","text":"<p>To generate an incidence file for a cohort, use:</p> <p><pre><code>Flow --rep &lt;repository&gt; --cohort_incidence \"from_year=2007;to_year=2014;from_age=40;to_age=80;age_bin=40;incidence_days_win=1825\" --cohort_fname &lt;cohort file&gt; --cohort_incidence &lt;incidence parameters&gt; --out_incidence &lt;incidence file&gt; --censor_reg &lt;censor registry&gt; --use_kaplan_meir 1\n</code></pre> <code>cohort_fname</code> and <code>censor_reg</code> are MedRegistry objects. You can convert a MedCohort to MedRegistry using:</p> <pre><code># Create MedRegistry from MedCohort\ncat &lt;cohort file&gt; | awk '{ if ($NF &gt; 0) { print $1 \"\\t\" $2 \"\\t\" $4 \"\\t\" \"0\";  print $1 \"\\t\" $4 \"\\t\" $3 \"\\t\" \"1\" } else { print $1 \"\\t\" $2 \"\\t\" $3  \"\\t\" \"0\" } }'\n# Create Censor registry from MedCohort\ncat &lt;cohort file&gt; | awk '{ if ($NF &gt; 0) { print $1 \"\\t\" $2 \"\\t\" $4 \"\\t\" \"1\" } else { print $1 \"\\t\" $2 \"\\t\" $3  \"\\t\" \"1\" } }'\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Flow%20To%20Prepare%20Samples%20and%20Get%20Incidences.html#incidence-parameters-in-cohort_incidence-argument-separated-by","title":"Incidence Parameters (in <code>cohort_incidence</code> argument, separated by <code>;</code>)","text":"<ul> <li><code>age_bin</code>: Size of age bins (e.g., 5)</li> <li><code>min_samples_in_bin</code>: Small bins are merged with neighbors</li> <li><code>from_year</code>, <code>to_year</code>: Year range</li> <li><code>start_date</code>: Date in year to test (mmyy, e.g., 508=May 8, 1201=Dec 1)</li> <li><code>gender_mask</code>, <code>train_mask</code>: As above</li> <li><code>from_age</code>, <code>to_age</code>: Age range</li> <li><code>incidence_years_window</code>: Years ahead to calculate incidence</li> <li><code>incidence_days_win</code>: Days ahead to calculate incidence (overrides years if set)</li> </ul> <p>Example:</p> <pre><code># Incidence for 2007\u20132010 (annual), TRAIN=1, age bins of 5, test date June 2nd\nINC_PARAMS=\"train_mask=1;age_bin=5;start_date=602;incidence_years_window=1;from_year=2007;to_year=2010\"\n</code></pre> <p>You can also control sampling directly with <code>--sampler_params</code>:</p> <ul> <li><code>start_year</code>, <code>end_year</code> or <code>start_time</code>, <code>end_time</code>: Full time/date</li> <li><code>prediction_month_day</code>: Prediction date</li> <li><code>time_jump</code>/<code>day_jump</code>: Interval between prediction dates</li> <li><code>time_jump_unit</code>: Jump unit (e.g., Day, Year)</li> <li><code>time_range_unit</code>: Time range unit (e.g., Date, Minutes) \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/Using%20the%20Flow%20App/Using%20Pre%20Processors.html","title":"Using Pre Processors","text":"<p>Given a trained model , one may need to apply some additional rep processors before the model is applied.\u00a0 A classical example is : the model was trained without history limits on the signals, and one needs to test the results when limiting the signals (or some of them) to say only 1 year of history.</p> <p>The way to do that is add a pre processor at apply time to the model.</p> <p>This can be done using the Flow --get_model_preds option and adding the pre processors using the --f_pre_json parameter</p> <p>Example:</p> <p>A pre processor json file that limits histrory:</p> <pre><code>{\n        \"pre_processors\" : [ {\"rp_type\" : \"history_limit\" , \"signal\" : \"ref:signals\", \"win_from\" : \"0\" , \"win_to\" : \"365\"} ] ,\n        \"signals\" : [\"Hemoglobin\", \"MCV\", \"MCH\"]\n}\n</code></pre> <p>A Flow get prediction example using pre processors</p> <pre><code>Flow --get_model_preds --rep myrep.repository --f_samples test.samples --f_model trained.model --f_preds out.preds --f_pre_json pre.json\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html","title":"bootstrap_app","text":"<p><code>bootstrap_app</code> is a powerful and highly efficient command-line tool designed for evaluating model performance using bootstrap analysis. It provides key statistical metrics such as confidence intervals and standard deviations for a wide range of performance measures.</p> <p>The tool uses a patient-level bootstrap sampling method. It first randomly selects a patient from the dataset, then either a random sample or all samples associated with that patient. This ensures that the analysis properly accounts for patients with multiple data points, which is a best practice in medical model assessment.</p> <p>The source code can be found in the <code>bootstrap_app</code> folder of the MR_Tools repository. It is compiled as part of AllTools.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#getting-started-your-first-run","title":"Getting Started: Your First Run","text":"<p>The easiest way to use <code>bootstrap_app</code> is from command line:</p> <pre><code>bootstrap_app --input /path/to/my_data.tsv --output /tmp/bootstrap_results\n</code></pre> <p>Mandatory Parameters Only two parameters are mandatory for any run:</p> <ul> <li><code>--input</code>: The file containing your model's predictions and the true outcomes. It must be a tab-separated TSV with a header row, including the columns <code>pid</code> (patient ID), <code>outcome</code>, and <code>pred_0</code> (the prediction score). Full File Format MedSamples</li> <li><code>--output</code>: The path to the output file where <code>bootstrap_app</code> will write the results of the analysis.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#core-features-at-a-glance","title":"Core Features at a Glance","text":"<ul> <li>Highly efficient: Quickly computes performance metrics on millions of predictions, making it suitable for large-scale datasets result files.</li> <li>Flexible Inputs: Supports various input types and allows for sample weighting.</li> <li>Comprehensive Metrics: Handles regression, categorical, and custom performance measures, including specialized metrics for binary classification.</li> <li>Powerful Cohort Analysis: Easily assesses model performance across thousands of defined cohorts (e.g., age groups, sex, time windows) using a simple configuration file.</li> <li>Standalone &amp; Integrated: Available as a standalone executable built on a fast C++ library, with a Python API available for integration into dataframe-based workflows.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#how-it-works-program-overview","title":"How it Works: Program Overview","text":"<p>The primary use case for <code>bootstrap_app</code> is to analyze a <code>MedSamples</code> TSV file containing patient id, outcome and prediction data. However, its main strength lies in its ability to analyze complex cohorts.</p> <p>The workflow for cohort analysis involves:</p> <ol> <li>Defining Cohorts: You define cohorts using a plain-text file where each filter is a simple <code>FEATURE_NAME:MIN_VALUE,MAX_VALUE</code> pair. Multiple conditions on the same line are combined with <code>AND</code> logic.</li> <li>Generating a Feature Matrix: The tool can generate a feature matrix from a MedModel JSON file and a data repository or given a matrix CSV directly in the input</li> <li>Applying Filters: The tool then uses simple filtering rules, similar to pandas (even though less expressive, it is suffecient for most usecases), to create and analyze performance for each defined cohort.</li> </ol> <p>This approach keeps everything within the fast C++ ecosystem, providing an efficient way to analyze performance across many sub-populations.</p> <p>A single line in the <code>cohorts_file</code> can generate multiple combinations of cohorts. For instance, this example will generate 6 distinct cohorts by combining every <code>Age</code> filter with every <code>Time-Window</code> filter:</p> <pre><code>MULTI   Age:40,89;Age:50,75;Age:45,75   Time-Window:0,365;Time-Window:180,365\n</code></pre> <p>For more information on the <code>cohorts_file</code> format/bootstrap query language, refer to the MedBootstrap wiki page.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#command-line-options","title":"Command-Line Options","text":"<p>You can view all available options by running:</p> <pre><code>./bootstrap_app --help\n</code></pre> <p>The options are organized into several logical groups:</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#general-options","title":"General Options","text":"<ul> <li><code>--sample_seed arg (=0)</code>: Seed for bootstrap sampling</li> <li><code>--use_splits</code>: Perform split-wise analysis in addition to full data</li> <li><code>--run_id arg (=run_id_not_set)</code>: Run ID to store in the result file</li> <li><code>--debug</code>: Enable verbose debugging</li> <li><code>--output_raw</code>: Output bootstrap filtering of label and prediction (for inspection)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#inputoutput-options","title":"Input/Output Options","text":"<ul> <li><code>--rep arg</code>: Repository path (for cohort filtering, if needed)</li> <li><code>--input_type arg (=samples)</code>: Input type (<code>samples</code>, <code>samples_bin</code>, <code>features</code>, <code>medmat_csv</code>, <code>features_csv</code>)</li> <li><code>--input arg</code>: Input file location</li> <li><code>--weights_file arg</code>: File with sample weights (same order as input), or use <code>attr:</code> prefix to extract attribute from samples</li> <li><code>--control_weight arg (=-1)</code>: If &gt;0, use this value to weight controls (simple alternative to <code>weights_file</code>)</li> <li><code>--cohorts_file arg</code>: Cohort definition file (format details)</li> <li><code>--cohort arg</code>: Analyze a single cohort by providing a string (e.g., <code>Time-Window:0,365;Age:40,80</code>); overrides <code>--cohorts_file</code></li> <li><code>--json_model arg</code>: JSON model for creating features for cohort filtering (input for <code>cohorts_file</code>)</li> <li><code>--output arg</code>: Output file location for bootstrap results</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#bootstrap-metricsmeasurement-options","title":"Bootstrap Metrics/Measurement Options","text":"<ul> <li><code>--measurement_type arg</code>: Which measures to perform. Options: <code>calc_regression</code>, <code>calc_kandel_tau</code>, <code>calc_harrell_c_statistic</code>, <code>calc_multi_class</code>, <code>calc_roc_measures_with_inc</code>, <code>calc_only_auc</code>, <code>calc_npos_nneg</code>. Default: <code>calc_roc_measures_with_inc</code> (recommended for binary classification).</li> <li><code>--general_measurements_args arg</code>: Arguments for the measurement init function (as <code>key=value;</code> string)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#binary-classification-metrics-options","title":"Binary Classification Metrics Options","text":"<p>For binary classification, you can specify many arguments directly (instead of using <code>general_measurements_args</code>): - <code>--max_diff_working_point arg (=0.05)</code>: Maximum allowed difference between calculated and requested working point (otherwise, returns missing value) - <code>--force_score_working_points</code>: Force using scores as working points (reports results as a function of score cutoff only) - <code>--min_score_quants_to_force_score_wp arg (=10)</code>: If the prediction file has fewer than this number of score bins, performance is reported only by score cutoffs - <code>--working_points_sens arg</code>: Sensitivity working points (comma-separated, 0-100%) - <code>--working_points_fpr arg</code>: FPR working points (comma-separated, 0-100%) - <code>--working_points_pr arg</code>: Precision-recall working points (comma-separated, 0-100%) - <code>--part_auc_params arg</code>: Partial AUC points (comma-separated, 0-1)</p> <p>Explanation: Performance metrics are reported as \"Metric @ Other Metric Cutoff\". There are two main ways to define cutoffs:</p> <ol> <li>By score threshold: reported as <code>@SCORE_X</code> (e.g., <code>SENS@SCORE_0.5</code>). If the score has few bins (e.g., binary), results are reported only this way. The parameters <code>min_score_quants_to_force_score_wp</code> and <code>force_score_working_points</code> control this behavior.</li> <li>By another metric: e.g., sensitivity at a fixed FPR (e.g., <code>SENS@FPR_03</code>). See the metrics legend for details. You can also extract the score cutoff at a given FPR (e.g., <code>SCORE@FPR_03</code>). This is useful for extracting KPIs by controlling FPR, PR, or sensitivity directly.</li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#multi-class-metrics-options","title":"Multi-Class Metrics Options","text":"<ul> <li><code>--multiclass_top_n arg (=1,5)</code>: In multiclass mode, top N predictions to consider (comma-separated)</li> <li><code>--multiclass_dist_name arg</code>: Name of distance function (e.g., Jaccard, Uniform)</li> <li><code>--multiclass_dist_file arg</code>: File with distance metric</li> <li><code>--multiclass_auc arg (=0)</code>: Perform class-wise AUC in multi-class analysis</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#bootstrapping-parameters","title":"Bootstrapping Parameters","text":"<ul> <li><code>--score_resolution arg (=0.0001)</code>: Score bin resolution for speed (set to 0 for no rounding)</li> <li><code>--score_bins arg (=0)</code>: Number of score bins for speed (set to 0 to disable)</li> <li><code>--nbootstrap arg (=500)</code>: Number of bootstrap iterations</li> <li><code>--fix_label_to_binary arg (=1)</code>: If set, treats labels as <code>label = outcome &gt; 0</code> (enforces binary labels)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#bootstrap-sampling-options","title":"Bootstrap Sampling Options","text":"<ul> <li><code>--sample_per_pid arg (=1)</code>: Number of samples to take per patient (0 = take all samples for a randomized patient)</li> <li><code>--sample_pid_label</code>: If true, randomization is based on patient and label (default: false)</li> <li><code>--do_autosim</code>: Perform auto simulation (requires <code>min_time</code> and <code>max_time</code>)</li> <li><code>--min_time arg</code>: Minimum time for auto simulation</li> <li><code>--max_time arg</code>: Maximum time for auto simulation</li> <li><code>--sim_time_window</code>: Treat cases as controls if not in time window (do not censor)</li> <li><code>--censor_time_factor arg (=2)</code>: When <code>sim_time_window</code> is on, factor for censoring cases (1 = all cases beyond window become controls without censoring)</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#bootstrap-filtering-options","title":"Bootstrap Filtering Options","text":"<ul> <li><code>--whitelist_ids_file arg</code>: File with whitelist of IDs to include</li> <li><code>--blacklist_ids_file arg</code>: File with blacklist of IDs to exclude</li> <li><code>--sample_min_year arg (=-1)</code>: Filter out samples before this year (no filtering if &lt; 0)</li> <li><code>--sample_max_year arg (=-1)</code>: Filter out samples after this year (no filtering if &lt; 0)</li> <li><code>--use_censor arg (=1)</code>: Use repository censor signal (deprecated)</li> </ul> <p>you may not provide this file or override all parameter with command arguments.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#incidence-adjustments-advanced","title":"Incidence Adjustments (Advanced)","text":"<p>See Fixing incidence for more details. - <code>--incidence_file arg</code>: Path to incidence file (age/sex stratified, e.g., from SEER) to adjust incidence for metrics sensitive to incidence rate - <code>--registry_path arg</code>: Registry path for calculating incidence by sampling in the current repository (use only for non-case-control data) - <code>--labeling_params arg (=conflict_method=max;label_interaction_mode=0:within,all|1:before_start,after_start;censor_interaction_mode=all:within,all)</code>: How to label from registry (see <code>LabelParams</code> type) - <code>--incidence_sampling_args arg (=start_time=20070101;end_time=20140101;time_jump=1;time_jump_unit=Year;time_range_unit=Date)</code>: Initialization string for <code>MedSamplingYearly</code> - <code>--do_kaplan_meir arg (=1)</code>: Use Kaplan-Meier calculation for incidence by registry (recommended, especially for large time windows)</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#example-usage","title":"Example Usage","text":"<p>The easiest way to use <code>bootstrap_app</code> is with a configuration (<code>.cfg</code>) file, which is an INI-style plain-text file instead of supplying all arguments in command line.</p> <pre><code>bootstrap_app --base_config /path/to/bootstrap_example.cfg\n</code></pre> <p>Here's an example of what a <code>bootstrap_example.cfg</code> file might look like:</p> <pre><code># Repository path:\nrep = /home/Repositories/THIN/thin_jun2017/thin.repository\n# MedSamples input (or use `input_type` for other types)\ninput = /server/Work/Users/Alon/UnitTesting/examples/bootstrap_app/validation_samples.preds\n# Output path:\noutput = /tmp/bootstrap_test\n# JSON model for additional features for cohort filtering\njson_model = /server/Work/Users/Alon/UnitTesting/examples/bootstrap_app/model_stats.json\n# Cohorts definition file:\ncohorts_file = /server/Work/Users/Alon/UnitTesting/examples/bootstrap_app/bootstrap_new.params\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#explanation","title":"Explanation","text":"<p>While <code>bootstrap_app</code> has many options, only two are mandatory for a basic run: <code>input</code> and <code>output</code>. The input file is a tab-separated TSV file that must contain columns for <code>pid</code> (patient ID), <code>outcome</code>, and <code>pred_0</code> (prediction score). For more details on this format, see the MedSamples documentation.</p> <p>The other parameters you see in the example, such as <code>rep</code>, <code>json_model</code>, and <code>cohorts_file</code>, are optional and used for advanced cohort analysis/filtering of sub population. The tool can automatically generate a feature matrix from a <code>json_model</code> and a data repository. Alternatively, you can directly provide a feature matrix as a CSV file by setting <code>input_type</code> to <code>features_csv</code> in this case, you don't need to specify <code>rep</code> or <code>json_model</code>.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#adjusting-incidence-with-an-incidence-file","title":"Adjusting Incidence with an Incidence File","text":"<p>The tool estimates average incidence in your cohort based on sex, age group, and patient counts. For positive predictive value (PPV), it multiplies sensitivity by incidence, then divides by <code>sensitivity * incidence + FPR * (1 - incidence)</code>. This is equivalent to weighting cases as <code>average incidence * total cohort / total cases</code> and controls as <code>(1 - average incidence) * total cohort / total controls</code>.</p> <p>If the model is random (sensitivity = FPR), then PPV equals incidence. The bootstrap program also evaluates how additional cohort filters (beyond sex and age) affect incidence. For example, selecting anemic patients may bias toward cases and increase incidence compared to the global population (e.g., SEER data). The tool assumes the effect on stratified outcome, age, and sex is similar between your input samples and the reference population, and measures the lift in odds ratio before and after applying bootstrap filters. This is an adjustment for incidence-sensitive metrics, not an exact calculation.</p> <p>Example incidence file (can be created via Flow App):</p> <p><pre><code>head /nas1/Work/Users/Alon/UnitTesting/examples/bootstrap_app/pre2d_incidence_thin.new_format\n</code></pre> <pre><code>AGE_BIN 3\nAGE_MIN 21\nAGE_MAX 90\nOUTCOME_VALUE   0.0\nOUTCOME_VALUE   1.0\nSTATS_ROW       MALE    21      1.0     5242\nSTATS_ROW       MALE    21      0.0     94758\nSTATS_ROW       FEMALE  21      1.0     5242\nSTATS_ROW       FEMALE  21      0.0     94758\nSTATS_ROW       MALE    24      1.0     5338\n</code></pre></p> <p>The <code>registry_path</code> is a text format of MedRegistry. See the code documentation for more details (e.g., <code>write_text_file</code> method).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#understanding-the-results-output","title":"Understanding the Results Output","text":"<p>The output is a TSV file with three columns: <code>Cohort</code>, <code>Measurement</code>, and <code>Value</code>.</p> <ul> <li>Cohort: The name of the cohort being analyzed.</li> <li>Measurement: The performance metric (e.g., AUC_Mean, SENS@FPR_03).</li> <li>Value: The calculated value for that metric.</li> </ul> <p>For each metric, the tool outputs several statistics:</p> <ul> <li><code>_Mean</code>: The average value across all bootstrap iterations.</li> <li><code>_Obs</code>: The value from the original, non-bootstrapped data.</li> <li><code>_Std</code>: The standard deviation across iterations.</li> <li> <p><code>_CI.Lower.95</code> and <code>_CI.Upper.95</code>: The lower and upper bounds of the 95% confidence interval.</p> </li> <li> <p>See Bootstrap legend for more details on output metrics.</p> </li> <li>Use the utility tool to process bootstrap result files to generate tables or plots (e.g., ROC curves) from one or more bootstrap files.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#example-result-format","title":"Example Result Format","text":"<pre><code>Cohort  Measurement Value\nAge:40-89,Time-Window:0,365 AUC_CI.Lower.95 0.555556\nAge:40-89,Time-Window:0,365 AUC_CI.Upper.95 1\nAge:40-89,Time-Window:0,365 AUC_Mean    0.867092\nAge:40-89,Time-Window:0,365 AUC_Obs 0.875\nAge:40-89,Time-Window:0,365 AUC_Std 0.129476\n...\nAge:40-89,Time-Window:0,365 SENS@FPR_03 10.34565\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/index.html#implementation-and-advanced-library-usage","title":"Implementation and Advanced Library Usage","text":"<p>You can use <code>bootstrap</code> in C++ code at three levels (main code in MR_Libs):</p> <ol> <li><code>bootstrap.cpp</code>, <code>bootstrap.h</code> (in <code>Internal/MedStat/MedStat</code>): Basic bootstrap analysis API using standard C++ objects (e.g., vector, map). Main function: <code>booststrap_analyze</code>.</li> <li><code>MedBootstrap.cpp</code>, <code>MedBootstrap.h</code> (in <code>Internal/MedStat/MedStat</code>): Builds on the base API, providing a friendlier interface using MES C++ objects like <code>MedSamples</code> and <code>MedFeatures</code>.</li> <li><code>bootstrap_app</code>: The application itself, which receives arguments and uses the API from the MedBootstrap library.</li> </ol> <p>For details on implementing custom metrics and extending bootstrap, see Extending bootstrap.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Bootstrap%20legend.html","title":"Bootstrap Result File: Column Legend","text":"<p>The bootstrap result file is a tab-delimited file with three columns:</p> <ol> <li> <p>Cohort</p> <ul> <li>The name of the filter used to define the cohort for which the results are reported. Examples: <code>All</code> (no filters), <code>Males</code> (only males), <code>Age:45-80</code>, etc.</li> <li>Cohort names are set in the bootstrap cohort configuration file (see the <code>--cohort_file</code> argument), which maps each cohort name to a set of filter rules. For more details, see: bootstrap_app.</li> </ul> </li> <li> <p>Measurement</p> <ul> <li>The name of the measured metric. For example, <code>AUC_Mean</code> is the mean value of the AUC metric across bootstrap runs.</li> <li>Measurement names have two parts: the metric measured in each bootstrap experiment, and the statistical summary calculated over all experiments. The suffix indicates the summary statistic:<ul> <li><code>_Mean</code>: Mean value across all bootstrap experiments</li> <li><code>_Std</code>: Standard deviation across all bootstrap experiments</li> <li><code>_CI.Lower.95</code>: Lower 95% confidence interval (2.5th percentile)</li> <li><code>_CI.Upper.95</code>: Upper 95% confidence interval (97.5th percentile)</li> <li><code>_Obs</code>: Observed value (calculated on the full data, without bootstrap randomization)</li> </ul> </li> <li>Metrics can be simple (e.g., <code>AUC</code>, <code>RMSE</code>) or compound, separated by <code>@</code>. For example, <code>SENS@FPR_10.000</code> means Sensitivity at a False Positive Rate cutoff of 10.000. The second part after <code>@</code> can be:<ul> <li><code>SCORE</code>: Cutoff by score</li> <li><code>FPR</code>: Cutoff by False Positive Rate</li> <li><code>SENS</code>: Cutoff by Sensitivity</li> <li><code>PR</code>: Cutoff by Positive Rate</li> </ul> </li> <li>The first token (metric) can be:<ul> <li><code>SENS</code>: Sensitivity / Recall / True Positive Rate (Y-axis in ROC)</li> <li><code>FPR</code>: False Positive Rate (X-axis in ROC)</li> <li><code>PR</code>: Positive Rate</li> <li><code>PPV</code>: Positive Predictive Value / Precision</li> <li><code>SCORE</code>: Model score</li> <li><code>LIFT</code>: Lift</li> <li><code>OR</code>: Odds Ratio</li> <li><code>RR</code>: Relative Risk</li> <li><code>NPV</code>: Negative Predictive Value</li> <li><code>SPEC</code>: Specificity (1 - FPR)</li> </ul> </li> <li>Additional measures:<ul> <li><code>NNEG</code>: Number of negatives (controls) in the cohort</li> <li><code>NPOS</code>: Number of positives (cases) in the cohort</li> <li><code>checksum</code>: A unique hash for the population pid,time combination without randomization to identify and compare executions were performed on the same cohort. It has no <code>_Mean</code>, <code>_Std</code> or other suffixes.</li> </ul> </li> </ul> </li> <li> <p>Value</p> <ul> <li>The numeric value corresponding to the cohort and measurement.</li> </ul> </li> </ol>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Bootstrap%20legend.html#example-result-format","title":"Example Result Format","text":"<pre><code>Cohort  Measurement Value\nAge:40-89,Time-Window:0,365 AUC_CI.Lower.95 0.555556\nAge:40-89,Time-Window:0,365 AUC_CI.Upper.95 1\nAge:40-89,Time-Window:0,365 AUC_Mean    0.867092\nAge:40-89,Time-Window:0,365 AUC_Obs 0.875\nAge:40-89,Time-Window:0,365 AUC_Std 0.129476\n...\nAge:40-89,Time-Window:0,365 SENS@FPR_03 10.34565\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Utility%20tools%20to%20process%20bootstrap%20results.html","title":"Utility Tools for Processing Bootstrap Results","text":"<p>The bootstrap output file can contain a lot of numbers and be scatter across different files.  We need a tool to visualize, compare and rearrange the results in a desired format.  Sometimes we want to visulize it as a graph.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Utility%20tools%20to%20process%20bootstrap%20results.html#formatting-bootstrap-results-as-tables","title":"Formatting Bootstrap Results as Tables","text":"<p>You can use the <code>bootstrap_format.py</code> script to convert bootstrap result files into well-formatted tables (Excel-like). This script is available in the MR_SCRIPTS repository and should be accessible in your <code>PATH</code> under the <code>Python-scripts</code> directory of MR_Scripts.</p> <p>Basic usage: <pre><code>bootstrap_format.py --report_path $BT_REPORT_PATH_1 $BT_REPORT_PATH_2 ... $BT_REPORT_PATH_N \\\n  --report_name $NAME_FOR_1 $NAME_FOR_2 ... $NAME_FOR_N \\\n  --cohorts_list $REGEX_TO_FILTER_COHORTS \\\n  --measure_regex $REGEX_TO_FILTER_MEASUREMENTS \\\n  --table_format $TABLE_FORMAT\n# Example of processing 2 files for previous mode and a current model:\nbootstrap_format.py --report_path /tmp/bt_previous.pivot_txt /tmp/bt_current.pivot_txt \\\n  --report_name OLD NEW \\\n  --cohorts_list . \\\n  --measure_regex \"AUC|OR@PR\" \\\n  --table_format \"cm,r\"\n</code></pre></p> <p>Key options:</p> <ul> <li>Specify one or more bootstrap result files and assign each a name.</li> <li>Filter cohorts using the <code>--cohorts_list</code> regex (use <code>.</code> to include all).</li> <li>Select which measurements to extract with <code>--measure_regex</code> (e.g., <code>AUC|SENS@FPR</code>).</li> <li>Control table layout with <code>--table_format</code>. There are three dimensions:<ul> <li><code>r</code>: report (the result file, e.g., baseline or MES_Full)</li> <li><code>c</code>: cohort (multiple cohorts per file)</li> <li><code>m</code>: measurement (e.g., AUC, SENS@FPR_05) Specify the three characters, separated by a comma, to map dimensions to rows and columns. One token will have two characters, expanding all combinations (Cartesian product) and using <code>$</code> as a delimiter. For example, <code>cm,r</code> means rows are cohort \u00d7 measurement, columns are reports.</li> </ul> </li> </ul> <p>Additional arguments:</p> <ul> <li><code>--break_cols</code>: Splits cohort filters into separate columns (default behavior).</li> <li><code>--break_mes</code>: Splits measurement values (e.g., <code>8.4[7.8 - 9.2]</code>) into three columns: Mean, Min, Max.</li> <li><code>--output_path</code>: Save the results as a CSV file.</li> </ul> <p>Example output (without <code>--break_cols</code>): <pre><code>Cohort$Measurements     OLD        NEW\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000$AUC        0.807[0.801 - 0.814]    0.816[0.809 - 0.822]\nTime-Window:90.000-360.000,Age:50.000-80.000,Suspected:0.000-0.000,Ex_or_Current:1.000-1.000$OR@PR_3    8.4[7.8 - 9.2]  9.5[8.7 - 10.3]\n... (truncated)\n</code></pre></p> <p>With <code>--break_cols</code> (default): <pre><code>Cohort  Age     Ex_or_Current   Suspected       Time-Window     Measurements    OLD        NEW\nTime-Window:90.000-360.000,Age:50.000-80.000,...   50-80   1   0   90-360  AUC     0.807[0.801 - 0.814]    0.816[0.809 - 0.822]\n... (truncated)\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Utility%20tools%20to%20process%20bootstrap%20results.html#plotting-graphs-from-bootstrap-results","title":"Plotting Graphs from Bootstrap Results","text":"<p>To generate graphs (such as ROC curves) from bootstrap result files, use the <code>plt_bt.py</code> script:</p> <p><pre><code>plt_bt.py --input $BT_REPORT_PATH_1 $BT_REPORT_PATH_2 ... $BT_REPORT_PATH_N \\\n  --names $NAME_FOR_1 $NAME_FOR_2 ... $NAME_FOR_N \\\n  --output $OUTPUT_PATH \\\n  --measure $MEASURE \\\n  --filter_cohorts $REGEX_TO_FILTER_COHORTS \\\n  --show_ci 1 \\\n  --add_y_eq_x 1  # Adds y=x reference line\n</code></pre> For ROC, use <code>SENS@FPR</code> as the measure.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html","title":"Extending Bootstrap","text":"<p>The bootstrap infrastructure is designed to be extensible, allowing you to add custom statistical measurements to the bootstrap analysis process (i.e., repeated experiments with replacement) to estimate confidence intervals, standard deviations, and more.</p> <p>To do this, you simply write a function that computes your desired measurement(s).</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#function-signature","title":"Function Signature","text":"<p>Your measurement function must use the following signature:</p> <ul> <li><code>Lazy_Iterator *iterator</code>: Used to fetch data for measurement calculation. This iterator is efficient, performing randomization on the fly without allocating memory for each bootstrap experiment.</li> <li><code>int thread_num</code>: The thread number for parallel execution, used by the iterator.</li> <li><code>Measurement_Params *function_params</code>: Optional pointer to function parameters (can be <code>nullptr</code>). Use this to pass custom parameters to your function.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#using-the-iterator","title":"Using the Iterator","text":"<p>Call <code>fetch_next(thread_num, y, pred, w)</code> to iterate through the data. It returns <code>true</code> until the end of input is reached.</p> <p>If you are using your function outside of <code>bootstrap.cpp</code>, use <code>fetch_next_external</code> instead (the only difference is a minor optimization in <code>fetch_next</code>).</p> <p>For more complex cases (e.g., multi-label outcomes), you can pass <code>ret_preds_order</code> to <code>fetch_next</code> when you have multiple predictions.</p> <p>Argument meanings:</p> <ul> <li><code>y</code>: Label/outcome</li> <li><code>pred</code>: Prediction/score</li> <li><code>w</code>: Weight (<code>-1</code> if not used). If your measurement does not support weights, check that <code>w == -1</code> and throw an error if weights are present.</li> </ul> <p>function_params: If your function uses parameters, check that <code>function_params</code> is not <code>nullptr</code> (otherwise use defaults), and cast it to your parameter type. <code>Measurement_Params</code> is a simple class you can extend (e.g., <code>ROC_Params</code>).</p> <p>Example: <pre><code>map&lt;string, float&gt; calc_roc_measures_with_inc(Lazy_Iterator *iterator, int thread_num, Measurement_Params *function_params) {\n    ROC_Params default_params;\n    ROC_Params *params = &amp;default_params;\n    if (function_params != NULL)\n        params = (ROC_Params *)function_params;\n    // Use params as needed\n    float max_diff_in_wp = params-&gt;max_diff_working_point;\n    // ...\n}\n</code></pre></p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#function-output","title":"Function Output","text":"<p>Your function should return a <code>map&lt;string, float&gt;</code>, where each key is the name of a measurement and the value is the result. This allows you to compute multiple measurements in a single function.</p> <p>The infrastructure will automatically append suffixes to each measurement: <code>_Mean</code>, <code>_Std</code>, <code>_CI.Lower.95</code>, <code>_CI.Upper.95</code>, <code>_Obs</code>. See: Bootstrap legend</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#example-counting-cases-and-controls","title":"Example: Counting Cases and Controls","text":"<p><pre><code>map&lt;string, float&gt; calc_npos_nneg(Lazy_Iterator *iterator, int thread_num, Measurement_Params *function_params) {\n    map&lt;string, float&gt; res;\n    map&lt;float, int&gt; cnts;\n    float y, w, pred;\n    while (iterator-&gt;fetch_next(thread_num, y, pred, w))\n        cnts[y] += w != -1 ? w : 1;\n    res[\"NPOS\"] = (float)cnts[(float)1.0];\n    res[\"NNEG\"] = (float)cnts[(float)0];\n    return res;\n}\n</code></pre> This function counts the number of positive and negative outcomes in the data, supporting weights if present.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/index.html#using-your-custom-function","title":"Using Your Custom Function","text":"<pre><code>#include &lt;MedStat/MedStat/MedBootstrap.h&gt;\n\n// Example custom measurement function\nmap&lt;string, float&gt; MY_CUSTOMIZED_MEASUREMENT_FUNCTION(Lazy_Iterator *iterator, int thread_num, Measurement_Params *function_params) {\n    // ...\n}\n\n// Example custom parameter class\nclass Custom_Measurement_Parameters : public Measurement_Params {\npublic:\n    int min_bin_size = 0;\n    float cases_weight = 1;\n    float controls_weight = 1;\n};\n\n// Helper function to generate matrix for filtering (needed if generating cohorts and filtering with bootstrap)\nvoid get_mat(const string &amp;json_model, const string &amp;rep_path, MedSamples &amp;samples, MedModel &amp;mdl) {\n    if (!json_model.empty())\n        mdl.init_from_json_file(json_model);\n    else\n        MLOG(\"No json for bootstrap - can only use Time-Window,Age,Gender filters\\n\");\n    bool need_age = true, need_gender = true;\n    for (FeatureGenerator *generator : mdl.generators) {\n        if (generator-&gt;generator_type == FTR_GEN_AGE)\n            need_age = false;\n        if (generator-&gt;generator_type == FTR_GEN_GENDER)\n            need_gender = false;\n    }\n    if (need_age)\n        mdl.add_age();\n    if (need_gender)\n        mdl.add_gender();\n    vector&lt;int&gt; pids_to_take;\n    samples.get_ids(pids_to_take);\n\n    MedPidRepository rep;\n    mdl.load_repository(rep_path, pids_to_take, rep, true);\n\n    if (mdl.learn(rep, &amp;samples, MedModelStage::MED_MDL_LEARN_REP_PROCESSORS,\n                  MedModelStage::MED_MDL_APPLY_FTR_PROCESSORS) &lt; 0)\n        MTHROW_AND_ERR(\"Error creating features for filtering\\n\");\n}\n\nint main(int argc, char *argv[]) {\n    // Set bootstrap parameters (see MedBootstrap::init for all options)\n    string bt_params = \"loopcnt=500;sample_per_pid=1\";\n    MedBootstrapResult bt;\n    bt.bootstrap_params.init_from_string(bt_params);\n\n    // Custom measurement parameters\n    Custom_Measurement_Parameters custom_args;\n\n    string json_model;    // Optional: for generating features for cohort filtering\n    string cohorts_file;  // Path to cohort definitions. If not specified, all samples are used as a single cohort.\n    string rep_path;      // Path to repository (needed if filtering by more than Time-Window)\n\n    MedSamples preds;     // Your MedSamples with predictions\n    preds.read_from_file(\"YOUR_SAMPLES_PATH\");\n\n    if (!cohorts_file.empty())\n        bt.bootstrap_params.parse_cohort_file(cohorts_file);\n\n    // Set measurement functions (default is calc_roc_measures_with_inc)\n    vector&lt;pair&lt;MeasurementFunctions, Measurement_Params *&gt;&gt; &amp;cl_m = bt.bootstrap_params.measurements_with_params;\n    cl_m.clear();\n    cl_m.push_back(pair&lt;MeasurementFunctions, Measurement_Params *&gt;(MY_CUSTOMIZED_MEASUREMENT_FUNCTION, &amp;custom_args));\n\n    // Create matrix for json and repository\n    MedModel mdl;\n    get_mat(json_model, rep_path, smps_preds, mdl);\n    MedFeatures &amp;bt_features_matrix = mdl.features;\n\n    // Prepare features for bootstrap analysis\n    vector&lt;float&gt; preds_v, labelsOrig;\n    vector&lt;int&gt; pidsOrig, preds_order;\n    map&lt;string, vector&lt;float&gt;&gt; bt_data;\n    bt.bootstrap_params.prepare_bootstrap(bt_features_matrix, preds_v, labelsOrig, pidsOrig, bt_data, preds_order);\n    // Run bootstrap analysis\n    bt.bootstrap(preds, bt_data);\n\n    // Access results: bt.bootstrap_results is a map of maps (cohort name -&gt; measurement name -&gt; value)\n    for (auto &amp;it_cohort : bt.bootstrap_results) {\n        const string &amp;cohort_name = it_cohort.first;\n        for (auto &amp;it_measurement : it_cohort.second) {\n            const string &amp;measurement_name = it_measurement.first;\n            float value = it_measurement.second;\n            fprintf(stdout, \"%s %s %f\\n\", cohort_name.c_str(), measurement_name.c_str(), value);\n        }\n    }\n}\n</code></pre> <p>You can append multiple measurement functions; the bootstrap process will apply all of them.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/bootstrap_app/Extending%20bootstrap/Using%20Harrell%20C%20Statistics.html","title":"Using Harrell C Statistics","text":"<p>activate: <pre><code>bootstrap_app --measurement_type \"calc_harrell_c_statistic\"\n</code></pre> Encoding samples for harrell's\u00a0c Statistics:</p> <ul> <li>Case/Control =&gt; effect outcome/y sign. positive is case, negative controls. Can't handle event in time zero.</li> <li>Time to event =&gt; abs value of outcome/y</li> <li>Score =&gt; the prediction</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html","title":"change_model","text":"<p>This tool allows you to modify components within an existing MedModel without the need to retrain the entire model. It's an efficient way to make targeted adjustments to a model's pipeline, especially for production, testing, or debugging purposes. This tool, is part of the AllTools compilation</p> <p>Common use cases include:</p> <ul> <li>Modifying Production Settings: nable special flags in components like <code>RepBasicOutlierCleaner</code> to store outlier information. This process is typically slower and more memory-intensive, so you might only want to activate it in production.</li> <li>Controlling Pre-processing: Temporarily disable or adjust pre-processing steps like normalization or imputation for specific testing or data matrix creation.</li> <li>Adjusting <code>ButWhy</code> Explanations: Change arguments for the explainability component, such as the number of contributors to output, without re-learning the model.</li> </ul> <p><code>change_model</code>supports both interactive and non-interactive command-line interfaces for controlling and modifying model objects.  This documentation focuses on the non-interactive use case, which is most common for scripting and automation.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#arguments","title":"Arguments","text":"<p>There are two primary methods for using change_model in non-interactive mode:  a simple one-liner for a single change or a JSON file for multiple, more complex changes.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#method-1-single-change-via-command-line","title":"Method 1: Single Change via Command Line","text":"<p>For a single, quick modification, use the <code>--search_object_name</code> and <code>--object_command</code> flags.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#example-disabling-outlier-attribute-storage","title":"Example: Disabling Outlier Attribute Storage","text":"<p>This command finds all <code>RepBasicOutlierCleaner</code> objects and clears the attributes used for storing outlier information, effectively disabling that feature to improve performance.</p> <pre><code>change_model --model_path &lt;model_path&gt; --output_path &lt;output_model_path&gt; --interactive_change 0 --search_object_name RepBasicOutlierCleaner --object_command \"nrem_attr=;nrem_suff=;ntrim_attr=;ntrim_suff=\"\n</code></pre> <ul> <li><code>--model_path</code>: Path to your existing model file.</li> <li><code>--output_path</code>: Path where the modified model will be saved.</li> <li><code>--interactive_change</code>: <code>0</code> Activates non-interactive mode. Default is <code>1</code></li> <li><code>--search_object_name</code>: The name of the object to find and modify. In this case, <code>RepBasicOutlierCleaner</code>.</li> <li><code>--object_command</code>: A string of semicolon-separated <code>key=value</code> pairs to set new parameters. To delete an object, use <code>\"DELETE\"</code>.</li> </ul>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#method-2-multiple-changes-via-json-file","title":"Method 2: Multiple Changes via JSON File","text":"<p>For complex or multiple modifications, you can provide a JSON file containing all your change instructions using the <code>--change_model_file</code> argument. This file will be used to initiate a <code>ChangeModelInfo</code> object, which then executes the requested changes.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#json-change-block-structure","title":"JSON Change Block Structure","text":"<p>The JSON file contains an array of \"change blocks,\" where each block defines a specific modification.</p> <pre><code>{ \n  \"changes\": [\n       // Define your change blocks here\n  ]\n}\n</code></pre> <p>Each block has the following parameters:</p> <pre><code>{\n    \"change_name\": \"&lt;name_for_logging&gt;\",\n    \"object_type_name\": \"&lt;The object we seek to change&gt;\",\n    \"json_query_whitelist\": [ \"&lt;string list of regex items&gt;\" ],\n    \"json_query_blacklist\": [ \"&lt;string list of regex items&gt;\" ],\n    \"change_command\": \"&lt;command string&gt;\",\n    \"verbose_level\": \"&lt;verbosity level for this action (optional)&gt;\"\n}\n</code></pre> <ul> <li><code>json_query_whitelist</code>: A list of regular expressions. A component will only be selected if its string representation matches all regex strings in this array (AND condition).</li> <li><code>json_query_blacklist</code>: A list of regular expressions. A component will be filtered out if its string representation matches any regex string in this array.</li> </ul> <p>Warning!: Be extremely cautious when modifying a model. Only change things that do not require re-training. For instance, do not remove features that are essential for the classifier to function correctly.</p>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#json-examples","title":"JSON Examples","text":""},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#example-1-remove-normalizers-and-imputers","title":"Example 1: Remove Normalizers and Imputers","text":"<p>This example shows how to selectively remove specific normalizers and all imputers. The first block uses <code>json_query_whitelist</code> to target only the <code>FeatureNormalizer</code> objects for <code>Age</code> and <code>Gender</code> while leaving others untouched. The second block removes all <code>FeatureImputer</code> objects.</p> <pre><code>{ \n  \"changes\": [\n       {\n            \"change_name\":\"Remove Normalizers\",\n            \"object_type_name\":\"FeatureNormalizer\",\n            \"json_query_whitelist\": [ \"Age|Gender\" ],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"DELETE\",\n            \"verbose_level\":\"2\"\n       },\n       {\n            \"change_name\":\"Remove Imputers\",\n            \"object_type_name\":\"FeatureImputer\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"DELETE\",\n            \"verbose_level\":\"2\"\n       }\n  ]\n}\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/index.html#example-2-multiple-modifications-in-one-file","title":"Example 2: Multiple Modifications in One File","text":"<p>This example demonstrates how to perform several changes in a single operation: </p> <ul> <li>Removing outlier attributes - that reports on outliers. makes think slower and consume more memory.</li> <li>Deleting RepCheckReq objects - that checks eligibility criteria. makes think slower and consume more memory.</li> <li>adjusting the <code>max_data_in_mem</code> parameter for the <code>MedModel</code> object itself, to adjusts the model's memory limit to process larger prediction batches.</li> </ul> <pre><code>{ \n  \"changes\": [\n       {\n            \"change_name\":\"Remove attributes from cleaners\",\n            \"object_type_name\":\"RepBasicOutlierCleaner\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"nrem_attr=;ntrim_attr=;nrem_suff=;ntrim_suff=\",\n            \"verbose_level\":\"1\"\n       },\n       {\n            \"change_name\":\"Remove attributes Req\",\n            \"object_type_name\":\"RepCheckReq\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"DELETE\",\n            \"verbose_level\":\"1\"\n       },\n       {\n            \"change_name\":\"MedeModel Memory\",\n            \"object_type_name\":\"MedModel\",\n            \"json_query_whitelist\": [],\n            \"json_query_blacklist\": [],\n            \"change_command\": \"max_data_in_mem=0\",\n            \"verbose_level\":\"1\"\n       }\n  ]\n}\n</code></pre>"},{"location":"Infrastructure%20Library/Medial%20Tools/change_model/How%20to%20limit%20memory%20usage%20in%20predict.html","title":"Limiting Memory Usage in predict","text":"<p>If you encounter a <code>bad alloc</code> error during prediction, you may need to restrict memory usage. Run <code>--get_model_preds</code> with an additional flag:</p> <pre><code>Flow --get_model_preds ... --change_model_file ${LIMIT_MEMORY_JSON}\n</code></pre> <p>Where <code>LIMIT_MEMORY_JSON</code> is defined as: <pre><code>{\n  \"changes\": [\n    {\n        \"change_name\":\"Decrease mem size\",\n        \"object_type_name\":\"MedModel\",\n        \"change_command\":\"max_data_in_mem=100000000\"\n    }\n  ]\n}\n</code></pre></p> <p>This setting limits the maximum matrix flat array size to 100M float elements (~400 MB). If out model has 1000 features it will split the predictions into 100K batches. Keep in mind:</p> <ul> <li>Actual memory usage can reach up to 2\u00d7 this value due to temporary duplication.</li> <li>Additional constant and object overhead also contributes to memory consumption that are not taking into account in the batch size</li> <li>The limit applies specifically to matrix creation and batch processing during prediction.</li> </ul>"},{"location":"Infrastructure%20Library/Research/index.html","title":"Research","text":"<ul> <li>What-If<ul> <li>An envelope script for Causal Inference on Synthetic Data</li> <li>Checking Causal Inference on Synthetic Data</li> <li>Generating Syntethic Data for Causal-Inference</li> </ul> </li> <li>But Why - Explainers<ul> <li>Sanity test experiment for debuging</li> <li>ButWhy experiments results<ul> <li>TestGibbs</li> </ul> </li> <li>Experiments - Stage B</li> <li>Experiments - Stage C (Freeze Version 1)</li> </ul> </li> <li>Conferences<ul> <li>AIME 2019</li> <li>Boston MLHC (ML in HealthCare) 2017</li> <li>KDD 2017</li> </ul> </li> <li>Journal Club Page</li> <li>Factorization Machines</li> <li>Raindrop</li> <li>Missing Values Based Uncertainty Analysis</li> <li>Right Censoring</li> </ul>"},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html","title":"Factorization Machines","text":""},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html#rationale","title":"Rationale","text":"<p>Classifiers currently used in Medial make use of different types of features, in particular features based on e.g. RC (read codes) or ATC (drug codes). Due to a large number of RC and ATC codes, the total number of RC/ATC features can be very high (200K in CKD Fast Progressors project). In addition, these features are extremely sparse. It would be computationally prohibitive to handle all this features in XGBoost. In order to solve this problem, there exists a \"category_depend\" mechanism, which reduces a number of RC/ATC features by keeping only features which are highly correlated to the outcome. Therefore, information about some read codes or drugs is lost.\u00a0 \u00a0 This project was undertaken in order to check whether it is possible to retain this information by means of\u00a0 \"Factorization Machines\" method, which is suited to work with a large number\u00a0 of extremely sparse features, and is\u00a0widely used in Recommender System.\u00a0 \u00a0</p>"},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html#factorization-machines_1","title":"Factorization machines","text":"<p>Factorization Machines (FM) are\u00a0generic supervised learning models that map arbitrary real-valued features into a low-dimensional latent factor space\u00a0and can be applied naturally to a wide variety of prediction tasks including regression, classification, and ranking.\u00a0FMs can estimate model parameters accurately under very sparse data and train with linear complexity, allowing them to scale to very large data sets. FMs are widely used for real-world recommendation problems. FM model is described in details in the article by Steffen Rendle.</p>"},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html#libfm","title":"libFM","text":"<p>libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least squares (ALS) optimization as well as Bayesian inference using Markov Chain Monte Carlo (MCMC). Links to the source code and Windows executable can be found at libfm.org LibFM manual is available here.</p>"},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html#workflow","title":"Workflow","text":"<p>We tested performance on libFM on a number of datasets, all based on the source dataset from the CKD Fast Progressors project. We decided to omit all the lab test-based features, since these are dense, while libFM is more suited to work on sparse binary and categorical features. Each subsequent set includes more features than the previous one, up to 200K+ features. Where possible we compared results returned by libFM to those of XGBoost, trained on respective data (using the same configuration that was used in CKD Fast Progressors project)</p>"},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html#datasets","title":"Datasets","text":"<p>1. Age, Gender columns only 2. Age,Gender and all RC-related and ATC-related columns of type \"category set\" used in\u00a0CKD Fast Progressors project\u00a0(not including features generated via \"category_depend\" mechanism) 3. \u00a0Age,Gender,RC-related and ATC-related columns of type \"category set\" and \"category_depend\" used in\u00a0CKD Fast Progressors project\u00a0 4. Age,Gender and a sparse matrix of RC and ATC-related features generated using Embeddings project\u00a0(with some columns dropped using \"shrinkage\" mechanism, descibed in the Embeddings project documentation)\u00a0\u00a0 5. Age,Gender and a sparse matrix of RC and ATC-related features generated using\u00a0Embeddings\u00a0project\u00a0(this time without using \"shrinkage'). Resulting matrix has 200K+ columns.</p>"},{"location":"Infrastructure%20Library/Research/Factorization%20Machines.html#results","title":"Results","text":"<ol> <li>Age, Gender columns only</li> <li>Age,Gender and all RC-related and ATC-related columns of type \"category set\" used in\u00a0CKD Fast Progressors project\u00a0 (not including features generated via \"category_depend\" mechanism) Results produced by libFM are marginally better in this scenario</li> </ol> Classifier AUC Linear 0.541 XGBoost 0.573 libFM 0.581 <ol> <li>Age,Gender,RC-related and ATC-related columns of type \"category set\" and \"category_depend\" used in\u00a0CKD Fast Progressors project\u00a0</li> <li>Age,Gender and a sparse matrix of RC and ATC-related features generated using\u00a0Embeddings\u00a0project\u00a0(with some columns dropped using \"shrinkage\" mechanism, descibed in the Embeddings project documentation)\u00a0\u00a0</li> <li>Age,Gender and a sparse matrix of RC and ATC-related features generated using\u00a0Embeddings\u00a0project\u00a0(this time without using \"shrinkage'). Resulting matrix has 200K+ columns. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</li> </ol>"},{"location":"Infrastructure%20Library/Research/Journal%20Club%20Page.html","title":"Journal Club Page","text":"<p>Following are papers/presentations/links presented in the Journal Club over time, for your pure enjoyment \u00a0</p>"},{"location":"Infrastructure%20Library/Research/Journal%20Club%20Page.html#xgboost","title":"XGBoost","text":"<p>xgboost_article.pdf xgboost_weight_quantile_sketch.pdf </p>"},{"location":"Infrastructure%20Library/Research/Journal%20Club%20Page.html#deep-learning","title":"Deep Learning","text":"<p>DeepLearningIntroduction.pptx </p>"},{"location":"Infrastructure%20Library/Research/Journal%20Club%20Page.html#openmp","title":"OpenMP","text":"<p>OpenMPIntro.pptx</p>"},{"location":"Infrastructure%20Library/Research/Missing%20Values%20Based%20Uncertainty%20Analysis.html","title":"Missing Values Based Uncertainty Analysis","text":"<p>Use case: KP, Lung Cancer, monthly samples (monthly, without suspected, at least 3 months before outcome for cases) Question / goal / idea: When we have missing values we can:</p> <ul> <li>a) ignore the patient (if he has 'too much missing info')</li> <li>b) impute the missing values, or</li> <li>c) using GIBBS estimation of score std to decide to\u00a0on which patients it is more important to fill the missing values What is better? \u00a0 Setting: MR/Projects/Shared/AlgoMarkers/LungCancer/scripts_uncertainty/adjust_and_calculate.sh MR/Projects/Shared/AlgoMarkers/LungCancer/configs_uncertainty/GIBBS.json</li> </ul> <p>Evaluation: Easier to start with a notebook, but we intend to add a function to bootstrap_app, to report result by Top N Options:</p> <ul> <li>a) One sample per patient (if you using bootstrap_app - we should chose the sample before the app)</li> <li>b) all samples per patient, or\u00a0</li> <li> <p>c) simulation (relatively complicated in this use case - hopefully we won't need it) \u00a0 Baseline Basic evaluation - in the graph, number of cases in the Top N, one sample for patient, age 50-80</p> </li> <li> <p>if we take all patients (3215 on average)</p> </li> <li>if we take just patients with smoking intensity information (1964 patients eligible on average)</li> <li>if we take just patients with WBC in the last 6 months (1438 eligible on average) A few notes:</li> <li>Results are for Ex or Current smokers - if we included never smokers results would have been a bit better ...</li> <li> <p>I got similar results when we take all samples per patient\u00a0 Thus, it is better to take all patients, using the standard imputer, but we will stick to\u00a0Ex or Current smokers Next, we will explore ways to be do even better using the variability in potential score, using GIBBS imputation statistics \u00a0 GIBBS -\u00a0na\u00efve Preliminaries:</p> </li> <li> <p>The script adjust_and_calculate.sh train the adjusted model on all samples - DONE (leakage! should be re-visited later)\u00a0</p> </li> <li>However, apply (calculate) on all samples is far too slow</li> <li>So the Notebook: KP_uncertainty_generate_samples take one sample per patient, and apply GIBBS just for the sample</li> <li> <p>We now have 15 examples so we can test results variance Naive approach would be to replace pred_0 with pred_0 + score std based on GIBBS, to prioritize high uncertainty (or by\u00a0\u00a0pred_0 - score std for the opposite) Both options do not help. Furthermore, if we take (for instance)\u00a0pred_0 +/- 3 X score std result deteriorates. \u00a0 Missing data completion Assume we have M tickets for data completion. We will choose the highest M patients with missing information, with the highest\u00a0pred_0 + score std based on GIBBS.</p> </li> <li> <p>For now we assume 100% compliance</p> </li> <li>Note that it night not be optimal to take the highest M, as (when we not the working point and expected cutoff) those in the top with small std do not need data completion. However, they are not many, and as the results are not sensitive to M, we keep it this way for code simplicity, and to get results for several working points in one run. To simulate data completion:</li> <li>Optional, use data from the future:<ul> <li>Relevant only for smoking features</li> <li>Data exists in ~1/3 of cases and ~1/3 of controls\u00a0</li> </ul> </li> <li>2nd option, find the m (for instance m=20) closest patients<ul> <li>Closets means same gender, same smoking status, distance is measured over all other (normalized) features ,with higher weight for age, and smoking features</li> <li>Choose 1 patient randomly from the 20, and take his values</li> </ul> </li> <li>For patients with missing data but not in the top M, use imputation by our standard imputer (i.e., use original score)</li> <li>Implemented in Notebook:\u00a0KP_uncertainty_missing_values_completion After that we re-calculate the score for those M patients (prediction is made based on split with the right model from the cross validation), take Top N and check performance. MAJOR DRAWBACK In smoking features calculation, we impute\u00a0Smok_Days_Since_Quitting by code (and not by the imputer), so:</li> <li>GIBBS cannot take it into account, and\u00a0</li> <li>for our test\u00a0Smok_Days_Since_Quitting is never missing Furthermore, as you can see in the research summary of the LEAN model,\u00a0Smok_Days_Since_Quitting alone cuts significantly the gap between with and without smoking features.</li> </ul> <p>MAIN RESULT - no significant change in performance .... Example from 1 run: </p> <ul> <li>axis x - N</li> <li>axis y- cancer detected</li> <li>parameters (most important - results are not sensitive to these parameters ...):<ul> <li>sample #09\u00a0</li> <li>M = 2000 (how many data completions)</li> <li>m = 20 (how many closest friends to randomly chose from for imputation)</li> <li>W - 5 for age, 3 for smoking features and BMI, 1 for labs</li> <li>Impute from future - True</li> <li>Priority for impute - pred_0 + std by GIBBS \u00a0 Statistics over 10 datasets, each with 3 run (imputation has random):  Side notes</li> </ul> </li> </ul> <ol> <li>The 'hope for success' was in the data completion procedure, as we complete missing values conditioned on the actual status (case/control). Indeed, for patients with data completion, the average score for cases increased more than for controls, but apparently not more enough. See example in the following table - change of score for M=5000</li> <li>The data completion increases the cutoff score for Top N. It makes sense, as the standard imputation impute to the mean, and we sample from the distribution, hence have more extreme results.</li> <li>Is the data completion by closest friends OK? Other then exploring some examples, we compare the std in score calculated in GIBBS, and the actual std we get (among patients with expected similar std). See the result in the graph - axis x is GIBBS and axis y is what we get. Trend is similar but we higher variability, could be because GIBBS calculate the std on an over fitted model (trained over all the data)</li> </ol>"},{"location":"Infrastructure%20Library/Research/Raindrop.html","title":"Raindrop","text":"<p>/home/Repositories/MHS/build_Feb2016_Mode_3/maccabi.repository </p> <pre><code>/home/Repositories/MHS/RepProcessed/static/ BYEAR.tsv GENDER.tsv /home/Repositories/MHS/RepProcessed/dynamic/ Hemoglobin.tsv ... (rest of the signals) \n</code></pre> <pre><code>/home/Repositories/MHS/RepProcessed/static/ /home/Repositories/MHS/RepProcessed/dynamic/\n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/static\n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/dynamic\n</code></pre> <pre><code> /home/Repositories/MHS/RepProcessed/dynamic.feather /home/Repositories/MHS/RepProcessed/static.feather \n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/dynamic.feather\n/nas1/Work/Users/Ilya/Repositories/MHS/RepProcessed/static.feather\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#training-data-preparation","title":"Training data preparation","text":""},{"location":"Infrastructure%20Library/Research/Raindrop.html#convert-into-the-raindrop-input-format","title":"Convert into the Raindrop input format","text":"<p>M12data_Large/process_scripts/LoadMaccabi.py</p> <pre><code>Repository data \n    /home/Repositories/MHS/RepProcessed/dynamic.feather\n    /home/Repositories/MHS/RepProcessed/static.feather\nSamples \n    /server/Work/Users/Ilya/LGI/outputs.MHS/Samples.no_exclusions/train.730.1_per_control.samples\nNOTE:   \n    These are the train samples used by the CRC model\n</code></pre> <pre><code>/home/Ilya/Raindrop/M12data_Large/processed_data/arr_outcomes.npy\n/home/Ilya/Raindrop/M12data_Large/processed_data/PTdict_list.npy\n/home/Ilya/Raindrop/M12data_Large/processed_data/subsampled_df.pickle\nNOTE:   \n    Here \"subsampled_df.pickle\" contains data from the .samples file\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#convert-absolute-time-into-days-clean-zero-length-entries","title":"Convert absolute time into days, clean zero-length entries","text":"<p>Convert absolute time into days since 20010101 and remove pids which do not have associated dynamic data TRAINING dataset</p> <pre><code>20221103_Fix_Train_FromScratch.ipynb\n</code></pre> <pre><code>/home/Ilya/Raindrop/M12data_Large/processed_data/\n    PTdict_list.npy\n    arr_outcomes.npy\n    subsampled_df.pickle\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTrain/processed_data/\n    PTdict_list.npy\n    arr_outcomes.npy\n    subsampled_df.pickle\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#switch-from-absolute-to-relative-time-wrt-sample-date","title":"Switch from absolute to relative time (wr.t. sample date)","text":"<p>http://node-05:9000/notebooks/ilya-internal/20221213_RelativeTimePlusOne.ipynb</p> <pre><code>/home/Ilya/Raindrop/LargeTrain/processed_data/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTrainRelative/\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#add-offset-1-to-the-relative-time-to-avoid-0-being-interpreted-as-missing-data","title":"Add offset 1 to the relative time (to avoid 0 being interpreted as missing data)","text":"<pre><code>/home/Ilya/Raindrop/LargeTrainRelative/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTrainRelativePlusOne/\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#testing-data-preparation","title":"Testing data preparation","text":""},{"location":"Infrastructure%20Library/Research/Raindrop.html#reducing-to-a-single-cohort","title":"Reducing to a single cohort","text":"<p>We started from applying the Raindrop model to   the WHOLE Test set, see</p> <p><pre><code>/nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest/bootstrap_raindbow_model.sh\n</code></pre> However, applying the model to the whole test set   resulted in high memory consumption (about 100GB had to be allocated)  Since we are mainly interested in the performance on one specific cohort:   \"MULTI Time-Window:0,365 Age:40,89\", we decided to restrict the test set to this cohort only.  The Test365 samples file was prepared as follows:</p> <pre><code>1)     Create file \"M12data_LargeTest/bootstrap/rainbow.crc.Raw\"\n    using following command\n    [see M12data_LargeTest/bootstrap_rainbow_model.sh]\n    bootstrap_app \\\n    --use_censor 0 \\\n    --rep /home/Repositories/MHS/build_Feb2016_Mode_3/maccabi.repository \\\n    --input ${RUN_PATH_FULL_LEAN}/bootstrap/rainbow.crc.preds \\\n    --output ${RUN_PATH_FULL_LEAN}/bootstrap/rainbow.crc \\\n    --cohort \"${COHORT}\" \\\n    --working_points_pr 1,3,5,7,10,15,20,25,30,35,40,45,50,55,60,65,70,80,90,95,99 \\\n    --working_points_fpr 1,3,5,7,10,15,20,25,30,35,40,45,50,55,60,65,70,80,90,95,99 \\\n    --working_points_sens 75,80,85,90,95,97,99 \\\n    --output_raw\n    Input:  /nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest/bootstrap/rainbow.crc.preds\n    Output:  /nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest/bootstrap/rainbow.crc.Raw\n2)    Take \"rainbow.crc.Raw\" as an input and run notebook 20221023_ExtractTestCohort.ipynb to\n    create cohort in \"samples\" file format.\n    Code:   http://node-05:9000/notebooks/ilya-internal/20221023_ExtractTestCohort.ipynb\n    Input:  M12data_LargeTest/bootstrap/rainbow.crc.Raw \n    Output: M12data_LargeTest/test_cohorts/test_365.samples\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#convert-test365-samples-into-the-raindrop-input-format","title":"Convert Test365 samples into the Raindrop input format","text":"<p>M12data_LargeTest365/LoadMaccabiTest.py</p> <pre><code>M12data_LargeTest/test_cohorts/test_365.samples\nRepProcessed/dynamic.feather \nRepProcessed/static.feather\n</code></pre> <pre><code>M12data_LargeTest365/processed_data/subsampled_df.pickle [contain (probably subsampled) input data]\nM12data_LargeTest365/processed_data/PTdict_list.npy\nM12data_LargeTest365/processed_data/arr_outcomes.npy\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#convert-absolute-time-into-days-clean-zero-length-entries_1","title":"Convert absolute time into days, clean zero-length entries","text":"<p>Convert absolute time into days since 20010101 and remove pids which do not have associated dynamic data TRAINING dataset</p> <pre><code>20221103_Fix_Test365_FromScratch.ipynb\n</code></pre> <p><pre><code> /nas1/UsersData/ilya-internal/PycharmProjects/Raindrop-main/M12data_LargeTest365/processed_data/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTest365/processed_data/\n    PTdict_list.npy\n    arr_outcomes.npy\n    subsampled_df.pickle\n</code></pre></p>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#switch-from-absolute-to-relative-time-wrt-sample-date_1","title":"Switch from absolute to relative time (wr.t. sample date)","text":"<p>http://node-05:9000/notebooks/ilya-internal/20221213_RelativeTimePlusOne.ipynb</p> <pre><code>/home/Ilya/Raindrop/LargeTest365/processed_data/\n</code></pre> <pre><code>/home/Ilya/Raindrop/LargeTest365Relative/\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#modifications-applied-to-the-raindrop-code","title":"Modifications applied to the Raindrop code","text":"<p>Original Raindrop code is not well-suited for experimentation   for following reasons:</p> <ul> <li>Raindrop.py contains code specific to handling different datasets.</li> <li>Training and testing are done in the same loop</li> <li>In order to get a confidence interval for AUC authors train models for five different splits, rather than using Bootstrap</li> <li> <p>Many configuration parameters are hardcoded We made following modifications to the code:</p> </li> <li> <p>Strip Raindrop.py of the code not relevant to MHS</p> </li> <li>Implement MHS processing based on the code handling Physionet 2012 (denoted P12 in the code). We will denote the code specific to MHS dataset as M12.</li> <li>Split the code into RaindropTrain.py and RaindropTest.py</li> <li>Add command-line parameters that allow setting configuration parameters from the command line</li> <li> <p>Implement model evaluation using Bootstrap, in order to be consistent with the SOTA model evaluation approach. New files:</p> </li> <li> <p>RaindropTrain.py</p> </li> <li>RaindropTest.py Obsoleted files:</li> <li>Raindrop.py</li> </ul>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#model-evaluation","title":"Model evaluation","text":"<p>After the model is applied to the test data during the call to RaindropTest.py we convert predictions into a format expected by the Medial's bootstrap_app.  The script that implements this conversion is called BootstrapPrepare.py  Code:</p> <pre><code>code/BootstrapPrepare.py\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#script-infrastructure-for-fast-experimentation","title":"Script infrastructure for fast experimentation","text":"<p>In order to be able to run experiments fast while logging all the necessary information for retrospective analysis,  we implemented a folder template which contains all or some of following files:  train_test_bootstrap.sh this file trains a model, applies it to the test set and then computes AUC with confidence interval using bootstrap_app  log.txt this file contains logs of the ./train_test_bootstrap.sh invokation</p>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#experiments","title":"Experiments","text":""},{"location":"Infrastructure%20Library/Research/Raindrop.html#largerelativeaftertrainfix","title":"LargeRelativeAfterTrainFix/","text":"<pre><code>Train model on RELATIVE TIME.\nThe \"fix\" mentioned in the folder name is that we make sure relative time is never zero by adding 1. This is necessary since 0 is interpreted as a missing value by the Raindrop code.\nAUC     0.829[0.814 - 0.843]\nNOTE:\n    We also applied model to the test data after EACH epoch\n    to see the dynamics of the AUC as a function of epoch.\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#largerelative_d_ob_16","title":"LargeRelative_D_OB_16/","text":"<pre><code>Based on LargeRelativeAfterTrainFix/\nbut this time we increase dimension of the observation embedding to\nStatus:\n    There's no performance improvement as compared to D_OB=4\n    AUC     0.828[0.811 - 0.842]\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#largerelative_nlayers_4","title":"LargeRelative_nlayers_4/","text":"<pre><code>Increase the number of TransformerEncoder layers from 2 to 4.\nStatus:\n    There's no performance improvement as compared to 2 layers\n    AUC     0.825[0.812 - 0.839]\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#largematchedfull","title":"LargeMatchedFull/","text":"<pre><code>Train classifier based on FULL matching data\nWe first converted FULL matched data into Relative format\nusing 20221218_PrepareFullMatched_Data_RelativeTime.ipynb,\nthe resulting data written to\n/home/Ilya/Raindrop/LargeTrainMatchedRelative/\nStatus:\n    AUC     0.814[0.799 - 0.828]\n    Performance of the model trained on the matched data \n    is degraded, as compared to the model trained on the unmatched\n    data (AUC     0.829[0.814 - 0.843])\n</code></pre>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#todo","title":"TODO","text":""},{"location":"Infrastructure%20Library/Research/Raindrop.html#increase-dimensionality-of-the-positional-embedding","title":"Increase dimensionality of the positional embedding","text":"<p>This will increase the time resolution of Raindrop</p>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#use-inverted-time-series-collect-output-of-the-first-position-in-traonsformerencoder-same-way-it-is-done-when-using-bert-for-classification","title":"Use inverted time series + collect output of the first position in TraonsformerEncoder (same way it is done when using BERT for classification)","text":"<p>This may help the network focus on the latest observation</p>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#train-network-on-data-where-missing-values-were-fixed-using-ad-hoc-algoritm","title":"Train network on data where missing values were fixed using ad-hoc algoritm","text":"<p>Since \"observation propagation\" stage of the Raindrop algorithm turn out to be degenerate, this may improve the performance</p>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#disable-observation-propagation-stage-at-all","title":"Disable observation-propagation stage at all","text":"<p>Since \"observation propagation\" stage of the Raindrop algorithm turn out to be degenerate, we expect almost no performance degradation here</p>"},{"location":"Infrastructure%20Library/Research/Raindrop.html#summary","title":"Summary","text":"<p><pre><code>Performance of the Raindrop model (AUC 0.829) is significantly below the performance of SOTA CRC model.\n</code></pre> </p>"},{"location":"Infrastructure%20Library/Research/Right%20Censoring.html","title":"Right Censoring","text":"<p>Definition: Right censoring is the phenomenon of finite length of follow up. When we try to predict an event within a time window using a retrospective database, not all patients that stand the criterion of prediction at a given time have a follow up that covers the whole time window.</p> <p>If the event does happen within a shorter time the longer follow up is not needed. If the event does not happen within the follow up we do not know if it could still happen within the time window but after end of folow up.</p> <p>This phenomena is important for long prediction window or in databases of high turn around and short follow up. Right censoring is expected to give higher weight to short range events as the long range events have a higher probability of being censored.</p> <p>Right censoring can happen when patient is lost to follow up, when the experiment period ends or when patient dies (if the predicted event is not death). The following paper discusses the IPCW method to compensate for this censoring: https://www.sciencedirect.com/science/article/pii/S1532046416000496?via%3Dihub</p> <p>They assume that the loss to follow up distribution is independent from outcome. For each included prediction that had enough follow up till event (case) or till end of prediction window (control) they give weight that represents all other predictions that were censored before an event or end of window happened.</p> <p>We implemented their method for a model that predicts severe aortic stenosis within 5 years and also all cause death for same time window. We calculated weights on prediction file (using all test samples, including the ones that were censored), and used the weighted bootstrap to get the performance.</p> <p>As expected AUC deteriorated by up to 2% when implementing this on the test only. Implementation on training set is complicated if you also want to match for years.</p>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/index.html","title":"But Why - Explainers","text":"<p>The But Why project.</p> <p>Goal: Explain prediction of model for specific sample - \"local feature importance\" Challenge: There are several definitions\\options of how to explain a specific prediction.\u00a0 Method: Using Shapley Values definition. You can read more about it in Wikipedia. \u00a0 Background: Shapley values have several properties which makes them more\u00a0suitable\u00a0for explaining model predictions. It gives better result in\u00a0experiments that are favorable\u00a0among testers compared to other methods (science jornals claim that).</p> <p>Shapley Values - simplified: Given a specific prediction sample and requirement to explain the prediction by its contributing variables, Shapley Values divides the contribution fairly (comes from game theory) and is unique solution\u00a0 For this division of contributions which answer some important properties. Each variable is given an importance score based on how the score is effected by the knowledge of this specific variable value. For example - if a variable is a copy of another variable (or\u00a0deterministic\u00a0function of the other variable) - it will\u00a0receive\u00a0the same importance as the other variable\u00a0regardless\u00a0of the model (the variables should contribute the same). If variable doesn't\u00a0affect\u00a0the score - it will get 0 importance. The sum of all variable's importance is the score. The problem is that Shapley value is hard to calculate and can only be estimated. The minimal properties for unique solution:</p> <ul> <li>Null player - If variable does't\u00a0affect\u00a0the score in any scenario (knowledge of the variable) - it will get 0 importance</li> <li>Symetric/Consitent/Fairness - If i,j are unknown features and in all cases when 1 of them is known the score changes the same as the other one is known - they will get the same contribution score</li> <li>Linearity - If we have 2 functions v,w and define a new function v+w to explain which sums the 2 functions - so the contribution value for each varaible\u00a0 in each function v,w can also be summed.\u00a0 \u00a0 Framework for experiments: The git repository is\u00a0MR_Projects - mapped into\u00a0$MR_ROOT/Projects/Shared/Projects</li> </ul> <p>The experiment will be done on multiple model to test for\u00a0robustness. All the models are on THIN, except CRC which is on MHS. The models are:</p> <ul> <li>Diabetes model<ul> <li>Repository: THIN 2017</li> <li>Json to train model:\u00a0$MR_ROOT/Projects/Shared/Projects/configs/But_Why/test_models/pre2d_model.json</li> <li>Train\\Test samples:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers_samples/diabetes</li> <li>GAN model to generate synthetic samples for shapley calculation if needed:\u00a0/server/Work/Users/Alon/But_Why/outputs/GAN/pre2d_gan_model.txt</li> </ul> </li> <li>CRC model<ul> <li>Repository: MHS</li> <li>Json to train model:\u00a0$MR_ROOT/Projects/Shared/Projects/configs/But_Why/test_models/crc.json</li> <li>Train\\Test samples:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers_samples/crc</li> <li>GAN model to generate synthetic samples for shapley calculation if needed - Not exist, need to create one</li> </ul> </li> <li>Flu+Complications simpler model (All relevatn drugs, all relevant diagnosis, Age, Gender and 5 top lab tests features)<ul> <li>Repository: NWP</li> <li>Json to train model:\u00a0in flu project fo nwp. The final binary of trained model is in here:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/base_model.bin</li> <li>Train\\Test samples:\u00a0/server/Work/Users/Alon/But_Why/outputs/explainers_samples/flu_nwp</li> <li>GAN model to generate synthetic samples for shapley calculation if needed -\u00a0Not exist, need to create one</li> </ul> </li> </ul> <p>The Experiment: Experiment will be performed on\u00a0each model. We will try to explain the test samples (special samples were collected for each problem - for example low hemoglobin and high score in CRC and the opposite... several examples for each model) with different explainers. Explainers are\u00a0PostProcessor of\u00a0ModelExplainer. We will try to explain the prediction with different configurations\\method for each sample in the model. After collecting all the results we will go over the alternatives and Coby Metzger(someone else?) will score them by:</p> <ul> <li>Good</li> <li>Fair</li> <li> <p>Bad General comments:</p> </li> <li> <p>Options to test - in blined (method changes each row randomly)</p> </li> <li>Test 10 examples for each\u00a0Explainer\u00a0setting for sanity before the full\u00a0experiment</li> <li>Final step to validate with someone else besides Coby (Karni?)\u00a0</li> </ul> <ul> <li>Grouping</li> <li>Output - return top 10 of most contributing with abs value. - will return contribution and normalized contribution in the output (both magnitudes).\u00a0 will also print value of representative feature in the group (the representative will be chosen by the most important feature in the group) Selected problems:</li> </ul> <ol> <li>CRC - about 1400 features and 40 test samples</li> <li>Diabetes - about 100 features and 143 test samples -\u00a0need to reduce number of samples (not need that much)</li> <li>Flu simpler - Drug, Diagnosis, Age, Gender + 7 lab features and 20 test samples</li> </ol> <p>Explainer settings:</p> <ol> <li>KNN - with threshold on 5%\u00a0without covariance fix</li> <li>tree without covariance fix</li> <li>tree with covariance fix</li> <li>LIME with GAN - do some test to choose the required number of masks.\u00a0without covariance fix</li> <li>Shapley with GAN\u00a0\u00a0- do some test to choose the required number of masks.\u00a0without covariance fix</li> <li>missing_shapley - retrain model with random missing values samples and use the retrained model with masks (much faster shapley, less accurate).\u00a0\u00a0\u00a0without covariance fix total test: 6 * (40+20+143) = 1218\u00a0 testIf diabetes will have less samples (for example 30) - 540 tests \u00a0</li> </ol> <p>The Explainers: All the different explainers are configured unders: <pre><code>ll $MR_ROOT/Projects/Shared/Projects/configs/But_Why/explainers_cfgs\n</code></pre> Each post_processor is json for adjust model with ModelExplainer to test - it can be different method or different parameters. the name of the explainer is the name between explainer. to .json</p> <ul> <li>KNN\u00a0- coby method for KNN</li> <li>LIME_GAN - LIME Shapley with GAN</li> <li>LIME_gibbs_LightGBM - Lime Shapley with Gibb sampling of LightGBM model to estimate each variable value probability based on other variables in the Gibb sampling</li> <li>LIME_gibbs_QRF - Lime Shapley with Gibb sampling of QRF model to estimate each variable value probability based on other variables in the Gibb sampling. Faster than LightGBM but less accurate</li> <li>shapley_GAN - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) thatthe sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses GAN to generate the samples in each sampled mask experiment.</li> <li>shapley_GAN_sample_by_size - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) and uses sampling that is not random that the sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses GAN to generate the samples in each sampled mask experiment.</li> <li>shapley_gibbs_lightGBM - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) thatthe sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses Gibb sampling of LightGBM model to estimate each variable value probabilitybased on other variables in the Gibb sampling</li> <li>shapley_gibbs_QRF - estimating Shapley values by sub-sampling masks from all possible masks to estimate the sum of all the masks. Uses fixed weights (because of the sampling) thatthe sum of the experiments with weights will aggregate to the expected sum over all masks (with Shapley weights). Uses Gibb sampling of QRF model to estimate each variable value probabilitybased on other variables in the Gibb sampling. Faster than LightGBM but less accurate</li> <li>tree - Uses tree algorithm for trees</li> <li> <p>missing_shap - learn a respond model to mimic model result on missing values Can also add more configurations - Play with processings like grouping, covariance sum, to zero missing values and more... Need to define and close all options to test! \u00a0 How to generate explain results for model: Use the scripts in: \u00a0 <pre><code>ll $MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.*.sh\n#or just use to run all experiments for crc,diabetes and flu:\n$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/run_all.sh\n</code></pre> script args are: 1. the explainer cfg name: look for the available names above under\u00a0The Explainers section. For example \"tree\" or \"KNN\" or \"LIME_gibbs_QRF\" 2. Run_mode - a number with 2 bits:</p> <ol> <li>First bit if on - will run explainer to explain predictions on the test samples - will override existing results</li> <li>Second bit if on - Will run train\\adjust model - Will force readjust modelFor Examples: 0 - do nothing just convert outputs. 1 - only apply and run explainer to explain test samples. 2 - will only run adjust_model. 3- will do both</li> <li>Summary run mode flag - if zero (default argument value) will finish run after Train\\Apply explainer (depend on Run_Mode). if got 1 - will create a blinded random_test with all the explainers. If got 2 - will summarize all scores for all the explainers and will create a report* Full example for diabetes: <pre><code>#Force to run Train and Apply on KNN_TH. 3 - means in binary 11 - run train &amp; Apply. Default is 0 to run only whats needed. If model noe exists will run train, if no results exists will run apply.\n$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.diabetes.sh KNN_TH 3\n#Or from (has symbolic link to the git scripts)\n/server/Work/Users/Alon/But_Why/scripts/test_explainers.diabetes.sh tree 3\n</code></pre> \u00a0 \u00a0 I have script that creates a blinded report (the first argument\u00a0KNN_TH is not being used, but you must pass it. it can be either any of the possible explainers in the experiment) <pre><code>$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.diabetes.sh KNN_TH 0 1\n</code></pre> Now look at /server/Work/Users/Alon/But_Why/outputs/explainers/diabetes/compare_blinded.tsv Example from diabetes blinded report:  As you may see after each sample of patient+time there is a line with \u201cSCORES\u201d to score each explainer - wrote for example \"Good\" for the first explainer. Each explainer ranks the top 10 groups of features to explain the prediction score pred_0. Example for line 1 in explainer_1: Glucose:=-1.30226(36.29%) {Glucose.avg.win_0_3650:=100.31133} \u2013 which means the contribution of Glucose group of signals is -1.30 (pay attention to the minus sign) which means Glucose levels reduce the score. In the brackets you may see the normalized contribution of the signal in percentage \u2013 Glucose is responsible for 36.29% of the score. Lastly, you may see the value of the most important feature in the group \u201cGlucose.avg.win_0_3650\u201d and the value is 100.31.. \u00a0 *** You need to be consistent with the scores (case insensitive), either \u201cgood\u201d, \u201cfair\u201d \u201cbad\u201d or number, just keep the same scores (don\u2019t use free text, if need to comment use the line below)\u00a0 *** \u00a0 the explainer's names for each line (patient+ prediction time) is recorded in the file\u00a0map.ids.tsv\u00a0 Example of map.ids.tsv for each record:  In each line we can see the order of explainers *Macro Analysis of explainers: Analyze each explainer results: how many Good, Fair, Bad for each model and all together. To create summary resport run: <pre><code>#after collecting scores from the expirment form Coby or other physician, to summarize and map to explainer names report run (the diffrent form previous command is the last argument which is 2 instead of 1)\n$MR_ROOT/Projects/Shared/Projects/scripts/But_Why/test_explainers.diabetes.sh KNN_TH 0 2\n</code></pre> The summary can be found in files:</li> </ol> </li> <li> <p>summary.tsv - For each record (patient + prediction time) and explainer (now with the name, not blinded) will reveal Coby scores. A table for each sample records X explainer's names - the values in the cells are Coby's scores</p> </li> <li>summary.sum.tsv - A summary for each explainer name and each score how mnay times it happened. Conclude into conclusion. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html","title":"Experiments - Stage B","text":"<p>We will preform an additional experiment (not blinded) and more focused to achieve those goals:</p> <ul> <li>Understand/decide about rules to clean the output.<ul> <li>For example: weird results like giving high contribution to missing value(we already have a parameter to zero it)</li> <li>Check score for threshold and validate we have enough positive contributions or negative, if the patient wasn't flagged?</li> <li>Use threshold for absolute and relative contribution scores - for example: patient with low score maybe should not see normal lab results with small contribution</li> </ul> </li> <li>Understand/decide about the outputs - currently we have groups of features and we reveal top contributing feature with it. Also presenting the feature value and percentage from all contribution. Do we need to change it?<ul> <li>Presents top 10 groups by absolute values- maybe we should sort otherwise, depend on the score?currently we have decided to do it that way</li> <li>Groups: will split each lab signal into 2 groups of : values, trends. categorical values are aggregated by time windows</li> <li>Preventative feature in each group - present the most contributing feature in the group. what to do if it has opposite contribution sign?</li> </ul> </li> <li>Complete the documentation of how much time it takes for LIME, trees to run.<ul> <li>CRC - 23.5 seconds for 120 samples. means 3-4 s sample in second (using parallel)...\u00a0 \u00a0A\u00a0 little slow out of scope: - how to present it in gui, separate into reason we can affect and not... \u00a0 Will be done on CRC, Flu and with trees_with_covarinace_fix, LIME_GAN \u00a0 Update!!! Added new method: tree_iterative.</li> </ul> </li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html#pre2d-results","title":"Pre2D results:","text":"<p>The results on pre2d were already pretty good (with maybe some improvement over tree explainer without covariance fix). Just wanted to make sure the tree_iterative doesn't make things worse. tree_iterative is similar (with\u00a0maybe\u00a0slightly improvement comapre to with_cov)</p> Explainer_name 2 3 4 5 &lt;EMPTY&gt; Total exp Average score Average L^0.5 tree_iterative 21 46 46 25 2 138 3.543478261 1.864308126 tree_with_cov 16 54 51 17 2 138 3.5 1.856313886 <p>*in pre2d the covariance fix made the results of but why a little worse in the first stage of experiments.\u00a0</p>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html#crc-results","title":"CRC results:","text":"run Explainer_name 2 3 4 5 &lt;EMPTY&gt; Mean_Score Mean_Score_0.5 first run tree_iterative_cov 1 4 15 7 94 4.037037037 1.999810838 first run tree_iterative 1 6 16 5 93 3.892857143 1.963816368 first run tree_with_cov 5 10 10 2 94 3.333333333 1.809767105"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20B.html#comments","title":"Comments:","text":"<ul> <li>coby noticed that some missing value features are getting high contribution - we should use the flag to zero it?\u00a0Coby Metzger - are you sure? maybe because of the groups some features in the group are not missing? - Coby thinks it's OK, the imputed value for example when the glucose is high and no HbA1C, the imputation for HbA1C is also high. He likes it. He gave more reasons...</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20C%20%28Freeze%20Version%201%29.html","title":"Experiments - Stage C (Freeze Version 1)","text":"<p>After some discussions and ideas to improve the covariance fix in order to better handle with similar and dependent features. As you probably remember, Shapley Values should split the contribution equally to features that are the same, so if important features has many similar features, it may result in a wrong ButWhy report. This method is used togheter with the \"iterative\" method. \u00a0 The current method is to calculate the covariance matrix of the features and multiply it by the contributions. The problem arises when you have groups. When using groups those are the new options (the first option is what we had in the last experiment): In all options these are the definitions: Let's mark the features covariance matrix as F(i,j) is size NxN (N is the number of features). Build \"covariance matrix\" for groups (if we have G groups, the matrix size is GxG), lets mark it C matrix. C(i,i)=1. C(i,j)=C(j,i) the matrix is symmetric</p> <ol> <li>C(i,j) := max{ F(k,l) | k is feature in group i, l is feature in group j }</li> <li>Let's mark feature contribution for the prediction as vector T in size N as the number of features (taking the contribution of the features without iterations) .C(i,j) := Sigma( k is feature in group i, l is feature in group j) { F(k,l)T(k)T(l) } / Sigma( k is feature in group i, l is feature in group j) { 1T(k)T(l) } The advantage in equation #2 is that it should maybe better since it's not taking \"max\" and using specific feature contributions. \u00a0 Added another options to the calculation of the \"covariance\" matrix of features differently by using mutual information between features instead of correlation. The idea is to catch better non-linear behaviors like BMI and some other and more complicate feature dependencies that linear model can miss. Used normalization factor to control the values to be between 0-1. 0 - the features are independent, 1 - you can determine the second feature from the first feature. The equation for mutual information is KLD between the joint features distribution and calculation of the joint probability assuming they are independent (you measure the information gain between the assumption of in-dependency to what you observe in the data). Normalization is done by dividing with the entropy of the joint distribution. The normalization causes all number to be between 0-1 as we want and duplicate features will get 1. \u00a0 The following file has 4 methods to compare (covariance or mutual information and equation 1 or equation 2) W:\\Users\\Alon\\But_Why\\outputs\\Stage_B\\explainers\\crc\\reports\\compare_new_cov_fix\\compare_all.xlsx. I did it for CRC which is the most challenging problems since we have many features and similar once.</li> </ol>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20C%20%28Freeze%20Version%201%29.html#results","title":"Results","text":"<p>Alon Results\u00a0compare_all.Alon.xlsx</p> Method/score 1 2 3 4 5 Average Average_0.5 Tree_iterative_covariance(New equation) 0 0 0 2 20 4.909091 2.214607252 Tree_iterative_mutual_information(New equation) 0 0 0 3 19 4.863636 2.20387689 Tree_iterative_mutual_information(MAX) 0 0 0 10 12 4.545455 2.128764351 Tree_iterative_covariance(MAX) 0 0 7 9 6 3.954545 1.979125614 <p>Coby results compare_all - Coby.xlsx:</p> Method/score 1 2 3 4 5 Average Average_0.5 Tree_iterative_covariance(New equation) 0 0 3 11 7 4.19047619 2.04041087 Tree_iterative_mutual_information(New equation) 0 0 8 10 3 3.761904762 1.931648114 Tree_iterative_mutual_information(MAX) 0 1 5 5 3 3.714285714 1.913047967 Tree_iterative_covariance(MAX) 0 6 10 4 0 2.9 1.690289472"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Experiments%20-%20Stage%20C%20%28Freeze%20Version%201%29.html#conclusions","title":"Conclusions","text":"<ul> <li>The new equation seem to improve the results. Use it instead of max, that's the default in ExplainProcessings (use_max_cov=0)</li> <li>The mutual information might improve the results. I can't see it when using the new equations since the average score is in saturation (almost all recieved 5 out of 5). Coby and I noticed a huge improvement for the mutual information compared to the covariance when using MAX instead of new equation. Yet, we both got that the best results for CRC are new equations with covaraince.Waiting for\u00a0Avi Shoshan and\u00a0Yaron to review the file and grade the results themselves if they want (maybe they will see different things). Currently my recommandation is to use the new equation with covaraince (also faster learning).\u00a0 \u00a0 To create a nice ButWhy report, you might use:</li> </ul> <ol> <li>adjust model app to add post_processor with explainer to the model. Later you can change some parameters if needed using change_model (without relearn)</li> <li>CreateExplainnReport app to generate a nice report \u00a0</li> </ol>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/Sanity%20test%20experiment%20for%20debuging.html","title":"Sanity test experiment for debuging","text":"<p>Date:\u00a018.11.2019It looks like all our methods are tuned (without a major bug).\u00a0The results of all the methods look reasonable.Coby passed over 8 samples (out of 140 available) for diabetes predictor (grouped by signals into 11 groups from 69 features) and scored them from 1-4 (the higher the better).We can see the distribution of the scores for each method and the average score: \u00a0</p> Explainer_name 1 2 3 4 &lt;EMPTY&gt; Average Score Tree 0 1 4 3 132 3.25 missing_shap 0 2 2 4 132 3.25 LIME_GAN 0 2 3 3 132 3.125 SHAP_GAN 0 2 3 3 132 3.125 Tree_with_cov 0 3 1 4 132 3.125 knn_with_th 2 2 1 3 132 2.625 knn 0 5 2 1 132 2.5 <p>* Tree = regular tree shapley implementation * missing_shap - shapley algorithm with 500 random masks. but instead of using GAN or gibbs to generate samples, we use additional predictor (xgboost regression model) to predict the diabetes model scores when seeing random masks of missing values.\u00a0\u00a0the grouping is calculated internally. * LIME_GAN - LIME algorithm with GAN - sampling random masks and fitting a model -\u00a0the grouping is calculated internally. * SHAP_GAN - Shapley algorithm with GAN. sampling random masks and not an exact calculation - the grouping is calculated internally.* Tree_with_cov - the tree implementation with covariance fix * knn_with_th - the KNN algorithm with threshold of 5% to explainknn - the KNN algorithm without threshold</p>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html","title":"ButWhy experiments results","text":"<ul> <li>Expirement models<ul> <li>NWP_Flu\u00a0</li> <li>CRC</li> <li>Pre2D</li> </ul> </li> <li>Conclusions</li> <li>Appendix - Gibbs in Flu NWP\u00a0</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#expirement-models","title":"Expirement models","text":""},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#nwp_flu","title":"NWP_Flu","text":"<p>The model has 22 features, most of them are binary (Drugs, Diagnosis category_set). The non categorical are: Age, Smoking, SpO2, Resp_Rate, Flu.nsamples, Complications.nsamples, Memebership. In this run, Added Shapley gibbs explainer (22 features is OK for shapley gibbs -see apendix for more details).\u00a0 scores are the higher the better from 1-5. this is the histogram of 18 examples of flu. Also added average on sqrt of the 1-5 scores to increase the importance for improving on low scores compare to higher scores - no big change here.</p> Explainer_name 1 2 3 4 5 Mean_Score Mean_of_Sqrt_Score Tree_with_cov 0 2 3 8 5 3.888889 1.955828857 Tree 0 2 3 10 3 3.777778 1.929599082 SHAP_Gibbs_LightGBM 0 1 7 8 2 3.611111 1.889483621 missing_shap 0 1 9 4 4 3.611111 1.885941263 LIME_GAN 1 4 7 4 2 3.111111 1.736296992 SHAP_GAN 2 4 6 6 0 2.888889 1.669397727 knn 0 7 6 5 0 2.888889 1.682877766 knn_with_th 8 2 5 2 1 2.222222 1.42915273 <p>Summary - in the simple case of 22 features Tree_with_Covariance preforms the best and than the regular tree. Not far behind\u00a0SHAP_Gibbs_LightGBM and\u00a0missing_shap which preforms similarly. \u00a0 Reference to expirement results:</p> <ul> <li>compare_blinded.tsv - the blinded experiment - for each sample random shuffle of explainers outputs. and in xlsx format:\u00a0compare_blinded.xlsx</li> <li>map.ids.tsv - the order of each explainer</li> <li>summary.tsv - results for each sample - with explainers aligned (not blinded) - after joining map.ids.tsv with compare_blinded.tvs</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#crc","title":"CRC","text":"Explainer_name 1 2 3 4 5 &lt;EMPTY&gt; Score Score_Sqrt Tree_with_cov 0 1 13 21 3 1 3.684211 1.911555 Tree 0 3 16 17 2 1 3.473684 1.853358 LIME_GAN 0 15 11 12 0 1 2.921053 1.691204 SHAP_GAN 0 14 16 6 2 1 2.894737 1.683788 missing_shap 4 18 7 8 1 1 2.578947 1.574112 knn 6 17 9 6 0 1 2.394737 1.516581 knn_with_th 6 18 10 4 0 1 2.315789 1.494115 <p>Reference to expirement results:</p> <ul> <li>compare_blinded.tsv\u00a0-\u00a0the blinded experiment - for each sample random shuffle of explainers outputs. and in xlsx format: compare_blinded_CRC.xlsx</li> <li>map.ids.tsv -\u00a0\u00a0the order of each explainer</li> <li>summary.sum.tsv\u00a0- results for each sample - with explainers aligned (not blinded) - after joining map.ids.tsv with compare_blinded.tvs</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#pre2d","title":"Pre2D","text":"Explainer_name 1 2 3 4 5 &lt;EMPTY&gt; Score Score_of_Sqrt SHAP_GAN 0 4 43 86 5 2 3.666667 1.908082456 LIME_GAN 0 3 49 78 7 3 3.649635 1.903398585 Tree 0 5 48 82 3 2 3.601449 1.890708047 Tree_with_cov 0 10 63 63 2 2 3.413043 1.838648351 missing_shap 1 26 76 34 0 3 3.043796 1.732886234 knn 3 43 61 29 2 2 2.884058 1.680713177 knn_with_th 22 36 56 22 2 2 2.608696 1.582454126 <p>Reference to expirement results:</p> <ul> <li>compare_blinded.tsv\u00a0- the blinded experiment - for each sample random shuffle of explainers outputs. and in xlsx format:\u00a0</li> <li>map.ids.tsv\u00a0- the order of each explainer</li> <li>summary.tsv\u00a0- results for each sample - with explainers aligned (not blinded) - after joining map.ids.tsv with compare_blinded.tvs</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#conclusions","title":"Conclusions","text":"<p>Summary Table all expirements:</p> Method Flu 1 Flu 0.5 CRC 1 CRC 0.5 Diabetes 1 Diabetes 0.5 L1 L0.5 Tree_with_cov 3.888889 1.955828857 3.684211 1.912 3.413043 1.8386484 3.662048 1.902011 Tree 3.777778 1.929599082 3.473684 1.853 3.601449 1.890708 3.617637 1.891222 SHAP_Gibbs_LightGBM 3.611111 1.889483621 3.611111 1.889484 LIME_GAN 3.111111 1.736296992 2.921053 1.691 3.649635 1.9033986 3.227266 1.776967 SHAP_GAN 2.888889 1.669397727 2.894737 1.684 3.666667 1.9080825 3.150098 1.753756 missing_shap 3.611111 1.885941263 2.578947 1.574 3.043796 1.7328862 3.077951 1.73098 knn 2.888889 1.682877766 2.394737 1.517 2.884058 1.6807132 2.722561 1.626724 knn_with_th 2.222222 1.42915273 2.315789 1.494 2.608696 1.5824541 2.382236 1.501907 <ul> <li>The tree algorithm works the best in gerneal when the predictor is tree based. the covariance fix also improves it slightly.</li> <li>The LIME\\SHAP are pretty similar. The LIME is slightly better and faster so it's preferable over SHAP. They are also model agnostic, but hareder to train. Gibbs might imporve the results (but be much slower) and might be usefull if we use it on not too many features/groups of features.</li> <li>The missing_shap - very simple and fast model (also model agnostic). It preforms good in some problems, but has some train parameters the are important to tune.\u00a0In previous experiments in Pre2D it was much better (used different train parameters that made it worse comapre to the previous run). After runing with better params, I see it's even better than Shapley,LIME methods..BUG found in paramters in missing_shap that had disabled the grouping and cause problem - need to run again (Can't run with Grouping and \"group_by_sum=1\", should use the grouping mechanisim in missing_shap)...Bug found when training with wrong weights in missing_shap when using groups!</li> <li>KNN - should be used without threshold. For now, it haven't prove itself enougth to be used.</li> <li>If we use Trees predictors without groups - the shapley values should do the job without covariance fix. It's a unique solution that preserve fairness.\u00a0</li> </ul>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/index.html#appendix-gibbs-in-flu-nwp","title":"Appendix - Gibbs in Flu NWP","text":"<p>The Gibbs shows seperation of 0.6 between the generated samples and the real ones (when using random mask with probability 0.5 for each feature) and seperation of 0.719 when generating all features. This is high quality generation of matrix. For example GAN show seperation of 0.99 when generating all features and 0.74 when choosing random masks. Test gibbs script</p> <p><pre><code>$MR_ROOT/Projects/Shared/But_Why/Linux/Release/TestGibbs --rep /home/Repositories/KPNW/kpnw_jun19/kpnw.repository --train_samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/flu_nwp/train.samples --test_samples /server/Work/Users/Alon/But_Why/outputs/explainers_samples/flu_nwp/validation_full.samples --model_path /server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/base_model.bin --run_feat_processors 1 --save_gibbs /server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/gibbs_tests/test_gibbs.bin --save_graphs_dir /server/Work/Users/Alon/But_Why/outputs/explainers/flu_nwp/gibbs_tests/gibbs_graphs --gibbs_params \"kmeans=0;select_with_repeats=0;max_iters=0;predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=0;num_trees=80;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=30;feature_fraction=0.8;bagging_fraction=0.25;bagging_freq=4;is_unbalance=true;num_leaves=80};num_class_setup=num_class;calibration_string={calibration_type=isotonic_regression;verbose=0};calibration_save_ratio=0.2;bin_settings={split_method=iterative_merge;min_bin_count=200;binCnt=150};selection_ratio=1.0\" --predictor_type xgb --predictor_args \"tree_method=auto;booster=gbtree;objective=binary:logistic;eta=0.1;alpha=0;lambda=0.1;gamma=0.1;max_depth=4;colsample_bytree=1;colsample_bylevel=0.8;min_child_weight=10;num_round=100;subsample=0.7\" --gibbs_random_range 1 --gibbs_sampling_params \"burn_in_count=500;jump_between_samples=20;samples_count=50000;find_real_value_bin=1\"   --test_random_masks 0 \n</code></pre> \u00a0 Some Feature graphs examples - gibbs generated VS real: Binary features: Gender - Males rate - 47.88% in real VS 47.68% in generated. Diagnosis.Asthma rate in real 17.17% VS 17.08%. Admission hostpital_observation rate in real 2.98% VS 3.15% In Age, there seems like there is binning issue in gibbs for odd age values. In smoking, there are a lot of unique values and the gibbs only returns 1 out of 150 bins (should bin the feature before trying to seperate. not done yet) - might cause some of the power to seperate between real and the generated... All the others looks very good. Some graphs (real date is in blue, the generated gibbs is in orange): Click here to expand...  </p>"},{"location":"Infrastructure%20Library/Research/But%20Why%20-%20Explainers/ButWhy%20experiments%20results/TestGibbs.html","title":"TestGibbs","text":"<p>There is utility to test the gibbs imputations method - it has several options for controlling gibbs parameters for training, testing it. The default test is to impute all values (harder) - generate a full example and compare it to the original population. You can also contol to test only \"earsing\" randomly masks of values and imputing them (easier for gibbs). Please git pull\u00a0https://github.com/Medial-EarlySign/MR_Projects/tree/main/But_Why\u00a0and compile TestGibbs, then run with TestGibbs --base_config $CONFIG_FILE\u00a0 or just specify all parameters in the command. \u00a0 Gibbs config examples with all params: <pre><code>save_gibbs=/nas1/Temp/Test_gibbs/gibbs_model #Binary model of gibbs object. Will start from if exists\nsave_gibbs_cmp_log=/nas1/Temp/Test_gibbs/compare_gibbs_log\nsave_graphs_dir=/nas1/Temp/Test_gibbs/gibbs_graphs\nmissing_value=-65336\ntest_random_masks=0\ndown_sample=0\n#When learn_to_impute_missing_value is 1, the imputer will also take missing value imputation as legal value to impute. Useful for simple compare to test matrix that has missing values VS generated data using gibbs\nlearn_to_impute_missing_value=1\n#optional file path with name of features to filter from matrix and keep only those\n#Flow --print_model_info --f_model /nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl 2&gt;&amp;1 | grep FEAT | awk -F\" \" '{print $3}' | sort &gt; /nas1/Temp/Test_gibbs/feats_list\n#Removed Smoking_Status features - they are strongly connected and it causes problems in gibbs\nsel_features=/nas1/Temp/Test_gibbs/feats_list \n#Input option 1 - directly provide train/test matrices\n#load_matrix_path= #Provide specific matrix for train\n#test_gibbs_mat= #Provide specific matrix for test\n#Input option 2 - Provide rep and samples and model to generate matrix\nrun_feat_processors=0\nrep=/home/Repositories/KP/kp.repository\nmodel_path=/nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl\ntrain_samples=/nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl.train_samples\ntest_samples=/nas1/Work/Users/Eitan/Lung/outputs/models/model_10.base/results/CV_MODEL_0.medmdl.test_samples\nmax_loops=1 #Rerrun multiple times to take only \"good\" samples each time. If bigger than 1\ngibbs_random_range=1 #Only used if test_random_masks is on - erase mask and put those values in random range\nstop_at_sens=0.95 #Rerrun multiple times to take only \"good\" samples each time. threshold for good samples\ngibbs_params=predictor_type=lightgbm;predictor_args={objective=multiclass;metric=multi_logloss;verbose=0;num_threads=0;num_trees=100;learning_rate=0.05;lambda_l2=0;metric_freq=50;is_training_metric=false;max_bin=255;min_data_in_leaf=20;feature_fraction=0.8;bagging_fraction=1;bagging_freq=4;is_unbalance=true;num_leaves=80};calibration_save_ratio=0.2;calibration_string={calibration_type=isotonic_regression;verbose=0};num_class_setup=num_class;bin_settings={split_method=iterative_merge;min_bin_count=500;binCnt=100};selection_count=500000\ngibbs_sampling_params=burn_in_count=1000;jump_between_samples=50;samples_count=10000;find_real_value_bin=1\n#For testing the generated samples and compare to test samples\npredictor_type=xgb\npredictor_args=tree_method=auto;booster=gbtree;objective=binary:logistic;eta=0.1;alpha=0;lambda=0.1;gamma=0.1;max_depth=4;colsample_bytree=1;colsample_bylevel=0.8;min_child_weight=10;num_round=100;subsample=0.7\n</code></pre> <pre><code>Linux/Release/TestGibbs --base_conf /server/UsersData/alon/MR/Projects/Shared/Projects/configs/UnitTesting/examples/MultipleImputations/TestGibbs_cfg.cfg\n</code></pre> \u00a0 The result will appear in\u00a0 /nas1/Temp/Test_gibbs</p>"},{"location":"Infrastructure%20Library/Research/Conferences/index.html","title":"Conferences","text":"<p>This page will contain\u00a0Conferences details</p>"},{"location":"Infrastructure%20Library/Research/Conferences/AIME%202019.html","title":"AIME 2019","text":"<p>Papers Internal: W:\\Conferences\\AIME_2019\\pdfs Papers External: P:\\Conferences\\AIME_2019 \u00a0 PPT from Sharepoint :\u00a0AIME 2019.pptx </p>"},{"location":"Infrastructure%20Library/Research/Conferences/Boston%20MLHC%20%28ML%20in%20HealthCare%29%202017.html","title":"Boston MLHC (ML in HealthCare) 2017","text":"<p>Boston MLHC</p> <p>https://mucmd.org/</p> <p>Search for the articles in arxive:</p>"},{"location":"Infrastructure%20Library/Research/Conferences/Boston%20MLHC%20%28ML%20in%20HealthCare%29%202017.html#accepted-papers","title":"Accepted Papers","text":"<ul> <li>Piecewise-constant parametric approximations for survival learning Jeremy Weiss*, Carnegie Mellon University</li> <li>Spatially-Continuous Plantar Pressure Reconstruction Using Compressive SensingAmirreza Farnoosh, Northeastern University; Mehrdad Nourani, University of Texas at Dallas; Sarah Ostadabbas*, Northeastern University</li> <li>Classifying Lung Cancer Severity with Ensemble Machine Learning in Health Care Claims DataSavannah Bergquist*, Harvard University; Gabriel Brooks, Dartmouth-Hitchcock Medical Center; Nancy Keating, Harvard Medical School, Brigham and Women's Hospital; Mary Beth Landrum, Harvard Medical School; Sherri Rose, Harvard Medical School</li> <li>Predicting long-term mortality with first week post-operative data after Coronary Artery Bypass Grafting using Machine Learning modelsJos\u017d Forte*, University of Groningen; Marco Wiering, University of Groningen; Hjalmar Bouma, University Medical Center Groningen; Fred de Geus, University Medical Center Groningen; Anne Epema, University Medical Center Groningen</li> <li>ShortFuse: Biomedical Time Series Representations in the Presence of Structured InformationMadalina Fiterau*, Stanford University; Suvrat Bhooshan, Stanford University; Jason Fries, Stanford University; Charles Bournhonesque, Stanford University; Jennifer Hicks, Stanford University; Eni Halilaj, Stanford University; Christopher Re, Stanford University; Scott Delp, Stanford University</li> <li>Towards Vision-based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene ComplianceAlbert Haque*, Stanford University; Michelle Guo, Stanford University; Alexandre Alahi, Stanford University; Amit Singh, Lucile Packard Children's Hospital; Serena Yeung, Stanford University; N. Lance Downing, Stanford; Terry Platchek, Lucile Packard Children's Hospital; Li Fei-Fei, Stanford University</li> <li>Surgeon Technical Skill Assessment using Computer Vision based AnalysisHei Law*, University of Michigan; Jia Deng, University of Michigan, Ann Arbor; Khurshid Ghani, University of Michigan</li> <li>Predicting Surgery Duration with Neural Heteroscedastic RegressionZachary Lipton*, UCSD; Nathan Ng, UCSD; Rodney Gabriel , UCSD; Charles Elkan, UCSD; Julian McAuley, UC San Diego</li> <li>Temporal prediction of multiple sclerosis evolution from patient-centered outcomesESamuele Fiorini, University of Genoa; Andrea Tacchino, Italian Multiple Sclerosis Foundation - Scientific Research Area; Giampaolo Brichetto, Italian Multiple Sclerosis Foundation - Scientific Research Area; Alessandro Verri, University of Genova, Italy; Annalisa Barla*, Universit\u02c6 degli Studi di Genova</li> <li>Clustering Patients with Tensor DecompositionMatteo Ruffini*, UPC; Ricard Gavald\u02c6, UPC; Esther Lim-n, Institut Catal\u02c6 de la Salut</li> <li>Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning ApproachAniruddh Raghu*, MIT; Marzyeh Ghassemi, MIT; Matthieu Komorowski, Imperial College London; Leo Celi, MIT; Pete Szolovits, MIT</li> <li>Modeling Progression Free Survival in Breast Cancer with Tensorized Recurrent Neural Networks and Accelerated Failure Time ModelYinchong Yang*, Siemens AG, LMU M\u0178nchen; Volker Tresp, Siemens AG and Ludwig Maximilian University of Munich ; Peter Fasching, Department of Gynecology and Obstetrics, University Hospital Erlangen</li> <li>Hawkes Process Modeling of Adverse Drug Reactions with Longitudinal Observational DataYujia Bao*, University of Wisconsin-Madison; Zhaobin Kuang, University of Wisconsin, Madison; Peggy Peissig, Marshfield Clinic Research Foundation; David Page, University of Wisconsin, Madison; Rebecca Willett, University of Wisconsin, Madison</li> <li>Patient Similarity Using Population Statistics and Multiple Kernel LearningBryan Conroy*, Philips Research North America; Minnan Xu-Wilson, Philips Research North America; Asif Rahman, Philips Reserach</li> <li>A Video-Based Method for Automatically Rating AtaxiaRonnachai Jaroensri*, MIT CSAIL; Amy Zhao, MIT; Fredo Durand, MIT; John Guttag, MIT; Jeremy Schmahmann, Massachusetts General Hospital; Guha Balakrishnan, MIT; Derek Lo, Yale University</li> <li>Visualizing Clinical Significance with Prediction and Tolerance RegionsMaria Jahja*, North Carolina State University; Daniel Lizotte, UWO</li> <li>Predictive Hierarchical Clustering: Learning clusters of CPT codes for improving surgical outcomesElizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, and Katherine Heller</li> <li>An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis DetectionJoseph Futoma, Sanjay Hariharan, Katherine Heller, Mark Sendak, Nathan Brajer, Meredith Clement, Armando Bedoya, and Cara O'BrienMarked Point Process for Severity of Illness AssessmentKazi Islam*, UC Riverside; Christian Shelton, UC Riverside</li> <li>Diagnostic Inferencing via Improving Clinical Concept Extraction with Deep Reinforcement Learning: A Preliminary StudyYuan Ling, Philips Research North America; Sadid A. Hasan*, Philips Research North America; Vivek Datla, Philips Research North America; Ashequl Qadir, Philips Research North America; Kathy Lee, Philips Research North America; Joey Liu, Philips Research North America; Oladimeji Farri, Philips Research North America</li> <li>Generating Multi-label Discrete Patient Records using Generative Adversarial NetworksEdward Choi*, Georgia Institute of Technology; Siddharth Biswal, Georgia Institute of Technology; Bradley Malin, Vanderbilt University; Jon Duke, Georgia Institute of Technology; Walter Stewart, Sutter Health; Jimeng Sun, CS</li> <li>Quantifying Mental Health from Social Media using Learned User EmbeddingsSilvio Moreira*, INESC-ID; Glen Copperfield, qntfy.io; Paula Carvalho, INESC-ID; M\u2021rio Silva, INESC-ID; Byron Wallace, Northeastern</li> <li>Clinical Intervention Prediction and Understanding using Deep NetworksNathan Hunt*, MIT; Marzyeh Ghassemi, MIT; Harini Suresh, MIT; Pete Szolovits, MIT; Leo Celi, MIT; Alistair Johnson, MIT</li> <li>Understanding Coagulopathy using Multi-view Data in the Presence of Sub-Cohorts: A Hierarchical Subspace ApproachArya Pourzanjani*, UCSB; Tie Bo Wu, UCSB; Richard M. Jiang, UCSB; Mitchell J. Cohen, Denver Health Medical Center; Linda R. Petzold, UCSB</li> <li>Towards a directory of rare disease specialists: Identifying experts from publication historyZihan Wang*, University of Toronto; Michael Brudno, U Turonto; Orion Buske, Centre for Computational Medicine, SickKids Hospital</li> <li>Reproducibility in critical care: a mortality prediction case studyAlistair Johnson*, MIT; Tom Pollard, MIT; Roger Mark, MIT</li> </ul>"},{"location":"Infrastructure%20Library/Research/Conferences/Boston%20MLHC%20%28ML%20in%20HealthCare%29%202017.html#accepted-clinical-abstracts","title":"Accepted Clinical Abstracts","text":"<ul> <li>Extracting Information from Electronic Health Records Using Natural Language Processing \u2013 Knowledge Discovery from Unstructured InformationVasua Chandrasekaran, Jinghua He, Monica Reed Chase, Aman Bhandari, Christopher Frederick, and Paul Dexter</li> <li>Using Machine Learning to Recommend Oncology Clinical TrialsAnasuya Das, Leifur Thorbergsson, Aleksandr Grigorenko, David Sontag, Iker Huerga</li> <li>Accounting for diagnostic uncertainty when training a Machine Learning algorithm to detect patients with the Acute Respiratory Distress SyndromeNarathip Reamaroon, Michael W. Sjoding, Kayvan Najarian</li> <li>Visual Supervision of Unsupervised Clustering of Patients with ClustervisionAdam Perer*, IBM Research; Bum Chul Kwon, IBM Research; Janu Verma, IBM Research; Kenney Ng, IBM Research; Ben Eysenbach, MIT; Christopher deFilippi, INOVA; Walter Stewart, Sutter Health</li> <li>MS Mosaic: First Steps (and Stumbles) Toward a Patient-Centered Mobile Platform for Multiple Sclerosis Research and CareLee Hartsell</li> <li>Light Field Otoscope 3D Imaging of Diseased Ears in an Alaska Native PopulationManuel Martinello, Harshavardhan Binnamangalam, Philip Hofstetter, John Kokesh, Samantha Kleindienst, Tiffany Romain, Noah Bedard, and Ivana Tosic</li> </ul>"},{"location":"Infrastructure%20Library/Research/Conferences/KDD%202017.html","title":"KDD 2017","text":"<p>Review of relevant papers in kdd 2017: KDD%202017</p>"},{"location":"Infrastructure%20Library/Research/What-If/index.html","title":"What-If","text":"<ul> <li>Synthetic Data Generation</li> <li>Checking Causal Inference on Synthetic Data</li> <li>An envelope script for Causal Inference on Synthetic Data</li> </ul>"},{"location":"Infrastructure%20Library/Research/What-If/An%20envelope%20script%20for%20Causal%20Inference%20on%20Synthetic%20Data.html","title":"An envelope script for Causal Inference on Synthetic Data","text":"<p>The script H:/MR/Projects/Shared/CausalEffects/CausalEffectScripts/run_process.py is an envelope for generating synthetic data for causal-inference, applying several methods, and analyzing. At the moment, the script should be executed on node-05. The scripts parmeters are: <pre><code>run_process.py --help\nusage: run_process.py [-h] --config CONFIG [--start START] [--end END]\n                      [--show]\nCheck Causal Effects Approaches on Toy Model\noptional arguments:\n  -h, --help       show this help message and exit\n  --config CONFIG  configuration file\n  --start START    start stage\n  --end END        end stage\n  --show           show stages\n  --fast           skip slow stages\n</code></pre></p> <ul> <li> <p>config is the only required parameter. An example of a configuration file is -\u00a0 <pre><code>nSamples=15000\nnums=--numA 10 --numB 10 --numC 10\npTreatment=0.25\nseed1=123\nseed2=22\nseed3=11\nparams=--pOutcome 0.1 --outSigMin 0.08 --outSigMax 0.95 --treatSigWidth 5.0 --outSigWidth 5.0 --outcomePolyDegree 2 --treatmentPolyDegree 2 --treatmentFactor 0.4\ndir=/nas1/Work/Users/yaron/CausalEffect/ToyModel_ITE/Test1\n</code></pre> Note that all keys are required, and no sapces allowed around the assignment sign. All keys are used for generating the synthetic data, and ** is also used for all further steps</p> </li> <li> <p>** tells the script to list all stages and stop: <pre><code>run_process.py --config dummy --show\n['Generate', 'Stats', 'Naive.LGBM', 'IPW.LGBM', 'Naive.LGBM.RCT', 'Naive.NN', 'IPW.NN', 'Naive.NN.RCT', 'Quasi.NN', 'Quasi.Full_NN', 'Quasi.LGBM', 'CFR', 'SHAP', 'IPW.SHAP', 'Performance']\n</code></pre></p> </li> <li> <p>** and ** allow running only a subset of the script's stages</p> </li> <li>slow stages are - IPW.NN and Quasi.Full_NN The stages of the script perform the following tasks-</li> </ul> Generate Generate the synthetic data Stats Some analyses on synthetic data, including generation of true validation ITEs file Naive.LGBM \u2192 IPW.SHAP check_toy_model using various methods Performance Check performance of various methods using correlation to true ITE <p>A part of the Stats stage is running the script H:/MR/Projects/Shared/CausalEffects/CausalEffectScripts/analyze_risk_matrix.py *\u00a0 *that generates a PDF file () with various graphs. However, due to Python issues we currently do not run it within the envelope script and it should be executed seperately The output of the envelope script is a file **/Summary which include both config file information, as well as the performance evaluation information: <pre><code>nSamples=15000\nnums=--numA 10 --numB 10 --numC 10\npTreatment=0.25\nseed1=123\nseed2=22\nseed3=11\nparams=--pOutcome 0.1 --outSigMin 0.08 --outSigMax 0.95 --treatSigWidth 5.0 --outSigWidth 5.0 --outcomePolyDegree 2 --treatmentPolyDegree 2 --treatmentFactor 0.4\ndir=/nas1/Work/Users/yaron/CausalEffect/ToyModel_ITE/Test1\nITEs.true : 1.000000\nITEs.Model.NN.RCT : 0.536027\nITEs.Model.LGBM.RCT : 0.531990\nITEs.Quasi.Full_NN : 0.315594\nITEs.Quasi.NN : 0.313484\nITEs.Quasi.LGBM : 0.284961\nITEs.CFR : 0.249583\nITEs.Model.LGBM : 0.222051\nITEs.IPW.LGBM : 0.216663\nITEs.shap : 0.201008\nITEs.Model.NN : 0.175503\nITEs.IPW.NN : 0.119160\nITEs.ipw_shap : 0.110271\n</code></pre> </p>"},{"location":"Infrastructure%20Library/Research/What-If/Checking%20Causal%20Inference%20on%20Synthetic%20Data.html","title":"Checking Causal Inference on Synthetic Data","text":"<p>The program for testing causal-inference methods on synthetic data is located in -\u00a0H:\\MR\\Projects\\Shared\\CausalEffects\\CausalEffectsUtils\\check_toy_model The program parameters are :\u00a0 <pre><code>check_toy_model --help\nProgram options:\n  --help                                produce help message\n  --trainMatrix arg                     train data file (bin)\n  --testMatrix arg                      test data file (bin)\n  --validationMatrix arg                validation data file (bin)\n  --params arg                          serialized true model file\n  --validationITE arg                   File of validation ITE\n  --out arg                             output file for ITE graph\n  --read_models                         read required models from file\n  --write_models                        write generated models to file\n  --models_prefix arg                   prefix of models for read/write\n  --gen_model_params arg (=lightgbm;num_threads=15;num_trees=200;learning_rate=0.05;lambda_l2=0;metric_freq=250;bagging_fraction=0.5;bagging_freq=1;feature_fraction=0.8;max_bin=50;min_data_in_leaf=250;num_leaves=120)\n                                        parameters and definition of\n                                        classifiers which are not specifically\n                                        given\n  --gen_reg_params arg (=xgb;alpha=0.1;colsample_bytree=0.5;eta=0.01;gamma=0.5;booster=gbtree;objective=reg:linear;lambda=0.5;max_depth=3;min_child_weight=100;num_round=250;subsample=0.5)\n                                        parameters and definition of regressors\n                                        which are not specifically given\n  --nbootstrap arg (=100)               # of bootstrap rounds\n  --nfolds arg (=8)                     # of folds for cross validation\n  --gen_nn_params arg (=batch_size=1000;function=relu;max_num_batches=30000;checkpoint_num_batches=250;data=features;keep_prob=0.8;learning_rate=1e-3;nhidden=200;nlayers=2)\n                                        parameters for nn regressionscript\n  --do_true                             get ITE from true model\n  --do_direct                           directly model true ite\n  --do_model                            get ITE from single model\n  --model_params arg                    parameters and definition of outcome\n                                        predictor\n  --add_propensity arg (=0)             If True will add propensity score to\n                                        direct model for outcome\n  --bonus arg (=0)                      bonums for splitting by treatment in\n                                        outcome prediction using xgboost\n  --do_nn_model                         get ITE from single NN model\n  --do_two_models                       get ITE from single model\n  --model0_params arg                   parameters and definition of outcome\n                                        predictor for untreated\n  --mode1_params arg                    parameters and definition of outcome\n                                        predictor for treated\n  --do_weighted                         get ITE from propensity weighted model\n  --prop_params arg                     parameters and defition of propensity\n                                        score\n  --weighed_model_params arg            parameters and definition of outcome\n                                        predictor\n  --do_g_comp                           get ITE using g-computation\n  --g_comp_params arg                   parameters and definition of outcome\n                                        predictor\n  --g_comp_prop_params arg              parameters and definition of propensity\n                                        predictor\n  --g_comp_reg_params arg               parameters and definition of\n                                        counter-factuals regressor\n  --g_comp_reg_script arg (=/nas1/UsersData/yaron/MR/Tools/quasi_oracle/PythonScripts/ite_predictor.py)\n                                        Python script for t-prediction (ITE)\n  --g_comp_reg_script_output arg        output for Python script for\n                                        counter-factuals regressor\n  --g_comp_reg_script_input arg (=g_comp_matrix)\n                                        input for Python script for\n                                        counter-factuals regressor\n  --g_comp_reg_script_params arg        parameters for python script for\n                                        counter-factuals regressor\n  --g_comp_cf_params arg                parameters and definition of\n                                        counter-factuals classifier\n  --gNumCopy arg (=10)                  number of copies per sample in\n                                        counterfactual matrix\n  --gAddTestMatrix                      add test matrix to counter-factual\n                                        regression matrix\n  --do_two_models_g_comp                get ITE using g-computation\n  --g_comp_params0 arg                  parameters and definition of outcome\n                                        predictor for treatment=0\n  --g_comp_params1 arg                  parameters and definition of outcome\n                                        predictor for treatment=1\n  --do_nn_quasi_oracle                  get ITE using quasi-oracle\n  --do_quasi_oracle                     get ITE using quasi-oracle\n  --e_params arg                        Quasi-Oracle e-prediction params\n                                        (propensity)\n  --m_params arg                        Quasi-Oracle m-prediction params\n                                        (outcome without explicit treatment)\n  --t_params arg                        Quasi-Oracle t-prediction params (ITE)\n  --t_script arg (=/nas1/UsersData/yaron/MR/Tools/quasi_oracle/PythonScripts/ite_predictor.py)\n                                        Python script for t-prediction (ITE)\n  --t_script_output arg                 output for Python script for\n                                        t-predictions (ITE)\n  --t_script_input arg (=ite_matrix)    input for Python script for\n                                        t-predictions (ITE)\n  --t_script_params arg                 parameters for python script for\n                                        t-predictions (ITE)\n  --do_oracle                           get ITE using an oracle\n  --treatment_params arg                serialized treatment model file\n  --extend_matrix                       add quadratic features to t-modeling\n                                        matrix\n  --preds_files_suffix arg              Siffix for predictions files\n  --optimization_file arg               File for optimization of t-predictor\n                                        parameters\n  --summary_file arg                    File for summary of results\n  --do_external                         read predictions from csv file\n  --preds_file arg                      predictions file (untreated/treated\n                                        pairs)\n  --do_external_predictor               generate predictions from a\n                                        MedPredictor\n  --predictor_file arg                  predictor files (ITE from features\n                                        without Treatment)\n  --do_external_script                  generate predictions using a script\n  --script arg                          external script to run\n  --script_input arg                    script data input (--data ...)\n  --script_params arg                   script parameters\n  --script_output arg                   script prediction output (--preds ...)\n  --do_cfr                              run counterfactual regression\n  --script_dir arg (=.)                 script data directory\n  --cfr_train_script arg (=/nas1/UsersData/yaron/MR/Projects/Shared/CausalEffects/CausalEffectScripts/cfrnet-master/cfr_net_train.py)\n  --cfr_trans_script arg (=/nas1/UsersData/yaron/MR/Projects/Shared/CausalEffects/CausalEffectScripts/csv2cfr.py)\n  --do_shap                             use Shapley values for ITE\n  --do_ipw_shap                         use Shapley values on IPW-corrected\n                                        model for ITE\n  --do_weighted_nn_model                get ITE from propensity weighted NN\n                                        model\n</code></pre> The program uses ** and * to learn model(s) for evaluation individual treatment effecsts (ITE) and then applies the model(s) on *. The programs outputs include descrptive information in , as weill as tabular output in **. Each line in ** countains three tab-delimited columns -\u00a0\u00a0</p> Method-Name True-ITE Estimated-ITE <p>The true ITE is either generated from the generative model (if given, in .bin and .treatment.bin) or read from file (**) Methods currently implemented are:</p> Method Description Comment <p>do_true</p> true ITE from generative models do_direct Learn a regression model to directly evaluate ITE on trainMatrix and apply on validationMatrix This is a debugging method as it assumes true ITE is known for the trainMatrix do_model Learn a naive model \u0192(x,T)\u2192y , and evaluate ITE = \u0192(x,1) - \u0192(x,0) do_nn_model Same as do_model but using external learning/predictions scripts for model Allows interfacing with TensorFlow do_two_models Learn two models\u0192<sub>T=1</sub>(x)\u2192y and \u0192<sub>T=0</sub>(x)\u2192y and evaluate ITE = \u0192<sub>T=1</sub>(x) - \u0192<sub>T=0</sub>(x) do_weighted Use Inverse Propensity Weighting (IPW) to learn \u0192(x,T)\u2192y , and evaluate ITE = \u0192(x,1) - \u0192(x,0) do_weighted_nn_model Same as do_weighted but using external learning/predictions scripts for model Allows interfacing with TensorFlow do_g_comp <p>Use \"G-Computation\" - create counter-factuals using a model, and then use them for learning a</p><p>second model.</p> IPW optional for first model do_two_models_g_comp A combination of do_g_comp &amp; do_two_models <p>do_quasi_oracle</p> Evalute ITE using Quasi-Oracle - e* and m* evaluated internally and ITE using an external script ITE is evaluated using an external script to allow using TensorFlow NN <p>do_nn_quasi_oracle</p> Evalute ITE using Quasi-Oracle - e* and m* also evaluated using external scripts Allows interfacing with TensorFlow on all stages do_oracle Similar to Quasi-Oracle, only using true e and m instead of estimated e* and m* his is a debugging method as it assumes true e and m are known do_external Import ITE from file and generate out file do_external_predictor Read a MedPredictor object and apply on validationMatrix to generate ITE do_external_script Apply an external script to generate ITE on validationMatrix do_cfr Apply Uri Shalit's CounterFactual Regression Methods Use downloaded scripts do_shap Use Shapley of naive model \u0192(x,T)\u2192y values as estimators for ITE do_ipw_shap Use Shapley of IPW-learned model \u0192(x,T)\u2192y values as estimators for ITE"},{"location":"Infrastructure%20Library/Research/What-If/Generating%20Syntethic%20Data%20for%20Causal-Inference.html","title":"Generating Syntethic Data for Causal-Inference","text":"<p>The program for generating synthetic data is located in -\u00a0H:\\MR\\Projects\\Shared\\CausalEffects\\CausalEffectsUtils\\generate_realistic_data the program implements the following model -\u00a0 \u00a0 Where -\u00a0</p> <ul> <li>Poly-Tree = Tree with polynomials at the\u00a0 nodes</li> <li>Transformed-input = apply the following transformation before calculating the polynomial (currently n=3 is hard-coded):</li> <li>Noisy-Indexing = apply logistic function on value, with minimal/maximal values moved from\u00a0[0.0,1.0] to\u00a0[\u03b5,1-\u03b5'], and then use that as probability for deciding on the dichotomic Treatment/Outcome</li> <li>+/- = requiring that the first-order contributions of a parameter to the polynomial is set to be positive (negative)</li> <li> <p>Parameters for running the generation are: <pre><code>generate_realistic_data --help\nProgram options:\n  --help                               produce help message\n  --seed arg (=12345)                  randomization seed\n  --scale arg (=1)                     scale of random features\n  --nSamples arg                       number of samples\n  --numA arg                           # of variables affecting only treatment\n  --numB arg                           # of variables affecting treatment and\n                                       outcome (confounders)\n  --numC arg                           # of variables affecting only outcome\n  --polyDegree arg (=3)                polynom model degree\n  --treatmentPolyDegree arg (=1)       polynom model degree for treatment\n  --outcomePolyDegree arg (=1)         polynom model degree for outcome\n  --pTreatment arg                     probablity of applying treatment\n  --treatSigMax arg (=0.899999976)     maximal value of treatment probability\n  --treatSigMin arg (=0.0500000007)    minimal value of treatment probability\n  --treatSigWidth arg (=0.100000001)   width of treatment sigmoid function\n  --pOutcome arg                       probablity of positive outcome\n  --outSigMax arg (=0.975000024)       maximal value of output probability\n  --outSigMin arg (=0.0250000004)      minimal value of output probability\n  --outSigWidth arg (=0.5)             width of output sigmoid function\n  --treatmentFactor arg (=0.150000006) Scaling factor of Treatment before\n                                       outcome calcluation\n  --matrix arg                         output matrix file (bin)\n  --params arg                         output params file (bin)\n  --output arg                         output log file\n  --risk arg                           output risk scores file\n</code></pre></p> </li> <li> <p>Note that -\u00a0</p> <ul> <li>Currently, the depth of the trees is hard-coded as depth=2</li> <li>SigWidth determine the scaling of the logistic function, normalized by the distribution of the input variable. The lower it is, the lower the slope at the step is (i.e., very large widths correspond to step function)</li> <li>Features\u00a0 are generated as Uniform[0,**]</li> <li>Treatment is scaled by treatmentFactor before application of final polynomial</li> <li>risk is a debugging output matrix giving various intermediate values (e.g. Outcome/Treatment/Risk scores, various probabilities, etc.)</li> <li>The models are written into *.bin and *.treatment.bin</li> </ul> </li> <li>Additional projects in the same solution include -\u00a0<ul> <li>Generate matrices given the model: <pre><code>generate_realistic_data_from_model --help\nProgram options:\n  --help                 produce help message\n  --seed arg (=12345)    randomization seed\n  --scale arg (=1)       scale of random features\n  --nSamples arg         number of samples\n  --numA arg             # of variables affecting only treatment\n  --numB arg             # of variables affecting treatment and outcome\n                         (confounders)\n  --numC arg             # of variables affecting only outcome\n  --matrix arg           output matrix file (bin)\n  --params arg           input model params file (bin)\n  --treatment_params arg treatment model params file (bin)\n  --output arg           output log file\n  --rct_p arg            RCT data. Randomize Treatment with given probability\n</code></pre> The program generates a random feaures matrix (Uniform[0,]), either uses\u00a0.treatment.bin to set the treatment or randomly selects it (if ** is given) for a Randomized Controlled Trial scenario, and then uses\u00a0**.bin to set the output \u00a0</li> <li>Various utilities for handling the synthetic data/model - <pre><code>utils --help\nProgram options:\n  --help                produce help message\n  --mode arg            what should I do ?\n  --matrix arg          matrix to use for evaluation\n  --dir arg             directory for outcome and treatment models\n  --preds arg           predictions to evaluate\n  --epreds arg          e predictions for evaluation\n  --mpreds arg          m predictions for evaluation\n  --model arg           model to print\n  --out arg             output file\n  --csv arg             input csv file\n  --bin arg             output bin file\n</code></pre></li> </ul> </li> </ul> <p>Possible modes include:</p> <ol> <li>print - get a human-readable version of a generative model (from params.bin****</li> <li>getProbs - generate a vector of the output probabilities given a matrix and a model</li> <li>getAUC - get the maximal possible AUC for outcome prediction (known true probabilities)</li> <li>csv2bin - translate a csv matrix to binary format (serialized MedFeatures) \u00a0</li> </ol>"},{"location":"Installation/index.html","title":"Installation Guide","text":""},{"location":"Installation/index.html#introduction","title":"Introduction","text":"<p>This guide describes how to install and set up the MES Infrastructure and its components. You can choose to install all or only the components you need, depending on your use case. These tools allow you to use published models, train new models with MES Tools, or work with the Python API.</p>"},{"location":"Installation/index.html#prebuilt-releases","title":"Prebuilt Releases","text":"<p>A prebuilt package (excluding MES Tools) is available for direct download. This eliminates the need for manual compilation. The binaries are built on Ubuntu 24.04 and are compatible with any Linux distribution using glibc \u2265 2.39. You must also install OpenMP support. A prebuild for <code>MES Tools</code> can be found in here: MES Tools</p>"},{"location":"Installation/index.html#prerequisites","title":"Prerequisites","text":""},{"location":"Installation/index.html#install-openmp-support-ubuntu","title":"Install OpenMP Support (Ubuntu)","text":"<p>Install OpenMP for parallel processing:</p> <pre><code>sudo apt install libgomp1 -y\n</code></pre> <p>Note: This step is required even if you don't plan to compile the tools. It is required for runtime. Equivalent package exists in other linux distros, but here we will cover Ubuntu.</p>"},{"location":"Installation/index.html#building-the-tools-yourself","title":"Building the tools yourself","text":"<p>Before building the tools, complete the following steps:</p>"},{"location":"Installation/index.html#1-install-compiler-and-build-tools-ubuntu","title":"1. Install Compiler and Build Tools (Ubuntu)","text":"<p>Install the required compiler and build tools if you want to build the software yourself:</p> <pre><code>sudo apt install binutils gcc g++ cmake make -y\n</code></pre>"},{"location":"Installation/index.html#2-install-boost-libraries-ubuntu","title":"2. Install Boost Libraries (Ubuntu)","text":""},{"location":"Installation/index.html#compiling-boost-from-source","title":"Compiling Boost from Source","text":"<p>You can download Boost and compile it manually. Example steps for version 1.85.0:</p> Boost Compilation<pre><code># Install tools for download and extraction\nsudo apt install bzip2 wget -y\n\n# Download Boost\nVERSION=1.85.0\nVERSION_2=$(echo ${VERSION} | awk -F. '{print $1 \"_\" $2 \"_\" $3}')\nwget https://archives.boost.io/release/${VERSION}/source/boost_${VERSION_2}.tar.bz2\n\n# Extract files\ntar -xjf boost_${VERSION_2}.tar.bz2\nrm -f boost_${VERSION_2}.tar.bz2\n\n# Set up Boost install directory\nWORK_BUILD_FOLDER=$(realpath .)\ncd boost_${VERSION_2}\n\n# Configure and clean\n./bootstrap.sh\n./b2 --clean\n\n# Build static libraries\n./b2 cxxflags=\"-march=x86-64\" link=static variant=release linkflags=-static-libstdc++ -j8 cxxflags=\"-fPIC\" --stagedir=\"${WORK_BUILD_FOLDER}/Boost\" --with-program_options --with-system --with-regex --with-filesystem\n\nmkdir -p ${WORK_BUILD_FOLDER}/Boost/include\n\n# Link headers to Boost/include\nln -sf ${WORK_BUILD_FOLDER}/boost_${VERSION_2}/boost  ${WORK_BUILD_FOLDER}/Boost/include\n\n# Build shared libraries (not needed for AlgoMarker, but needed for MES tools if you choose to compile)\n./b2 cxxflags=\"-march=x86-64\" link=shared variant=release linkflags=-static-libstdc++ -j8 cxxflags=\"-fPIC\" --stagedir=\"${WORK_BUILD_FOLDER}/Boost\" --with-program_options --with-system --with-regex --with-filesystem\n</code></pre>"},{"location":"Installation/index.html#installing-boost-via-package-manager","title":"Installing Boost via Package Manager","text":"<pre><code>sudo apt install libboost-system1.83-dev libboost-filesystem1.83-dev libboost-regex1.83-dev libboost-program-options1.83-dev -y\n</code></pre> <p>Note: On Ubuntu 22.04, Boost version 1.74 is available and compatible. It was also tested with newest Boost version 1.89.0(2025-August-14) as today</p> <p>Important: This method does not work for the AlgoMarker library or the Python API. For those, you must compile Boost from source.</p>"},{"location":"Installation/index.html#available-components","title":"Available Components","text":"<p>You can install any of the following five components:</p> <ol> <li>AlgoMarker Shared Library: A shared Linux C library for accessing the AlgoMarker API and generating predictions/outputs from a model. Designed for production use, it supports only the essential \"predict\" and related APIs. Follow those steps only if you want to productize your model.</li> <li>AlgoMarker Wrapper: A REST API wrapper for the AlgoMarker Shared Library. Follow those steps only if you want to productize your model.</li> <li>MES Tools to Train and Test Models: Command-line executables for training, testing, and manipulating models using the MR_LIBS infrastructure. Required for training new models. Alternatively, you can use the Python API.</li> <li>Python API for MES Infrastructure: Python API, enabling model training, testing, and manipulation from Python. Some features may only be available via MES Tools or by extending the Python API.</li> <li>[MR_Scripts]: Useful Python and Bash scripts. Clone the repository with <code>git clone git@github.com:Medial-EarlySign/MR_Scripts.git</code>. No need to install it.</li> </ol>"},{"location":"Installation/index.html#environment-setup-script","title":"Environment Setup Script","text":"<p>After installing the required components, it is recommended to use the following script to configure your shell environment for all tools and scripts:</p> Start-Up Script<pre><code>#!/bin/bash\n\n# Path to Boost Library (If you compiled the boost library)\nBOOST_ROOT=${HOME}/Documents/MES/Boost\n# Path to Git repository clones - here in the example, we clones all repositories under ${HOME}/Documents/MES\nMR_LIBS=${HOME}/Documents/MES/MR_LIBS\nMR_TOOLS=${HOME}/Documents/MES/MR_Tools\nMR_SCRIPTS=${HOME}/Documents/MES/MR_Scripts\nPY_VERSION=$(python -c \"import platform; print(''.join(platform.python_version().split('.')[:2]))\")\n\nLD_PATH=${BOOST_ROOT}/lib\nexport PATH=$PATH:${MR_TOOLS}/AllTools/Linux/Release:${MR_SCRIPTS}/Python-scripts:${MR_SCRIPTS}/Bash-Scripts:${MR_SCRIPTS}/Perl-scripts\nif [ ! -z \"$LD_LIBRARY_PATH\" ]; then\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${LD_PATH}\nelse\n    export LD_LIBRARY_PATH=${LD_PATH}\nfi\nexport AUTOTEST_LIB=${MR_TOOLS}/AutoValidation/kits\nexport PYTHONPATH=${MR_LIBS}/Internal/MedPyExport/generate_binding/Release/medial-python${PY_VERSION}:${MR_TOOLS}/RepoLoadUtils/common\nexport BOOST_ROOT\n</code></pre> <p>Tip: Adjust the <code>LD_PATH</code>, <code>MR_LIBS</code>, <code>MR_TOOLS</code>, and <code>MR_SCRIPTS</code> variables as needed for your system.</p>"},{"location":"Installation/AlgoMarker_Library.html","title":"AlgoMarker Library","text":""},{"location":"Installation/AlgoMarker_Library.html#algomarker-library","title":"AlgoMarker Library","text":""},{"location":"Installation/AlgoMarker_Library.html#overview","title":"Overview","text":"<p>The AlgoMarker shared library enables deployment of the AlgoMarker model. It provides a C-level API to load, apply, and retrieve results from your binary model output.</p> <p>You can download a prebuilt release from the Release page. The release is built with glibc 2.39 and will work out of the box on systems with glibc \u2265 2.39 (e.g., Ubuntu 24.04). If you need to build from source, follow the instructions below.</p>"},{"location":"Installation/AlgoMarker_Library.html#installation-steps","title":"Installation Steps","text":"<ol> <li>Complete the Preliminary Steps to set up your build environment (install cmake, gcc, and libgomp1).</li> <li>Compile the Boost Libraries. You must build Boost from source.</li> <li>Clone the repository:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_LIBS.git\n</code></pre></li> <li>Edit <code>Internal/AlgoMarker/CMakeLists.txt</code> and add/edit line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    Set this path to your Boost build directory (<code>WORK_BUILD_FOLDER</code> from step 2). Ensure the compiled libraries are in <code>/libs</code> and headers in <code>/include</code>.    Alternatively you can just set your environment variable <code>BOOST_ROOT</code> to reference the Boost build directory. </li> <li>Build the library:    <pre><code>Internal/AlgoMarker/full_build.sh\n</code></pre></li> </ol> <p>A full docker image for compilation steps can be found under this link:</p> <ul> <li>01.basic_boost A base docker image with Boost</li> <li>03.algomarker A build with algomarker library</li> </ul>"},{"location":"Installation/MES%20Tools%20to%20Train%20and%20Test%20Models.html","title":"MES Tools to Train and Test Models","text":""},{"location":"Installation/MES%20Tools%20to%20Train%20and%20Test%20Models.html#mes-tools-to-train-and-test-models","title":"MES Tools to Train and Test Models","text":""},{"location":"Installation/MES%20Tools%20to%20Train%20and%20Test%20Models.html#overview","title":"Overview","text":"<p>These are command-line executables for training, testing, and manipulating models, as well as other utilities developed by MES.</p> <p>You can download a prebuilt release from the Release page. The release is built with glibc 2.35 and will work out of the box on systems with glibc \u2265 2.35 (e.g., Ubuntu 22.04). If you need to build from source, follow the instructions below.</p>"},{"location":"Installation/MES%20Tools%20to%20Train%20and%20Test%20Models.html#installation-steps","title":"Installation Steps","text":"<ol> <li>Install Boost Libraries. Compilation is not required, you might install from pacakge manager.</li> <li>Clone the required repositories:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_Tools.git\ngit clone git@github.com:Medial-EarlySign/MR_LIBS.git\n</code></pre></li> <li>Change to the <code>MR_Tools</code> directory:    <pre><code>cd MR_Tools\n</code></pre></li> <li>Edit <code>All_Tools/CMakeLists.txt</code> to set <code>LIBS_PATH</code> to the path of your MR_LIBS clone. If your directory structure is:    <pre><code>Root Directory\n\u251c\u2500\u2500 MR_LIBS\n\u2514\u2500\u2500 MR_Tools\n</code></pre>    then no changes are needed. If you compiled Boost, also set <code>BOOST_ROOT</code> in the CMakeLists.txt file.</li> <li>Edit <code>All_Tools/CMakeLists.txt</code> and add/edit line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    Set this path to your Boost build directory (<code>WORK_BUILD_FOLDER</code> from step 2). Make sure the compiled libraries are in <code>/libs</code> and headers in <code>/include</code>.    Alternatively you can just set your environment variable <code>BOOST_ROOT</code> to reference the Boost build directory. </li> <li>Build the tools:    <pre><code>AllTools/full_build.sh\n</code></pre></li> </ol> <p>A full docker image for compilation steps can be found under this link:</p> <ul> <li>01.basic_boost A base docker image with Boost</li> <li>02.build_tools A build with tools prepared for usage</li> </ul>"},{"location":"Installation/MES%20Tools%20to%20Train%20and%20Test%20Models.html#common-issues","title":"Common Issues","text":"<ol> <li> <p>Can't find Boost libray errors in compilation - Please delete the \"./build\" folder to recreate all Makefiles again. It holds some bad settings of Boost in cache.</p> </li> <li> <p>Can't find <code>boost_atomic.so</code> in runtime. Please add <code>atomic</code> in CMakeLists.txt under <code>BOOST_LIBS</code> and recompile</p> </li> <li> <p>Running an executable may fail with: <pre><code>$&gt; Flow ...\nFlow: error while loading shared libraries: libboost_regex.so.1.85.0: cannot open shared object file: No such file or directory\n</code></pre> This indicates the Boost shared libraries are not found at runtime. Set <code>LD_LIBRARY_PATH</code> to point to your Boost lib directory. Add to your ~/.bashrc or run once per session: <pre><code>export LD_LIBRARY_PATH=/path/to/BOOST_ROOT/lib\n</code></pre> If Boost was installed via the system packages (Ubuntu 22.04 uses 1.74, 24.04 uses 1.83), install the appropriate dev packages instead of adjusting <code>LD_LIBRARY_PATH</code>: <pre><code>sudo apt install libboost-system1.83-dev libboost-filesystem1.83-dev libboost-regex1.83-dev libboost-program-options1.83-dev -y\n</code></pre> Replace 1.83 with 1.74 on Ubuntu 22.04 when needed.</p> </li> </ol>"},{"location":"Installation/Python%20API%20for%20MES%20Infrastructure.html","title":"Python API for MES Infrastructure","text":""},{"location":"Installation/Python%20API%20for%20MES%20Infrastructure.html#python-api-for-mes-infrastructure","title":"Python API for MES Infrastructure","text":""},{"location":"Installation/Python%20API%20for%20MES%20Infrastructure.html#overview","title":"Overview","text":"<p>This is a Python API wrapper for the MES Infrastructure, allowing you to interact with MES models from Python code. Train and manipulate new models.</p> <p>A prebuilt release is available on the Release page. The release is built with glibc 2.39 and works out of the box on systems with glibc \u2265 2.39 (such as Ubuntu 24.04). If you need to build from source, follow the steps below.</p>"},{"location":"Installation/Python%20API%20for%20MES%20Infrastructure.html#installation-steps","title":"Installation Steps","text":"<ol> <li>Install required system libraries:    <pre><code>sudo apt install python3-dev swig -y\n</code></pre></li> <li>Compile the Boost library from source.</li> <li>Edit <code>Internal/MedPyExport/generate_binding/CMakeLists.txt</code> and add/edit line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    Set this path to your Boost build directory (<code>WORK_BUILD_FOLDER</code> from step 2). Make sure the compiled libraries are in <code>/libs</code> and headers in <code>/include</code>.    Alternatively you can just set your environment variable <code>BOOST_ROOT</code> to reference the Boost build directory. </li> <li>Ensure NumPy is installed:    <pre><code>python -m pip install numpy\n</code></pre> <p>This API supports both NumPy 1.x and 2.x. For maximum compatibility, compile with NumPy 2.x (works for clients with either version). Compiling with NumPy 1.x will not work for clients using 2.x.</p> </li> <li>Build the Python API:    <pre><code>Internal/MedPyExport/generate_binding/make-simple.sh\n</code></pre></li> <li>Make the library accessible to your python by adding it to <code>PYTHONPATH</code> variable:     <pre><code>export PYTHONPATH=${MR_LIBS}/Internal/MedPyExport/generate_binding/Release/medial-python${PY_VERSION}\n</code></pre>    Change MR_LIBS to your cloned path of MR_LIBS and ${PY_VERSION} to your python version, eg. 312 for python 3.12.</li> </ol> <p>A full docker image for compilation steps can be found under this link:</p> <ul> <li>01.basic_boost A base docker image with Boost</li> <li>04.medpython A build with the python. Please edit, install your python version in the build. This will use the python 3.10 that was shipped with ubuntu 22.04. If you need a different version, please install it inside the docker before executing the setup script</li> </ul>"},{"location":"Installation/AlgoMarker%20Wrapper/index.html","title":"AlgoMarker Wrapper","text":""},{"location":"Installation/AlgoMarker%20Wrapper/index.html#description","title":"Description","text":"<p>The AlgoMarker Wrapper provides a REST API for the AlgoMarker C++ Library. There are two wrappers available:</p> <ol> <li>C++ Native Wrapper: Minimal dependencies, very fast and efficient. Uses Boost Beast. Can be installed in a minimal Ubuntu Chiselled Docker image with just glibc.</li> <li>Python Server Wrapper: Built with FastAPI, more flexible for changes, and supports the old AlgoMarker API. It is slower and has more dependencies but is friendlier for testing. </li> </ol>"},{"location":"Installation/AlgoMarker%20Wrapper/C%2B%2B%20Native%20Wrapper.html","title":"C++ Native Wrapper","text":""},{"location":"Installation/AlgoMarker%20Wrapper/C%2B%2B%20Native%20Wrapper.html#c-native-wrapper","title":"C++ Native Wrapper","text":"<p>A prebuilt release is available on the Release page. The release is built with glibc 2.39 and works out of the box on systems with glibc \u2265 2.39 (such as Ubuntu 24.04). If you need to build from source, follow the steps below.</p>"},{"location":"Installation/AlgoMarker%20Wrapper/C%2B%2B%20Native%20Wrapper.html#compilation-steps","title":"Compilation Steps","text":"<ol> <li>Follow the Preliminary Steps to setup build enviroment with cmake, cmake, gcc and libgopmp1</li> <li>Set up Boost Libraries. You can also install it from package manager and you are not forced to compile the Boost package.</li> <li>Clone the repository:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_Tools.git\n</code></pre></li> <li>If you compiled the Boost library, edit <code>AlgoMarker_python_API/ServerHandler/CMakeLists.txt</code> to include the following line:    <pre><code>set(BOOST_ROOT \"$ENV{HOME}/boost-pic-install\")\n</code></pre>    This should point to your Boost compiled home directory (<code>WORK_BUILD_FOLDER</code> from step 2) from the compilation step. Ensure the compiled libraries are in <code>/libs</code> and the headers are in <code>/include</code>.    Alternatively you can just set your environment variable <code>BOOST_ROOT</code> to reference the Boost build directory. </li> <li>Compile the wrapper:    <pre><code>AlgoMarker_python_API/ServerHandler/compile.sh\n</code></pre></li> <li> <p>Execute the server:    <pre><code>AlgoMarker_python_API/ServerHandler/Linux/Release/AlgoMarker_Server --algomarker_path $AM_CONFIG --library_path $AM_LIB --port 1234\n</code></pre></p> <ul> <li><code>AM_CONFIG</code>: Path to the AlgoMarker configuration file.</li> <li><code>AM_LIB</code>: Path to the AlgoMarker shared library.    Refer to AlgoMarker Library) for compilation steps.</li> </ul> </li> </ol> <p>A full docker image for compilation steps can be found under this link:</p> <ul> <li>01.basic_boost A base docker image with Boost</li> <li>05.algomarker_wrapper A build with algomarker wrapper</li> </ul>"},{"location":"Installation/AlgoMarker%20Wrapper/Python%20Server%20Wrapper.html","title":"Python Server Wrapper","text":""},{"location":"Installation/AlgoMarker%20Wrapper/Python%20Server%20Wrapper.html#python-server-wrapper","title":"Python Server Wrapper","text":"<ol> <li>Clone the repository:    <pre><code>git clone git@github.com:Medial-EarlySign/MR_Tools.git\n</code></pre></li> <li>Edit <code>AlgoMarker_python_API/run_server.sh</code> and update the following:</li> </ol> <ul> <li><code>AM_CONFIG</code>: Path to the AlgoMarker configuration file.</li> <li><code>AM_LIB</code>: Path to the AlgoMarker shared library. Refer to AlgoMarker Library for compilation steps.</li> <li>If using the old ColonFlag, follow the steps in the ColonFlag setup page to compile the ICU library. Add the ICU library path to <code>LD_LIBRARY_PATH</code> in the script before calling <code>uvicorn</code>. 3. Make sure you have all python dependencies installed (The AlgoMarker.py itself has no dependencies if you want to use it directly without FastAPI):    <pre><code>python -m pip install fastapi\n</code></pre> 4. Run the Server <code>AlgoMarker_python_API/run_server.sh</code></li> </ul> <p>For more details follow: Python AlgoMarker API Server</p>"},{"location":"Models/index.html","title":"Available Models","text":"<p>The Models section describes predictive models developed and deployed using the Medial Research Framework. These models are designed to identify patients at elevated risk for a range of clinical outcomes, supporting earlier interventions and improved healthcare decision-making.</p> <p>Each model in this repository is built on standardized EMR data processed through the Medial Infrastructure - ensuring reproducibility, scalability, and compliance across diverse healthcare systems. The framework supports modular data pipelines, feature generation, and evaluation workflows, enabling seamless deployment of models across sites.</p> <p>Most models presented here have been validated across multiple real-world datasets and healthcare partners.</p>"},{"location":"Models/index.html#what-youll-find-here","title":"What You\u2019ll Find Here","text":"<ul> <li>Model Overviews - Brief summaries describing the clinical motivation, prediction targets, and input features.</li> <li>Evidence - List of publication, deployments</li> <li>Implementation Details - Intended usage and contact details</li> </ul> <p>Each model page provides both conceptual and technical details, helping users understand why it was developed and in which contexts it performs best.</p> Model Name Model description Contact Details for Usage LGI/Colon-Flag Detects colon cancer using age, sex, and CBCs Roche LungFlag Detects lung cancer using age, sex, smoking information, and common blood tests Roche GastroFlag Detects gastric cancer using age, sex, and common blood tests Roche AAA Predicts AAA events Geisinger/TBD FluComplications Predicts flu followed by complications such as pneumonia, hospitalization, or death TBD Pred2D Predicts progression from prediabetes to diabetes Planned to be open source FastProgressors Predicts rapid decline in eGFR Planned to be open source Mortality Predicts mortality using CMS claims data TBD Unplanned COPD Admission Prediction Model Predicts COPD hospitalization using CMS claims data TBD <p>Instructions for using an existing model can be found here</p>"},{"location":"Models/AAA.html","title":"AAA","text":""},{"location":"Models/AAA.html#overview","title":"Overview","text":"<p>Developed and implemented at Geisinger Health System (Pennsylvania), this model identifies cases of Abdominal Aortic Aneurysm (AAA). Beyond standard criteria, it has been specifically adapted to enhance fairness and to detect AAA in females, even though this is not part of the USPSTF guidelines.</p>"},{"location":"Models/AAA.html#deployments","title":"Deployments","text":"<ul> <li>Geisinger Health System (since 2022)</li> </ul>"},{"location":"Models/AAA.html#intended-usage","title":"Intended Usage","text":"<p>The model is designed to assist in referring patients for ultrasound (US) screening for AAA. While USPSTF guidelines recommend screening for males aged 65 and older who are current or former smokers, Geisinger has broadened the criteria to also include females aged 65 and older with a history of smoking.</p>"},{"location":"Models/AAA.html#publications","title":"Publications","text":"<p>Selected publication:</p> Manuscript Population Year Development and validation of a machine-learning prediction model to improve abdominal aortic aneurysm screening Geisinger - US, prospective usage 2024"},{"location":"Models/AAA.html#contact","title":"Contact","text":"<p>For further information, please contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/COPDCMS.html","title":"Unplanned COPD Admission Prediction Model","text":""},{"location":"Models/COPDCMS.html#overview","title":"Overview","text":"<p>This model predicts the risk of unplanned hospital admission due to COPD within 30 days, using claims data from the CMS Health AI Challenge. It is designed for Medicare and Medicaid populations and targets patients with a history of COPD.</p> <p>Eligibility Criteria:</p> <ul> <li>Patient must have a prior COPD diagnosis before the prediction date</li> <li>Prediction date must not fall within an inpatient admission</li> </ul> <p>The model provides explainability outputs and has undergone fairness testing.</p>"},{"location":"Models/COPDCMS.html#publications","title":"Publications","text":"<p>Presentation slides with detailed results are available upon request.</p>"},{"location":"Models/COPDCMS.html#contact","title":"Contact","text":"<p>For more information, contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/ColonFlag.html","title":"LGI/Colon-Flag","text":""},{"location":"Models/ColonFlag.html#overview","title":"Overview","text":"<p>The LGI/Colon-Flag model was developed to detect colon and rectal cancer using data from Maccabi Healthcare Services (MHS), the second largest HMO in Israel. First version published in 2015, with the final version completed in 2018.</p> <p>The model has undergone external validation at numerous sites across various regions. A partial list of related publications is provided below.</p> <p>This model is effective at detecting gastric cancer, although our specialized GastroFlag model offers superior performance in this area. It also performs well in identifying lower gastrointestinal (GI) disorders and pre-cancerous conditions.</p>"},{"location":"Models/ColonFlag.html#model-inputs","title":"Model Inputs","text":"<p>The model uses the following signals:</p> <ul> <li>Birth year (for age calculation)</li> <li>Sex</li> <li>CBC panel: Hemoglobin, Hematocrit, RBC, MCH, MCV, MCHC, Platelets, RDW, WBC, MPV, Lymphocytes (absolute and %), Monocytes (absolute and %), Eosinophils (absolute and %), Basophils (absolute and %), Neutrophils (absolute and %)</li> </ul>"},{"location":"Models/ColonFlag.html#deployments","title":"Deployments","text":"<ul> <li>Maccabi Healthcare Services - Israel (since 2015)</li> <li>Geisinger Health System - US (since 2018)</li> <li>List of more sites...</li> </ul> <p>Note: partial list - there are more deployments</p>"},{"location":"Models/ColonFlag.html#intended-usage","title":"Intended Usage","text":"<ul> <li>Not for screening exclusion: This tool is not intended to rule out patients from screening. Its sensitivity is insufficient for use as a primary screening tool.</li> <li>Purpose: The model is designed to help increase compliance and improve the yield of colonoscopies among patients who are non-adherent to screening recommendations.</li> <li>Professional use only: The tool is intended for interpretation and use by healthcare professionals, not patients.</li> </ul> <p>The model is designed to calculate a score when new CBC data is available. Requests to calculate a score without CBC data will be rejected. A score can still be generated even if some other input signals are missing.</p> <p>For further information, please see the \"Contact Details for Usage\" section and request the User Guide.</p>"},{"location":"Models/ColonFlag.html#list-of-publications","title":"List of Publications","text":"<ul> <li>Partial list of publications.</li> </ul> Manuscript Population Year Model Study Type Research Organization Development and validation of a predictive model for detection of colorectal cancer in primary care by analysis of complete blood counts: a binational retrospective study MHS Israel, THIN - UK 2016 Old ColonFlag Retrospective, Outcomes were retrieved after scoring + External validation MES Performance analysis of a machine learning flagging system used to identify a group of individuals at a high risk for colorectal cancer MHS Israel 2017 Old ColonFlag Prospective Observational MES Evaluation of a prediction model for colorectal cancer: retrospective analysis of 2.5 million patient records UK - CPRD 2017 Old ColonFlag Retrospective, External Validation Oxford Early Colorectal Cancer Detected by Machine Learning Model Using Gender, Age, and Complete Blood Count Data US - Kaiser Permanente North West 2017 ColonFlag ??? Retrospective, External Validation MES Computer-Assisted Flagging of Individuals at High Risk of Colorectal Cancer in a Large Health Maintenance Organization Using the ColonFlag Test MHS Israel 2018 Old ColonFlag Prospective Interventional MHS Prediction of findings at screening colonoscopy using a machine learning algorithm based on complete blood counts (ColonFlag) Canada 2018 ColonFlag Retrospective, External Validation MES Potential roles of artificial intelligence learning and faecal immunochemical testing for prioritisation of colonoscopy in anaemia UK - gastroenterology clinic in Plymouth, Royal London Hospital 2019 ColonFlag Prospective Observational Barts Validation of an Algorithm to Identify Patients at Risk for Colorectal Cancer Based on Laboratory Test and Demographic Data in Diverse, Community-Based Population US - Kaiser Permanente North West 2020 ColonFlag Retrospective, External Validation University of Washington + KP Collaboration to Improve Colorectal Cancer Screening Using Machine Learning US - Geisinger Health System 2022 ColonFlag Prospective Interventional Geisinger Diagnostic application of the ColonFlag AI tool in combination with faecal immunochemical test in patients on an urgent lower gastrointestinal cancer pathway UK - Barts Health - Urgency pathway 2024 ColonFlag Prospective Interventional Barts Machine Learning-Guided Cancer Screening: The Benefits of Proactive Care Geisinger 2024 ColonFlag Prospective Interventional Geisinger <p>Less important publication:</p> Manuscript Population Year Model Study Type Research Organization Comment Use of ColonFlag score for prioritisation of endoscopy in colorectal cancer UK - Barts Health - Urgency pathway 2021 ColonFlag Prospective Interventional Barts There is a new publication from 2024 with more data Predicting the presence of colon cancer in members of a health maintenance organisation by evaluating analytes from standard laboratory records MHS Israel 2017 ColonFlag variations with all labs Retrospective MES Different model Development and Validation of a Colorectal Cancer Prediction Model: A Nationwide Cohort-Based Study Clalit - Israel 2024 Clalit Model Retrospective, Developent and internal validation Clalit Not our model and no comparison to ColonFlag"},{"location":"Models/ColonFlag.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>Roche Navify Algosuit - Details to be announced.</p>"},{"location":"Models/FastProgressors.html","title":"FastProgressor","text":""},{"location":"Models/FastProgressors.html#overview","title":"Overview","text":"<p>This model is designed to screen patients after a creatinine test to identify those at risk of rapid eGFR decline. The definition of fast deterioration aligns with KDIGO guidelines:</p> <ul> <li>A yearly average drop of more than 5 points in eGFR</li> <li>The observation period is at least 18 months</li> <li>Excludes acute conditions (e.g., AKI)</li> <li>Excludes patients who begin SGLT2 drug treatment, as they are already being managed</li> </ul> <p>This model is not the KFRE, which predicts ESRD (end-stage renal disease). Instead, it targets otherwise healthy patients to help slow, stop, or prevent kidney function decline earlier in the disease process.</p> <p>The model was developed using the UK THIN (The Health Improvement Network) dataset and externally validated at a US site. However, there are no peer-reviewed publications due to competing interests.</p>"},{"location":"Models/FastProgressors.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the \"/discovery\" API, including signal names and required units.</p>"},{"location":"Models/FastProgressors.html#intended-usage","title":"Intended Usage","text":"<p>Refer patients identified as high risk for rapid deterioration to the endocrinology track.</p>"},{"location":"Models/FastProgressors.html#list-of-publications","title":"List of Publications","text":""},{"location":"Models/FastProgressors.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>The model may be released as open source upon request. please contact : alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/FluComplications.html","title":"FluComplications","text":""},{"location":"Models/FluComplications.html#overview","title":"Overview","text":"<p>This model was developed using data from Kaiser Permanente Northwest. Its primary goal is to identify unvaccinated patients who would most benefit from receiving a flu shot by predicting those at higher risk for influenza or influenza-like illness followed within three months by a complication. Complications may include pneumonia (the most common), hospitalization, or death.</p> <p>The model is especially sensitive to severe complications such as hospitalization and death, but also performs well in predicting all types of complications. In the related publication, we also analyzed the additional complications observed after flu compared to patients who did not contract the flu. Since the model's risk factors often identify frail patients who may experience complications regardless of flu, we specifically examined the \"additional\" complications following influenza.</p> <p>The model was compared to a linear model based on the World Health Organization (WHO) list of approximately 10 risk factors (e.g., asthma, immunosuppression, pregnancy, age, etc.). Our model outperforms the WHO-based model and also incorporates hospitalization/admission history, medications, and other diagnosis codes. Some variants included blood and spirometry tests, but these were ultimately excluded from the final public model for simplicity, as they are less common or less impactful.</p> <p>The model assumes good vaccine efficacy. While ideally the model would target \"who will benefit most from a flu shot,\" this is difficult to determine due to varying vaccine efficacy from year to year. Instead, the model focuses on predicting who is likely to get sick and experience complications, highlighting those for whom vaccination would be especially important.</p>"},{"location":"Models/FluComplications.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the \"/discovery\" API, including signal names and required units.</p>"},{"location":"Models/FluComplications.html#deployments","title":"Deployments","text":"<ul> <li>MHS (Maccabi Health Services), 2019</li> <li>Kaiser Permanente Northwest, 2019</li> <li>Geisinger, 2019 and 2021\u20132025</li> </ul>"},{"location":"Models/FluComplications.html#intended-usage","title":"Intended Usage","text":"<p>The model is intended to increase flu shot adherence among populations at higher risk for flu complications.</p>"},{"location":"Models/FluComplications.html#publications","title":"Publications","text":"<p>Partial list:</p> Manuscript Population Year Prediction of Influenza Complications: Development and Validation of a Machine Learning Prediction Model to Improve and Expand the Identification of Vaccine-Hesitant Patients at Risk of Severe Influenza Complications Geisinger - US 2022"},{"location":"Models/FluComplications.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>please contact : alon (dot) medial at gmail (dot) com for more details</p>"},{"location":"Models/GastroFlag.html","title":"GastroFlag","text":""},{"location":"Models/GastroFlag.html#overview","title":"Overview","text":"<p>The GastroFlag model was created to identify gastric cancer using data from Maccabi Healthcare Services (MHS), Israel\u2019s second largest HMO.</p> <p>This model is effective not only in detecting colon and rectal cancer but also performs well in recognizing lower gastrointestinal (GI) disorders and pre-cancerous conditions.</p> <p>The finalized model has been validated across eight different sites:</p> <ul> <li>THIN (UK dataset)</li> <li>Japan</li> <li>Taiwan</li> <li>United States</li> <li>Latvia</li> <li>Korea (2 sites)</li> <li>MHS</li> </ul> <p>A publication may be available in the future.</p>"},{"location":"Models/GastroFlag.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the \"/discovery\" API, including signal names and required units.</p> <ul> <li>Birth year (for age calculation)</li> <li>Sex</li> <li>CBC panel: Hemoglobin, Hematocrit, RBC, MCH, MCV, MCHC, Platelets, RDW, WBC, MPV, Lymphocytes (absolute and %), Monocytes (absolute and %), Eosinophils (absolute and %), Basophils (absolute and %), Neutrophils (absolute and %)</li> <li>Iron panel: Ferritin, Iron_Fe</li> <li>Basic Metabolic Panel (BMP): Glucose, Sodium, Creatinine, Urea, Potassium</li> <li>Comprehensive Metabolic Panel (CMP) additions: Albumin, Protein_Total, ALT, AST, ALKP, Bilirubin</li> </ul>"},{"location":"Models/GastroFlag.html#intended-usage","title":"Intended Usage","text":"<ul> <li>Not for screening exclusion: This tool is not intended to rule out patients from screening. Its sensitivity is insufficient for use as a primary screening tool.</li> <li>Purpose: The model is designed to help increase compliance and improve the yield of colonoscopies among patients who are non-adherent to screening recommendations.</li> <li>Professional use only: The tool is intended for interpretation and use by healthcare professionals, not patients.</li> </ul> <p>The model is designed to calculate a score when new CBC data is available. Requests to calculate a score without CBC data will be rejected. A score can still be generated even if some other input signals are missing.</p> <p>For further information, please see the \"Contact Details for Usage\" section and request the User Guide.</p>"},{"location":"Models/GastroFlag.html#list-of-publications","title":"List of Publications","text":""},{"location":"Models/GastroFlag.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>Roche Navify Algosuit - Details to be announced.</p>"},{"location":"Models/LungFlag.html","title":"LungFlag","text":""},{"location":"Models/LungFlag.html#overview","title":"Overview","text":"<p>The LungFlag model was developed at Kaiser Permanente Southern California (KPSC) to identify all types of lung cancer, including Squamous, Adenocarcinoma, Small Cell, and Non-Small Cell variants.</p> <p>It has been validated at approximately 10 different sites.</p>"},{"location":"Models/LungFlag.html#model-inputs","title":"Model Inputs","text":"<p>Inputs are listed in the <code>/discovery</code> API, including signal names and required units.</p> <ul> <li>Birth year (for age calculation)</li> <li>Sex</li> <li>Measurements: BMI, Weight, Height</li> <li>Smoking Information: Smoking Status (mandatory), Smoking duration (years), smoking intensity (ciggarets per day), Pack years, Smoking quit date (if applicable)</li> <li>Diagnosis signal: ICD10/ICD9</li> <li>Spirometry test: Fev1</li> <li>CBC panel: Hemoglobin, Hematocrit, RBC, MCH, MCV, MCHC, Platelets, RDW, WBC, Lymphocytes (absolute and %), Monocytes (absolute and %), Eosinophils (absolute and %), Basophils (absolute and %), Neutrophils (absolute and %)</li> <li>Lipid panel: Cholesterol, Triglycerides, LDL, HDL, NonHDLCholesterol</li> <li>Basic Metabolic Panel (BMP): Glucose, Creatinine, Urea</li> <li>Comprehensive Metabolic Panel (CMP) additions: Albumin, Protein_Total, ALT, ALKP</li> </ul>"},{"location":"Models/LungFlag.html#deployments","title":"Deployments","text":"<ul> <li>Geisinger Health System (since 2022)</li> </ul>"},{"location":"Models/LungFlag.html#intended-usage","title":"Intended Usage","text":"<p>For further information, please see the \"Contact Details for Usage\" section and request the User Guide.</p>"},{"location":"Models/LungFlag.html#list-of-publications","title":"List of Publications","text":"<ul> <li>Partial list of publications.</li> </ul> Manuscript Population Year Machine Learning for Early Lung Cancer Identification Using Routine Clinical and Laboratory Data KPSC - US 2021 Flagging High-Risk Individuals with a ML Model Improves NSCLC Early Detection in a USPSTF-Eligible Population KPSC - US 2023 Improved Efficiency with Lungflag vs Opportunistic Selection in a Theoretical East Asian Lung Cancer Screening Program Budget impact 2023 Use of the predictive risk model LungFlagTM for lung cancer screening in screening in a Spanish reference center: A cost-effectiveness analysis Spain - Budget Impact 2023 LungFlag, a machine-learning (ML) personalized tool for assessing lung cancer risk in a community setting, to evaluate performance in flagging non-small cell lung cancer (NSCLC) regardless of sex or race KPSC - US 2023 1561P Targeted screening methodologies to select high risk individuals: LungFlag performance in Estonia Lung Cancer Screening Pilot Estonia 2024 Validation of LungFlag\u2122 Prediction Model Using Electronic Medical Records (EMR) On Taiwan Data National Taiwan University Hospital - Taiwan 2024 Artificial intelligence\u2013aided lung cancer screening in routine clinical practice: A pilot of LungFlag at Geisinger Geisinger - US 2024 Budget impact model of LungFlag, a predictive risk model for lung cancer screening US - Budget Impact 2024 Validation of LungFlag Lean machine-learning model to identify individuals with lung cancer using multinational data KPSC - US, Geisinger - US, THIN - UK, additional site in US 2025 Cost-effectiveness of a machine learning risk prediction model (LungFlag) in the selection of high-risk individuals for non-small cell lung cancer screening in Spain Spain - Budget Impact 2025 Maximizing Lung Cancer Screening in High-Risk Population Leveraging ML-Developed Risk-Prediction Algorithms: Danish Retrospective Validation of LungFlag Southern Denmark 2025"},{"location":"Models/LungFlag.html#posters","title":"Posters","text":"Poster Population Year Flagging high-risk individuals with a ML model improves NSCLC early detection in a USPSTF-eligible population KPSC - US 2022 Computer-assisted Flagging of Never Smokers at High Risk of NSCLC in a Large US-based HMO using the LungFlag Model KPSC - US 2022 Cost-effectiveness of a machine learning risk prediction model (LungFlag ) in the selection of high-risk individuals for non-small cell lung cancer screening in Spain Budget impact 2023 Internal Poster - Real-World Evidence as the centerpiece for the evaluation of LungFlag pre-screening digital algorithm Budget Impact 2023 LungFlag, a Machine-Learning (ML) Personalized Tool for Assessing Pulmonary Complications a Community Setting, Demonstrates Comparable Performance in Flagging Non-Small Cell Lung Cancer (NSCLC) Regardless of Sex or Race KPSC - US 2023 Improved Efficiency with LungFlag vs. Opportunistic Selection in a Theoretical East Asian Lung Cancer Screening Program Budget Impact 2023 Use of the predictive risk model LungFlag for lung cancer screening in a Spanish reference center: A cost-effectiveness analysis Budget Impact 2023 Artificial intelligence\u2013aided lung cancer screening in routine clinical practice: A pilot of LungFlag at Geisinger Geisinger - US 2024 Budget impact model of LungFlag, a predictive risk model for lung cancer screening Budget Impact 2024 Validation of LungFlag\u2122 Prediction Model Using Electronic Medical Records (EMR) On Taiwan Data National Taiwan University Hospital - Taiwan 2024 TARGETED SCREENING METHODOLOGIES TO SELECT HIGH RISK INDIVIDUALS: LUNGFLAG\u2122 PERFORMANCE IN ESTONIAN LUNG CANCER SCREENING PILOT Estonia 2024 LungFlag Risk Prediction Validation on Canadian Ever Smokers Pre-Classified as High Risk for Lung Cancer Canada 2025"},{"location":"Models/LungFlag.html#on-going-studies","title":"On Going Studies","text":"<ul> <li>Validation on CPRD</li> <li>Validation on 4 datasets</li> </ul>"},{"location":"Models/LungFlag.html#contact-details-for-usage","title":"Contact Details for Usage","text":"<p>Roche Navify Algosuit - Details to be announced.</p>"},{"location":"Models/MortalityCMS.html","title":"Mortality Model","text":""},{"location":"Models/MortalityCMS.html#overview","title":"Overview","text":"<p>This model was created using claims data from the CMS Health AI Challenge. It focuses on Medicare and Medicaid populations, with the goal of predicting all-cause mortality within one year, as defined by CMS.</p> <p>The model provides explainability outputs and has undergone fairness testing.</p>"},{"location":"Models/MortalityCMS.html#publications","title":"Publications","text":"<p>Slides detailing the results are available upon request from the author.</p>"},{"location":"Models/MortalityCMS.html#contact-information","title":"Contact Information","text":"<p>For inquiries, please contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Models/Pred2D.html","title":"Pre2D","text":""},{"location":"Models/Pred2D.html#overview","title":"Overview","text":"<p>Pre2D is a predictive model developed using data from THIN (The Health Improvement Network) in the UK. Its purpose is to identify patients in the pre-diabetes stage who are likely to develop diabetes within the next two years.</p> <p>The criteria for defining diabetes are as follows:</p> <ul> <li>Two consecutive fasting glucose tests above 125 mg/dL</li> <li>HbA1C above 6.5</li> <li>A single glucose measurement above 200 mg/dL</li> <li>A diabetes diagnosis code with at least one abnormal test (glucose or HbA1C)</li> <li>Evidence of treatment with any glucose-lowering agent other than metformin</li> </ul>"},{"location":"Models/Pred2D.html#model-inputs","title":"Model Inputs","text":"<p>The model utilizes the following input signals:</p> <ul> <li>Birth year (used to calculate age)</li> <li>Sex</li> <li>Measurements: BMI</li> <li>Diabetes screening: Glucose, HbA1C</li> <li>Lipid panel: Triglycerides, HDL</li> <li>WBC</li> <li>ALT</li> <li>Prescriptions (optional): NDC, RX NORM, which may be converted to ATC codes in the future</li> </ul>"},{"location":"Models/Pred2D.html#intended-usage","title":"Intended Usage","text":"<p>For the user guide, please contact the author.</p>"},{"location":"Models/Pred2D.html#publications","title":"Publications","text":"<p>Partial list of publications:</p> Manuscript Population Year Prediction of progression from pre-diabetes to diabetes: Development and validation of a machine learning model THIN - UK, MHS - Israel, AppleTree - Canada 2020"},{"location":"Models/Pred2D.html#contact-information","title":"Contact Information","text":"<p>The model may be released as open source upon request. For inquiries, please contact: alon (dot) medial at gmail (dot) com</p>"},{"location":"Tutorials/index.html","title":"Medial EarlySign Tutorials","text":"<p>Welcome to the Medial EarlySign (MES) tutorials! This guide will walk you through our end-to-end machine learning platform for healthcare. From initial setup to model deployment, you'll learn how to leverage our powerful tools to build and deploy impactful models.</p> <p>All our software is open-source under the MIT license. While some models will be released publicly, others will be available exclusively through our partners.</p>"},{"location":"Tutorials/index.html#how-it-works","title":"How It Works","text":"<p>MES offers three primary ways to interact with the platform:</p> <ol> <li>Python API: A comprehensive library for accessing data, creating samples, training models, and running inference. This is the recommended starting point.</li> <li>Command-Line Tools: Standalone executables for specific workflows, especially for features not yet available in the Python API.</li> <li>AlgoMarker: A minimal, lightweight API designed for model deployment. It accepts JSON-formatted patient records and runs inference without requiring the full training infrastructure. It's so lightweight it can even run in distroless Linux containers.</li> </ol>"},{"location":"Tutorials/index.html#who-are-these-tutorials-for","title":"Who Are These Tutorials For?","text":"<p>These tutorials are designed for data scientists and developers who want to build and deploy machine learning models using the MES platform. Whether you're a new or experienced user, these guides will help you master our tools.</p>"},{"location":"Tutorials/index.html#getting-started-setup","title":"Getting Started: Setup","text":"<ol> <li> <p>Python API (Recommended): Start with our Python integration. It provides a convenient way to access most of the platform's features. Follow the Python setup instructions to get started. While the Python package is extensive, you can extend it to add custom functionality.</p> </li> <li> <p>Command-Line Tools: If you need features not yet available in the Python API, install our command-line tools. Follow the installation guide. Each tool includes a <code>--help</code> flag for usage instructions and a <code>--version</code> flag to check its version.</p> </li> <li> <p>Custom Applications: To build your own applications on top of our infrastructure, check out the examples of simple applications.</p> </li> </ol> <p>In most cases, the Python API combined with a few command-line tools will be all you need. Writing new C++ code is rarely necessary.</p>"},{"location":"Tutorials/index.html#the-tutorials","title":"The Tutorials","text":"<p>This series of tutorials will guide you through the entire machine learning workflow:</p> <ul> <li>01. Load the Data: Start here to learn how to load your data into the MES platform.</li> <li>02. Access Data: Learn how to access and manipulate your data.</li> <li>03. Create Samples: Prepare your data for model training.</li> <li>04. Train Model: Train your machine learning model.</li> <li>05. Apply Model: Use your trained model to make predictions.</li> <li>06. Model Evaluation: Evaluate the performance of your model.</li> <li>07. Deployment: Deploy your model for use in a production environment.</li> </ul>"},{"location":"Tutorials/index.html#lets-get-started","title":"Let's Get Started!","text":"<p>Ready to begin? Head over to the first tutorial: 01. Load the Data.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/index.html","title":"\ud83d\udcc4 ETL Workflow: The Recommended Tool","text":"<p>Using our ETL tool streamlines data loading by providing a standardized, robust, and reproducible process.  While manual loading file creation is possible, our tool is highly recommended for its numerous benefits:</p> <ul> <li> <p>Validation and Error Reporting: The tool performs extensive checks to ensure correct data format, proper sorting, and valid lab values. It provides clear, informative error messages that help you quickly fix issues, saving significant time during the loading process.</p> </li> <li> <p>Automated Configuration: It eliminates the need for manual configuration by automatically generating all necessary files for the final loading process. This includes:</p> <ul> <li>Signals Config Defines signal types and units, allowing you to easily add new signals or override existing ones.</li> <li>Dictionary Files Automatically creates conversion dictionaries for all categorical signals (e.g., converting strings to numerical values). It can even pull hierarchical data from known ontologies, such as ICD-10 codes, when a specific prefix is used.</li> <li>Conversion Config A central configuration file that references all necessary dictionaries, signals, and data files for the load. Will be used by <code>Flow</code> app to convert those raw data files into InfraMed Binary format.</li> <li>Loading Script Generates a final script with a <code>Flow</code> to run the entire loading process in one go.</li> </ul> </li> <li> <p>Efficient Batch Processing: The tool supports batch processing, which is crucial for handling large datasets efficiently and managing memory usage. If a loading process fails, it can resume from the last successful step, preventing the need to restart from the beginning.</p> </li> <li> <p>Comprehensive Logging: The tool logs all processing steps, test results, and creates distribution graphs for all signals, providing a clear overview of the data and the loading process.</p> </li> <li> <p>Code Reusability: It reuses common data elements and testing procedures, drastically reducing the amount of code you need to write. Our scripts are typically 3-4 times shorter than older, manual ETL scripts.</p> </li> </ul> <p>ETL_infra.pptx (slide deck) Sphinx API Link</p> <p>The code for this infrastructure was written with less strict standards as it is not part of our main production environment. While using the infrastructure is designed to be comfortable, modifying it may be challenging due to this less rigorous policy. Bugs may also be present.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/index.html#best-practices","title":"Best Practices","text":"<p>Keep it Simple: Load signals with minimal preprocessing. Handle outliers and clean data within the model itself, not during the ETL stage.  This approach simplifies implementation, reduces ETL errors, and results in a more robust model.  Future implementations will be easier with a straightforward ETL process. The only thing to take care of in the ETL is formating and right units for labs - that's it.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/index.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>The first step is to setup the ETL infrastructure. Follow the detailed instructions in the Setup documentation to begin.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html","title":"ETL Process \u2013 Dynamic Testing of Signals","text":"<p>You can define both global tests (applied across all ETL processes) and local tests (specific to a given ETL process). Local tests can override global tests if they share the same name in the local path.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#test-locations","title":"Test Locations","text":"<ul> <li>Global tests: <code>MR_Tools/RepoLoadUtils/common/ETL_Infra/tests</code></li> <li>Local tests: <code>$CODE_DIR/tests</code></li> </ul> <p>The code is executed from <code>MR_Tools/RepoLoadUtils/common/ETL_Infra</code>. This means you can use relative paths to access config files, dictionaries, etc.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#test-organization","title":"Test Organization","text":"<ul> <li>Each test directory (global or local) contains subdirectories for groups of tests.</li> <li>Subdirectory names correspond to either:<ul> <li>A signal name, or  </li> <li>A group of signals (e.g., <code>\"labs\"</code>, <code>\"cbc\"</code>).  </li> </ul> </li> <li>Only signals matching the directory name will be tested.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#test-function-format","title":"Test Function Format","text":"<p>Each test file must include a function called <code>Test</code> with the following signature:</p> <pre><code>def Test(df: pd.DataFrame, si, codedir: str, workdir: str) -&gt; bool:\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#arguments","title":"Arguments:","text":"<ul> <li>df: Input dataframe containing the signal to test</li> <li>si: Signal information object<ul> <li>si.t_ch: Array of time channel types (i = int, f = float, etc.)</li> <li>si.v_ch: Array of value channel types</li> </ul> </li> <li>codedir: Path to the ETL code (useful for accessing the config folder)</li> <li>workdir: Working directory for storing outputs</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#return-value","title":"Return value:","text":"<ul> <li>True if the test passes</li> <li>False if the test fails</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#example-test","title":"Example Test","text":"<p>Path: <code>MR_Tools/RepoLoadUtils/common/tests/labs/test_non_nulls.py</code></p> <p><pre><code>import pandas as pd\n\ndef Test(df: pd.DataFrame, si, codedir: str, workdir: str):\n    if len(df) == 0:\n        return True\n    cols = [x for x in df.columns if x == \"pid\" or \"value\" in x or \"time\" in x]\n    sig_name = df[\"signal\"].iloc[0]\n    # si.t_ch - contains array of each time channel type (for example \"i\" is integer, \"f\" float). v_ch is the same for value channels.\n    signal_columns = [\"time_%d\" % (i) for i in range(len(si.t_ch))] + [\n        \"value_%d\" % (i) for i in range(len(si.v_ch))\n    ]\n    signal_columns.append(\"pid\")\n    for col in cols:\n        if col not in signal_columns:\n            print(f\"Skip columns {col} which is not needed in signal {sig_name}\")\n            continue\n        null_date_cnt = len(df[df[col].isnull()])\n        if null_date_cnt / len(df) &gt; 0.001:\n            print(\n                \"Failed! There are %d(%2.3f%%) missing values for signal %s in col %s\"\n                % (null_date_cnt, 100 * null_date_cnt / len(df), sig_name, col)\n            )\n            return False\n        if null_date_cnt &gt; 0:\n            print(\n                \"There are %d(%2.3f%%) missing values for signal %s in col %s\"\n                % (null_date_cnt, 100 * null_date_cnt / len(df), sig_name, col)\n            )\n        df.drop(df.loc[df[col].isnull()].index, inplace=True)  # clean nulls\n        df.reset_index(drop=True, inplace=True)\n    print(\"Done testing nulls in signal %s\" % (sig_name))\n    return True\n</code></pre> This test verifies that no more than 1% null values exist in pid, time_0, value_0 for all labs signals. You can copy it into a local directory and adjust thresholds as needed.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#plotting-graphs","title":"Plotting Graphs","text":"<p>To generate HTML plots, use the plot_graph function:</p> <pre><code>import sys, os\nfrom ETL_Infra.plot_graph import plot_graph\n</code></pre> <ul> <li>Input:<ul> <li>A dataframe with two columns, or</li> <li>A dictionary {name: dataframe} (to plot multiple series)</li> </ul> </li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/ETL_process%20dynamic%20testing%20of%20signals.html#running-tests-on-signals","title":"Running Tests on Signals","text":"<p>You can run or rerun tests with:</p> <p><pre><code>python MR_Tools/RepoLoadUtils/common/ETL_Infra/run_test_on_sig.py \\\n  --workdir $WORKDIR \\\n  --codedir $CODEDIR \\\n  --signal $SIGNAL\n</code></pre> * --signal can accept multiple signals (comma-separated).</p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html","title":"Setup","text":"<p>To begin working with the ETL infrastructure, you need to clone the repository.  Note that this is not a PyPI package; it is designed to be used as part of the environment/codebase.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#clone-the-repository","title":"Clone the Repository","text":"<p>Start by cloning the MR_Tools repository from GitHub to a folder on your computer.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#configure-python-to-recognize-the-library","title":"Configure Python to Recognize the Library","text":"<p>Install required python packages: <pre><code>pip install numpy pandas plotly sqlalchemy ipython\n</code></pre></p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#option-1-update-your-environment-variable","title":"Option 1: Update Your Environment Variable","text":"<p>The easiest way to use the infrastructure is by adding the \"RepoLoadUtils/common\" directory to your <code>PYTHONPATH</code> environment variable. This allows you to directly import modules from the ETL infrastructure in any Python file.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#option-2-modify-individual-python-files","title":"Option 2: Modify Individual Python Files","text":"<p>If you prefer not to modify your environment settings, you can add the following code snippet at the beginning of each Python file that needs access to the ETL infrastructure. This temporarily adds the required directory to the system path, enabling module imports.</p> <pre><code>import sys\n# Replace \"ABSOLUTE PATH TO\" with the actual path on your computer\nsys.path.insert(0, \"ABSOLUTE PATH TO RepoLoadUtils/common\") \n# Now you can import the needed modules from ETL_Infra\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#verify-the-setup","title":"Verify the Setup","text":"<p>To confirm everything is set up correctly, run the following command:</p> <pre><code>python -c 'import ETL_Infra'\n</code></pre> <p>Alternatively, if using Option 2, wrap the snippet in a script and add <code>import ETL_Infra</code> at the end to verify it works.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#notes","title":"Notes:","text":"<p>The <code>ETL_Infra</code> directory that we imported, is refered as ETL_INFRA_DIR in this guide. You don't need to touch it, unless you want to extend the infrastrucutre, fix bugs for all ETLs, add tests for all ETLs. </p>"},{"location":"Tutorials/01.ETL%20Tutorial/00.Setup/index.html#next-steps","title":"Next Steps","text":"<p>Sphinx API Link</p> <p>The first step in the ETL process is to create a module or script that fetches data in batches. This method is highly efficient and preferred over returning a single <code>DataFrame</code> from a simple function.</p> <p>Follow the detailed instructions in the Data Fetcher documentation to begin.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html","title":"Data Fetching Step","text":"<p>The goal of this step is to create a function that returns a lazy iterator for data. This function, <code>fetch_function</code>, will fetch data in manageable batches, defined by a <code>batch_size</code> and a <code>start_batch</code> index. This approach is memory-efficient and ideal for large datasets.</p> <p><pre><code>def fetch_function(batch_size:int, start_batch:int) -&gt; Generator[pd.DataFrame, None, None]\n</code></pre> Sphinx API Link</p> <p>This step happens after Setup and after creating a directory for source code of this current ETL process. Note: The folder for our current ETL process is refered as <code>CODE_DIR</code> and list of files/strucutre can be seen in CODE_DIR. We will start with an empty direcotry and the ETL infrastracture will create some of the files for you.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#step-by-step-guide-file-based-example","title":"Step-by-Step Guide: File-Based Example","text":"<p>This guide uses a file-based example to demonstrate the process, but the same principles apply to other data sources like databases (We also have helper functions for DBs).</p>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#1-setup-and-imports","title":"1. Setup and Imports","text":"<p>First, import the necessary helper libraries. The example uses pre-built functions for file handling.</p> <pre><code>from ETL_Infra.data_fetcher.files_fetcher import files_fetcher, list_directory_files, big_files_fetcher\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#2-list-files-helper-function","title":"2. List Files Helper Function","text":"<p>Create a helper function to list all relevant files from your raw data directory. This function should return a list of full file paths. You can use a regular expression (<code>file_regex</code>) to filter the files. It's a good practice to handle one file type or format at a time.</p> <p>We will use the helper function <code>list_files</code> that does that using regex. If there are multiple folders, you can write your own code to list full file paths to read/process.</p> <p><pre><code>def list_files(file_regex: str)-&gt; list[str]:\n    base_path = '/nas1/Data/client_data'\n    return list_directory_files(base_path, file_regex)\n</code></pre> </p>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#3-file-path-parsing-function","title":"3. File Path Parsing Function","text":"<p>Next, write a function to read and parse a single file into a Pandas DataFrame. The only required constraint is that the resulting DataFrame must include a <code>pid</code> (patient identifier) column.</p> <pre><code>def read_single_file(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a single file into a DataFrame.\n\n    :param file_path: Path to the file to be read.\n    :return: DataFrame containing the data.\n    \"\"\"\n    df = pd.read_csv(file_path, sep=\"\\t\")\n    # Ensure the DataFrame contains a \"pid\" column\n    return df\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#4-the-data-fetcher-function","title":"4. The Data Fetcher Function","text":"<p>The final step is to create the main data fetcher, which acts as a \"lazy\" generator.  This function uses the <code>files_fetcher</code> helper to read files in batches, returning a concatenated DataFrame for each batch.  This lazy execution is a key feature; data is only loaded when you iterate over the result, which is useful for debugging and fast testing or reruning the full load script again and skiping actaul reading of the data if the processings was already completed.</p> <p>We will use <code>list_files</code> that we wrote before and <code>read_single_file</code> and will return a function with desired signature as described in <code>fetch_function</code>.</p> <p><pre><code>def generic_file_fetcher(\n    file_pattern: str,\n) -&gt; Callable[[int, int], Generator[pd.DataFrame, None, None]]:\n    \"\"\"\n    Creates a file fetcher function that reads files matching the given pattern.\n\n    :param file_pattern: Pattern to match files.\n    :return: A function that fetches files in batches based on requested batch size and starting index.\n    \"\"\"\n    file_fetcher_function = lambda batch_size, start_batch: files_fetcher(\n        list_files(file_pattern),\n        batch_size,\n        read_single_file,\n        start_batch,\n    )\n    return file_fetcher_function\n</code></pre> </p>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#example-usage","title":"Example Usage","text":"<p>Here\u2019s how you can use the generic_file_fetcher to test and debug your data pipeline.  This example creates a dummy file and then uses the fetcher to read it in batches.</p> <pre><code># Example Usage\nimport pandas as pd\nimport os\n\ndata = pd.DataFrame(\n    {\"pid\": [1, 2, 3, 4, 5], \"value\": [1988, 1999, 2000, 2001, 2002]}\n)\ndata[\"signal\"] = \"BYEAR\"\n# Let's store the file somewhere and update `list_files`  to use same path\nBASE_PATH = \"/tmp\"\ndata.to_csv(f\"{BASE_PATH}/res.tsv\", sep=\"\\t\", index=False)\n\n# Create a data fetcher for the dummy file\nfunc_fetcher = generic_file_fetcher(\"res.tsv\")\nfile_fetcher = func_fetcher(1, 0)  # Read one file at a time, starting from index 0\n\nprint(\"Iterating on batches:\")\nfor i, df in enumerate(file_fetcher):\n    print(f\"Batch {i}:\\n{df}\")\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#dummy-example","title":"Dummy example","text":"<p>Another dummy example of writing a data fetcher directly, but without helper functions in step 2+3. Just demonstarting the usage of <code>yield</code> in python. <pre><code>def dummy_data_fethcer_of_fake_data(batch_size, start_batch):\n    #In this simple funtion, we ignore the \"batch_size, start_batch\" and returns lazy iterator with 2 DataFrames\n    yield pd.DataFrame({'pid':[1,2,3], 'data':['A','B', 'C']})\n    yield pd.DataFrame({'pid':[4,5,6], 'data':['D','E', 'F']})\n</code></pre> This will result in lazy iterator with 2 batches of dataframe.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#handling-big-files","title":"Handling big files","text":"<p>For very large files, you might need to process them by rows rather than by entire files. The <code>big_files_fetcher</code> helper function is designed for this. It requires a different type of <code>read_single_file</code> function that can handle row-based chunks.</p> <pre><code>def read_single_file_by_rows(\n    filepath: str, batch_size: int, start_from_row: int\n) -&gt; Generator[pd.DataFrame, None, None]:\n    \"\"\"\n    Parses a file in chunks, yielding DataFrames of specified batch size as number of rows.\n\n    :param filepath: Path to the file to be parsed.\n    :param batch_size: Number of rows per batch.\n    :param start_from_row: Row index to start reading from.\n    :return: Generator yielding DataFrames.\n    \"\"\"\n    # manipulate the file reading parameters as needed (keep chunksize and skiprows):\n    has_header = True  # Set to False if the file has no header\n    header = None\n    if has_header and start_from_row &gt; 0: # read header if skiping first row\n        header = pd.read_csv(filepath, sep=\"\\t\", nrows=0).columns.tolist()\n        start_from_row += 1\n\n    df_iterator = pd.read_csv(\n        filepath,\n        names=header,\n        sep=\"\\t\",\n        skiprows=start_from_row,\n        chunksize=batch_size,\n    )\n    for df in df_iterator:\n        # Do manipulations on df if needed or just return `df_iterator`\n        yield df\n</code></pre> <p>This modified function allows the <code>big_files_fetcher</code> to read a specified number of rows per batch, moving to the next file only when the current one is exhausted.</p> <p>Full usage/replacement for <code>generic_file_fetcher</code> can be seen here:</p> <pre><code>def generic_big_files_fetcher(\n    file_pattern: str,\n) -&gt; Callable[[int, int], Generator[pd.DataFrame, None, None]]:\n    \"\"\"\n    Creates a data fetcher function for large files, reading them in batches.\n    Iterates over files matching the given pattern and processes them in chunks by reading rows.\n\n    :param file_pattern: Pattern to match files.\n    :return: A function that fetches files in batches based on requested batch size, starting index.\n    batch_size controls the number of rows read from each file. When file ends, it will read the next file.\n    \"\"\"\n    # Use the labs_file_parser_lazy to read files in chunks\n    files = list_files(file_pattern)\n    data_fetcher_function = lambda batch_size, start_batch: big_files_fetcher(\n        files,\n        batch_size,\n        read_single_file_by_rows,\n        has_header=True,\n        start_batch=start_batch,\n    )\n    return data_fetcher_function\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/01.Data%20Fetching%20step/index.html#next-step-process-pipeline","title":"Next Step: Process Pipeline","text":"<p>Once you have your data fetcher function, you can pass it as an argument to the next step: creating a process pipeline for each data type. Follow this guide to continue</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html","title":"Building the ETL Processing Pipeline","text":"<p>This section explains how to construct a processing pipeline in the ETL workflow using the <code>prepare_final_signals</code> function. This function applies your custom processing logic to data fetched in the previous step.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Import Necessary Functions:  Start by importing the <code>prepare_final_signals</code> function and your custom data_fetcher. Sphinx API Link</li> </ol> <p><pre><code>from ETL_Infra.etl_process import prepare_final_signals\nfrom parser import generic_file_fetcher\n</code></pre> 2. Define a Working Directory:  Specify the path where test results and output files will be stored.</p> <pre><code>WORK_DIR = \"...\"  # Specify your working directory path\n</code></pre> <ol> <li>Run the Pipeline:  Execute the pipeline with the following call:</li> </ol> <pre><code>prepare_final_signals(\n    generic_file_fetcher(\"^demo.*\"),  # Fetch files starting with \"demo\"\n    WORK_DIR,\n    \"demographic\",  # Name of this processing pipeline\n    batch_size=0,   # Process all files in a single batch\n    override=\"n\"    # Skip if already successfully completed\n)\n</code></pre> <p>\u27a1\ufe0f In this example:</p> <ul> <li>All files starting with <code>\"demo\"</code> are processed under the <code>\"demographic\"</code> pipeline (will be explained next).</li> <li>All files are read in a single batch (<code>batch_size=0</code>)`.</li> <li><code>override=\"n\"</code> prevents re-running an already completed process.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#important-notes","title":"Important Notes:","text":"<ul> <li>You should repeat this process for each data type.</li> <li>Best Practice for PID Mapping: Start by processing demographic signals like BDATE and GENDER. This is because if patient IDs (pid) are strings, the ETL will create a numeric-to-string mapping based on these demographic signals. This mapping is then used for all other signals, ensuring consistent pid values across the entire dataset. More details on the pipeline can be seen next Higher-Level Pipeline Flow</li> <li>Use the <code>start_write_batch</code> parameter when multiple pipelines write to the same output signals. This ensures unique batch indices and prevents overwrites</li> <li>A single fetcher can feed multiple pipelines by specifying comma-separated names. For example: <pre><code>\"gender,byear\"\n</code></pre> This sends demographic data to both gender.py and byear.py.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#defining-processing-logic","title":"Defining Processing Logic","text":"<p>Each pipeline automatically looks for a processing script named after the pipeline (e.g., <code>demographic.py</code>). This script must be located in the <code>signal_processings</code> folder alongside your main code.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#inside-the-script","title":"Inside the Script","text":"<ul> <li>Input: A <code>DataFrame df</code> from the fetcher.<ul> <li>You can also use <code>workdir</code> as WORKDIR folder path and <code>sig_types</code> and object with signal type definitions. A usefull function is to retrived other processed signal, in that example <code>BDATE</code> with <code>df_bdate = load_final_sig(workdir, sig_types, \"BDATE\")</code></li> </ul> </li> <li>Output: A transformed DataFrame in the required format, still called <code>df</code>.</li> <li>Any <code>print</code> statements will be captured in a log file.</li> </ul> <p>A Note on Signal Processing: </p> <ul> <li>If the DataFrame contains a <code>signal</code> column, the system generates a mapping file for easier renaming and mapping. Please refer to Signal Mapping.</li> <li>If <code>signal</code> is non-null, processing is grouped by signal names instead of pipeline names.</li> <li>Each signal belongs to hierarchical groups (tags and aliases). Example: <pre><code>SIGNAL  BDATE   102 16:my_SInt  demographic,singleton,date  0   date\nSIGNAL  Hemoglobin  1000    16:my_SDateVal  labs,cbc    0   g/dL\n</code></pre></li> <li>signal <code>\"BDATE\"</code> is linked to <code>demographic</code>, <code>singleton</code>, and <code>date</code>.</li> <li><code>\"Hemoglobin\"</code> belongs to <code>labs</code> and <code>cbc</code></li> </ul> <p>\u27a1\ufe0f Processing order: If a dedicated script exists (e.g., <code>Hemoglobin.py</code>), it will be used.  Otherwise, the system looks from the most specific to least specific tag (e.g., first <code>cbc.py</code>, then <code>labs.py</code>).</p> <p>Link to File Definition of All Signals</p> <p>Debugging To debug, add <code>breakpoint()</code> in your script. No changes to ETL infrastructure code are required.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#required-output-format","title":"Required Output Format","text":"<p>The final DataFrame must include:</p> <ul> <li><code>pid</code>: Patient identifier (already required in fetching step)</li> <li><code>signal</code>: The name of the signal (e.g., 'GENDER', 'Hemoglobin'). This must be a recognized signal. The system will provide an error with instructions if a signal is unknown (How to define a new signal if needed).</li> <li><code>time_X</code>: Time channels, where X is an index. These should be stored as integers in YYYYMMDD format.</li> <li><code>value_X</code>: Value channels, where X is an index. These can be float, integer, or string (for categorical data)</li> </ul> <p>\u274c Any other columns will be ignored during the final loading process.</p> <p>\u2705 Column order doesn\u2019t matter - the ETL system arranges them automatically.</p> <p>Special Cases</p> <ul> <li>For categorical signals, see Categorical Signals</li> <li>For signals that needs unit conversion, this guide can be helpful: Unit Conversion</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#example-processing-code","title":"Example Processing Code","text":"<p>Here's an example of the code you would write inside the <code>demographic.py</code> file to process a DataFrame and create a <code>GENDER</code> signal.</p> <pre><code># Input: DataFrame 'df' with patient data\n# Goal: Create 'GENDER' signal from 'Sex' column\n\n# The output DataFrame should have these columns:\n#   pid\n#   signal\n#   value_0 (string categorical, e.g., 'Male', 'Female')\n\n# 1. Select and rename columns\ndf = df[['pid', 'Sex']].rename(columns={'Sex': 'value_0'})\n\n# 2. Add the 'signal' column with the signal name\ndf['signal'] = 'GENDER'\n\n# 3. Standardize the values\ndf.loc[df['value_0'] == 'male', 'value_0'] = 'Male'\ndf.loc[df['value_0'] == 'female', 'value_0'] = 'Female'\n\n# 4. Finalize the DataFrame by keeping only required columns and removing duplicates\ndf = df[['pid', 'signal', 'value_0']].drop_duplicates().reset_index(drop=True)\n\n# The 'df' is now ready for the next stage of the pipeline\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#higher-level-pipeline-flow","title":"Higher-Level Pipeline Flow","text":"<p>Every pipeline includes the following stages:</p> <ol> <li> <p>PID Mapping: </p> <ul> <li>Convert non-numeric IDs to numeric.</li> <li>Mapping stored in <code>[WORK_DIR]/FinalSignals/ID2NR</code>.</li> <li>Demographic signals (signals with \"demographic\" tag) create the mapping; all others join on it.</li> </ul> </li> <li> <p>Signal Mapping: </p> <ul> <li>If <code>signal</code> column exists and not-null, optional name mappings can be done using <code>configs/map.tsv</code>. </li> <li>File lists source \u2192 destination mappings, ordered by frequency. Empty lines without destination value will be ignored in the mapping.</li> </ul> </li> <li> <p>Data Transformation: </p> <ul> <li>Transform the DataFrame to the final format. This is the process unit that we wrote here in the ETL.</li> </ul> </li> <li> <p>Testing: </p> <ul> <li>Run automatic format tests, followed by deeper tests (e.g., lab distributions).</li> <li>See Extending and adding tests</li> </ul> </li> <li> <p>Storing: </p> <ul> <li>Sort and save the processed data in <code>[WORK_DIR]/FinalSignals</code>.</li> </ul> </li> </ol> <p></p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/index.html#next-step-prepare-dicts-if-needed","title":"Next Step: Prepare Dicts if needed","text":"<p>If your project requires client-specific dictionaries, follow this guide: Prepare Special Dicitonaries</p> <p>Once you have you finished, follow this guide to finalize the loading</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html","title":"Categorical Signals &amp; Custom Dictionaries","text":"<p>This page explains how categorical signals are handled in the ETL process, with examples of when to use known ontologies, how to deal with client-provided values, and how to integrate custom mapping dictionaries.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#use-case-1-known-signals-with-standard-ontologies","title":"Use Case 1 \u2013 Known Signals with Standard Ontologies","text":"<p>When using a known categorical signal from ETL_INFRA_DIR/rep_signals/general.signals (e.g., DIAGNOSIS, Drug, PROCEDURE), the ETL automatically applies existing ontologies and mappings between codes.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#example","title":"Example:","text":"<p>For the Drug signal, if you create values with the <code>RX_CODE</code> prefix, the ETL will detect this and automatically pull:</p> <ul> <li>The <code>RX_CODE</code> dictionary,</li> <li>The <code>ATC</code> dictionary, and</li> <li>The mapping between <code>RX_CODE</code> and <code>ATC</code>.</li> </ul> <p>You only need to set the correct prefixes in <code>prepare_final_signals</code> processings. The call to <code>finish_prepare_load</code> takes care of the rest. No need to do anything special.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#known-ontologies-and-prefixes","title":"Known Ontologies and Prefixes","text":"Coding system prefix description ICD10_CODE: Diagnosis or procedure with ICD10 codes. For PROCEDURE signal, uses procedure ontology ICD9_CODE: Diagnosis or procedure with ICD9 codes. For PROCEDURE signal, uses procedure ontology ATC_CODE: Medication prescriptions in ATC codes RX_CODE: Medications prescriptions in RX norm NDC_CODE: Medications in NDC codes <p>Notes:</p> <ol> <li>Please strip \".\" from ICD10/ICD9 codes</li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#use-case-2-new-signals-from-client-list-of-values","title":"Use Case 2 \u2013 New Signals from Client (List of Values)","text":"<p>Sometimes we receive a signal that is not part of a known ontology and comes only as a list of values from the client.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#example_1","title":"Example:","text":"<p>A signal like Cancer_Type with values such as:</p> <ul> <li><code>Adenocarcinoma</code></li> <li><code>Small_Cells</code></li> <li>etc. (extracted from cancer patients)</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#what-to-do","title":"What to do:","text":"<ul> <li>Define the new categorical signal in CODE_DIR/configs/rep.signals \u27a1\ufe0f No manual mapping is needed. It will processed in the end as part of <code>finish_prepare_load</code> call later</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#use-case-3-new-or-known-signals-with-additional-client-dictionaries","title":"Use Case 3 \u2013 New or Known Signals with Additional Client Dictionaries","text":"<p>Sometimes the signal is known (e.g., DIAGNOSIS) or new, but the client provides extra mapping dictionaries.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#example_2","title":"Example:","text":"<p>The client uses an internal coding system (<code>EDG_CODE</code>) and provides:</p> <ol> <li>Translation dictionary - maps internal codes to descriptions.<ul> <li>Example: <code>`EDG_CODE:1234</code> \u2192 Diabetes type II</li> </ul> </li> <li>Mapping dictionary - maps internal codes to another known ontology.<ul> <li>Example: <code>EDG_CODE:1234</code> (Diabetes type II) \u2192 <code>ICD10_CODE:E11</code></li> </ul> </li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#notes","title":"Notes:","text":"<ul> <li>Sometimes only #1 (translation) is available \u2192 still valid.</li> <li>Sometimes only #2 (mapping) is available \u2192 also valid.</li> <li>If the ontology is common and reusable, we may store the mapping dictionary in ETL for future use. We will need to change the code in <code>create_dicts.py</code>, currently it is not very easily extended.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#how-to-use","title":"How to use","text":"<p>Use the function <code>prepare_dicts</code> with up to two optional dataframes:</p> <ul> <li>Translation dictionary:</li> </ul> Column Meaning <code>code</code> Internal code <code>description</code> Human-readable description <ul> <li>Mapping dictionary:</li> </ul> Column Meaning <code>client_value</code> Value from client <code>ontology_code</code> Code from our known ontology"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#how-to-reading-the-output-of-prepare_dicts-finish_prepare_load","title":"How-To: Reading the Output of prepare_dicts / finish_prepare_load","text":"<p>During processing, the ETL produces log messages with statistics about how the dictionaries were handled.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#what-to-expect","title":"What to Expect","text":"<ul> <li>Known codes detected - how many values already exist in our mappings.</li> <li>New codes detected - how many values were introduced for the first time (e.g., new ICD10 codes for new diseases).</li> <li>Automatic mapping attempts - in some cases, new codes are mapped by truncating strings to a higher-level category (e.g., grouping a specific disease into a broader disease family). \u00a0</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/Categorical%20signal_%20Custom%20dictionaries.html#why-it-matters","title":"Why It Matters","text":"<ul> <li>Helps identify if client data aligns well with existing ontologies.</li> <li>Flags new codes that may need review or long-term integration.</li> <li>Provides confidence that signal values were normalized as expected. \u00a0</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html","title":"unit_conversion","text":"<p>The <code>unit_conversion.py</code> module in ETL_Infra provides utilities for managing and applying unit conversions in lab data. It includes two primary functions:</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#functions","title":"Functions","text":"<p><code>process_unit_conversion</code></p> <p>Generates a statistics file with:</p> <ul> <li>Signals and units ranked by frequency</li> <li>A conversion table for review</li> </ul> <p>Parameters:  * <code>fullDF</code>: DataFrame with:     - <code>signal</code> column (signal name)     - <code>unit</code> column (unit name)     - <code>value_0</code> column (numeric value) * <code>outFile</code>: Path to save the conversion table and statistics * <code>samples_per_signal</code>: Number of sample rows per group</p> <p>Output: * None (writes results to <code>outFile</code>)</p> <p><code>fix_units</code></p> <p>Applies unit conversions using a prepared configuration file.</p> <p>Parameters:  * <code>fullDF</code>: DataFrame with <code>signal</code>, <code>unit</code>, and <code>value_0</code> columns * <code>inFile</code>: Path to the unit conversion configuration file</p> <p>Output: * Returns a DataFrame with units converted</p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#recommended-workflow-for-lab-data","title":"Recommended Workflow for Lab Data","text":"<ol> <li>Generate conversion config - Run generate_labs_mapping_and_units_config from etl_process with your DataFrame (requires signal and unit columns).</li> <li>Edit the conversion table - Review and adjust the generated table as needed.</li> <li>Apply conversions - Use <code>map_and_fix_units</code> from etl_process with the same inputs to apply conversions. This returns the updated DataFrame.</li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#example-usage-in-labspy","title":"Example usage in <code>labs.py</code>:","text":"<pre><code>generate_labs_mapping_and_units_config(df)\n# After editing the conversion file, you can comment out the line above to avoid regenerating it.\ndf=map_and_fix_units(df)\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/index.html#references","title":"References","text":"<ul> <li>Example configuration file</li> <li>Full code example</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/Full%20code%20example%20-%20LungFlag%20Taiwan%20labs.html","title":"Full code example","text":"<p>The scripts does:</p> <ol> <li>Renaming of columns</li> <li>date columns conversion</li> <li>mapping of LABLOINC codes to signals using $CODE_DIR/configs/map.tsv file</li> <li>Removing rows without \"numeric\" results </li> <li>Filtering null columns</li> <li>removing \"ignored\" signal list that was mapped, but we don't need/want to load </li> <li>Cleaning prefix for \"value\" columns suffix like \"(Manual checked)\"</li> <li>numeric conversion of values - removal of non numeric values</li> <li>Transforming \"unit\" column and taking lowercase for unit</li> <li>Calling unit conversion functions: <code>generate_labs_mapping_and_units_config</code>, map_and_fix_units</li> </ol> <p>$CODE_DIR/signal_processings/labs.py <pre><code>\u00a0#You have dataframe called \"df\". Please process it to generare signal/s of class \"labs\"\n#Example signal from this class might be BP\n#The target dataframe should have those columns:\n#    pid\n#    signal\n#    time_0 - type i\n#    value_0 - string categorical (rep channel type s)\n#    value_1 - string categorical (rep channel type s)\n#Source Dataframe - df.head() output:\n#     pid ACCOUNTIDSE2    ORDERTASKIDSE HOSPITALCODE EXEHOSPITALCODE DEPTCODE  ...    SPECIMENNO     SAMPLINGDATETIME SPECIMENTYPECODE RESULTUNIT                      REGULAREXPRESSIONDEFINEDRANGE  signal\n#0   4728    A00015780  201301020010156           T0              T0      NaN  ...  130102903195  2013-01-02 10:31:04              BLO       g/dL  [0-14d]12.0-20.0 [15-30d]10.0-15.3 [31d-0.5y]8...    None\n#1  39977    A00028394  201301310013839           T0              T0      NaN  ...  130131904446  2013-01-31 10:42:04              BLO         pg  [0-14d]31.1-35.9 [15-30d]29.9-35.3 [31d-0.5y]2...    None\n#2  67616    A00036755  201302110003945           T0              T0      NaN  ...  130211062196  2013-02-11 14:49:22              BLO       g/dL  [0-14d]12.0-20.0 [15-30d]10.0-15.3 [31d-0.5y]8...    None\n#3  23150    A00041791  201302220023413           T0              T0      NaN  ...  130222001672  2013-02-22 15:56:49              BLO          %  [0-14d]36.0-60.0 [15-30d]30.5-45.0 [31d-0.5y]2...    None\n#4  11117    A00045620  201303040026380           T0              T0      NaN  ...  130304908805  2013-03-04 15:35:12              BLO          %                                                NaN    None\n#\n#[5 rows x 22 columns]\ndef clean_suffix(df, suffix):\n    df['value_0']=df['value_0'].apply(lambda x: x.strip()[:-len(suffix)] if x.strip().endswith(suffix) else x)\n    return df\ndf=df.rename(columns={'SAMPLINGDATETIME': 'time_0', 'CONFIRMRESULT': 'value_0'})\ndf['time_0']=df['time_0'].map(lambda x: x.split()[0].replace('-','')).astype(int)\ndf=df[['pid', 'time_0', 'value_0', 'RESULTUNIT', 'LABLOINC', 'ORIGINALLABORDERFULLNAME']]\n#df[['LABLOINC', 'ORIGINALLABORDERFULLNAME']].groupby('LABLOINC').agg(['count','min']).reset_index().sort_values(('ORIGINALLABORDERFULLNAME', 'count'), ascending=False).to_csv('configs/maps.tsv', index=False, sep='\\t')\nmap_labs=pd.read_csv(os.path.join('configs', 'map.tsv'), sep='\\t', usecols=['LABLOINC', 'target']).set_index('LABLOINC')\ndf=df.set_index('LABLOINC').join(map_labs, how='left').reset_index().rename(columns={'target':'signal'}) #.drop(columns=['ORIGINALLABORDERFULLNAME'])\nmissing_map=len(df[df['signal'].isnull()])\nif missing_map &gt;0:\n    print(f'missing codes {missing_map}')\ndf=df[['pid', 'signal', 'time_0', 'value_0', 'RESULTUNIT', 'LABLOINC']]\nbefore_nana=len(df)\ndf=df[df['value_0'].notnull()].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed empty values. size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\nbefore_nana=len(df)\ndf=df[df.signal.notnull()].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed unmapped signals. size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\nbefore_nana=len(df)\nignore_labs=set(['CK-MB', 'Normobl', 'INR', 'IGNORE', 'PT', 'PTT', 'CK-isoenzyme'])\ndf=df[(~df.signal.isin(ignore_labs))].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed IGNORE signal. size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\n#Covert number\ndf=clean_suffix(df, '(Manual checked)')\ndf=clean_suffix(df, '(NRBC excluded)')\n#extract number:\ndf['value_00']=pd.to_numeric(df.value_0.apply(lambda x: re.compile('^([0-9]+(\\.[0-9]+)?)(\\s.*)').sub(r'\\1', x).strip()) ,errors='coerce')\n\ndf['has_num']=df['value_0'].apply(lambda x: len(re.compile('[0-9]').findall(x))&gt;0)\nprint('Excluded values:')\nprint(df[~df['has_num']].value_0.value_counts())\nbefore_nana=len(df)\ndf=df[df['has_num']].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed non numeric size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\ndf=df.drop(columns=['has_num'])\n\nprint('Excluded more non numeric values:')\nprint(df[df['value_00'].isnull()].value_0.value_counts())\nbefore_nana=len(df)\ndf=df[df['value_00'].notnull()].reset_index(drop=True)\nif len(df)!=before_nana:\n    print(f'Removed bad values size was {before_nana}, now {len(df)}. Excluded {before_nana-len(df)}')\ndf['value_0']=df['value_00']\ndf=df.drop(columns=['value_00'])\n#Now handle units = &gt; All looks good and in the same unit\ndf['RESULTUNIT']=df['RESULTUNIT'].astype(str).apply(lambda x: x.lower())\ndf=df.rename(columns={'RESULTUNIT': 'unit'})\n#generate_labs_mapping_and_units_config(df, 5)\n#Please edit the file \"/mnt/earlysign/workspace/LungFlag/ETL/configs/map_units_stats.cfg\" and then comment out previous line for speedup in next run\ndf=map_and_fix_units(df)\ndf=df.drop(columns=['signal.original', 'mapped'])\n</code></pre> </p>"},{"location":"Tutorials/01.ETL%20Tutorial/02.Process%20Pipeline/unit_conversion/config%20file%20of%20unit%20conversion.html","title":"Example config/output file of unit conversion","text":"<p>Based on Taiwan Conversions (almost all signals has single unit). transformed all units to lowercase to merge same units before calling this function. The file was created Automatically and only the row \"Ca\" was edited =&gt; mmol/l. Rows without edits will be kept \"as is\", no unit conversion and in most cases that's OK.</p> <ul> <li>signal is the \"source\" signal in Taiwan - already did the mapping before with a different file. when \"to_signal\" is empty will keep the same name as \"signal\"</li> <li>count - how common this signal</li> <li>unit - the unit with this signal</li> <li>unitcount - How common is the signal+unit combination. If single unit, it will be equal to count. Will sum up to count when taking all rows for the same signal</li> <li>to_signal - \u00a0when \"to_signal\" is empty will keep the same name as \"signal\"</li> <li>to_unit - was taken from global+local signals file definitions configuration (just for convenient, not in use). Each signal definition has units</li> <li>multiple_by, additive_b0 - the linear transformation. Default is 1 for multiple_by and 0 for additive_b0 if not given (dioing nothing be default to multiple by 1 and add 0)</li> <li>value_0 - example values</li> </ul> <p>Example of top 20 rows: <pre><code>signal  count   unit    unitCount   to_signal   to_unit multiple_by additive_b0 value_0\nGlucose 201128  mg/dl   201128      mg/dL           [107.0, 319.0, 127.0, 140.0, 91.0]\nHemoglobin  145250  g/dl    145216      g/dL            [10.2, 13.2, 11.5, 14.5, 13.2]\nHemoglobin  145250  %   34      g/dL            [13.8, 15.1, 19.9, 12.4, 17.8]\nHematocrit  144843  %   144840      %           [27.6, 30.0, 26.3, 48.2, 27.4]\nHematocrit  144843  %pcv    3       %           [27.0, 27.0, 26.0, 26.0, 21.0]\nCreatinine  139001  mg/dl   139001      mg/dL           [1.3, 1.0, 0.9, 0.7, 1.0]\nMCV 135235  fl  135235      fL          [88.6, 89.8, 92.6, 86.1, 99.2]\nPlatelets   135159  k/?gl   135091      10*9/L          [189.0, 330.0, 409.0, 39.0, 297.0]\nPlatelets   135159  10^3/ul 68      10*9/L          [100.0, 36.0, 134.0, 31.0, 25.0]\nWBC 134959  k/?gl   134893      10*9/L          [5.13, 12.84, 4.39, 4.8, 1.83]\nWBC 134959  10^3/ul 66      10*9/L          [49.22, 0.14, 1.3, 37.49, 0.24]\nRBC 134620  m/?gl   134558      10*12/L         [4.41, 4.2, 5.32, 4.32, 3.16]\nRBC 134620  10^6/ul 62      10*12/L         [4.03, 4.03, 3.04, 2.79, 2.46]\nMCHC-M  134404  g/dl    134404      g/dL            [32.5, 32.5, 32.6, 33.6, 34.7]\nMCH 134065  pg  134065      pg          [29.0, 29.2, 31.2, 28.4, 23.3]\nRDW 129021  %   129021      %           [15.9, 15.5, 12.2, 17.2, 15.0]\nALT 122626  u/l 122626      U/L         [92.0, 20.0, 58.0, 21.0, 21.0]\neGFR    90973   ml/min/1.73 m^2 90973                   [108.8, 57.5, 130.8, 91.5, 106.7]\n....\nCa  35375   mmol/l  35349       mg/dL   4.01        [2.2, 2.13, 2.3, 1.84, 2.25]\nCa  35375   mg/dl   26      mg/dL           [9.6, 10.9, 9.8, 10.3, 9.9]\n</code></pre></p> <p>In this example, the <code>Ca</code> signal with the <code>mmol/l</code> unit was given a conversion factor of <code>4.01</code>. This factor is used to transform the values from <code>mmol/l</code> to the target unit of <code>mg/dL</code>. You can see this in the example values: the original values are around 2, while data from <code>mg/dL</code> unit has values around 10. For all other signals in the example, the units were consistent and did not require any conversion, so the conversion fields were left blank. \u00a0 Full file: \u00a0 map_units_stats.cfg</p>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html","title":"Finalizing the Load Process","text":"<p>This step completes the ETL pipeline and prepares the repository for use.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#recap-of-earlier-steps","title":"Recap of earlier steps","text":"<ul> <li>Prepare signals: Run <code>prepare_final_signals</code> for each data type (see previous step)</li> <li>Handle client dictionaries (if needed): Use <code>prepare_dicts</code> for categorical signals</li> </ul> <p>Now we will finalize the preparation and generate all configuration files needed for loading by using a third function: <code>finish_prepare_load</code>.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#finish_prepare_load","title":"<code>finish_prepare_load</code>","text":"<p>Finalizes preparation and loads your data into the repository. Sphinx API Link</p> <pre><code>finish_prepare_load(WORK_DIR, '/nas1/Work/CancerData/THIN/thin_20XX', 'thin')\n</code></pre> <p>Parameters:</p> <ul> <li><code>WORK_DIR</code> - path to the working directory (string)</li> <li><code>REPOSITORY_OUTPUT_DIR</code> - destination folder for the repository (string)</li> <li><code>REPO_NAME</code> - name of the repository (string)</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#full-workflow-example","title":"Full Workflow Example","text":"<p>Here\u2019s a complete example combining all steps:</p> <pre><code>import pandas as pd\nfrom ETL_Infra.etl_process import *\nfrom parser import generic_file_fetcher, generic_big_files_fetcher\n\nWORK_DIR = '/nas1/Work/demo_ETL'\n\n# Step 1: Prepare signals\nprepare_final_signals(\n    generic_file_fetcher(\"^demo.*\"),  # Fetch files starting with \"demo\"\n    WORK_DIR,\n    \"demographic\",  # Name of this processing pipeline\n    batch_size=0,   # Process all files in a single batch\n    override=\"n\"    # Skip if already successfully completed\n)\nprepare_final_signals(\n    generic_big_files_fetcher(\"^labs.*\"),  # Fetch files starting with \"labs\"\n    WORK_DIR,\n    \"labs\",  \n    batch_size=1e6,   # Process each 1M lines in a single batch\n    override=\"n\"    \n)\n\n# Step 2 (optional): Handle custom dictionaries\n# Provide client dicts as DataFrames: def_dict, set_dict (or None)\nprepare_dicts(WORK_DIR, 'DIAGNOSIS', def_dict, set_dict)\n\n# Step 3: Finalize and load\nfinish_prepare_load(WORK_DIR, '/nas1/Work/CancerData/THIN/thin_20XX', 'thin')\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#final-script-to-create-the-data-repository","title":"Final Script to Create the Data Repository","text":"<p>Look for a message on your screen similar to the last two lines below, which provide the full path to the script that runs <code>Flow</code> and generates the repository:</p> <p><pre><code>Full script to execute :\n.../rep_configs/load_with_flow.sh\n</code></pre> This script is located at <code>WORK_DIR</code>/rep_configs/load_with_flow.sh. Run this script and confirm that it completes successfully with a success message at the end.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#function-reference","title":"Function Reference","text":"<p>1. <code>prepare_final_signals</code> Sphinx API Link Processes and tests each data type. Handles batching if needed.</p> <ul> <li>Arguments:</li> <li><code>data_fetcher</code> or <code>DataFrame</code>: Source of your data</li> <li><code>workdir</code>: Working directory for outputs</li> <li><code>signal_type</code>: Name/type of the signal (used for classification)</li> <li><code>batch_size</code>: Batch size (0 = no batching)</li> <li><code>override</code>: <code>'y'</code> to overwrite, <code>'n'</code> to skip completed signals</li> </ul> <p>2. <code>prepare_dicts</code> Sphinx API Link Creates mapping dictionaries for categorical signals.</p> <ul> <li>Arguments: </li> <li><code>workdir</code>: Working directory</li> <li><code>signal</code>: Signal name</li> <li><code>def_dict</code>: DataFrame with internal codes and descriptions (optional)</li> <li><code>set_dict</code>: DataFrame mapping client codes to known ontology</li> </ul> <p>3. <code>finish_prepare_load</code> Sphinx API Link Finalizes preparation, generates signals, and loads the repository.</p> <ul> <li>Arguments:</li> <li><code>workdir</code>: Working directory</li> <li><code>dest_folder</code>: Destination for the repository</li> <li><code>dest_rep</code>: Repository name (prefix)</li> <li><code>to_remove</code> (optional): List of signals to skip</li> <li><code>load_only</code> (optional): List of signals to load only</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#extending-and-testing","title":"Extending and Testing","text":"<p>For guidance on extending the process and adding automated tests, see Test Extention</p>"},{"location":"Tutorials/01.ETL%20Tutorial/03.Finalize%20Load/index.html#validating-the-etl-outputs-and-tests","title":"Validating The ETL Outputs and Tests","text":"<p>Follow this guide</p>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html","title":"Reviewing ETL loading results","text":""},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#purpose","title":"Purpose","text":"<p>This page explains how to inspect and interpret the ETL outputs after running the ETL process. It tells you where to find logs and visualizations, what the most important checks are, and how to spot common issues such as unit mismatches or unexpected value distributions.</p> <p>Where outputs are stored</p> <p>All ETL outputs are written under the repository <code>WORK_DIR</code>. See the high-level path: WORK_DIR.</p> <p>Key folders to review:</p> <ul> <li><code>signal_processings_log/</code> - per-signal processing logs and diagnostics.</li> <li><code>outputs/</code> - summary test logs (named <code>tests.*log</code>) produced by the validation checks. Directory for each signal and its distribution values.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#how-to-quick-checklist","title":"How-to: quick checklist","text":"<ol> <li>Open <code>outputs/tests.*log</code> (for example <code>outputs/tests.labs.log</code>) to read automated validation results.</li> <li>If the test log reports anomalies, inspect the per-signal directory under <code>outputs/$SIGNAL/</code> for plots/visualizations and batch-level reports to help confirm unit consistency and distribution shapes.</li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#example-reading-a-test-log","title":"Example - reading a test log","text":"<p>Here is a representative excerpt from <code>outputs/tests.labs.log</code> (Hemoglobin checks):</p> <pre><code>Done testing nulls in signal Hemoglobin\nunit:g/L || KLD (127)= 0.005790, KLD_to_Uniform=0.740022, entory_p=4.105875, grp_cnt=8444, group_counts=1/4\nunit:g/l || KLD (127)= 0.010441, KLD_to_Uniform=0.740022, entory_p=4.105875, grp_cnt=6995, group_counts=2/4\nThere are issues with low range, please have a look (more than factor 3)\n       q  value_0  reference     ratio1    ratio2      ratio\n0  0.001   80.908        7.2  11.237223  0.088990  11.237223\n1  0.010  102.000        9.0  11.333333  0.088235  11.333333\n2  0.100  126.000       11.2  11.250000  0.088889  11.250000\nThere are issues with the median, please have a look (more than factor 2)\n     q  value_0  reference     ratio1    ratio2      ratio\n3  0.5    145.0       13.3  10.902255  0.091724  10.902255\nThere are issues with high range, please have a look (more than factor 3)\n       q  value_0  reference     ratio1    ratio2      ratio\n4  0.900    163.0  15.300000  10.653595  0.093865  10.653595\n5  0.990    178.0  16.799999  10.595239  0.094382  10.595239\n6  0.999    189.0  17.900000  10.558659  0.094709  10.558659\nDone testing values of signal Hemoglobin\n</code></pre>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#mechanism-what-this-output-tells-you","title":"Mechanism - what this output tells you","text":"<ul> <li>KLD (Kullback\u2013Leibler divergence) per source indicates how similar that source's value distribution is to the overall signal distribution. Small KLD (&lt;&lt; 1) means similar.</li> <li>The sections labelled \"There are issues with ...\" flag quantile-level discrepancies between the current dataset and a reference distribution. These point to potential unit mismatches, data-entry issues or population differences.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#columns-in-the-discrepancy-tables","title":"Columns in the discrepancy tables","text":"<ul> <li><code>q</code>: quantile being compared (for example 0.001, 0.5, 0.999)</li> <li><code>value_0</code>: quantile value in the current dataset</li> <li><code>reference</code>: quantile value in the reference dataset</li> <li><code>ratio1</code>: value_0 / reference</li> <li><code>ratio2</code>: 1 / ratio1</li> <li><code>ratio</code>: max(ratio1, ratio2) - used to highlight large deviations</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#interpreting-large-ratios","title":"Interpreting large ratios","text":"<p>Large ratios (for example ~10) often indicate unit mismatches. A common case for Hemoglobin is <code>g/L</code> vs <code>g/dL</code> (multiply <code>g/dL</code> by 10 to get <code>g/L</code>). If you see a consistent factor across quantiles, consider converting units or normalizing the source before further processing.</p> <p>[!IMPORTANT] If there are mismatches in the input, loading does not fail by default. Warnings will appear in the logs. It is your responsibility to review and correct data issues where needed.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#deep-dive-log-locations-and-visual-aids","title":"Deep-dive: log locations and visual aids","text":"<ul> <li>Signal specific test log: <code>ETL/outputs/test.$SIGNAL.log</code></li> <li>Signal processing log (runtime messages, dropped lines): <code>ETL/signal_processings_log/$SIGNAL.log</code></li> <li>Batch-level charts and aggregated reports: <code>ETL/signal_processings_log/$SIGNAL/batches/</code> and <code>ETL/signal_processings_log/$SIGNAL</code></li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#practical-checks-and-recommended-workflow","title":"Practical checks and recommended workflow","text":"<ol> <li>Scan <code>outputs/tests.*log</code> for flagged issues.</li> <li>For flagged signals, open <code>signal_processings_log/$SIGNAL/</code> and inspect per-signal log for dropped records and warnings.</li> <li>Use the charts in <code>ETL/outputs/$SIGNAL</code> to confirm whether a discrepancy comes from unit differences, data entry errors, or genuine population shifts.</li> <li>If a unit mismatch is found, apply a unit conversion and re-run the pipeline for that signal.</li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#visual-examples","title":"Visual examples","text":""},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#example-1-monthly-vs-yearly-distribution-hemoglobin","title":"Example 1 - monthly vs yearly distribution (Hemoglobin)","text":"<p>Monthly charts can look noisy when most data come from a single year. In that case, check both monthly and yearly views to avoid false alarms.</p> <p> </p>"},{"location":"Tutorials/01.ETL%20Tutorial/04.Read%20Results/index.html#example-2-distribution-shape-check","title":"Example 2 - distribution shape check","text":"<p>On the right is a smooth (expected) distribution. On the left are unexplained \"vibrations\"; they may indicate data-quality issues or batch artifacts. Discuss with the dataset owner to confirm.</p> <p> </p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html","title":"High-Level Overview of the ETL Process","text":"<p>This document outlines the high-level structure and flow of the ETL (Extract, Transform, Load) process. It covers the core file structure and the sequential steps involved in loading data.</p> <p>Sphinx API Link</p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html#etl-file-structure","title":"\ud83d\udcc1 ETL File Structure","text":"<p>The ETL process uses three main directories:</p> <ul> <li>ETL_INFRA_DIR: This directory, located at MR_TOOLS git repo <code>RepoLoadUtils\\common\\ETL_Infra</code>, contains the standalone ETL infrastructure. It's portable and doesn't require other files to function.</li> <li>CODE_DIR: This is where you write the code specific to your ETL task.</li> <li>WORK_DIR: This directory serves as the output location for the ETL process.</li> </ul> <p>For more details on the contents of each directory, click the links above.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html#the-etl-process-flow","title":"\u2699\ufe0f The ETL Process Flow","text":"<p>The ETL process is managed by a <code>load.py</code> script located in the CODE_DIR. This script orchestrates the loading process by calling the <code>prepare_final_signals</code> function, which in turn initiates a \"parser\" for each signal. <code>load.py</code> is a convention name, it is not necessarily needs to be like that.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html#step-1-handling-signals","title":"Step 1: Handling Signals","text":"<ol> <li>Status Check: The <code>prepare_final_signals</code> function first checks the status of the required signal.<ul> <li>If <code>override</code> is specified, the signal is loaded from the start.</li> <li>If the signal is already loaded, it's skipped.</li> <li>Otherwise, it resumes loading from the last interrupted batch or starts from the beginning.</li> </ul> </li> <li>PID Validation: The <code>pid</code> column is mandatory in the parser's output.<ul> <li>If <code>pid</code> column is missing, the process will ERROR and STOP.</li> <li>If <code>pid</code> is a string, it's mapped to a numeric value. This mapping is only performed for demographic signals (e.g., BDATE, GENDER). A <code>pid</code> in another signal (e.g., DIAGNOSIS) that hasn't been seen in a demographic signal will be excluded. Therefore, demographic signals must be processed first with <code>prepare_final_signals</code>.</li> </ul> </li> <li>Preliminary Check: Before processing, the system checks if the signal data is already complete and valid.<ul> <li>If the dataframe contains all necessary columns and attributes as defined in <code>rep_signals/general.signals</code> and passes all preliminary tests, no further processing is needed. The data is sorted, and the signal processing is marked as complete.</li> <li>Otherwise, the process continues to the next step.</li> </ul> </li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html#step-2-determining-the-processing-unit","title":"Step 2: Determining the Processing Unit","text":"<p>The system identifies the appropriate \"processing unit\" based on the data.</p> <ol> <li>When <code>signal</code> column exists:<ul> <li>The most specific code is executed based on the signal name and its classifications. For example, for the \"BDATE\" signal (classified as \"demographic\" and \"singleton\"), the system would search for <code>BDATE.py</code>, then <code>demographic.py</code>.</li> <li>The signals configuration file and their classifications is located in the <code>general.signals</code> file in ETL_INFRA_DIR or in \"configs/rep.signals\" added to CODE_DIR.</li> </ul> </li> <li>Without a <code>signal</code> column:<ul> <li>The <code>prepare_final_signals</code> function's parameter is used to determine the signal. This also allows for multiple signals to be passed (e.g., \"BDATE,GENDER\"), and the most specific code is found for each.</li> </ul> </li> </ol> <p>If no relevant processing unit is found, a new one is created with instructions. In interactive mode, a Python environment opens for real-time processing and inspection. In non-interactive mode, the process fails, waiting for the unit's code to be filled in.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html#step-3-testing-and-finalizing-the-signal","title":"Step 3: Testing and Finalizing the Signal","text":"<ol> <li>Post-Processing Tests: After the processing unit returns the signal file, two types of tests are performed:<ul> <li>General Tests: These check for valid time/value channels, numeric values, valid dates, and illegal characters.</li> <li>Specific Tests: These are associated with the signal name and its classifications, such as comparing signal distribution or counting outliers. These tests can be added globally in ETL_INFRA_DIR or locally in CODE_DIR.</li> </ul> </li> <li>Post-Test Actions: Once the signal passes all tests:<ul> <li>Columns are sorted and organized.</li> <li>Statistics for categorical signals are collected to aid in dictionary creation.</li> <li>The batch or signal state is updated to reflect successful completion.</li> </ul> </li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/index.html#step-4-dictionaries-and-final-load","title":"Step 4: Dictionaries and Final Load","text":"<ol> <li>Dictionary Creation: If your data requires specific dictionaries, you can call the <code>prepare_dicts</code> function.<ul> <li>For categorical features like \"Drug\", \"PROCEDURES\", and \"DIAGNOSIS\" that have specific prefixes (e.g., <code>ICD10_CODE:*</code>), the system can automatically recognize and use the correct ontology.</li> <li>Statistics from these steps are collected for the final report.</li> </ul> </li> <li>Finalization: The final step is to call <code>finish_prepare_load</code>. This function:<ul> <li>Processes all categorical signal dictionaries.</li> <li>Generates a merged \"signals\" file.</li> <li>Creates a <code>convert_config</code> file.</li> <li>Prepares the <code>Flow</code> command to execute the loading process.</li> </ul> </li> </ol>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/CODE_DIR.html","title":"\ud83d\udcc1 CODE_DIR: Your ETL Workspace","text":"<p>The <code>CODE_DIR</code> is the primary location for all code specific to your ETL process. It typically contains two main Python files: a helper library for parsing and <code>load.py</code>, which is the entry point for the entire loading process. </p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/CODE_DIR.html#key-directories-and-files","title":"Key Directories and Files","text":"<ul> <li><code>configs</code>: This folder stores configurations specific to your ETL. While it might be empty, it commonly contains settings for unit and signal mapping.<ul> <li><code>rep.signals</code>: This file is created for you as a template. You can override or add new signal definitions that are specific to your repository. New signals should use an ID of <code>3000</code> or greater. The file format is consistent with the global signal format.</li> <li><code>map_units_stats.cfg</code>: Used if you are performing signal mapping and unit conversions. This file contains the configuration for those conversions.</li> <li>Additional configuration files can be added as needed. For more details: unit_conversion</li> </ul> </li> <li> <p><code>signal_processings</code>: This folder contains the custom logic for processing different signals and data types.</p> <ul> <li><code>XXXX.py</code>: Each file here is a Python script with specific processing logic. For example, <code>labs.py</code> would process all signals tagged as \"labs\" (unless a more specific file exists). The system uses a hierarchical search: for a signal tagged \"labs,cbc,Hemoglobin,\" it first looks for <code>Hemoglobin.py</code>, then <code>cbc.py</code>, then <code>labs.py</code>. This structure promotes code reuse.</li> <li>If no suitable logic was found, a template python file with signal name and instructions will be created automatically. The template shows you the expected input dataframe (<code>df</code>) and the required output format, guiding you to write the appropriate processing code. For example: <pre><code>#You have dataframe called \"df\". Please process it to generare signal/s of class \"smoking\"\n#Example signal from this class might be Smoking_Status\n#The target dataframe should have those columns:\n#    pid\n#    signal\n#    time_0 - type i\n#    value_0 - string categorical (rep channel type i)\n#Source Dataframe - df.head() output:\n#     time_0     ahdcode ahdflag      data1   data2      data3 data4 data5 data6  medcode signal  pid\n#0  19880705  1008050000       Y             INP001                N           Y  4K22.00   None    1\n#1  19921209  1005010200       Y  81.000000          28.300000                    22A..00   None    1\n#2  19921209  1003050000       Y          Y       2                               136..00   None    1\n#3  19921209  1003040000       Y          N       0                               137L.00   None    1\n#4  19930000  1001400086       Y                                                  537..00   None    1\n</code></pre></li> </ul> </li> <li> <p><code>tests</code>: An optional and rarely used folder for adding extra tests specific to your ETL process. It follows the same format as the <code>tests</code> folder in <code>ETL_INFRA_DIR</code>, and its contents are merged with the global tests during execution.</p> </li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/ETL_INFRA_DIR.html","title":"\ud83d\udcc2 ETL_INFRA_DIR: A Closer Look","text":"<p>The <code>ETL_INFRA_DIR</code> contains the standalone ETL infrastructure, available in the MR_Tools repository under <code>RepoLoadUtils/common/ETL_Infra</code>. It's designed to be portable and can be used on a remote machine without external dependencies.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/ETL_INFRA_DIR.html#core-files-and-folders","title":"Core Files and Folders","text":"<ul> <li><code>etl_process.py</code>: The main Python module to import into your specific ETL code.</li> <li><code>dicts</code>: Stores dictionaries for known medical ontologies and their mappings. This is a resource for the infrastructure; only edit this folder to update global dictionaries for all future work.</li> <li><code>examples</code>: Provides sample loading processes for various repositories. The <code>THIN</code> example is fully implemented.</li> <li><code>rep_signals</code>: Contains global definitions for standard signals. These definitions can be overridden in your local ETL code but should generally remain as is. Only edit this folder to update signal definitions for all future work.<ul> <li><code>general.signals</code>: A file with definitions for all known signals, including type, unit, categorical status, and \"tags\". Tags are crucial as they determine the processing logic and tests to be executed. The File Format: Repository Signals file format<ul> <li>For example, a signal tagged as \"labs\" and \"cbc\" will first look for <code>Hemoglobin.py</code>, then <code>labs.py</code>, then <code>cbc.py</code> for processing logic.</li> <li>All tests associated with <code>hemoglobin</code>, <code>labs</code>, and <code>cbc</code> will be executed.</li> </ul> </li> <li><code>signals_prctile.cfg</code>: Contains quantile information for each signal listed in <code>general.signals</code>. This data is used to test lab signals.</li> <li><code>lab_zero_value_allowed.txt</code>: A list of signals where a value of <code>0</code> is permitted. An error will be raised if a zero value is found for a signal not on this list and the rate is above a defined threshold.</li> </ul> </li> <li><code>tests</code>: Holds additional, specific tests for signals. The ETL process always performs basic structural checks (e.g., correct channels, data types).<ul> <li><code>TAG_NAME</code> (e.g., \"labs\"): Each file within this folder represents a different test that will be executed for every signal with that specific tag. To add additional tests see\u00a0ETL_process dynamic testing of signals</li> </ul> </li> <li><code>data_fetcher</code>: A library with helper functions for the parsing phase, which fetches data from databases or files. For example, it includes functions for batching large files.</li> </ul>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/WORK_DIR.html","title":"\ud83d\udcc1 WORK_DIR: The Output Directory","text":"<p>The <code>WORK_DIR</code> is where all output files from the ETL process are automatically generated. You should not manually edit any files in this folder. It serves as the single source of truth for the state of the ETL and holds all final outputs and logs.</p>"},{"location":"Tutorials/01.ETL%20Tutorial/High%20level%20-%20important%20paths/WORK_DIR.html#core-files-and-folders","title":"Core Files and Folders","text":"<ul> <li><code>loading_status.state</code>: A state file that tracks the loading status of each signal.<ul> <li>It has three columns: <code>signal</code>, <code>date+time</code> (last creation date), and <code>status</code>.</li> <li>Signals with a \"Completed\" status are skipped unless the <code>override</code> flag is used. Overriding a signal is functionally equivalent to manually deleting its row from this file.</li> <li>Signals with an \"In Process\" status will continue from the first unprocessed batch.</li> </ul> </li> <li><code>loading_batch_status.state</code>: Tracks the loading status at the batch level. If the process crashes, it will resume from the last unprocessed batch. Unless  <code>override</code> was set to true</li> <li><code>FinalSignals</code>: This directory contains the final, processed signal files ready for loading with Flow. It may also include an <code>ID2NR</code> file for converting patient IDs to a numeric format.</li> <li><code>rep_configs</code>: A directory that holds the necessary configuration files for the repository loading process, including the signals file, <code>convert_config</code>, and a script to run Flow. This is created automatically.</li> <li><code>outputs</code>: Contains reports and detailed test analysis for each signal.<ul> <li><code>$SIGNAL_NAME</code>: A dedicated folder for each signal (e.g., <code>Hemoglobin</code>). It holds test results, comparisons to reference distributions, and analysis for each batch and whole data combined in the end. HTML plots with distribution of values will be presented</li> <li><code>test.$SIGNAL_NAME.log</code>: A log file containing the results of all tests run on a signal. If a test fails, the process stops, and you must fix the issue before proceeding. This file is overwritten with each new run (override = true) or appended to previous logs and stored by batch and final test for all batches in the end.</li> </ul> </li> <li><code>signal_processings_log</code>: A directory for logging the signal processing steps.<ul> <li><code>process_XXXX.log</code>: Captures all print statements from your processing scripts (e.g., <code>process_labs.log</code>). It shows the output for each batch sequentially.</li> <li><code>XXXX.log</code>: If you use interactive mode, this file logs the commands and outputs from your data inspection, serving as a record of your ETL development process.</li> </ul> </li> </ul>"},{"location":"Tutorials/02.Access%20Data/index.html","title":"Accessing Repository Data","text":"<p>This guide explains two methods for accessing data from the repository: using the Python API for programmatic access or using MES Tools for a user-interface-based approach.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#method-1-using-the-python-api","title":"Method 1: Using the Python API","text":"<p>This method is ideal for programmatic data access and analysis within a Python environment.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ol> <li>Installed the Python API for MES Infrastructure.</li> <li>Loaded a Data Repository by following the ETL Tutorial.</li> </ol>"},{"location":"Tutorials/02.Access%20Data/index.html#basic-data-retrieval","title":"Basic Data Retrieval","text":"<p>To get started, import the <code>med</code> library and initialize the <code>PidRepository</code> with the path to your loaded repository file. Full API Sphinx Link</p> <pre><code>import med\n\n# Initialize the repository\nrep = med.PidRepository()\nrep.init('/path/to/your/repository_file')\n\n# Retrieve a signal, for example, 'Albumin'\nalbumin = rep.get_sig('Albumin')\n\n# 'albumin' is now a pandas DataFrame containing patient IDs ('pid'), \n# time channels ('timeX'), and value channels ('valX').\n</code></pre>"},{"location":"Tutorials/02.Access%20Data/index.html#filtering-by-patients-and-signals","title":"Filtering by Patients and Signals","text":"<p>You can optimize data loading by initializing the repository with a specific list of patient IDs and signals. This is particularly useful for large datasets.</p> <pre><code>list_of_patient_ids = [101, 102, 103] # Example patient IDs\nlist_of_signals = ['Albumin', 'DIAGNOSIS']\n\n# Read data only for specified patients and signals\nrep.read_all('/path/to/your/repository_file', list_of_patient_ids, list_of_signals)\n\n# Subsequent calls to get_sig will now only return data for the filtered patients.\n# If list_of_patient_ids is empty, it will query all patients in the repository.\n</code></pre>"},{"location":"Tutorials/02.Access%20Data/index.html#working-with-categorical-signals","title":"Working with Categorical Signals","text":"<p>Categorical signals, like 'DIAGNOSIS', can be handled in two ways.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#1-translated-string-values","title":"1. Translated (String) Values","text":"<p>By default, get_sig returns a DataFrame with human-readable string values. While convenient, this can be memory-intensive.</p> <pre><code># This can consume a lot of memory for large datasets\ndiagnosis_strings = rep.get_sig('DIAGNOSIS') \n</code></pre>"},{"location":"Tutorials/02.Access%20Data/index.html#2-untranslated-numeric-codes","title":"2. Untranslated (Numeric) Codes","text":"<p>For more efficient memory usage and advanced querying, you can retrieve the raw numeric codes for each category.</p> <pre><code># Set translate=False to get numeric codes instead of strings\ndiagnosis_codes = rep.get_sig('DIAGNOSIS', translate=False)\n</code></pre> <p>This returns a DataFrame with integer codes, which is more memory-efficient but requires an extra step for querying based on categories.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#efficiently-querying-categorical-data","title":"Efficiently Querying Categorical Data","text":"<p>To efficiently query categorical signals by their meaning (e.g., finding all diagnoses related to a specific disease group), use lookup tables (LUTs).</p> <p>A lookup table is an array where each index corresponds to a numeric category code. By marking relevant indices, you can perform very fast filtering.</p> <p>Example: Filtering for Respiratory Diseases</p> <p>Let's filter the 'DIAGNOSIS' signal for all codes corresponding to respiratory diseases, defined by the ICD-10 range <code>J00-J99</code>.</p> <p>Step 1: Create a Lookup Table</p> <p>First, get the dictionary section for the 'DIAGNOSIS' signal. Then, create a lookup table for the desired code range.</p> <pre><code># Get the dictionary section ID for the 'DIAGNOSIS' signal\nsig_dict_section_id = rep.dict.section_id('DIAGNOSIS')\n\n# Create a lookup table for the ICD-10 range 'J00-J99'\nlut = rep.dict.prep_sets_lookup_table(sig_dict_section_id, [\"ICD10_CODE:J00-J99\"])\n</code></pre> <p>The <code>lut</code> now contains <code>1</code> at indices corresponding to the <code>J00-J99</code> codes and <code>0</code> otherwise. This mapping can handle complex relationships, such as mapping NDC drug codes to ATC codes, not just simple string matching.</p> <p>[!NOTE] You can pass multiple values to prep_sets_lookup_table, it acceptes a list of codes to create a single lookup table with OR condition between all codes.</p> <p>Step 2: Apply the Lookup Table</p> <p>Now, use the lookup table to filter your DataFrame of diagnosis codes.</p> <pre><code># diagnosis_codes is the DataFrame from the get_sig('DIAGNOSIS', translate=False) call\nfiltered_diagnosis = diagnosis_codes[lut[diagnosis_codes[\"val0\"]] != 0]\n</code></pre> <p><code>filtered_diagnosis</code> now contains only the diagnosis records that fall within the <code>J00-J99</code> range.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#method-2-using-mes-tools-and-ui","title":"Method 2: Using MES Tools and UI","text":"<p>If you prefer command line or a graphical interface, you can use the MES Tools.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#prerequisites_1","title":"Prerequisites","text":"<p>First, complete the MES Tools Setup.</p>"},{"location":"Tutorials/02.Access%20Data/index.html#examples","title":"Examples","text":"<ul> <li>View or Export Data with <code>Flow</code>: See the guide on how to use Flow to view signals and export data.</li> <li>Inspect a Single Patient: Use the Repository Viewers UI to open and explore the data for a single patient.</li> </ul>"},{"location":"Tutorials/03.Create%20Samples/index.html","title":"Create MedSamples","text":"<p>Create a tab-separated MedSamples file describing prediction times and outcomes for each patient: MedSamples format.</p> <p>Use the Python API when possible to generate MedSamples. Each row typically contains a patient identifier, prediction time, outcome (0/1 or numeric for regression), outcome time, and optional split/metadata fields. This is the core step for defining the prediction target for training and testing. This is one of the main differences between each model.</p> <p>Suggested process:</p> <ol> <li>Generate candidate samples (patient id + date) without labels.</li> <li>Label and exclude ineligible samples, documenting exclusions for cohort diagrams and validation.</li> </ol> <p>Example (creating samples for date <code>20251011</code>):</p> <pre><code>import med\nrep = med.PidRepository()\n\nrep.read_all(\"/path/to/repository\", [], [\"BDATE\"])\nbdate_sig = rep.get_sig(\"BDATE\").rename(columns={\"pid\": \"id\"})\n\nbdate_sig[\"EVENT_FIELDS\"] = \"SAMPLE\"\nbdate_sig[\"time\"] = 20251011\nbdate_sig[\"outcome\"] = 0\nbdate_sig[\"split\"] = -1\nbdate_sig[\"outcomeTime\"] = 20500101\n\n# Keep fields in the MedSamples order\nbdate_sig = bdate_sig[[\"EVENT_FIELDS\", \"id\", \"time\", \"outcome\", \"outcomeTime\", \"split\"]]\n\nbdate_sig.to_csv(\"/path/to/samples\", index=False, sep=\"\\t\")\n</code></pre> <p>[!NOTE]  Keep in mind that training and test sample sets are often processed differently. For example, training sets may be rebalanced by year to avoid the model learning calendar trends. Those filtering and subsampling choices are part of study design and are under the responsibility of the data scientist to mitigate biases and potential information leakage in training.</p>"},{"location":"Tutorials/04.Train%20Model/index.html","title":"Training a Model","text":"<p>This guide covers two primary methods for training a model: using the Python API for a code-driven approach or using command-line tools.</p>"},{"location":"Tutorials/04.Train%20Model/index.html#step-1-define-the-model-architecture","title":"Step 1: Define the Model Architecture","text":"<p>Before training, you must define the model's architecture in a JSON file. This file specifies the entire pipeline, including feature generators, processors, and the algorithm to be used. This is conceptually similar to defining layers and stacking them in a deep learning framework.</p> <p>For detailed instructions on the file format, see the MedModel JSON format documentation.</p>"},{"location":"Tutorials/04.Train%20Model/index.html#method-1-training-with-the-python-api","title":"Method 1: Training with the Python API","text":"<p>This method provides control over the training process within a Python script.</p>"},{"location":"Tutorials/04.Train%20Model/index.html#prerequisites","title":"Prerequisites","text":"<p>Ensure you have completed the following setup steps:</p> <ol> <li>Install Python API: Python API for MES Infrastructure.</li> <li>Load Data Repository: Follow the ETL Tutorial.</li> <li>Create Samples: Prepare your training data as described in the Create Samples Tutorial.</li> </ol>"},{"location":"Tutorials/04.Train%20Model/index.html#python-training-example","title":"Python Training Example","text":"<p>The following script demonstrates the end-to-end process of training a model.</p> <pre><code>import med\n\n# --- 1. Configuration ---\nrep_path = '/path/to/your/repository_file'\nmodel_json_path = '/path/to/your/model_architecture.json'\nsamples_path = '/path/to/your/training_samples.tsv'\noutput_model_path = 'trained_model.bin'\n\n# --- 2. Initialize Model and Fit to Repository ---\nprint(\"Initializing model and fitting to the repository...\")\n\n# Load the model architecture from the JSON file\nmodel = med.Model()\nmodel.init_from_json_file(model_json_path)\n\n# Before loading all data, initialize a repository to analyze its structure\n# This helps the model determine which signals are \"virtual\" (i.e., can be\n# generated from other signals, like BMI from Height and Weight) versus\n# which need to be fetched directly.\nrep = med.PidRepository()\nrep.init(rep_path)\nmodel.fit_for_repository(rep)\n\n# Get the list of signals that must be fetched from the repository\nrequired_signals = model.get_required_signal_names()\n\n# --- 3. Load Data ---\nprint(\"Loading training samples and repository data...\")\n\n# Load the training samples (patient IDs, dates, and labels)\nsamples = med.Samples()\nsamples.read_from_file(samples_path)\n# Alternatively, load from a pandas DataFrame:\n# samples.from_df(your_dataframe)\n\n# Get the unique patient IDs required for training\npatient_ids = samples.get_ids()\n\n# Load the actual data for the required signals and patients\nrep = med.PidRepository()\nrep.read_all(rep_path, patient_ids, required_signals)\n\n# --- 4. Train the Model ---\nprint(\"Starting model training...\")\nmodel.learn(rep, samples)\nprint(\"Training complete.\")\n\n# --- 5. Save the Trained Model ---\nmodel.write_to_file(output_model_path)\nprint(f\"Model saved to {output_model_path}\")\n\n# --- Optional: Post-Training Steps ---\n\n# Access the generated feature matrix as a pandas DataFrame\nfeature_matrix_df = model.features.to_df()\n\n# Apply the trained model to a new set of samples (e.g., a test set)\n# test_samples = med.Samples()\n# test_samples.read_from_file('path/to/test_samples.tsv')\n# predictions = model.apply(rep, test_samples)\n</code></pre>"},{"location":"Tutorials/04.Train%20Model/index.html#loading-a-trained-model","title":"Loading a Trained Model","text":"<p>You can load a previously trained binary model file directly without needing the original JSON or data.</p> <pre><code>import med\n\nmodel = med.Model()\nmodel.read_from_file('trained_model.bin')\n</code></pre>"},{"location":"Tutorials/04.Train%20Model/index.html#method-2-training-with-command-line-tools","title":"Method 2: Training with Command-Line Tools","text":"<p>For users who prefer command-line operations, MES provides tools to train models without writing Python code.</p>"},{"location":"Tutorials/04.Train%20Model/index.html#prerequisites_1","title":"Prerequisites","text":"<p>First, ensure you have set up the MES Tools for Training and Testing Models.</p>"},{"location":"Tutorials/04.Train%20Model/index.html#training-tools","title":"Training Tools","text":"<ul> <li>Flow App: For a straightforward training run without hyperparameter tuning.<ul> <li>Guide: Training with Flow</li> </ul> </li> <li>Optimizer: For grid-search style hyperparameter tuning with built-in regularization to improve generalization. For more advanced tuning, consider using external libraries like Optuna.<ul> <li>Guide: Optimizer Tool</li> </ul> </li> </ul>"},{"location":"Tutorials/05.Apply%20Model/index.html","title":"Applying Your Model","text":"<p>This guide demonstrates how to apply a trained model to generate predictions. You can use either the Python API for programmatic control or the command-line tools for a code-free approach.</p>"},{"location":"Tutorials/05.Apply%20Model/index.html#method-1-python-api","title":"Method 1: Python API","text":"<p>This method offers model application process within a Python script.</p>"},{"location":"Tutorials/05.Apply%20Model/index.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ol> <li>Installed the Python API for MES Infrastructure.</li> <li>Loaded your data repository by following the ETL Tutorial.</li> <li>Created your testing dataset as outlined in the Create Samples Tutorial.</li> </ol>"},{"location":"Tutorials/05.Apply%20Model/index.html#inference-script","title":"Inference Script","text":"<p>The following script shows how to perform model inference from start to finish.</p> <pre><code>import med\n\n# --- Configuration ---\n# Path to your data repository\nrep_path = ''\n# Path to your trained MedModel file\nmodel_file = ''\n# Path to your samples file. Alternatively, load from a DataFrame\n# using: samples.from_df(dataframe_object)\nsamples_file = ''\n\n# --- Initialization ---\nprint(\"Initializing repository...\")\nrep = med.PidRepository()\n# Initialize for the first processing step\nrep.init(rep_path)\n\n# --- Load Model ---\nprint(\"Loading model...\")\nmodel = med.Model()\nmodel.read_from_file(model_file)\nmodel.fit_for_repository(rep)\n# Get the list of signals required by the model\nsignalNamesSet = model.get_required_signal_names()\n\n# --- Load Samples ---\nprint(\"Loading samples...\")\nsamples = med.Samples()\nsamples.read_from_file(samples_file)\n# Get the IDs from the samples to fetch relevant data\nids = samples.get_ids()\n\n# --- Read Data ---\nprint(\"Reading data from repository...\")\n# Read only the necessary data for the specified IDs and signals\nrep.read_all(rep_path, ids, signalNamesSet)\n\n# --- Apply Model ---\nprint(\"Applying model to samples...\")\nmodel.apply(rep, samples)\n\n# --- Save Results ---\n# The 'samples' object now contains the model scores\n# Convert to a DataFrame for further analysis\ndf = samples.to_df()\n# Save the results to a CSV file\ndf.to_csv('output_file.csv', index=False)\n# Or save as a samples file\nsamples.write_to_file('output_samples.tsv')\n\n# The feature matrix is available via: model.features.to_df()\nprint(\"Inference complete. Results saved.\")\n</code></pre>"},{"location":"Tutorials/05.Apply%20Model/index.html#method-2-command-line-tools","title":"Method 2: Command-Line Tools","text":"<p>If you prefer to work from the command line, MES offers tools to apply models without writing any Python code.</p>"},{"location":"Tutorials/05.Apply%20Model/index.html#prerequisites_1","title":"Prerequisites","text":"<p>First, install the MES Tools for Training and Testing Models.</p>"},{"location":"Tutorials/05.Apply%20Model/index.html#prediction-tools","title":"Prediction Tools","text":"<p>Use the following tool to generate predictions or extract the full feature matrix from your model's pipeline:</p> <ul> <li>Apply a model with Flow</li> </ul>"},{"location":"Tutorials/06.Model%20Evaluation/index.html","title":"Model Evaluation","text":"<p>Please refer to our suggested Checklist for model evaluation Common evaluation tools and workflows used in MES:</p> <ol> <li>bootstrap_app - bootstrap-based performance analysis with cohort and subgroup filtering. For example, we might want to test performance in different sub-cohorts: time window, age range, etc.</li> <li>Feature importance and post-processing - see Flow post-processors.</li> <li>Explainability - add model explainers as post-processors. See the Explainers Guide and our patent US20240161005A1 for the MES-specific approach. Recognizing that standard Shapley values struggle with high-dimensional, correlated medical data, we developed a specialized extension. This new method was validated by our clinicians in a blinded study against other explainability techniques and was a key component of our award-winning submission to the CMS Health AI Outcome Challenge. The results are published, but some of the process can be seen in the Research tab of this wiki.</li> <li>Covariate-shift / simulation tools - Simulator.</li> <li>Automated checks - AutoTest, a pipeline of tests derived from the Model Checklist.</li> </ol>"},{"location":"Tutorials/07.Deployment/index.html","title":"Deployment","text":"<p>The deployment is through wrapping the model file by AlgoMarker.</p>"},{"location":"Tutorials/07.Deployment/index.html#how-to-deploy-algomarker","title":"How to Deploy AlgoMarker","text":"<p>This guide explains how to set up AlgoMarker to run and expose a predictive model API.</p>"},{"location":"Tutorials/07.Deployment/index.html#1-obtain-the-model","title":"1. Obtain the Model","text":"<ul> <li>For public models, simply download the directory containing the model and its configuration files. Full List of Models</li> <li>For your own model, you\u2019ll need to create a configuration file.   See: Setup a new AlgoMarker</li> </ul>"},{"location":"Tutorials/07.Deployment/index.html#2-build-the-algomarker-library","title":"2. Build the AlgoMarker Library","text":"<ul> <li>Follow the AlgoMarker Library Setup to build the <code>libdyn_AlgoMarker.so</code> library, or use a pre-built version if available.</li> </ul>"},{"location":"Tutorials/07.Deployment/index.html#3-build-algomarker_server","title":"3. Build AlgoMarker_Server","text":"<ul> <li>Follow the AlgoMarker_Server Setup to build <code>AlgoMarker_Server</code>, or use an existing compiled binary.</li> </ul>"},{"location":"Tutorials/07.Deployment/index.html#4-run-the-server","title":"4. Run the Server","text":"<p>You can run the server either locally or using Docker/Podman.</p>"},{"location":"Tutorials/07.Deployment/index.html#local-server","title":"Local Server","text":"<p>Run the following command, adjusting the port as needed: <pre><code>AlgoMarker_Server --algomarker_path $PATH_TO_AM_CONFIG_FILE --port 1234\n# If the AlgoMarker library (`libdyn_AlgoMarker.so`) is not in a \"lib\" directory next to the config file,\n# specify its location with the \"--library_path\" argument.\n</code></pre> Alternatively, you can use the Python wrapper with FastAPI to expose the AlgoMarker model. Note: The Python wrapper is slower, requires a larger setup, and depends on Python and its libraries. In contrast, <code>AlgoMarker_Server</code> only needs glibc and can run in a minimal (distroless) image. However, the Python wrapper allows for easier code modifications and supports legacy ColonFlag models.</p> <p>To use the Python wrapper, run <code>run_server.sh</code> after setting <code>AM_CONFIG</code> and <code>AM_LIB</code> in the script. Clone the repository containing <code>run_server.sh</code> if needed. More details can be found here</p>"},{"location":"Tutorials/07.Deployment/index.html#using-docker","title":"Using Docker","text":"<ol> <li> <p>Create a Base Image    - The recommended base is the chiselled Ubuntu image (~10MB, minimal attack surface).    - Alternatively, use a full Ubuntu image and add the following to your Dockerfile:      <pre><code>apt-get update &amp;&amp; apt-get install libgomp1 -y\n</code></pre>    - To build the chiselled Ubuntu image, run <code>create.sh</code> in Docker/chiselled-ubuntu.</p> </li> <li> <p>Prepare the Application Directory    - Copy the AlgoMarker directory into the <code>data/app</code> folder next to your Dockerfile. Ensure <code>libdyn_AlgoMarker.so</code> is in a <code>lib</code> directory next to the <code>.amconfig</code> file.    - Copy the <code>AlgoMarker_Server</code> binary into <code>data/app</code>.</p> </li> <li> <p>Build the Docker Image    Use the following Dockerfile template:    <pre><code>FROM chiselled-ubuntu:latest\n\nCOPY data /\n\nENTRYPOINT [ \"/app/AlgoMarker_Server\", \"--algomarker_path\", \"/app/PATH_TO_AM_CONFIG_FILE\", \"--port\", \"1234\", \"--no_print\", \"1\" ]\n</code></pre>    Adjust <code>PATH_TO_AM_CONFIG_FILE</code> and the port as needed. You can also specify the path to <code>libdyn_AlgoMarker.so</code> using the <code>--library_path</code> argument.</p> </li> </ol> <p>Build the image with:    <pre><code>podman build -t algomarker_X --no-cache .\n</code></pre>    You can also use <code>create_image.sh</code> helper script to generate the AlgoMarker</p> <ol> <li>Run a Container    execute this command <code>podman run -id algomarker_X --name algomarker_container_X -p 1234:1234</code> and expose port 1234 to 1234 in your local machine.</li> </ol> <p>[!NOTE] You can replace all the <code>podman</code> commands with <code>docker</code>, it is fully compatible.</p>"},{"location":"Tutorials/07.Deployment/index.html#how-to-use-the-deployed-algomarker","title":"How to Use the Deployed AlgoMarker","text":"<p>Once the server is running, it exposes two main API endpoints:</p> <ol> <li> <p>GET <code>/discovery</code>    - No parameters required.    - Returns a JSON specification describing the AlgoMarker (name, inputs, etc.).</p> </li> <li> <p>POST <code>/calculate</code>    - Accepts a JSON request with either a single patient or a batch of patients.    - Request format details: Request json format</p> </li> </ol> <p>Example Response: Example Response<pre><code>{\n  \"type\": \"response\",\n  \"request_id\": \"999ef0f4-1099-4178-8c86-ecbfac6578e2\",\n  \"responses\": [\n    {\n      \"patient_id\": \"1\",\n      \"time\": \"20250806\",\n      \"prediction\": \"0.004082\",\n      \"flag_threshold\": \"USPSTF_50-80$PR_03.000\",\n      \"flag_result\": 0,\n      \"messages\": [\n        \"(320)An outlier was found and ignored in signal: RBC\"\n      ]\n    }\n  ]\n}\n</code></pre></p> Response with Explainability <pre><code>{\n    \"type\": \"response\",\n    \"request_id\": \"999ef0f4-1099-4178-8c86-ecbfac6578e2\",\n    \"responses\": [\n        {\n            \"patient_id\": \"1\",\n            \"time\": \"20230611\",\n            \"messages\": [\n                \"(320)An outlier was found and ignored in signal: RBC\",\n            ],\n            \"prediction\": \"0.004082\",\n            \"explainability_output_field_name_for_your_control\": {\n                \"static_info\": [\n                    {\n                        \"signal\": \"Age\",\n                        \"unit\": \"Year\",\n                        \"value\": \"45\"\n                    },\n                    {\n                        \"signal\": \"GENDER\",\n                        \"unit\": \"\",\n                        \"value\": \"1.000000\"\n                    },\n                    {\n                        \"signal\": \"Hemoglobin\",\n                        \"unit\": \"g/dL\",\n                        \"value\": \"14.500000\"\n                    }\n                ],\n                \"explainer_output\": [\n                    {\n                        \"contributor_name\": \"Age\",\n                        \"contributor_value\": -1.257716417312622,\n                        \"contributor_percentage\": 42.327555760464165,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"Age\",\n                                \"feature_value\": 45.0\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 2,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [\n                            {\n                                \"signal\": \"Age\",\n                                \"unit\": [\n                                    \"Year\"\n                                ],\n                                \"timestamp\": [],\n                                \"value\": [\n                                    \"45.000000\"\n                                ]\n                            }\n                        ]\n                    },\n                    {\n                        \"contributor_name\": \"MCH_Values\",\n                        \"contributor_value\": -0.2684820294380188,\n                        \"contributor_percentage\": 9.035572693121699,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000008.MCH.last.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000019.MCH.last.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000030.MCH.last.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000041.MCH.last.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000052.MCH.last.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000118.MCH.first.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000129.MCH.first.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000140.MCH.first.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000151.MCH.first.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000162.MCH.first.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000173.MCH.last2.win_0_10000\",\n                                \"feature_value\": 29.091323852539063\n                            },\n                            {\n                                \"feature_name\": \"FTR_000184.MCH.last2.win_0_1000\",\n                                \"feature_value\": 29.041757583618164\n                            },\n                            {\n                                \"feature_name\": \"FTR_000195.MCH.last2.win_0_730\",\n                                \"feature_value\": 29.01051139831543\n                            },\n                            {\n                                \"feature_name\": \"FTR_000206.MCH.last2.win_0_360\",\n                                \"feature_value\": 28.922163009643555\n                            },\n                            {\n                                \"feature_name\": \"FTR_000217.MCH.last2.win_0_180\",\n                                \"feature_value\": 28.734722137451172\n                            },\n                            {\n                                \"feature_name\": \"FTR_000228.MCH.avg.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000239.MCH.avg.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000250.MCH.avg.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000261.MCH.avg.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000272.MCH.avg.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000283.MCH.min.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000294.MCH.min.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000305.MCH.min.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000316.MCH.min.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000327.MCH.min.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000338.MCH.max.win_0_10000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000349.MCH.max.win_0_1000\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000360.MCH.max.win_0_730\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000371.MCH.max.win_0_360\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000382.MCH.max.win_0_180\",\n                                \"feature_value\": 32.21999740600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000503.MCH.last2_time.win_0_10000\",\n                                \"feature_value\": 449.85882568359375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000514.MCH.last2_time.win_0_1000\",\n                                \"feature_value\": 315.7171325683594\n                            },\n                            {\n                                \"feature_name\": \"FTR_000525.MCH.last2_time.win_0_730\",\n                                \"feature_value\": 268.54010009765625\n                            },\n                            {\n                                \"feature_name\": \"FTR_000536.MCH.last2_time.win_0_360\",\n                                \"feature_value\": 161.85084533691406\n                            },\n                            {\n                                \"feature_name\": \"FTR_000547.MCH.last2_time.win_0_180\",\n                                \"feature_value\": 85.85099792480469\n                            },\n                            {\n                                \"feature_name\": \"FTR_001148.MCH.last.win_730_10000\",\n                                \"feature_value\": 29.159976959228516\n                            },\n                            {\n                                \"feature_name\": \"FTR_001159.MCH.min.win_730_10000\",\n                                \"feature_value\": 28.687816619873047\n                            },\n                            {\n                                \"feature_name\": \"FTR_001170.MCH.max.win_730_10000\",\n                                \"feature_value\": 29.749162673950195\n                            },\n                            {\n                                \"feature_name\": \"FTR_001208.MCH.Estimate.180\",\n                                \"feature_value\": 28.93360137939453\n                            },\n                            {\n                                \"feature_name\": \"FTR_001208.MCH.Estimate.360\",\n                                \"feature_value\": 28.951967239379883\n                            },\n                            {\n                                \"feature_name\": \"FTR_001208.MCH.Estimate.730\",\n                                \"feature_value\": 28.965957641601563\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    },\n                    {\n                        \"contributor_name\": \"MCV_Values\",\n                        \"contributor_value\": -0.1519976258277893,\n                        \"contributor_percentage\": 5.11537231036717,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000007.MCV.last.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000018.MCV.last.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000029.MCV.last.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000040.MCV.last.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000051.MCV.last.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000117.MCV.first.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000128.MCV.first.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000139.MCV.first.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000150.MCV.first.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000161.MCV.first.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000172.MCV.last2.win_0_10000\",\n                                \"feature_value\": 87.35523986816406\n                            },\n                            {\n                                \"feature_name\": \"FTR_000183.MCV.last2.win_0_1000\",\n                                \"feature_value\": 87.30293273925781\n                            },\n                            {\n                                \"feature_name\": \"FTR_000194.MCV.last2.win_0_730\",\n                                \"feature_value\": 87.2611312866211\n                            },\n                            {\n                                \"feature_name\": \"FTR_000205.MCV.last2.win_0_360\",\n                                \"feature_value\": 87.1921615600586\n                            },\n                            {\n                                \"feature_name\": \"FTR_000216.MCV.last2.win_0_180\",\n                                \"feature_value\": 86.85134887695313\n                            },\n                            {\n                                \"feature_name\": \"FTR_000227.MCV.avg.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000238.MCV.avg.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000249.MCV.avg.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000260.MCV.avg.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000271.MCV.avg.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000282.MCV.min.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000293.MCV.min.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000304.MCV.min.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000315.MCV.min.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000326.MCV.min.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000337.MCV.max.win_0_10000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000348.MCV.max.win_0_1000\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000359.MCV.max.win_0_730\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000370.MCV.max.win_0_360\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000381.MCV.max.win_0_180\",\n                                \"feature_value\": 82.0\n                            },\n                            {\n                                \"feature_name\": \"FTR_000502.MCV.last2_time.win_0_10000\",\n                                \"feature_value\": 449.8634948730469\n                            },\n                            {\n                                \"feature_name\": \"FTR_000513.MCV.last2_time.win_0_1000\",\n                                \"feature_value\": 315.7387390136719\n                            },\n                            {\n                                \"feature_name\": \"FTR_000524.MCV.last2_time.win_0_730\",\n                                \"feature_value\": 268.5698547363281\n                            },\n                            {\n                                \"feature_name\": \"FTR_000535.MCV.last2_time.win_0_360\",\n                                \"feature_value\": 161.91400146484375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000546.MCV.last2_time.win_0_180\",\n                                \"feature_value\": 85.87918853759766\n                            },\n                            {\n                                \"feature_name\": \"FTR_001147.MCV.last.win_730_10000\",\n                                \"feature_value\": 87.29901123046875\n                            },\n                            {\n                                \"feature_name\": \"FTR_001158.MCV.min.win_730_10000\",\n                                \"feature_value\": 85.86404418945313\n                            },\n                            {\n                                \"feature_name\": \"FTR_001169.MCV.max.win_730_10000\",\n                                \"feature_value\": 88.5106430053711\n                            },\n                            {\n                                \"feature_name\": \"FTR_001207.MCV.Estimate.180\",\n                                \"feature_value\": 87.37998962402344\n                            },\n                            {\n                                \"feature_name\": \"FTR_001207.MCV.Estimate.360\",\n                                \"feature_value\": 87.38853454589844\n                            },\n                            {\n                                \"feature_name\": \"FTR_001207.MCV.Estimate.730\",\n                                \"feature_value\": 87.39167022705078\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [\n                            {\n                                \"signal\": \"MCV\",\n                                \"unit\": [\n                                    \"fL\"\n                                ],\n                                \"timestamp\": [\n                                    20230611\n                                ],\n                                \"value\": [\n                                    \"82.000000\"\n                                ]\n                            }\n                        ],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    },\n                    {\n                        \"contributor_name\": \"MCH_Trends\",\n                        \"contributor_value\": -0.14473219215869904,\n                        \"contributor_percentage\": 4.8708592685138346,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000063.MCH.slope.win_0_10000\",\n                                \"feature_value\": -0.09105083346366882\n                            },\n                            {\n                                \"feature_name\": \"FTR_000074.MCH.slope.win_0_1000\",\n                                \"feature_value\": -0.03978761285543442\n                            },\n                            {\n                                \"feature_name\": \"FTR_000085.MCH.slope.win_0_730\",\n                                \"feature_value\": -0.008064992725849152\n                            },\n                            {\n                                \"feature_name\": \"FTR_000096.MCH.slope.win_0_360\",\n                                \"feature_value\": 0.014474527910351753\n                            },\n                            {\n                                \"feature_name\": \"FTR_000107.MCH.slope.win_0_180\",\n                                \"feature_value\": -0.00038833924918435514\n                            },\n                            {\n                                \"feature_name\": \"FTR_000393.MCH.std.win_0_10000\",\n                                \"feature_value\": 0.5682898759841919\n                            },\n                            {\n                                \"feature_name\": \"FTR_000404.MCH.std.win_0_1000\",\n                                \"feature_value\": 0.4634387791156769\n                            },\n                            {\n                                \"feature_name\": \"FTR_000415.MCH.std.win_0_730\",\n                                \"feature_value\": 0.436860591173172\n                            },\n                            {\n                                \"feature_name\": \"FTR_000426.MCH.std.win_0_360\",\n                                \"feature_value\": 0.3896379768848419\n                            },\n                            {\n                                \"feature_name\": \"FTR_000437.MCH.std.win_0_180\",\n                                \"feature_value\": 0.35533618927001953\n                            },\n                            {\n                                \"feature_name\": \"FTR_000558.MCH.last_delta.win_0_10000\",\n                                \"feature_value\": -0.06643807888031006\n                            },\n                            {\n                                \"feature_name\": \"FTR_000569.MCH.last_delta.win_0_1000\",\n                                \"feature_value\": -0.03644682839512825\n                            },\n                            {\n                                \"feature_name\": \"FTR_000580.MCH.last_delta.win_0_730\",\n                                \"feature_value\": -0.024617791175842285\n                            },\n                            {\n                                \"feature_name\": \"FTR_000591.MCH.last_delta.win_0_360\",\n                                \"feature_value\": 0.008983400650322437\n                            },\n                            {\n                                \"feature_name\": \"FTR_000602.MCH.last_delta.win_0_180\",\n                                \"feature_value\": 0.0546439066529274\n                            },\n                            {\n                                \"feature_name\": \"FTR_001108.MCH.win_delta.win_0_180_360_10000\",\n                                \"feature_value\": -0.12988980114459991\n                            },\n                            {\n                                \"feature_name\": \"FTR_001128.MCH.win_delta.win_0_180_730_10000\",\n                                \"feature_value\": -0.1698904186487198\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    },\n                    {\n                        \"contributor_name\": \"MCHC-M_Values\",\n                        \"contributor_value\": -0.13522081077098846,\n                        \"contributor_percentage\": 4.550760668340038,\n                        \"contributor_elements\": [\n                            {\n                                \"feature_name\": \"FTR_000009.MCHC-M.last.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000020.MCHC-M.last.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000031.MCHC-M.last.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000042.MCHC-M.last.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000053.MCHC-M.last.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000119.MCHC-M.first.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000130.MCHC-M.first.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000141.MCHC-M.first.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000152.MCHC-M.first.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000163.MCHC-M.first.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000174.MCHC-M.last2.win_0_10000\",\n                                \"feature_value\": 33.27936935424805\n                            },\n                            {\n                                \"feature_name\": \"FTR_000185.MCHC-M.last2.win_0_1000\",\n                                \"feature_value\": 33.24119186401367\n                            },\n                            {\n                                \"feature_name\": \"FTR_000196.MCHC-M.last2.win_0_730\",\n                                \"feature_value\": 33.22017288208008\n                            },\n                            {\n                                \"feature_name\": \"FTR_000207.MCHC-M.last2.win_0_360\",\n                                \"feature_value\": 33.142608642578125\n                            },\n                            {\n                                \"feature_name\": \"FTR_000218.MCHC-M.last2.win_0_180\",\n                                \"feature_value\": 33.04962158203125\n                            },\n                            {\n                                \"feature_name\": \"FTR_000229.MCHC-M.avg.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000240.MCHC-M.avg.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000251.MCHC-M.avg.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000262.MCHC-M.avg.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000273.MCHC-M.avg.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000284.MCHC-M.min.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000295.MCHC-M.min.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000306.MCHC-M.min.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000317.MCHC-M.min.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000328.MCHC-M.min.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000339.MCHC-M.max.win_0_10000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000350.MCHC-M.max.win_0_1000\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000361.MCHC-M.max.win_0_730\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000372.MCHC-M.max.win_0_360\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000383.MCHC-M.max.win_0_180\",\n                                \"feature_value\": 45.30999755859375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000504.MCHC-M.last2_time.win_0_10000\",\n                                \"feature_value\": 449.85882568359375\n                            },\n                            {\n                                \"feature_name\": \"FTR_000515.MCHC-M.last2_time.win_0_1000\",\n                                \"feature_value\": 315.7171325683594\n                            },\n                            {\n                                \"feature_name\": \"FTR_000526.MCHC-M.last2_time.win_0_730\",\n                                \"feature_value\": 268.54010009765625\n                            },\n                            {\n                                \"feature_name\": \"FTR_000537.MCHC-M.last2_time.win_0_360\",\n                                \"feature_value\": 161.85084533691406\n                            },\n                            {\n                                \"feature_name\": \"FTR_000548.MCHC-M.last2_time.win_0_180\",\n                                \"feature_value\": 85.85099792480469\n                            },\n                            {\n                                \"feature_name\": \"FTR_001149.MCHC-M.last.win_730_10000\",\n                                \"feature_value\": 33.386741638183594\n                            },\n                            {\n                                \"feature_name\": \"FTR_001160.MCHC-M.min.win_730_10000\",\n                                \"feature_value\": 33.00123977661133\n                            },\n                            {\n                                \"feature_name\": \"FTR_001171.MCHC-M.max.win_730_10000\",\n                                \"feature_value\": 33.99413299560547\n                            },\n                            {\n                                \"feature_name\": \"FTR_001209.MCHC-M.Estimate.180\",\n                                \"feature_value\": 33.09005355834961\n                            },\n                            {\n                                \"feature_name\": \"FTR_001209.MCHC-M.Estimate.360\",\n                                \"feature_value\": 33.112545013427734\n                            },\n                            {\n                                \"feature_name\": \"FTR_001209.MCHC-M.Estimate.730\",\n                                \"feature_value\": 33.133628845214844\n                            }\n                        ],\n                        \"contributor_description\": \"\",\n                        \"contributor_level\": 1,\n                        \"contributor_level_max\": 4,\n                        \"contributor_records\": [],\n                        \"contributor_records_info\": {\n                            \"contributor_max_time\": 1825,\n                            \"contributor_max_time_unit\": \"Days\",\n                            \"contributor_max_count\": 3\n                        }\n                    }\n                ]\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Nagative <code>contributor_value</code> means the concept resulted in lower score, positive value means the concept contributed to increas risk score</li> <li>The contribution magnitude is reported as <code>contributor_level</code> and the maximal scale is <code>contributor_level_max</code>. Since <code>contributor_value</code> magnitude is hard to interapt we used those scaling to report contribution magnitude </li> </ul> <p>Additional fields may be included in the response, for example, if explainability is requested.</p> <p>For the full API specification, refer to: AlgoMarker Spec</p>"}]}